import asyncio
import contextlib
import json
import math
import re
import sqlite3
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from decimal import Decimal, InvalidOperation
from typing import Any, Iterable, Mapping, Sequence

try:
    from PIL import Image
except ImportError:  # pragma: no cover
    Image = None

try:
    import numpy as np
except ImportError:  # pragma: no cover
    np = None  # type: ignore

try:
    import cv2
except ImportError:  # pragma: no cover
    cv2 = None  # type: ignore

try:
    import fitz
except ImportError:  # pragma: no cover
    fitz = None  # type: ignore

try:
    from skimage.metrics import structural_similarity as ssim
except ImportError:  # pragma: no cover
    ssim = None  # type: ignore

try:
    from playwright.async_api import async_playwright
except ImportError:  # pragma: no cover
    async_playwright = None  # type: ignore

from .date_utils import get_col_type, mk_between_pred_for_date
from .contract_adapter import ContractAdapter, format_decimal_str
# ------------------------------------------------------------------
# Tolerant batch-block detection + stripping (explicit/implicit)
# ------------------------------------------------------------------
_BATCH_BLOCK_ANY_TAG = re.compile(
    r'(?is)'
    r'<(?P<tag>section|div|article|main|tbody|tr)\b'
    r'[^>]*\bclass\s*=\s*["\'][^"\']*\bbatch-block\b[^"\']*["\']'
    r'[^>]*>'
    r'(?P<inner>.*?)'
    r'</(?P=tag)>'
)

_TOKEN_REGEX_CACHE: dict[str, re.Pattern] = {}
_TR_BLOCK_RE = re.compile(r"(?is)<tr\b[^>]*>.*?</tr>")


def _token_regex(token: str) -> re.Pattern:
    cleaned = (token or "").strip()
    if not cleaned:
        raise ValueError("Token must be a non-empty string")
    cached = _TOKEN_REGEX_CACHE.get(cleaned)
    if cached is None:
        cached = re.compile(rf"\{{\{{?\s*{re.escape(cleaned)}\s*\}}\}}?")
        _TOKEN_REGEX_CACHE[cleaned] = cached
    return cached


def _segment_has_any_token(segment: str, tokens: Iterable[str]) -> bool:
    for token in tokens:
        if not token:
            continue
        if _token_regex(token).search(segment):
            return True
    return False


def _find_rowish_block(html_text: str, row_tokens: Iterable[str]) -> tuple[str, int, int] | None:
    candidate_tokens = [tok for tok in row_tokens if isinstance(tok, str) and tok.strip()]
    if not candidate_tokens:
        return None

    matches = [
        m for m in _TR_BLOCK_RE.finditer(html_text) if _segment_has_any_token(m.group(0), candidate_tokens)
    ]
    if not matches:
        return None

    prototype = matches[0].group(0).strip()
    start_index = matches[0].start()
    end_index = matches[-1].end()
    return prototype, start_index, end_index


def _select_prototype_block(html_text: str, row_tokens: Iterable[str]) -> tuple[str, int, int]:
    explicit_blocks = list(_BATCH_BLOCK_ANY_TAG.finditer(html_text))
    if explicit_blocks:
        chosen_match = explicit_blocks[0]
        if row_tokens:
            for match in explicit_blocks:
                if _segment_has_any_token(match.group(0), row_tokens):
                    chosen_match = match
                    break
        prototype = chosen_match.group(0).strip()
        start0 = explicit_blocks[0].start()
        end_last = explicit_blocks[-1].end()
        return prototype, start0, end_last

    rowish = _find_rowish_block(html_text, row_tokens)
    if rowish:
        return rowish

    block_full, _, _ = _find_or_infer_batch_block(html_text)
    start0 = html_text.find(block_full)
    if start0 < 0:
        raise RuntimeError("Inferred batch block could not be located in HTML via .find()")
    end_last = start0 + len(block_full)
    return block_full.strip(), start0, end_last

def _find_or_infer_batch_block(html_text: str) -> tuple[str, str, str]:
    """
    Return (full_match, tag_name, inner_html) of the repeating unit.
    Preference order:
      1) Any element with class="batch-block"
      2) First <tr> inside the first <tbody>
      3) First row-like <div> (class includes row|item|card)
      4) First large container (<section|main|div|article> under <body>)
    """
    m = _BATCH_BLOCK_ANY_TAG.search(html_text)
    if m:
        return m.group(0), m.group('tag').lower(), m.group('inner')

    m_tbody = re.search(r'(?is)<tbody\b[^>]*>(?P<body>.*?)</tbody>', html_text)
    if m_tbody:
        tbody = m_tbody.group('body')
        m_tr = re.search(r'(?is)<tr\b[^>]*>(?P<tr>.*?)</tr>', tbody)
        if m_tr:
            return m_tr.group(0), 'tr', m_tr.group('tr')

    m_div = re.search(r'(?is)<div\b[^>]*\b(row|item|card)\b[^>]*>(?P<inner>.*?)</div>', html_text)
    if m_div:
        return m_div.group(0), 'div', m_div.group('inner')

    m_body = re.search(r'(?is)<body\b[^>]*>(?P<body>.*?)</body>', html_text)
    if m_body:
        body = m_body.group('body')
        m_cont = re.search(r'(?is)<(section|main|div|article)\b[^>]*>(?P<inner>.*?)</\1>', body)
        if m_cont:
            return m_cont.group(0), m_cont.group(1).lower(), m_cont.group('inner')

    raise RuntimeError("No explicit batch-block and no suitable repeating unit could be inferred.")

def _strip_found_block(html_text: str, block_full: str, block_tag: str) -> str:
    """Remove the found/inferred block once (used to build shell)."""
    return html_text.replace(block_full, "", 1)

def html_without_batch_blocks(html_text: str) -> str:
    """Legacy stripper kept for compatibility."""
    pat = re.compile(r'(?is)\s*<section\s+class=["\']batch-block["\']\s*>.*?</section>\s*')
    return pat.sub("", html_text)

def _raise_no_block(html: str, cause: Exception | None = None) -> None:
    """Build a short <section ...> preview and raise ValueError from here."""
    sec_tags = re.findall(r'(?is)<section\b[^>]*>', html)
    preview_lines = []
    for i, t in enumerate(sec_tags[:12]):
        snip = t[:140].replace("\n", " ")
        preview_lines.append(f'{i+1:02d}: {snip}{" ..." if len(t) > 140 else ""}')
    preview = "\n".join(preview_lines)
    msg = (
        "Could not find any <section class='batch-block'> blocks and no suitable fallback could be inferred.\n"
        "First few <section> tags present:\n" + preview
    )
    raise ValueError(msg) from cause


# ======================================================
# ENTRYPOINT: DB-driven fill + PDF (no LLM here anymore)
# ======================================================
def fill_and_print(
    OBJ: dict,
    TEMPLATE_PATH: Path,
    DB_PATH: Path,
    OUT_HTML: Path,
    OUT_PDF: Path,
    START_DATE: str,
    END_DATE: str,
    batch_ids: list[str] | None = None,
    IMAGE_CONTENTS: list[dict] | None = None,  # kept for signature compat; unused
    KEY_VALUES: dict | None = None,
    GENERATOR_BUNDLE: dict | None = None,
    __force_single: bool = False,
):
    """
    DB-driven renderer:
      - Assumes TEMPLATE_PATH is already the *final shell* produced at Approve (auto_fill.py)
        containing a single prototype batch block.
      - Renders header tokens (parent row per batch), row repeater (child rows), totals, literals.
      - Writes OUT_HTML and prints OUT_PDF via Playwright.

    API contract preserved (same signature).
    """

    # ---- Guard required inputs ----
    for name in ("OBJ", "TEMPLATE_PATH", "DB_PATH", "START_DATE", "END_DATE"):
        if locals().get(name) is None:
            raise NameError(f"Missing required variable: `{name}`")

    # Ensure output dir exists
    OUT_DIR = OUT_HTML.parent
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    # ---- Load the final shell HTML (created during Approve) ----
    html = TEMPLATE_PATH.read_text(encoding="utf-8")

    TOKEN_RE = re.compile(r"\{\{?\s*([A-Za-z0-9_\-\.]+)\s*\}\}?")
    TEMPLATE_TOKENS = {m.group(1) for m in TOKEN_RE.finditer(html)}

    # ---- Unpack contract ----
    OBJ = OBJ or {}
    contract_adapter = ContractAdapter(OBJ)

    PLACEHOLDER_TO_COL = contract_adapter.mapping

    join_raw = OBJ.get("join", {}) or {}
    JOIN = {
        "parent_table": contract_adapter.parent_table or join_raw.get("parent_table", ""),
        "child_table": contract_adapter.child_table or join_raw.get("child_table", ""),
        "parent_key": contract_adapter.parent_key or join_raw.get("parent_key", ""),
        "child_key": contract_adapter.child_key or join_raw.get("child_key", ""),
    }

    DATE_COLUMNS = contract_adapter.date_columns or (OBJ.get("date_columns", {}) or {})

    HEADER_TOKENS = contract_adapter.scalar_tokens or OBJ.get("header_tokens", [])
    ROW_TOKENS = contract_adapter.row_tokens or OBJ.get("row_tokens", [])
    TOTALS = contract_adapter.totals_mapping or OBJ.get("totals", {})
    ROW_ORDER = contract_adapter.row_order or OBJ.get("row_order", ["ROWID"])
    LITERALS = {
        str(token): "" if value is None else str(value)
        for token, value in (OBJ.get("literals", {}) or {}).items()
    }
    FORMATTERS = contract_adapter.formatters
    key_values_map: dict[str, list[str]] = {}
    if KEY_VALUES:
        for token, raw_value in KEY_VALUES.items():
            name = str(token or "").strip()
            if not name:
                continue
            values: list[str] = []
            if isinstance(raw_value, (list, tuple, set)):
                seen = set()
                for item in raw_value:
                    text = str(item or "").strip()
                    if text and text not in seen:
                        seen.add(text)
                        values.append(text)
            else:
                text = str(raw_value or "").strip()
                if text:
                    values = [text]
            if values:
                key_values_map[name] = values
    for token, values in key_values_map.items():
        LITERALS[token] = ", ".join(values)

    def _first_key_value(values: list[str]) -> str | None:
        for val in values:
            text = str(val or "").strip()
            if text:
                return text
        return None

    multi_key_selected = any(len(values) > 1 for values in key_values_map.values())

    def _expand_key_value_groups(values_map: dict[str, list[str]]) -> list[dict[str, list[str]]]:
        if not values_map:
            return [{}]
        groups: list[dict[str, list[str]]] = [{}]
        for token, raw_values in values_map.items():
            trimmed: list[str] = []
            seen_local: set[str] = set()
            for val in raw_values:
                text = str(val or "").strip()
                if not text or text in seen_local:
                    continue
                seen_local.add(text)
                trimmed.append(text)
            if not trimmed:
                continue
            next_groups: list[dict[str, list[str]]] = []
            for base in groups:
                for value in trimmed:
                    combined = {k: list(v) for k, v in base.items()}
                    combined[token] = [value]
                    next_groups.append(combined)
            groups = next_groups or groups
        return groups or [{}]

    async def html_to_pdf_async(html_path: Path, pdf_path: Path, base_dir: Path):
        if async_playwright is None:
            print("Playwright not available; skipping PDF generation.")
            return

        html_source = html_path.read_text(encoding="utf-8", errors="ignore")
        base_url = (base_dir or html_path.parent).as_uri()

        async with async_playwright() as p:
            browser = await p.chromium.launch()
            context = None
            try:
                context = await browser.new_context(base_url=base_url)
                page = await context.new_page()
                await page.set_content(html_source, wait_until="networkidle")
                await page.emulate_media(media="print")
                await page.pdf(
                    path=str(pdf_path),
                    format="A4",
                    print_background=True,
                    margin={"top": "10mm", "right": "10mm", "bottom": "10mm", "left": "10mm"},
                    prefer_css_page_size=True,
                )
            finally:
                if context is not None:
                    await context.close()
                await browser.close()

    def _combine_html_documents(html_sections: list[str]) -> str:
        if not html_sections:
            return ""
        combined_body: list[str] = []
        doc_type = ""
        head_html = ""

        head_pattern = re.compile(r"(?is)<head\b[^>]*>(?P<head>.*)</head>")
        body_pattern = re.compile(r"(?is)<body\b[^>]*>(?P<body>.*)</body>")
        doctype_pattern = re.compile(r"(?is)^\s*<!DOCTYPE[^>]*>", re.MULTILINE)

        for idx, raw_html in enumerate(html_sections):
            text = raw_html or ""
            if idx == 0:
                doctype_match = doctype_pattern.search(text)
                if doctype_match:
                    doc_type = doctype_match.group(0).strip()
                    text = text[doctype_match.end():]
                head_match = head_pattern.search(text)
                if head_match:
                    head_html = head_match.group(0).strip()
                body_match = body_pattern.search(text)
                if body_match:
                    combined_body.append(body_match.group("body").strip())
                else:
                    combined_body.append(text.strip())
            else:
                body_match = body_pattern.search(text)
                section = body_match.group("body").strip() if body_match else text.strip()
                combined_body.append(f"<!-- NR:key-section {idx + 1} -->\n{section}")

        doc_lines = []
        if doc_type:
            doc_lines.append(doc_type)
        doc_lines.append("<html>")
        if head_html:
            doc_lines.append(head_html)
        doc_lines.append("<body>")
        doc_lines.append("\n\n".join(combined_body))
        doc_lines.append("</body>")
        doc_lines.append("</html>")
        return "\n".join(doc_lines)

    def _value_has_content(value: Any) -> bool:
        if value is None:
            return False
        if isinstance(value, (int, float, Decimal)):
            return value != 0
        text = str(value).strip()
        if not text:
            return False
        try:
            num = Decimal(text)
        except Exception:
            return True
        else:
            return num != 0

    def _row_has_significant_data(row: Mapping[str, Any], columns: list[str]) -> bool:
        return _row_has_any_data(row, (), columns)

    def _token_values_have_data(row: Mapping[str, Any], tokens: list[str]) -> bool:
        return _row_has_any_data(row, tokens, ())

    def _row_has_any_data(row: Mapping[str, Any], tokens: Sequence[str], columns: Sequence[str]) -> bool:
        for token in tokens:
            if not token:
                continue
            if _value_has_content(_value_for_token(row, token)):
                return True
        for col in columns:
            if not col:
                continue
            if _value_has_content(row.get(col)):
                return True
        return False

    def _reindex_serial_fields(rows: list[dict], tokens: Sequence[str], columns: Sequence[str]) -> None:
        serial_tokens = [
            tok for tok in tokens if tok and any(keyword in tok.lower() for keyword in ("row", "serial", "sl"))
        ]
        serial_columns = [
            col for col in columns if col and any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
        ]
        if not serial_tokens and not serial_columns:
            return
        for idx, row in enumerate(rows, start=1):
            for tok in serial_tokens:
                row[tok] = idx
            for col in serial_columns:
                row[col] = idx

    def _value_for_token(row: Mapping[str, Any], token: str) -> Any:
        if token in row:
            return row[token]
        col_name = _extract_col_name(PLACEHOLDER_TO_COL.get(token))
        if col_name and col_name in row:
            return row[col_name]
        return row.get(token)

    if not key_values_map:
        key_value_groups: list[dict[str, list[str]]] = [{}]
    elif multi_key_selected:
        key_value_groups = _expand_key_value_groups(key_values_map)
    else:
        key_value_groups = [key_values_map]

    if multi_key_selected and not __force_single:
        html_sections: list[str] = []
        tmp_outputs: list[tuple[Path, Path]] = []
        try:
            for idx, selection in enumerate(key_value_groups, start=1):
                tmp_html = OUT_HTML.with_name(f"{OUT_HTML.stem}__key{idx}.html")
                tmp_pdf = OUT_PDF.with_name(f"{OUT_PDF.stem}__key{idx}.pdf")
                result = fill_and_print(
                    OBJ=OBJ,
                    TEMPLATE_PATH=TEMPLATE_PATH,
                    DB_PATH=DB_PATH,
                    OUT_HTML=tmp_html,
                    OUT_PDF=tmp_pdf,
                    START_DATE=START_DATE,
                    END_DATE=END_DATE,
                    batch_ids=None,
                    IMAGE_CONTENTS=IMAGE_CONTENTS,
                    KEY_VALUES=selection or None,
                    GENERATOR_BUNDLE=GENERATOR_BUNDLE,
                    __force_single=True,
                )
                if not result.get("rows_rendered"):
                    print(f"Skipping key selection {selection} - no rows rendered.")
                    for key in ("html_path", "pdf_path"):
                        path = result.get(key)
                        if path:
                            with contextlib.suppress(FileNotFoundError):
                                Path(path).unlink()
                    continue
                html_sections.append(Path(result["html_path"]).read_text(encoding="utf-8", errors="ignore"))
                tmp_outputs.append((Path(result["html_path"]), Path(result["pdf_path"])))

            if not html_sections:
                print("No key selections produced output; returning empty report.")
                return {"html_path": str(OUT_HTML), "pdf_path": str(OUT_PDF), "rows_rendered": False}

            combined_html = _combine_html_documents(html_sections)
            OUT_HTML.write_text(combined_html, encoding="utf-8")
            asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent))
            return {"html_path": str(OUT_HTML), "pdf_path": str(OUT_PDF), "rows_rendered": True}
        finally:
            for tmp_html_path, tmp_pdf_path in tmp_outputs:
                for path in (tmp_html_path, tmp_pdf_path):
                    try:
                        path.unlink()
                    except FileNotFoundError:
                        pass

    def _get_literal_raw(token: str) -> str:
        if token not in LITERALS:
            return ""
        raw = LITERALS[token]
        return "" if raw is None else str(raw)

    def _literal_has_content(token: str) -> bool:
        return bool(_get_literal_raw(token).strip())

    def _first_nonempty_literal(tokens: Iterable[str]) -> tuple[str | None, str | None]:
        for tok in tokens:
            raw = _get_literal_raw(tok)
            if raw.strip():
                return tok, raw
        return None, None

    def _record_special_value(target: dict[str, str], token: str, value: str) -> None:
        existing_raw = _get_literal_raw(token)
        if existing_raw.strip():
            target[token] = existing_raw
        else:
            target[token] = value
            if token in LITERALS:
                LITERALS[token] = value

    def _filter_tokens_without_literal(tokens: set[str]) -> set[str]:
        return {tok for tok in tokens if not _literal_has_content(tok)}

    BEGIN_TAG = "<!-- BEGIN:BATCH (auto) -->"
    END_TAG = "<!-- END:BATCH (auto) -->"
    try:
        prototype_block, start0, end_last = _select_prototype_block(html, ROW_TOKENS)
    except Exception as exc:
        _raise_no_block(html, exc)
    shell_prefix = html[:start0] + BEGIN_TAG
    shell_suffix = END_TAG + html[end_last:]

    parent_table = JOIN.get("parent_table", "")
    parent_key   = JOIN.get("parent_key", "")
    child_table  = JOIN.get("child_table", "")
    child_key    = JOIN.get("child_key", "")
    parent_date  = DATE_COLUMNS.get(parent_table, "")
    child_date   = DATE_COLUMNS.get(child_table, "")
    order_col    = ROW_ORDER[0] if ROW_ORDER else "ROWID"

    def _normalize_token_name(name: str) -> str:
        return re.sub(r'[^a-z0-9]', '', name.lower())

    token_index: dict[str, set[str]] = defaultdict(set)
    all_candidate_tokens = set(TEMPLATE_TOKENS) | set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())

    def _token_synonym_keys(norm: str) -> set[str]:
        """
        Generate lightweight normalization aliases so that abbreviated tokens like
        `pg_total` or `page_num` still map onto the same lookup keys as their
        longer forms without needing every variant enumerated manually.
        """
        if not norm:
            return set()
        aliases = {norm}
        replacements: tuple[tuple[str, str], ...] = (
            ("pg", "page"),
            ("num", "number"),
            ("no", "number"),
            ("cnt", "count"),
            ("ttl", "total"),
        )
        for src, dest in replacements:
            if src in norm and dest not in norm:
                aliases.add(norm.replace(src, dest))
        # Avoid generating implausible short aliases (e.g., converting a lone "no"
        # in tokens unrelated to pagination), but include a fallback where a token
        # is exactly "pg" so that later lookups on "page" resolve.
        if norm == "pg":
            aliases.add("page")
        return {alias for alias in aliases if alias}

    for tok in all_candidate_tokens:
        norm = _normalize_token_name(tok)
        for key in _token_synonym_keys(norm):
            token_index[key].add(tok)

    def _tokens_for_keys(keys: set[str]) -> set[str]:
        found: set[str] = set()
        for key in keys:
            found.update(token_index.get(key, set()))
        return found

    def _parse_date_like(value) -> datetime | None:
        if value is None:
            return None
        val = str(value).strip()
        if not val:
            return None

        iso_try = val.replace('Z', '+00:00')
        if ' ' in iso_try and 'T' not in iso_try:
            iso_try = iso_try.replace(' ', 'T', 1)
        try:
            return datetime.fromisoformat(iso_try)
        except ValueError:
            pass

        if re.fullmatch(r"\d{10,}", val):
            try:
                seconds = int(val)
                if len(val) > 10:
                    scale = 10 ** (len(val) - 10)
                    return datetime.fromtimestamp(seconds / scale)
                return datetime.fromtimestamp(seconds)
            except ValueError:
                pass

        try:
            from email.utils import parsedate_to_datetime
        except ImportError:  # pragma: no cover
            parsedate_to_datetime = None  # type: ignore

        if parsedate_to_datetime is not None:
            try:
                dt = parsedate_to_datetime(val)
                if dt:
                    return dt if dt.tzinfo is None else dt.astimezone()
            except (TypeError, ValueError):
                pass

        candidates = [
            "%Y-%m-%d",
            "%Y/%m/%d",
            "%d/%m/%Y",
            "%m/%d/%Y",
            "%d-%m-%Y",
            "%m-%d-%Y",
            "%d.%m.%Y",
            "%d %b %Y",
            "%d %B %Y",
            "%b %d %Y",
            "%B %d %Y",
            "%Y-%m-%d %H:%M",
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M",
            "%Y/%m/%d %H:%M:%S",
            "%d/%m/%Y %H:%M",
            "%d/%m/%Y %H:%M:%S",
            "%m/%d/%Y %H:%M",
            "%m/%d/%Y %H:%M:%S",
            "%d-%m-%Y %H:%M",
            "%d-%m-%Y %H:%M:%S",
            "%d.%m.%Y %H:%M",
            "%d.%m.%Y %H:%M:%S",
            "%d %b %Y %H:%M",
            "%d %b %Y %H:%M:%S",
            "%d %B %Y %H:%M",
            "%d %B %Y %H:%M:%S",
            "%b %d %Y %H:%M",
            "%b %d %Y %H:%M:%S",
        ]
        for fmt in candidates:
            try:
                return datetime.strptime(val, fmt)
            except ValueError:
                continue
        return None

    def _has_time_component(raw_value, dt_obj: datetime | None) -> bool:
        if dt_obj and (dt_obj.hour or dt_obj.minute or dt_obj.second or dt_obj.microsecond):
            return True
        if raw_value is None:
            return False
        text = str(raw_value)
        if re.search(r'\d{1,2}:\d{2}', text):
            return True
        if re.search(r'\b(am|pm)\b', text, flags=re.IGNORECASE):
            return True
        if 'T' in text or 't' in text:
            return True
        return False

    def _format_for_token(token: str, dt_obj: datetime | None, include_time_default: bool = False) -> str:
        if not dt_obj:
            return ""

        token_lower = token.lower()
        token_clean = re.sub(r'[^a-z0-9]', '', token_lower)

        def _has(*needles: str) -> bool:
            return any(needle in token_clean for needle in needles)

        include_time = include_time_default or _has("time", "clock", "datetime", "timestamp")
        include_seconds = _has("second", "seconds", "sec", "timestamp", "precise", "fulltime")
        use_ampm = _has("ampm", "12h", "twelvehour")
        if include_seconds and not include_time:
            include_time = True
        if use_ampm and not include_time:
            include_time = True

        include_timezone = _has("timezone", "tz", "utc", "offset", "gmtoffset", "withtz", "withzone", "zulu")
        iso_like = _has("iso", "iso8601", "ymd", "rfc3339")
        rfc822_like = _has("rfc2822", "rfc822")
        http_like = _has("httpdate", "rfc7231")
        compact_like = _has("compact", "slug", "filename", "filestamp", "yyyymmdd", "numeric", "digits")
        us_like = _has("us", "usa", "mdy", "mmdd")
        dashed_like = _has("dash", "hyphen")
        long_like = _has("long", "verbose", "friendly", "pretty", "human")
        short_like = _has("short", "abbr", "mini", "brief")
        month_long_like = _has("monthname", "monthlong")
        month_short_like = _has("monthabbr", "monthshort")
        weekday_like = _has("weekday", "dayname")
        weekday_short = _has("weekdayshort", "weekdayabbr", "daynameshort")
        epoch_ms_like = _has("epochms", "millis", "milliseconds", "unixms")
        epoch_like = _has("epoch", "unixtime", "unix")

        dt_for_format = dt_obj
        if include_timezone and dt_for_format.tzinfo is None:
            try:
                dt_for_format = dt_for_format.astimezone()
            except ValueError:
                pass

        if epoch_ms_like:
            try:
                return str(int(dt_for_format.timestamp() * 1000))
            except (OSError, OverflowError, ValueError):
                pass
        if epoch_like:
            try:
                return str(int(dt_for_format.timestamp()))
            except (OSError, OverflowError, ValueError):
                pass

        if rfc822_like or http_like:
            try:
                from email.utils import format_datetime as _email_format_datetime
                base_dt = dt_for_format
                if base_dt.tzinfo is None:
                    base_dt = base_dt.astimezone()
                return _email_format_datetime(base_dt)
            except Exception:
                pass

        if iso_like:
            dt_use = dt_for_format
            if include_time:
                timespec = "seconds" if include_seconds else "minutes"
                try:
                    return dt_use.isoformat(timespec=timespec)
                except TypeError:
                    return dt_use.isoformat()
            return dt_use.date().isoformat()

        if compact_like:
            date_part = dt_for_format.strftime("%Y%m%d")
            if include_time:
                if use_ampm:
                    time_fmt = "%I%M%S%p" if include_seconds else "%I%M%p"
                else:
                    time_fmt = "%H%M%S" if include_seconds else "%H%M"
                date_part = f"{date_part}_{dt_for_format.strftime(time_fmt)}"
            if include_timezone:
                tz = dt_for_format.strftime("%z")
                if tz:
                    date_part = f"{date_part}{tz}"
            return date_part

        date_part = "%d/%m/%Y"
        if us_like:
            date_part = "%m/%d/%Y"
        elif dashed_like:
            date_part = "%d-%m-%Y"
        elif long_like:
            date_part = "%B %d, %Y"
        elif short_like or month_short_like:
            date_part = "%d %b %Y"
        elif month_long_like:
            date_part = "%d %B %Y"

        if weekday_like:
            prefix = "%a, " if weekday_short else "%A, "
            date_part = prefix + date_part

        fmt = date_part
        if include_time:
            if use_ampm:
                time_fmt = "%I:%M:%S %p" if include_seconds else "%I:%M %p"
            else:
                time_fmt = "%H:%M:%S" if include_seconds else "%H:%M"
            fmt = f"{fmt} {time_fmt}"
        if include_timezone:
            fmt = f"{fmt} %Z".strip()

        try:
            rendered = dt_for_format.strftime(fmt).strip()
            if not rendered and "%Z" in fmt:
                rendered = dt_for_format.strftime(fmt.replace("%Z", "%z")).strip()
            return rendered
        except Exception:
            if include_time:
                try:
                    return dt_for_format.isoformat(timespec="seconds")
                except TypeError:
                    return dt_for_format.isoformat()
            return dt_for_format.date().isoformat()

    def _format_for_db(dt_obj: datetime | None, raw_value, include_time_default: bool) -> str:
        """
        Normalize input dates for SQLite bindings:
          - prefer ISO 8601 date or datetime strings
          - fall back to trimmed raw strings when parsing fails
        """
        if dt_obj:
            include_time = include_time_default or bool(
                dt_obj.hour or dt_obj.minute or dt_obj.second or dt_obj.microsecond
            )
            if include_time:
                return dt_obj.strftime("%Y-%m-%d %H:%M:%S")
            return dt_obj.strftime("%Y-%m-%d")
        return "" if raw_value is None else str(raw_value).strip()

    def _run_generator_entrypoints(entrypoints: dict, sql_params: dict[str, object]) -> dict[str, list[dict[str, object]]]:
        if not entrypoints:
            return {}
        con = sqlite3.connect(str(DB_PATH))
        con.row_factory = sqlite3.Row
        alias_fix_patterns = [
            re.compile(r"\brows\.", re.IGNORECASE),
            re.compile(r"\brows_agg\.", re.IGNORECASE),
        ]

        def _wrap_date_param(sql_text: str, param: str) -> str:
            pattern = re.compile(rf":{param}\b")

            def _replace(match: re.Match[str]) -> str:
                start = match.start()
                prefix = sql_text[:start].rstrip()
                prefix_lower = prefix.lower()
                if prefix_lower.endswith("date(") or prefix_lower.endswith("datetime("):
                    return match.group(0)
                return f"DATE({match.group(0)})"

            return pattern.sub(_replace, sql_text)

        def _prepare_sql(sql_text: str) -> str:
            updated = re.sub(r"PARAM:([A-Za-z0-9_]+)", r":\1", sql_text)
            if "DATE(" not in updated.upper():
                return updated
            for param in ("from_date", "to_date"):
                updated = _wrap_date_param(updated, param)
            return updated

        def _attempt_sql_fix(sql_text: str, exc: Exception | None) -> str | None:
            if exc is None:
                return None
            message = str(exc).lower()
            if "no such column" not in message:
                return None
            fixed_sql = sql_text
            for pattern in alias_fix_patterns:
                fixed_sql = pattern.sub("", fixed_sql)
            return fixed_sql if fixed_sql != sql_text else None

        try:
            results: dict[str, list[dict[str, object]]] = {}
            for name in ("header", "rows", "totals"):
                sql = entrypoints.get(name)
                if not sql:
                    results[name] = []
                    continue
                current_sql = _prepare_sql(sql)
                last_error: Exception | None = None
                for attempt in range(2):
                    try:
                        cur = con.execute(current_sql, sql_params)
                    except sqlite3.OperationalError as exc:
                        last_error = exc
                        fixed_sql = _attempt_sql_fix(current_sql, exc)
                        if fixed_sql is not None and fixed_sql != current_sql:
                            current_sql = fixed_sql
                            continue
                        raise
                    else:
                        last_error = None
                        rows = [dict(row) for row in cur.fetchall()]
                        results[name] = rows
                        break
                else:
                    assert last_error is not None
                    raise last_error
            return results
        finally:
            con.close()

    start_dt = _parse_date_like(START_DATE)
    end_dt = _parse_date_like(END_DATE)
    print_dt = datetime.now()

    start_has_time = _has_time_component(START_DATE, start_dt)
    end_has_time = _has_time_component(END_DATE, end_dt)

    START_DATE_KEYS = {"fromdate", "datefrom", "startdate", "periodstart", "rangefrom", "fromdt", "startdt"}
    END_DATE_KEYS   = {"todate", "dateto", "enddate", "periodend", "rangeto", "todt", "enddt"}
    PRINT_DATE_KEYS = {"printdate", "printedon", "printeddate", "generatedon", "generateddate", "rundate", "runon", "generatedat"}
    PAGE_NO_KEYS    = {
        "page",
        "pageno",
        "pagenum",
        "pagenumber",
        "pageindex",
        "pageidx",
        "pagecurrent",
        "currentpage",
        "currpage",
        "pgno",
        "pgnum",
        "pgnumber",
        "pgindex",
        "pgcurrent",
    }
    PAGE_COUNT_KEYS = {
        "pagecount",
        "pagecounts",
        "totalpages",
        "pagestotal",
        "pages",
        "pagetotal",
        "totalpage",
        "pagecounttotal",
        "totalpagecount",
        "pagescount",
        "countpages",
        "lastpage",
        "finalpage",
        "maxpage",
        "pgtotal",
        "totalpg",
        "pgcount",
        "countpg",
        "pgs",
        "pgscount",
        "pgstotal",
        "totalpgs",
    }
    PAGE_LABEL_KEYS = {
        "pagelabel",
        "pageinfo",
        "pagesummary",
        "pagefooter",
        "pagefootertext",
        "pageindicator",
        "pagecaption",
        "pagefooterlabel",
        "pagetext",
        "pagefooterinfo",
        "pagehint",
    }

    special_values: dict[str, str] = {}

    start_tokens = _tokens_for_keys(START_DATE_KEYS)
    end_tokens = _tokens_for_keys(END_DATE_KEYS)
    print_tokens = _tokens_for_keys(PRINT_DATE_KEYS)
    page_number_tokens = _tokens_for_keys(PAGE_NO_KEYS)
    page_count_tokens = _tokens_for_keys(PAGE_COUNT_KEYS)
    page_label_tokens = _tokens_for_keys(PAGE_LABEL_KEYS)

    for tok in start_tokens:
        _record_special_value(
            special_values,
            tok,
            _format_for_token(tok, start_dt, include_time_default=start_has_time),
        )
    for tok in end_tokens:
        _record_special_value(
            special_values,
            tok,
            _format_for_token(tok, end_dt, include_time_default=end_has_time),
        )

    _, print_literal_value = _first_nonempty_literal(print_tokens)
    parsed_print_dt = _parse_date_like(print_literal_value) if print_literal_value else None
    print_dt_source = parsed_print_dt or print_dt
    print_has_time = _has_time_component(print_literal_value, parsed_print_dt)

    for tok in print_tokens:
        if print_literal_value and not parsed_print_dt:
            value = print_literal_value
        else:
            value = _format_for_token(tok, print_dt_source, include_time_default=print_has_time)
        _record_special_value(special_values, tok, value)

    page_number_tokens = _filter_tokens_without_literal(page_number_tokens)
    page_count_tokens = _filter_tokens_without_literal(page_count_tokens)
    page_label_tokens = _filter_tokens_without_literal(page_label_tokens)

    post_literal_specials = {tok: val for tok, val in special_values.items() if tok not in LITERALS}

    _ident_re = re.compile(r"^[A-Za-z_][A-Za-z0-9_]*$")
    def qident(name: str) -> str:
        if _ident_re.match(name):
            return name
        safe = name.replace('"', '""')
        return f'"{safe}"'

    # ---- Composite-key helpers ----
    def _parse_key_cols(key_spec: str) -> list[str]:
        return [c.strip() for c in str(key_spec).split(",") if c and c.strip()]

    def _key_expr(cols: list[str]) -> str:
        parts = [f"COALESCE(CAST({qident(c)} AS TEXT),'')" for c in cols]
        if not parts:
            return "''"
        expr = parts[0]
        for p in parts[1:]:
            expr = f"{expr} || '|' || {p}"
        return expr

    def _split_bid(bid: str, n: int) -> list[str]:
        parts = str(bid).split("|")
        if len(parts) != n:
            raise ValueError(f"Composite key mismatch: expected {n} parts, got {len(parts)} in {bid!r}")
        return parts

    def _looks_like_composite_id(x: str, n: int) -> bool:
        return isinstance(x, str) and x.count("|") == (n - 1)

    pcols = _parse_key_cols(parent_key)
    ccols = _parse_key_cols(child_key)

    has_child = bool(child_table and ccols)
    parent_table_lc = parent_table.lower()
    child_table_lc = child_table.lower()
    parent_filter_map: dict[str, list[str]] = {}
    child_filter_map: dict[str, list[str]] = {}
    if key_values_map:
        for token, values in key_values_map.items():
            mapping_value = PLACEHOLDER_TO_COL.get(token)
            if not isinstance(mapping_value, str):
                continue
            target = mapping_value.strip()
            if not target or target.upper().startswith("PARAM:") or "." not in target:
                continue
            table_name, column_name = target.split(".", 1)
            table_name = table_name.strip(' "`[]')
            column_name = column_name.strip(' "`[]')
            if not column_name:
                continue
            table_key = table_name.lower()
            if table_key in (parent_table_lc, "header"):
                bucket = list(parent_filter_map.get(column_name, []))
                for val in values:
                    if val not in bucket:
                        bucket.append(val)
                if bucket:
                    parent_filter_map[column_name] = bucket
            if has_child and table_key in (child_table_lc, "rows"):
                bucket = list(child_filter_map.get(column_name, []))
                for val in values:
                    if val not in bucket:
                        bucket.append(val)
                if bucket:
                    child_filter_map[column_name] = bucket
    parent_filter_items = list(parent_filter_map.items())
    child_filter_items = list(child_filter_map.items())
    parent_filter_sqls: list[str] = []
    parent_filter_values: list[str] = []
    for col, values in parent_filter_items:
        normalized: list[str] = []
        for val in values:
            if not isinstance(val, str):
                continue
            text = val.strip()
            if text and text not in normalized:
                normalized.append(text)
        if not normalized:
            continue
        if len(normalized) == 1:
            parent_filter_sqls.append(f"{qident(col)} = ?")
        else:
            placeholders = ", ".join("?" for _ in normalized)
            parent_filter_sqls.append(f"{qident(col)} IN ({placeholders})")
        parent_filter_values.extend(normalized)
    parent_filter_values_tuple = tuple(parent_filter_values)

    child_filter_sqls: list[str] = []
    child_filter_values: list[str] = []
    for col, values in child_filter_items:
        normalized: list[str] = []
        for val in values:
            if not isinstance(val, str):
                continue
            text = val.strip()
            if text and text not in normalized:
                normalized.append(text)
        if not normalized:
            continue
        if len(normalized) == 1:
            child_filter_sqls.append(f"{qident(col)} = ?")
        else:
            placeholders = ", ".join("?" for _ in normalized)
            child_filter_sqls.append(f"{qident(col)} IN ({placeholders})")
        child_filter_values.extend(normalized)
    child_filter_values_tuple = tuple(child_filter_values)

    def _merge_predicate(base_sql: str, extras: list[str]) -> str:
        if not extras:
            return base_sql
        extras_joined = " AND ".join(extras)
        base_sql = (base_sql or "1=1").strip()
        return f"({base_sql}) AND {extras_joined}"

    # --- Date predicates and adapters (handle missing/invalid date columns)
    parent_type = get_col_type(DB_PATH, parent_table, parent_date)
    child_type = get_col_type(DB_PATH, child_table, child_date)
    parent_pred, adapt_parent = mk_between_pred_for_date(parent_date, parent_type)
    child_pred, adapt_child = mk_between_pred_for_date(child_date, child_type)
    parent_where_clause = _merge_predicate(parent_pred, parent_filter_sqls)
    child_where_clause = _merge_predicate(child_pred, child_filter_sqls) if has_child else child_pred
    db_start = _format_for_db(start_dt, START_DATE, start_has_time)
    db_end = _format_for_db(end_dt, END_DATE, end_has_time)
    PDATE = tuple(adapt_parent(db_start, db_end))  # () if 1=1
    CDATE = tuple(adapt_child(db_start, db_end)) if has_child else tuple()  # () if 1=1
    parent_params_all = tuple(PDATE) + parent_filter_values_tuple
    child_params_all = tuple(CDATE) + child_filter_values_tuple if has_child else tuple()

    sql_params: dict[str, object] = {
        "from_date": db_start,
        "to_date": db_end,
    }

    for token in contract_adapter.param_tokens:
        if token in ("from_date", "to_date"):
            continue
        if token in key_values_map:
            first_value = _first_key_value(key_values_map[token])
            if first_value is not None:
                sql_params[token] = first_value
        elif token in LITERALS:
            sql_params[token] = LITERALS[token]
        elif token in special_values:
            sql_params[token] = special_values[token]
        else:
            sql_params.setdefault(token, "")

    if GENERATOR_BUNDLE is None:
        generator_dir = TEMPLATE_PATH.parent / "generator"
        meta_path = generator_dir / "generator_assets.json"
        if meta_path.exists():
            try:
                meta_payload = json.loads(meta_path.read_text(encoding="utf-8"))
            except Exception:
                meta_payload = None
            else:
                bundle: dict[str, object] = {"meta": meta_payload}
                output_schemas_path = generator_dir / "output_schemas.json"
                if output_schemas_path.exists():
                    try:
                        bundle["output_schemas"] = json.loads(output_schemas_path.read_text(encoding="utf-8"))
                    except Exception:
                        pass
                GENERATOR_BUNDLE = bundle

    generator_results: dict[str, list[dict[str, object]]] | None = None
    if GENERATOR_BUNDLE and not multi_key_selected:
        meta_payload = GENERATOR_BUNDLE.get("meta") or {}
        entrypoints = meta_payload.get("entrypoints")
        if not isinstance(entrypoints, dict):
            entrypoints = {}

        if any(entrypoints.values()):
            params_spec = meta_payload.get("params") or {}
            required_params = list(params_spec.get("required") or [])
            optional_params = list(params_spec.get("optional") or [])
            for name in required_params + optional_params:
                sql_params.setdefault(name, None)

            if "plant_name" in sql_params:
                sql_params["plant_name"] = LITERALS.get("plant_name") or special_values.get("plant_name", "")
            if "location" in sql_params:
                sql_params["location"] = LITERALS.get("location") or special_values.get("location", "")
            if "recipe_code" in sql_params:
                if "recipe_code" in key_values_map:
                    sql_params["recipe_code"] = _first_key_value(key_values_map["recipe_code"])
                else:
                    fallback_val = LITERALS.get("recipe_code")
                    sql_params["recipe_code"] = fallback_val if fallback_val not in (None, "", []) else None
            if "page_info" in sql_params:
                sql_params["page_info"] = LITERALS.get("page_info") or ""
            if key_values_map:
                for name, values in key_values_map.items():
                    sql_params[name] = _first_key_value(values)

            try:
                generator_results = _run_generator_entrypoints(entrypoints, sql_params)
            except Exception as exc:  # pragma: no cover - defensive logging
                print(f"Generator SQL execution failed; falling back to contract mapping: {exc}")
                generator_results = None

    if generator_results is None and not multi_key_selected:
        try:
            default_sql_pack = contract_adapter.build_default_sql_pack()
        except Exception as exc:  # pragma: no cover - defensive logging
            print(f"Contract-derived SQL synthesis failed: {exc}")
        else:
            default_params = default_sql_pack.get("params") or {}
            required_default = list(default_params.get("required") or [])
            optional_default = list(default_params.get("optional") or [])
            for name in required_default + optional_default:
                sql_params.setdefault(name, None)
            try:
                fallback_results = _run_generator_entrypoints(default_sql_pack["entrypoints"], sql_params)
            except Exception as exc:  # pragma: no cover - defensive logging
                print(f"Contract SQL execution failed; continuing with discovery fallback: {exc}")
            else:
                if any(fallback_results.get(section) for section in ("rows", "header", "totals")):
                    generator_results = fallback_results

    # ---- Normalize / auto-discover BATCH_IDS ----
    if generator_results is None:
        need_discover = False
        existing = batch_ids

        if isinstance(existing, str):
            existing = [existing]

        if not existing:
            need_discover = True
        else:
            if not isinstance(existing, (list, tuple)):
                need_discover = True
            else:
                existing = list(existing)
                if len(pcols) > 1:
                    if any(not _looks_like_composite_id(i, len(pcols)) for i in existing):
                        print("Provided BATCH_IDS do not match composite key format; falling back to auto-discovery.")
                        need_discover = True

        if need_discover:
            with sqlite3.connect(str(DB_PATH)) as con:
                con.row_factory = sqlite3.Row
                cur = con.cursor()

                # Parent discovery
                if len(pcols) == 1:
                    parent_sql = f"""
                        SELECT DISTINCT {qident(pcols[0])} AS bid
                        FROM {qident(parent_table)}
                        WHERE {parent_where_clause}
                    """
                    parent_ids = [r["bid"] for r in cur.execute(parent_sql, parent_params_all)]
                else:
                    parent_sql = f"""
                        SELECT DISTINCT {_key_expr(pcols)} AS bid
                        FROM {qident(parent_table)}
                        WHERE {parent_where_clause}
                    """
                    parent_ids = [r["bid"] for r in cur.execute(parent_sql, parent_params_all)]

                # Child discovery
                if len(ccols) == 1:
                    child_sql = f"""
                        SELECT DISTINCT {qident(ccols[0])} AS bid
                        FROM {qident(child_table)}
                        WHERE {child_where_clause}
                    """
                    child_ids = [r["bid"] for r in cur.execute(child_sql, child_params_all)]
                else:
                    child_sql = f"""
                        SELECT DISTINCT {_key_expr(ccols)} AS bid
                        FROM {qident(child_table)}
                        WHERE {child_where_clause}
                    """
                    child_ids = [r["bid"] for r in cur.execute(child_sql, child_params_all)]

                all_ids = sorted({str(x) for x in (parent_ids + child_ids)})

                if len(all_ids) <= 1:
                    # Relax discovery if filtered too tightly by date (retain key filters)
                    if len(pcols) == 1:
                        p_all = f"SELECT DISTINCT {qident(pcols[0])} AS bid FROM {qident(parent_table)}"
                    else:
                        p_all = f"SELECT DISTINCT {_key_expr(pcols)} AS bid FROM {qident(parent_table)}"
                    if parent_filter_sqls:
                        p_all = f"{p_all} WHERE " + " AND ".join(parent_filter_sqls)
                        parent_ids = [r["bid"] for r in cur.execute(p_all, parent_filter_values_tuple)]
                    else:
                        parent_ids = [r["bid"] for r in cur.execute(p_all)]

                    if len(ccols) == 1:
                        c_all = f"SELECT DISTINCT {qident(ccols[0])} AS bid FROM {qident(child_table)}"
                    else:
                        c_all = f"SELECT DISTINCT {_key_expr(ccols)} AS bid FROM {qident(child_table)}"
                    if has_child and child_filter_sqls:
                        c_all = f"{c_all} WHERE " + " AND ".join(child_filter_sqls)
                        child_ids = [r["bid"] for r in cur.execute(c_all, child_filter_values_tuple)]
                    else:
                        child_ids = [r["bid"] for r in cur.execute(c_all)]
                    all_ids = sorted({str(x) for x in (parent_ids + child_ids)})

                BATCH_IDS = all_ids
        else:
            BATCH_IDS = existing
    else:
        BATCH_IDS = ["__GENERATOR_SINGLE__"]

    print("BATCH_IDS:", len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])
    # ---- Only touch tokens outside <style>/<script> ----
    STYLE_OR_SCRIPT_RE = re.compile(r"(?is)(<style\b[^>]*>.*?</style>|<script\b[^>]*>.*?</script>)")

    def _apply_outside_styles_scripts(html_in: str, transform_fn):
        parts = STYLE_OR_SCRIPT_RE.split(html_in)
        for i in range(len(parts)):
            if i % 2 == 0:
                parts[i] = transform_fn(parts[i])
        return "".join(parts)

    def _sub_token_text(text: str, token: str, val: str) -> str:
        pat = re.compile(r"(\{\{\s*" + re.escape(token) + r"\s*\}\}|\{\s*" + re.escape(token) + r"\s*\})")
        return pat.sub(val, text)

    def sub_token(html_in: str, token: str, val: str) -> str:
        return _apply_outside_styles_scripts(html_in, lambda txt: _sub_token_text(txt, token, val))

    def format_token_value(token: str, raw_value: Any) -> str:
        return contract_adapter.format_value(token, raw_value)

    def _convert_css_length_to_mm(raw: str) -> float | None:
        if not raw:
            return None
        text = raw.strip().lower()
        if not text or text == "auto":
            return None
        m = re.match(r"([-+]?\d*\.?\d+)\s*(mm|cm|in|pt|pc|px)?", text)
        if not m:
            return None
        value = float(m.group(1))
        unit = m.group(2) or "px"
        if unit == "mm":
            return value
        if unit == "cm":
            return value * 10.0
        if unit in {"in", "inch", "inches"}:
            return value * 25.4
        if unit == "pt":
            return value * (25.4 / 72.0)
        if unit == "pc":
            return value * (25.4 / 6.0)
        if unit == "px":
            return value * (25.4 / 96.0)
        return None

    def _parse_page_size_value(value: str) -> tuple[float, float] | None:
        if not value:
            return None
        text = value.strip().lower()
        if not text:
            return None
        size_map: dict[str, tuple[float, float]] = {
            "a3": (297.0, 420.0),
            "a4": (210.0, 297.0),
            "a5": (148.0, 210.0),
            "letter": (215.9, 279.4),
            "legal": (215.9, 355.6),
            "tabloid": (279.4, 431.8),
        }
        tokens = text.split()
        orientation = None
        size_tokens = tokens
        if tokens and tokens[-1] in {"portrait", "landscape"}:
            orientation = tokens[-1]
            size_tokens = tokens[:-1]
        if len(size_tokens) == 1 and size_tokens[0] in size_map:
            width_mm, height_mm = size_map[size_tokens[0]]
        elif len(size_tokens) >= 2:
            first = _convert_css_length_to_mm(size_tokens[0])
            second = _convert_css_length_to_mm(size_tokens[1])
            if first is None or second is None:
                return None
            width_mm, height_mm = first, second
        else:
            return None
        if orientation == "landscape":
            width_mm, height_mm = height_mm, width_mm
        return width_mm, height_mm

    def _parse_margin_shorthand(value: str) -> tuple[float | None, float | None, float | None, float | None]:
        parts = [p for p in re.split(r"\s+", value.strip()) if p]
        values = [_convert_css_length_to_mm(p) for p in parts]
        if not values:
            return None, None, None, None
        if len(values) == 1:
            top = right = bottom = left = values[0]
        elif len(values) == 2:
            top = bottom = values[0]
            right = left = values[1]
        elif len(values) == 3:
            top = values[0]
            right = left = values[1]
            bottom = values[2]
        else:
            top, right, bottom, left = values[:4]
        return top, right, bottom, left

    def _extract_page_metrics(html_in: str) -> dict[str, float]:
        default_width_mm, default_height_mm = 210.0, 297.0
        margin_top_mm = 0.0
        margin_bottom_mm = 0.0
        page_match = re.search(r"@page\b[^{]*\{(?P<body>.*?)\}", html_in, re.IGNORECASE | re.DOTALL)
        if page_match:
            block = page_match.group("body")
            size_match = re.search(r"size\s*:\s*([^;]+);?", block, re.IGNORECASE)
            if size_match:
                parsed_size = _parse_page_size_value(size_match.group(1))
                if parsed_size:
                    default_width_mm, default_height_mm = parsed_size
            margin_match = re.search(r"margin\s*:\s*([^;]+);?", block, re.IGNORECASE)
            if margin_match:
                mt, _, mb, _ = _parse_margin_shorthand(margin_match.group(1))
                if mt is not None:
                    margin_top_mm = mt
                if mb is not None:
                    margin_bottom_mm = mb
            for name, setter in (("margin-top", "top"), ("margin-bottom", "bottom")):
                specific = re.search(rf"{name}\s*:\s*([^;]+);?", block, re.IGNORECASE)
                if specific:
                    as_mm = _convert_css_length_to_mm(specific.group(1))
                    if as_mm is None:
                        continue
                    if setter == "top":
                        margin_top_mm = as_mm
                    else:
                        margin_bottom_mm = as_mm
        return {
            "page_width_mm": default_width_mm,
            "page_height_mm": default_height_mm,
            "margin_top_mm": max(margin_top_mm, 0.0),
            "margin_bottom_mm": max(margin_bottom_mm, 0.0),
        }

    def _inject_page_counter_spans(
        html_in: str,
        page_tokens: set[str],
        count_tokens: set[str],
        label_tokens: set[str] | None = None,
    ) -> str:
        label_tokens = label_tokens or set()
        updated = html_in
        page_markup = '<span class="nr-page-number" data-nr-counter="page" aria-label="Current page number"></span>'
        count_markup = '<span class="nr-page-count" data-nr-counter="pages" aria-label="Total page count"></span>'

        for tok in page_tokens:
            updated = sub_token(updated, tok, page_markup)
        for tok in count_tokens:
            updated = sub_token(updated, tok, count_markup)
        for tok in label_tokens:
            if count_tokens:
                label_markup = f'<span class="nr-page-label" data-nr-counter-label="1">Page {page_markup} of {count_markup}</span>'
            else:
                label_markup = f'<span class="nr-page-label" data-nr-counter-label="1">Page {page_markup}</span>'
            updated = sub_token(updated, tok, label_markup)

        if (page_tokens or count_tokens or label_tokens) and 'nr-page-counter-style' not in updated:
            style_block = """
<style id="nr-page-counter-style">
  .nr-page-number,
  .nr-page-count { white-space: nowrap; font-variant-numeric: tabular-nums; }
  .nr-page-label { white-space: nowrap; }
  @media screen {
    .nr-page-number::after { content: attr(data-nr-screen); }
    .nr-page-count::after { content: attr(data-nr-screen); }
  }
  @media print {
    body { counter-reset: page; }
    .nr-page-number { counter-increment: page; }
    .nr-page-number::after { content: counter(page); }
    .nr-page-count::after { content: counter(pages); }
    html[data-nr-total-pages] .nr-page-count::after { content: attr(data-nr-total-pages); }
  }
</style>
"""
            if '</head>' in updated:
                updated = updated.replace('</head>', style_block + '</head>', 1)
            else:
                updated = style_block + updated

        if (page_tokens or count_tokens or label_tokens) and 'nr-page-counter-script' not in updated:
            metrics = _extract_page_metrics(updated)
            metrics_json = json.dumps(metrics)
            script_block = f"""
<script id="nr-page-counter-script">
(function() {{
  const METRICS = {metrics_json};
  const PX_PER_MM = 96 / 25.4;

  function computeTotals() {{
    try {{
      const doc = document.documentElement;
      const body = document.body;
      if (!doc || !body) return;
      const usableHeightMm = Math.max(METRICS.page_height_mm - (METRICS.margin_top_mm + METRICS.margin_bottom_mm), 0.1);
      const usableHeightPx = usableHeightMm * PX_PER_MM;
      const contentHeight = Math.max(
        body.scrollHeight,
        body.offsetHeight,
        doc.scrollHeight,
        doc.offsetHeight
      );
      const totalPages = Math.max(1, Math.ceil(contentHeight / usableHeightPx));
      doc.setAttribute('data-nr-total-pages', String(totalPages));
      const countNodes = document.querySelectorAll('[data-nr-counter="pages"]');
      countNodes.forEach((node) => {{
        node.setAttribute('data-nr-screen', String(totalPages));
      }});
      const pageNodes = document.querySelectorAll('[data-nr-counter="page"]');
      pageNodes.forEach((node) => {{
        if (!node.getAttribute('data-nr-screen')) {{
          node.setAttribute('data-nr-screen', '1');
        }}
      }});
    }} catch (err) {{
      if (typeof console !== 'undefined' && console.warn) {{
        console.warn('nr-page-counter: unable to compute preview counters', err);
      }}
    }}
  }}

  function scheduleCompute() {{
    computeTotals();
    setTimeout(computeTotals, 180);
  }}

  if (document.readyState === 'loading') {{
    document.addEventListener('DOMContentLoaded', () => {{
      scheduleCompute();
    }}, {{ once: true }});
  }} else {{
    scheduleCompute();
  }}

  if (typeof window !== 'undefined') {{
    window.addEventListener('resize', computeTotals, {{ passive: true }});
  }}
}})();
</script>
"""
            if '</body>' in updated:
                updated = updated.replace('</body>', script_block + '</body>', 1)
            else:
                updated = updated + script_block
        return updated

    def _blank_known_tokens_text(text: str, tokens) -> str:
        for t in tokens:
            text = re.sub(r"\{\{\s*" + re.escape(t) + r"\s*\}\}", "", text)
            text = re.sub(r"\{\s*" + re.escape(t) + r"\s*\}", "", text)
        return text

    def blank_known_tokens(html_in: str, tokens) -> str:
        return _apply_outside_styles_scripts(html_in, lambda txt: _blank_known_tokens_text(txt, tokens))

    # ---- Helpers to find tbody / row template (improved) ----
    def best_rows_tbody(inner_html: str, allowed_tokens: set):
        tbodys = list(re.finditer(r'(?is)<tbody\b[^>]*>(.*?)</tbody>', inner_html))
        best = (None, None, -1)  # (match, inner, hits)
        for m in tbodys:
            tin = m.group(1)
            hits = 0
            for trm in re.finditer(r"(?is)<tr\b[^>]*>.*?</tr>", tin):
                tr_html = trm.group(0)
                toks = re.findall(r"\{\{\s*([^}\n]+?)\s*\}\}|\{\s*([^}\n]+?)\s*\}", tr_html)
                flat = [a.strip() if a else b.strip() for (a, b) in toks]
                hits += sum(1 for t in flat if t in allowed_tokens)
            if hits > best[2]:
                best = (m, tin, hits)
        if best[0] is not None:
            return best[0], best[1]
        return (tbodys[0], tbodys[0].group(1)) if tbodys else (None, None)

    def find_row_template(tbody_inner: str, allowed_tokens: set):
        for m in re.finditer(r"(?is)<tr\b[^>]*>.*?</tr>", tbody_inner):
            tr_html = m.group(0)
            toks = re.findall(r"\{\{\s*([^}\n]+?)\s*\}\}|\{\s*([^}\n]+?)\s*\}", tr_html)
            flat = []
            for a, b in toks:
                if a: flat.append(a.strip())
                if b: flat.append(b.strip())
            flat = [t for t in flat if t in allowed_tokens]
            if flat:
                return tr_html, (m.start(0), m.end(0)), sorted(set(flat), key=len, reverse=True)
        return None, None, []

    def majority_table_for_tokens(tokens, mapping):
        from collections import Counter
        tbls = []
        for t in tokens:
            tc = mapping.get(t, "")
            if "." in tc:
                tbls.append(tc.split(".", 1)[0])
        return Counter(tbls).most_common(1)[0][0] if tbls else None

    # ---- Pre-compute minimal column sets ----
    def _extract_col_name(mapping_value: str | None) -> str | None:
        if not isinstance(mapping_value, str):
            return None
        target = mapping_value.strip()
        if "." not in target:
            return None
        return target.split(".", 1)[1].strip() or None

    header_cols = sorted(
        {
            col
            for t in HEADER_TOKENS
            for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))]
            if col
        }
    )
    row_cols = sorted(
        {
            col
            for t in ROW_TOKENS
            for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))]
            if col
        }
    )

    totals_by_table = defaultdict(lambda: defaultdict(list))
    total_token_to_target = {}

    for token, raw_target in TOTALS.items():
        target = (raw_target or PLACEHOLDER_TO_COL.get(token, "")).strip()
        if not target or "." not in target:
            continue
        table_name, col_name = [part.strip() for part in target.split(".", 1)]
        if not table_name or not col_name:
            continue
        totals_by_table[table_name][col_name].append(token)
        total_token_to_target[token] = (table_name, col_name)

    def _coerce_total_value(raw):
        if raw is None:
            return None, "0"
        try:
            decimal_value = Decimal(str(raw).strip())
        except (InvalidOperation, ValueError, TypeError, AttributeError):
            return None, "0"
        if not decimal_value.is_finite():
            return None, "0"
        formatted = format_decimal_str(decimal_value, max_decimals=3) or "0"
        return float(decimal_value), formatted

    totals_accum = defaultdict(float)
    last_totals_per_token = {token: "0" for token in TOTALS}

    child_totals_cols = {col: list(tokens) for col, tokens in totals_by_table.get(child_table, {}).items()}
    # ---- Render all batches ----
    rendered_blocks = []
    if generator_results is not None:
        block_html = prototype_block

        header_rows = generator_results.get("header") or []
        header_row = header_rows[0] if header_rows else {}
        for t in HEADER_TOKENS:
            if t in header_row:
                value = header_row[t]
                block_html = sub_token(block_html, t, format_token_value(t, value))

        allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)
        rows_data = generator_results.get("rows") or []
        if rows_data:
            tbody_m, tbody_inner = best_rows_tbody(block_html, allowed_row_tokens)
            filtered_rows: list[dict[str, Any]] = []
            row_tokens_in_template: list[str] = []
            significant_tokens = [
                tok for tok in allowed_row_tokens
                if tok and not any(keyword in tok.lower() for keyword in ("row", "serial", "sl"))
            ]
            significant_columns = [
                col for col in {
                    _extract_col_name(PLACEHOLDER_TO_COL.get(tok))
                    for tok in allowed_row_tokens
                }
                if col and not any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
            ]

            if tbody_m and tbody_inner:
                row_template, row_span, row_tokens_in_template = find_row_template(tbody_inner, allowed_row_tokens)
                if row_template and row_tokens_in_template:
                    serial_columns_all = [
                        _extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or ""
                        for tok in row_tokens_in_template
                    ]
                    significant_tokens = [
                        tok for tok in row_tokens_in_template
                        if tok and not any(keyword in tok.lower() for keyword in ("row", "serial", "sl"))
                    ]
                    significant_columns = [
                        col for col in (_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) for tok in row_tokens_in_template)
                        if col and not any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
                    ]
                    for row in rows_data:
                        if (significant_tokens or significant_columns) and not _row_has_any_data(row, significant_tokens, significant_columns):
                            continue
                        filtered_rows.append(dict(row))
                    if filtered_rows:
                        _reindex_serial_fields(filtered_rows, row_tokens_in_template, serial_columns_all)
                        parts = []
                        for row in filtered_rows:
                            tr = row_template
                            for tok in row_tokens_in_template:
                                val = _value_for_token(row, tok)
                                tr = sub_token(tr, tok, format_token_value(tok, val))
                            parts.append(tr)
                        new_tbody_inner = tbody_inner[:row_span[0]] + "\n".join(parts) + tbody_inner[row_span[1]:]
                        block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]
            else:
                tr_tokens = [m.group(1) or m.group(2)
                             for m in re.finditer(r"\{\{\s*([^}\n]+?)\s*\}\}|\{\s*([^}\n]+?)\s*\}", block_html)]
                row_tokens_in_template = [t.strip() for t in tr_tokens if t and t.strip() in allowed_row_tokens]
                if row_tokens_in_template:
                    serial_columns_all = [
                        _extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or ""
                        for tok in row_tokens_in_template
                    ]
                    significant_tokens = [
                        tok for tok in row_tokens_in_template
                        if tok and not any(keyword in tok.lower() for keyword in ("row", "serial", "sl"))
                    ]
                    significant_columns = [
                        col for col in (_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) for tok in row_tokens_in_template)
                        if col and not any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
                    ]
                    for row in rows_data:
                        if (significant_tokens or significant_columns) and not _row_has_any_data(row, significant_tokens, significant_columns):
                            continue
                        filtered_rows.append(dict(row))
                    if filtered_rows:
                        _reindex_serial_fields(filtered_rows, row_tokens_in_template, serial_columns_all)
                        parts = []
                        for row in filtered_rows:
                            tr = prototype_block
                            for tok in row_tokens_in_template:
                                val = _value_for_token(row, tok)
                                tr = sub_token(tr, tok, format_token_value(tok, val))
                            parts.append(tr)
                        block_html = "\n".join(parts)

            if filtered_rows:
                totals_row = (generator_results.get("totals") or [{}])[0]
                for token in TOTALS:
                    value = totals_row.get(token)
                    formatted = format_token_value(token, value)
                    block_html = sub_token(block_html, token, formatted)
                    last_totals_per_token[token] = formatted
                    target = total_token_to_target.get(token)
                    if target:
                        fv, _formatted = _coerce_total_value(value)
                        if fv is not None:
                            totals_accum[target] = totals_accum.get(target, 0.0) + fv

                rendered_blocks.append(block_html)
            else:
                print("Generator SQL produced no usable row data after filtering; skipping block.")
    else:
            for batch_id in (BATCH_IDS or []):
                block_html = prototype_block

                # (a) Header fill (parent row)
                if header_cols:
                    if len(pcols) == 1:
                        sql = (
                            f"SELECT {', '.join(qident(c) for c in header_cols)} "
                            f"FROM {qident(parent_table)} "
                            f"WHERE {qident(pcols[0])} = ? AND {parent_where_clause} "
                            f"LIMIT 1"
                        )
                        hdr_params = (batch_id,) + tuple(PDATE) + parent_filter_values_tuple
                    else:
                        where = " AND ".join([f"{qident(c)} = ?" for c in pcols])
                        sql = (
                            f"SELECT {', '.join(qident(c) for c in header_cols)} "
                            f"FROM {qident(parent_table)} "
                            f"WHERE {where} AND {parent_where_clause} "
                            f"LIMIT 1"
                        )
                        hdr_parts = _split_bid(batch_id, len(pcols))
                        hdr_params = tuple(hdr_parts) + tuple(PDATE) + parent_filter_values_tuple

                    con = sqlite3.connect(str(DB_PATH)); con.row_factory = sqlite3.Row
                    cur = con.cursor(); cur.execute(sql, hdr_params)
                    row = cur.fetchone(); con.close()
                    if row:
                        r = dict(row)
                        for t in HEADER_TOKENS:
                            if t in PLACEHOLDER_TO_COL:
                                col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                                if not col:
                                    continue
                                val = r.get(col, "")
                                block_html = sub_token(block_html, t, format_token_value(t, val))

                # (b) Row repeater (child rows)
                allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)

                # Try standard tbody-based path first
                tbody_m, tbody_inner = best_rows_tbody(block_html, allowed_row_tokens)
                if tbody_m and tbody_inner:
                    row_template, row_span, row_tokens_in_template = find_row_template(tbody_inner, allowed_row_tokens)
                    if row_template and row_tokens_in_template:
                        row_cols_needed = sorted(
                            {
                                col
                                for t in row_tokens_in_template
                                for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))]
                                if col
                            }
                        )

                        if order_col.upper() != "ROWID" and order_col not in row_cols_needed:
                            row_cols_needed.append(order_col)

                        order_clause = f"ORDER BY ROWID" if order_col.upper() == "ROWID" else f"ORDER BY {qident(order_col)}, ROWID"

                        if len(ccols) == 1:
                            sql = (
                                f"SELECT {', '.join(qident(c) for c in row_cols_needed)} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {qident(ccols[0])} = ? AND {child_where_clause} "
                                f"{order_clause}"
                            )
                            row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
                        else:
                            where = " AND ".join([f"{qident(c)} = ?" for c in ccols])
                            sql = (
                                f"SELECT {', '.join(qident(c) for c in row_cols_needed)} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {where} AND {child_where_clause} "
                                f"{order_clause}"
                            )
                            row_parts = _split_bid(batch_id, len(ccols))
                            row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple

                        con = sqlite3.connect(str(DB_PATH)); con.row_factory = sqlite3.Row
                        cur = con.cursor(); cur.execute(sql, row_params)
                        rows = [dict(r) for r in cur.fetchall()]
                        con.close()

                        # Fallback: date-only by majority table if needed
                        if not rows:
                            maj_table = majority_table_for_tokens(row_tokens_in_template, PLACEHOLDER_TO_COL)
                            if maj_table:
                                date_col = DATE_COLUMNS.get(maj_table, "")
                                if date_col:
                                    cols_needed = sorted(
                                        {
                                            col
                                            for t in row_tokens_in_template
                                            for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))]
                                            if col
                                        }
                                    )
                                    if date_col not in cols_needed:
                                        cols_needed.append(date_col)
                                    sql_fb = (
                                        f"SELECT {', '.join(qident(c) for c in cols_needed)} "
                                        f"FROM {qident(maj_table)} "
                                        f"WHERE datetime({qident(date_col)}) BETWEEN datetime(?) AND datetime(?) "
                                        f"ORDER BY {qident(date_col)} ASC, ROWID ASC"
                                    )
                                    con = sqlite3.connect(str(DB_PATH)); con.row_factory = sqlite3.Row
                                    cur = con.cursor(); cur.execute(sql_fb, (START_DATE, END_DATE))
                                    rows = [dict(r) for r in cur.fetchall()]
                                    con.close()
                                    print(f"Row fallback used: table={maj_table}, rows={len(rows)}")

                        if not rows:
                            print(f"No child rows found for batch {batch_id}; skipping block.")
                            continue

                        significant_cols = [
                            col
                            for col in row_cols_needed
                            if col and not any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
                        ]
                        filtered_rows = []
                        for r in rows:
                            if significant_cols and not _row_has_significant_data(r, significant_cols):
                                continue
                            filtered_rows.append(dict(r))

                        if not filtered_rows:
                            print(f"No significant child rows for batch {batch_id}; skipping block.")
                            continue

                        _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)

                        parts: list[str] = []
                        for r in filtered_rows:
                            tr = row_template
                            for t in row_tokens_in_template:
                                col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                                if not col:
                                    continue
                                tr = sub_token(tr, t, format_token_value(t, r.get(col)))
                            parts.append(tr)

                        new_tbody_inner = tbody_inner[:row_span[0]] + "\n".join(parts) + tbody_inner[row_span[1]:]
                        block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]

                else:
                    # Inferred single-<tr> block (no <tbody> path) ΓÇö duplicate the <tr> itself
                    tr_tokens = [m.group(1) or m.group(2)
                                 for m in re.finditer(r"\{\{\s*([^}\n]+?)\s*\}\}|\{\s*([^}\n]+?)\s*\}", block_html)]
                    tr_tokens = sorted({t.strip() for t in tr_tokens if t}, key=len, reverse=True)

                    row_tokens_in_template = [t for t in tr_tokens if t in allowed_row_tokens]
                    if row_tokens_in_template:
                        row_cols_needed = sorted(
                            {
                                col
                                for t in row_tokens_in_template
                                for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))]
                                if col
                            }
                        )
                        if order_col.upper() != "ROWID" and order_col not in row_cols_needed:
                            row_cols_needed.append(order_col)
                        order_clause = f"ORDER BY ROWID" if order_col.upper() == "ROWID" else f"ORDER BY {qident(order_col)}, ROWID"

                        if len(ccols) == 1:
                            sql = (
                                f"SELECT {', '.join(qident(c) for c in row_cols_needed)} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {qident(ccols[0])} = ? AND {child_where_clause} "
                                f"{order_clause}"
                            )
                            row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
                        else:
                            where = " AND ".join([f"{qident(c)} = ?" for c in ccols])
                            sql = (
                                f"SELECT {', '.join(qident(c) for c in row_cols_needed)} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {where} AND {child_where_clause} "
                                f"{order_clause}"
                            )
                            row_parts = _split_bid(batch_id, len(ccols))
                            row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple

                        con = sqlite3.connect(str(DB_PATH)); con.row_factory = sqlite3.Row
                        cur = con.cursor(); cur.execute(sql, row_params)
                        rows = [dict(r) for r in cur.fetchall()]
                        con.close()

                        if not rows:
                            print(f"No child rows found (no tbody path) for batch {batch_id}; skipping block.")
                            continue

                        significant_cols = [
                            col
                            for col in row_cols_needed
                            if col and not any(keyword in col.lower() for keyword in ("row", "serial", "sl"))
                        ]
                        filtered_rows = []
                        for r in rows:
                            if significant_cols and not _row_has_significant_data(r, significant_cols):
                                continue
                            filtered_rows.append(dict(r))

                        if not filtered_rows:
                            print(f"No significant child rows (no tbody path) for batch {batch_id}; skipping block.")
                            continue

                        _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)

                        parts = []
                        for r in filtered_rows:
                            tr = prototype_block  # the <tr> itself
                            for t in row_tokens_in_template:
                                col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                                if not col:
                                    continue
                                tr = sub_token(tr, t, format_token_value(t, r.get(col)))
                            parts.append(tr)

                        block_html = "\n".join(parts)

                # (c) Per-batch totals
                batch_total_values = {token: "0" for token in TOTALS}

                if child_totals_cols:
                    child_cols = sorted(child_totals_cols.keys())
                    if child_cols:
                        exprs = ", ".join([f"COALESCE(SUM({qident(c)}),0) AS {qident(c)}" for c in child_cols])

                        if len(ccols) == 1:
                            sql = (
                                f"SELECT {exprs} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {qident(ccols[0])} = ? AND {child_where_clause}"
                            )
                            tot_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
                        else:
                            where = " AND ".join([f"{qident(c)} = ?" for c in ccols])
                            sql = (
                                f"SELECT {exprs} "
                                f"FROM {qident(child_table)} "
                                f"WHERE {where} AND {child_where_clause}"
                            )
                            tot_parts = _split_bid(batch_id, len(ccols))
                            tot_params = tuple(tot_parts) + tuple(CDATE) + child_filter_values_tuple

                        con = sqlite3.connect(str(DB_PATH)); con.row_factory = sqlite3.Row
                        cur = con.cursor(); cur.execute(sql, tot_params)
                        sums = dict(cur.fetchone() or {}); con.close()

                        for col in child_cols:
                            raw_val = sums.get(col, 0)
                            fv, formatted = _coerce_total_value(raw_val)
                            if fv is not None:
                                key = (child_table, col)
                                totals_accum[key] = totals_accum.get(key, 0.0) + fv
                            for token in child_totals_cols[col]:
                                batch_total_values[token] = formatted

                for token, value in batch_total_values.items():
                    block_html = sub_token(block_html, token, value)
                    last_totals_per_token[token] = value

                rendered_blocks.append(block_html)

    # ---- Assemble full document ----
    rows_rendered = bool(rendered_blocks)
    if not rows_rendered:
        print("No rendered blocks generated for this selection.")

    html_multi = shell_prefix + "\n".join(rendered_blocks) + shell_suffix

    for tok, val in post_literal_specials.items():
        html_multi = sub_token(html_multi, tok, val if val is not None else '')

    if page_number_tokens or page_count_tokens or page_label_tokens:
        html_multi = _inject_page_counter_spans(html_multi, page_number_tokens, page_count_tokens, page_label_tokens)

    if total_token_to_target:
        overall_formatted = {}
        for (table_name, col_name), total in totals_accum.items():
            _, formatted = _coerce_total_value(total)
            overall_formatted[(table_name, col_name)] = formatted

        for token, target in total_token_to_target.items():
            table_name, col_name = target
            value = overall_formatted.get((table_name, col_name), last_totals_per_token.get(token, "0"))
            html_multi = sub_token(html_multi, token, value)

    # Apply literals globally
    for t, s in LITERALS.items():
        html_multi = sub_token(html_multi, t, s)

    # Blank any remaining known tokens
    ALL_KNOWN_TOKENS = set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())
    html_multi = blank_known_tokens(html_multi, ALL_KNOWN_TOKENS)

    # write to the path requested by the API
    OUT_HTML.write_text(html_multi, encoding="utf-8")
    print("Wrote HTML:", OUT_HTML)

    print("BATCH_IDS:", len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])

    asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent))
    print("Wrote PDF via Playwright:", OUT_PDF)

    return {"html_path": str(OUT_HTML), "pdf_path": str(OUT_PDF), "rows_rendered": rows_rendered}


# keep CLI usage (unchanged)
if __name__ == "__main__":
    print("Module ready for API integration. Call fill_and_print(...) from your FastAPI endpoint.")










