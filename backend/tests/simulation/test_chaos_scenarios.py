"""
Destructive Real-World Simulation Tests.

Chaos engineering-style tests that simulate production failure scenarios:
1. Process crashes mid-operation
2. State file corruption and recovery
3. Resource exhaustion scenarios
4. Cascading failures
5. Recovery after system restart
"""
import pytest
import json
import os
import time
import threading
import tempfile
from datetime import datetime, timedelta, timezone
from pathlib import Path
from unittest.mock import patch, MagicMock
import concurrent.futures

from backend.app.repositories.state.store import StateStore


@pytest.fixture
def state_store(tmp_path: Path) -> StateStore:
    """Create a temporary StateStore for testing."""
    return StateStore(base_dir=tmp_path)


# =============================================================================
# Process Crash Simulation
# =============================================================================

class TestProcessCrashScenarios:
    """Simulate process crashes at various points."""

    def test_crash_during_idempotency_store(self, tmp_path: Path):
        """
        Simulate crash during idempotency key storage.
        System should recover gracefully on restart.
        """
        store = StateStore(base_dir=tmp_path)

        # Store a valid key first
        store.store_idempotency_key("key-1", "job-1", "hash-1", {"status": "ok"})

        # Simulate partial write by corrupting state file mid-operation
        state_file = tmp_path / "state.json"

        # Write partial/corrupt JSON (simulate crash during write)
        with open(state_file, "w") as f:
            f.write('{"jobs": {}, "idempotency_keys": {"key-1": {"job_id": "job-1"')
            # Note: file is truncated mid-write

        # Create new store instance (simulating restart)
        try:
            new_store = StateStore(base_dir=tmp_path)
            # If it initializes, check if it recovered
            # It may have reset to default state or recovered partial data
            assert new_store is not None
        except json.JSONDecodeError:
            # Acceptable - corrupted file detected
            # In production, we'd have backup/recovery mechanisms
            pass

    def test_crash_during_dlq_move(self, state_store: StateStore):
        """
        Simulate crash while moving job to DLQ.
        The job should end up in a consistent state.
        """
        job = state_store.create_job(job_type="run_report", template_id="t1")
        state_store.record_job_completion(job["id"], status="failed")

        # Start moving to DLQ
        dlq_record = state_store.move_job_to_dlq(job["id"])

        # Verify state is consistent
        assert dlq_record is not None

        # Verify we can still retrieve the job
        retrieved = state_store.get_job(job["id"])
        assert retrieved is not None

        # Verify DLQ record exists
        dlq_job = state_store.get_dead_letter_job(job["id"])
        assert dlq_job is not None

    def test_multiple_simultaneous_failures(self, state_store: StateStore):
        """
        Multiple operations failing simultaneously should not corrupt state.
        """
        errors = []
        completed = []
        lock = threading.Lock()

        def failing_operation(i: int):
            try:
                job = state_store.create_job(job_type="run_report", template_id=f"t-{i}")
                state_store.record_job_start(job["id"])

                # Simulate random failures
                if i % 3 == 0:
                    raise Exception(f"Simulated failure for job {i}")

                state_store.record_job_completion(job["id"], status="succeeded")
                with lock:
                    completed.append(job["id"])
            except Exception as e:
                with lock:
                    errors.append((i, str(e)))

        # Run many operations with some failing
        threads = [threading.Thread(target=failing_operation, args=(i,)) for i in range(30)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # Should have some errors and some successes
        assert len(errors) > 0
        assert len(completed) > 0

        # State should still be consistent
        stats = state_store.get_dlq_stats()
        assert isinstance(stats, dict)


# =============================================================================
# State File Corruption and Recovery
# =============================================================================

class TestStateCorruptionRecovery:
    """Test recovery from various state corruption scenarios."""

    def test_empty_state_file_recovery(self, tmp_path: Path):
        """System should handle empty state file."""
        state_file = tmp_path / "state.json"
        state_file.write_text("")

        # Should initialize with default state
        store = StateStore(base_dir=tmp_path)
        assert store is not None

        # Should be able to create jobs
        job = store.create_job(job_type="run_report", template_id="t1")
        assert job is not None

    def test_invalid_json_recovery(self, tmp_path: Path):
        """System should handle invalid JSON in state file."""
        state_file = tmp_path / "state.json"
        state_file.write_text("not valid json {{{")

        # Should handle the error and potentially reset
        try:
            store = StateStore(base_dir=tmp_path)
            # If it succeeds, verify it's usable
            job = store.create_job(job_type="run_report", template_id="t1")
            assert job is not None
        except json.JSONDecodeError:
            # Also acceptable - depends on implementation
            pass

    def test_missing_required_sections_recovery(self, tmp_path: Path):
        """System should handle state file missing required sections."""
        state_file = tmp_path / "state.json"
        state_file.write_text('{"random": "data"}')

        store = StateStore(base_dir=tmp_path)

        # Should be able to create jobs (missing sections should be initialized)
        job = store.create_job(job_type="run_report", template_id="t1")
        assert job is not None
        assert job["status"] == "queued"

    def test_recovery_preserves_valid_data(self, tmp_path: Path):
        """Recovery should preserve any valid data that exists."""
        # Create valid state with some jobs
        store = StateStore(base_dir=tmp_path)
        job1 = store.create_job(job_type="run_report", template_id="t1")
        job2 = store.create_job(job_type="run_report", template_id="t2")

        # Read state, corrupt one section, write back
        state_file = tmp_path / "state.json"
        with open(state_file, "r") as f:
            state = json.load(f)

        # Corrupt idempotency_keys but leave jobs intact
        state["idempotency_keys"] = "corrupted"
        with open(state_file, "w") as f:
            json.dump(state, f)

        # Create new store
        new_store = StateStore(base_dir=tmp_path)

        # Jobs should still be accessible
        retrieved1 = new_store.get_job(job1["id"])
        retrieved2 = new_store.get_job(job2["id"])
        assert retrieved1 is not None
        assert retrieved2 is not None


# =============================================================================
# Resource Exhaustion Scenarios
# =============================================================================

class TestResourceExhaustionScenarios:
    """Test behavior under resource exhaustion."""

    def test_very_high_job_volume(self, state_store: StateStore):
        """System should handle very high job volumes."""
        job_ids = []

        # Create many jobs
        for i in range(500):
            job = state_store.create_job(job_type="run_report", template_id=f"t-{i}")
            job_ids.append(job["id"])

        # All jobs should be retrievable
        for job_id in job_ids[:10]:  # Sample check
            job = state_store.get_job(job_id)
            assert job is not None

        # Listing should work
        with state_store._lock:
            state = state_store._read_state()
            assert len(state.get("jobs", {})) >= 500

    def test_high_dlq_volume(self, state_store: StateStore):
        """System should handle very high DLQ volumes."""
        # Create many failed jobs and move to DLQ
        for i in range(200):
            job = state_store.create_job(job_type="run_report", template_id=f"dlq-{i}")
            state_store.record_job_completion(job["id"], status="failed")
            state_store.move_job_to_dlq(job["id"])

        # Stats should still work
        stats = state_store.get_dlq_stats()
        assert stats["total"] == 200

        # Listing with pagination should work
        page1 = state_store.list_dead_letter_jobs(limit=50)
        assert len(page1) == 50

    def test_high_idempotency_key_volume(self, state_store: StateStore):
        """System should handle many idempotency keys."""
        # Create many keys
        for i in range(1000):
            state_store.store_idempotency_key(
                f"key-{i}", f"job-{i}", f"hash-{i}", {"index": i}
            )

        # All should be retrievable
        for i in range(0, 1000, 100):  # Sample check
            exists, cached = state_store.check_idempotency_key(f"key-{i}", f"hash-{i}")
            assert exists is True
            assert cached["index"] == i


# =============================================================================
# Cascading Failure Scenarios
# =============================================================================

class TestCascadingFailures:
    """Test cascading failure scenarios."""

    def test_failed_job_cascade_to_dlq(self, state_store: StateStore):
        """Multiple job failures should cascade properly to DLQ."""
        job_ids = []

        # Create jobs
        for i in range(20):
            job = state_store.create_job(
                job_type="run_report",
                template_id=f"cascade-{i}",
                max_retries=3,
            )
            job_ids.append(job["id"])

        # Simulate cascade of failures
        for job_id in job_ids:
            state_store.record_job_start(job_id)
            state_store.record_job_completion(
                job_id,
                status="failed",
                error="Cascading failure - upstream dependency failed"
            )
            state_store.move_job_to_dlq(job_id)

        # All should be in DLQ
        stats = state_store.get_dlq_stats()
        assert stats["total"] == 20
        assert stats["pending"] == 20

    def test_recovery_from_cascade(self, state_store: StateStore):
        """System should allow recovery from cascading failures."""
        # Create cascade scenario
        job_ids = []
        for i in range(10):
            job = state_store.create_job(job_type="run_report", template_id=f"rec-{i}")
            state_store.record_job_completion(job["id"], status="failed")
            state_store.move_job_to_dlq(job["id"])
            job_ids.append(job["id"])

        # Requeue all - simulating "fix and retry all"
        new_jobs = []
        for job_id in job_ids:
            new_job = state_store.requeue_from_dlq(job_id)
            new_jobs.append(new_job)

        # All new jobs should be queued
        for new_job in new_jobs:
            assert new_job["status"] == "queued"

        # Original DLQ entries should show requeued
        stats = state_store.get_dlq_stats()
        assert stats["requeued"] == 10


# =============================================================================
# System Restart Scenarios
# =============================================================================

class TestSystemRestartScenarios:
    """Test behavior after system restart."""

    def test_running_jobs_become_stale_after_restart(self, tmp_path: Path):
        """Jobs that were running should be detected as stale after restart."""
        store = StateStore(base_dir=tmp_path)

        # Create running jobs
        running_jobs = []
        for i in range(5):
            job = store.create_job(job_type="run_report", template_id=f"run-{i}")
            store.record_job_start(job["id"])
            running_jobs.append(job["id"])

        # Simulate passage of time by backdating heartbeats
        with store._lock:
            state = store._read_state()
            old_time = (datetime.now(timezone.utc) - timedelta(minutes=30)).isoformat()
            for job_id in running_jobs:
                if job_id in state.get("jobs", {}):
                    state["jobs"][job_id]["last_heartbeat_at"] = old_time
                    state["jobs"][job_id]["started_at"] = old_time
            store._write_state(state)

        # Create new store (simulating restart)
        new_store = StateStore(base_dir=tmp_path)

        # Check for stale jobs (30 minutes = 1800 seconds, timeout is 15 minutes = 900 seconds)
        stale = new_store.find_stale_running_jobs(heartbeat_timeout_seconds=900)
        assert len(stale) == 5

    def test_state_persistence_across_restarts(self, tmp_path: Path):
        """State should persist across store restarts."""
        # Create initial state
        store1 = StateStore(base_dir=tmp_path)
        job = store1.create_job(job_type="run_report", template_id="persist-1")
        store1.store_idempotency_key("persist-key", job["id"], "hash", {"test": True})

        job_id = job["id"]

        # Create new store instance
        store2 = StateStore(base_dir=tmp_path)

        # Verify state persisted
        retrieved_job = store2.get_job(job_id)
        assert retrieved_job is not None

        exists, cached = store2.check_idempotency_key("persist-key", "hash")
        assert exists is True
        assert cached["test"] is True

    def test_pending_retry_jobs_after_restart(self, tmp_path: Path):
        """Jobs pending retry should be picked up after restart."""
        store = StateStore(base_dir=tmp_path)

        # Create jobs pending retry
        pending_retry_ids = []
        past = (datetime.now(timezone.utc) - timedelta(minutes=10)).isoformat()

        for i in range(5):
            job = store.create_job(
                job_type="run_report",
                template_id=f"retry-{i}",
                max_retries=3,
            )
            store.record_job_start(job["id"])

            # Manually set pending_retry status with retry_at in the past
            with store._lock:
                state = store._read_state()
                state["jobs"][job["id"]]["status"] = "pending_retry"
                state["jobs"][job["id"]]["retry_at"] = past
                store._write_state(state)

            pending_retry_ids.append(job["id"])

        # Create new store instance (simulating restart)
        new_store = StateStore(base_dir=tmp_path)

        # Find jobs ready for retry
        ready = new_store.find_jobs_ready_for_retry()
        assert len(ready) == 5


# =============================================================================
# Data Integrity Under Stress
# =============================================================================

class TestDataIntegrityUnderStress:
    """Test data integrity under stressful conditions."""

    def test_integrity_during_concurrent_writes(self, state_store: StateStore):
        """Data integrity should be maintained during concurrent writes."""
        results = {"jobs": [], "keys": [], "dlq": []}
        errors = []
        lock = threading.Lock()

        def create_jobs(n: int, prefix: str):
            for i in range(n):
                try:
                    job = state_store.create_job(
                        job_type="run_report",
                        template_id=f"{prefix}-{i}"
                    )
                    with lock:
                        results["jobs"].append(job["id"])
                except Exception as e:
                    with lock:
                        errors.append(("job", e))

        def create_keys(n: int, prefix: str):
            for i in range(n):
                try:
                    state_store.store_idempotency_key(
                        f"{prefix}-key-{i}",
                        f"job-{i}",
                        f"hash-{i}",
                        {"index": i}
                    )
                    with lock:
                        results["keys"].append(f"{prefix}-key-{i}")
                except Exception as e:
                    with lock:
                        errors.append(("key", e))

        def move_to_dlq(job_ids: list):
            for job_id in job_ids:
                try:
                    state_store.record_job_completion(job_id, status="failed")
                    state_store.move_job_to_dlq(job_id)
                    with lock:
                        results["dlq"].append(job_id)
                except Exception as e:
                    with lock:
                        errors.append(("dlq", e))

        # Create base jobs first
        base_jobs = []
        for i in range(20):
            job = state_store.create_job(job_type="run_report", template_id=f"base-{i}")
            base_jobs.append(job["id"])

        # Run concurrent operations
        threads = [
            threading.Thread(target=create_jobs, args=(30, "thread1")),
            threading.Thread(target=create_jobs, args=(30, "thread2")),
            threading.Thread(target=create_keys, args=(50, "thread1")),
            threading.Thread(target=create_keys, args=(50, "thread2")),
            threading.Thread(target=move_to_dlq, args=(base_jobs[:10],)),
            threading.Thread(target=move_to_dlq, args=(base_jobs[10:],)),
        ]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # Verify no errors
        assert len(errors) == 0, f"Errors occurred: {errors}"

        # Verify counts
        assert len(results["jobs"]) == 60
        assert len(results["keys"]) == 100

        # Verify all data is accessible
        for job_id in results["jobs"][:10]:
            job = state_store.get_job(job_id)
            assert job is not None

        for key in results["keys"][:10]:
            exists, _ = state_store.check_idempotency_key(key, key.replace("-key-", "-hash-").replace("thread1", "hash").replace("thread2", "hash"))
            # Note: hash mismatch expected, but key should exist
            assert exists is True or exists is False  # Just checking it doesn't crash

    def test_no_data_loss_under_high_load(self, state_store: StateStore):
        """No data should be lost under high concurrent load."""
        job_count = 100
        created_ids = set()
        lock = threading.Lock()

        def create_and_track(start: int, count: int):
            for i in range(start, start + count):
                job = state_store.create_job(
                    job_type="run_report",
                    template_id=f"nolose-{i}"
                )
                with lock:
                    created_ids.add(job["id"])

        # Create jobs from multiple threads
        threads = [
            threading.Thread(target=create_and_track, args=(0, 25)),
            threading.Thread(target=create_and_track, args=(25, 25)),
            threading.Thread(target=create_and_track, args=(50, 25)),
            threading.Thread(target=create_and_track, args=(75, 25)),
        ]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        assert len(created_ids) == job_count

        # Verify all jobs exist
        missing = []
        for job_id in created_ids:
            job = state_store.get_job(job_id)
            if job is None:
                missing.append(job_id)

        assert len(missing) == 0, f"Missing jobs: {missing}"
