# Backend Line-by-Line Analysis

## backend\__init__.py
- L1 docstring: "Connections service placeholder."

## backend\adapters\__init__.py
- L1 docstring: "Adapters layer - IO operations isolated behind interfaces."
- L3 from .persistence import Repository, UnitOfWork
- L4 from .databases import DataSource, SQLiteDataSource
- L5 from .rendering import Renderer, RenderResult
- L6 from .llm import LLMClient, LLMResponse, LLMMessage, OpenAIClient
- L7 from .extraction import Extractor, ExtractionResult, PDFExtractor, ExcelExtractor
- L9 assign __all__ = ['Repository', 'UnitOfWork', 'DataSource', 'SQLiteDataSource', 'Renderer', 'RenderResult', 'LLMClient', 'LLMResponse', 'LLMMessage', 'OpenAIClient', 'Extractor', 'ExtractionResult', 'PDFExtractor', 'ExcelExtractor']

## backend\adapters\databases\__init__.py
- L1 docstring: "Database adapters for querying data sources."
- L3 from .base import DataSource, QueryResult, SchemaDiscovery
- L4 from .sqlite import SQLiteDataSource
- L6 assign __all__ = ['DataSource', 'QueryResult', 'SchemaDiscovery', 'SQLiteDataSource']

## backend\adapters\databases\base.py
- L1 docstring: "Base interfaces for database adapters."
- L3 from __future__ import annotations
- L5 from abc import ABC, abstractmethod
- L6 from dataclasses import dataclass, field
- L7 from pathlib import Path
- L8 from typing import Any, Dict, Iterator, List, Optional, Protocol, Sequence
- L10 import pandas as pd
- L12 from backend.domain.connections import SchemaInfo, TableInfo, ConnectionTest
- L16 class QueryResult:
  - L17 docstring: "Result of a database query."
  - L19 annotated assign columns: List[str]
  - L20 annotated assign rows: List[List[Any]]
  - L21 annotated assign row_count: int
  - L22 annotated assign query: str
  - L23 annotated assign execution_time_ms: float
  - L25 def to_dataframeself:
    - L26 docstring: "Convert to pandas DataFrame."
    - L27 return pd.DataFrame(self.rows, columns=self.columns)
  - L29 def to_dictsself:
    - L30 docstring: "Convert to list of dictionaries."
    - L31 return [dict(zip(self.columns, row)) for row in self.rows]
- L34 class DataSource(Protocol):
  - L35 docstring: "Interface for data source access.\n\n    DataSources provide read-only access to..."
  - L42 def pathself:
    - L43 docstring: "Get the path to the data source."
    - L44 expr ...
  - L46 def test_connectionself:
    - L47 docstring: "Test the connection and return health info."
    - L48 expr ...
  - L50 def discover_schemaself:
    - L51 docstring: "Discover the database schema."
    - L52 expr ...
  - L54 def execute_queryself, query: str, parameters: Optional[Sequence[Any]]=None:
    - L59 docstring: "Execute a query and return results."
    - L60 expr ...
  - L62 def stream_queryself, query: str, parameters: Optional[Sequence[Any]]=None, batch_size: int=1000:
    - L68 docstring: "Stream query results in batches."
    - L69 expr ...
  - L71 def get_table_columnsself, table_name: str:
    - L72 docstring: "Get column names for a table."
    - L73 expr ...
  - L75 def get_row_countself, table_name: str:
    - L76 docstring: "Get row count for a table."
    - L77 expr ...
- L80 class SchemaDiscovery(ABC):
  - L81 docstring: "Abstract base for schema discovery implementations."
  - L84 def discover_tablesself:
    - L85 docstring: "Discover all tables."
    - L86 pass
  - L89 def discover_columnsself, table_name: str:
    - L90 docstring: "Discover columns for a table."
    - L91 pass
  - L94 def build_catalogself, tables: List[TableInfo]:
    - L95 docstring: "Build table.column catalog."
    - L96 pass
  - L98 def discoverself:
    - L99 docstring: "Full schema discovery."
    - L100 assign tables = self.discover_tables()
    - L101 assign catalog = self.build_catalog(tables)
    - L102 return SchemaInfo(tables=tables, catalog=catalog)

## backend\adapters\databases\sqlite.py
- L1 docstring: "SQLite database adapter implementation using DataFrames.\n\nAll database access ..."
- L7 from __future__ import annotations
- L9 import logging
- L10 import threading
- L11 import time
- L12 from contextlib import contextmanager
- L13 from datetime import datetime, timezone
- L14 from pathlib import Path
- L15 from typing import Any, Dict, Iterator, List, Optional, Sequence
- L17 from backend.app.services.dataframes import SQLiteDataFrameLoader, DuckDBDataFrameQuery, sqlite_shim, get_dataframe_store, ensure_connection_loaded
- L24 from backend.domain.connections import ConnectionTest, SchemaInfo, TableInfo
- L25 from .base import DataSource, QueryResult, SchemaDiscovery
- L27 assign logger = logging.getLogger('neura.adapters.sqlite')
- L30 class DataFrameConnectionPool:
  - L31 docstring: "DataFrame-based connection pool using sqlite_shim.\n\n    Instead of managing ra..."
  - L37 def __init__self, path: Path, readonly: bool=True, pool_size: int=5, max_overflow: int=10:
    - L44 assign self._path = path.resolve()
    - L45 assign self._readonly = readonly
    - L46 assign self._pool_size = pool_size
    - L47 assign self._max_overflow = max_overflow
    - L48 assign self._lock = threading.Lock()
    - L49 assign self._closed = False
    - L50 assign self._active_count = 0
    - L53 from backend.app.services.dataframes.sqlite_loader import get_loader
    - L54 assign self._loader = get_loader(self._path)
    - L55 expr self._loader.frames()
    - L56 expr logger.info(f'Loaded {len(self._loader.table_names())} tables into DataFrames for {self._path}')
  - L58 def _create_connectionself:
    - L59 docstring: "Create a new DataFrame connection."
    - L60 return sqlite_shim.connect(str(self._path))
  - L63 def acquireself:
    - L64 docstring: "Acquire a connection from the pool."
    - L65 if self._closed:
      - L66 raise RuntimeError('Connection pool is closed')
    - L68 with self._lock:
      - L69 aug assign self._active_count Add 1
    - L71 assign conn = self._create_connection()
    - L72 try:
      - L73 expr (yield conn)
      - L74 except Exception:
        - L75 raise
      - L77 finally:
        - L77 try:
          - L78 expr conn.close()
          - L79 except Exception:
            - L80 pass
        - L81 with self._lock:
          - L82 assign self._active_count = max(0, self._active_count - 1)
  - L84 def closeself:
    - L85 docstring: "Close the pool."
    - L86 assign self._closed = True
  - L88 def statusself:
    - L89 docstring: "Get pool status."
    - L90 return {'pool_size': self._pool_size, 'active_connections': self._active_count, 'tables_loaded': len(self._loader.table_names()), 'closed': self._closed}
- L99 assign SQLiteConnectionPool = DataFrameConnectionPool
- L102 class DataFrameSchemaDiscovery(SchemaDiscovery):
  - L103 docstring: "Schema discovery using DataFrames instead of direct SQLite queries."
  - L105 def __init__self, loader: SQLiteDataFrameLoader:
    - L106 assign self._loader = loader
    - L107 annotated assign self._table_info_cache: Dict[str, List[dict]] = {}
  - L109 def discover_tablesself:
    - L110 docstring: "Discover all tables from DataFrames."
    - L111 assign table_names = self._loader.table_names()
    - L113 if not table_names:
      - L114 return []
    - L117 expr self._prefetch_table_info(table_names)
    - L120 assign row_counts = self._batch_get_row_counts(table_names)
    - L122 assign tables = []
    - L123 for table_name in table_names:
      - L124 assign (columns, pk) = self._get_cached_table_info(table_name)
      - L125 expr tables.append(TableInfo(name=table_name, columns=columns, row_count=row_counts.get(table_name, 0), primary_key=pk))
    - L133 return tables
  - L135 def _prefetch_table_infoself, table_names: List[str]:
    - L136 docstring: "Prefetch table info using DataFrame loader."
    - L137 for table_name in table_names:
      - L138 try:
        - L139 assign info = self._loader.pragma_table_info(table_name)
        - L140 assign self._table_info_cache[table_name] = info
        - L141 except Exception as e:
          - L142 expr logger.debug(f'Failed to get table info for {table_name}: {e}')
          - L143 assign self._table_info_cache[table_name] = []
  - L145 def _get_cached_table_infoself, table_name: str:
    - L146 docstring: "Get columns and primary key from cache."
    - L147 assign info = self._table_info_cache.get(table_name, [])
    - L148 assign columns = [row.get('name', '') for row in info]
    - L149 assign pk = None
    - L150 for row in info:
      - L151 if row.get('pk'):
        - L152 assign pk = row.get('name')
        - L153 break
    - L154 return (columns, pk)
  - L156 def _batch_get_row_countsself, table_names: List[str]:
    - L157 docstring: "Get row counts from DataFrames."
    - L158 assign counts = {}
    - L159 for table_name in table_names:
      - L160 try:
        - L161 assign frame = self._loader.frame(table_name)
        - L162 assign counts[table_name] = len(frame)
        - L163 except Exception as e:
          - L164 expr logger.debug(f'Failed to get row count for {table_name}: {e}')
          - L165 assign counts[table_name] = 0
    - L166 return counts
  - L168 def discover_columnsself, table_name: str:
    - L169 docstring: "Discover columns from DataFrame."
    - L170 if table_name in self._table_info_cache:
      - L171 return [row.get('name', '') for row in self._table_info_cache[table_name]]
    - L172 try:
      - L173 assign frame = self._loader.frame(table_name)
      - L174 return list(frame.columns)
      - L175 except Exception:
        - L176 return []
  - L178 def build_catalogself, tables: List[TableInfo]:
    - L179 docstring: "Build table.column catalog."
    - L180 assign catalog = []
    - L181 for table in tables:
      - L182 for column in table.columns:
        - L183 expr catalog.append(f'{table.name}.{column}')
    - L184 return catalog
  - L186 def _get_row_countself, table_name: str:
    - L187 docstring: "Get row count from DataFrame."
    - L188 try:
      - L189 assign frame = self._loader.frame(table_name)
      - L190 return len(frame)
      - L191 except Exception as e:
        - L192 expr logger.debug(f'Failed to get row count for {table_name}: {e}')
        - L193 return 0
  - L195 def _get_primary_keyself, table_name: str:
    - L196 docstring: "Get primary key column from table info."
    - L197 try:
      - L198 assign info = self._loader.pragma_table_info(table_name)
      - L199 for row in info:
        - L200 if row.get('pk'):
          - L201 return row.get('name')
      - L202 return None
      - L203 except Exception as e:
        - L204 expr logger.debug(f'Failed to get primary key for {table_name}: {e}')
        - L205 return None
- L209 assign SQLiteSchemaDiscovery = DataFrameSchemaDiscovery
- L212 class DataFrameDataSource:
  - L213 docstring: "DataFrame-based implementation of DataSource interface.\n\n    All queries execu..."
  - L219 def __init__self, path: Path, *, readonly: bool=True, use_pool: bool=False, pool_size: int=5:
    - L227 assign self._path = path.resolve()
    - L228 assign self._readonly = readonly
    - L229 assign self._use_pool = use_pool
    - L230 annotated assign self._pool: Optional[DataFrameConnectionPool] = None
    - L233 from backend.app.services.dataframes.sqlite_loader import get_loader
    - L234 assign self._loader = get_loader(self._path)
    - L235 expr self._loader.frames()
    - L237 if use_pool:
      - L238 assign self._pool = DataFrameConnectionPool(path=self._path, readonly=readonly, pool_size=pool_size)
    - L244 expr logger.info(f'DataFrameDataSource initialized with {len(self._loader.table_names())} tables')
  - L247 def pathself:
    - L248 return self._path
  - L251 def _get_connectionself:
    - L252 docstring: "Get a DataFrame connection."
    - L253 if self._pool is not None:
      - L254 with self._pool.acquire() as conn:
        - L255 expr (yield conn)
      - L256 return None
    - L259 assign conn = sqlite_shim.connect(str(self._path))
    - L260 try:
      - L261 expr (yield conn)
      - L263 finally:
        - L263 expr conn.close()
  - L265 def test_connectionself:
    - L266 docstring: "Test the connection and return health info."
    - L267 assign start = time.perf_counter()
    - L268 try:
      - L270 assign table_names = self._loader.table_names()
      - L271 assign table_count = len(table_names)
      - L274 if table_names:
        - L275 assign _ = self._loader.frame(table_names[0])
      - L277 assign latency = (time.perf_counter() - start) * 1000
      - L278 return ConnectionTest(success=True, latency_ms=latency, tested_at=datetime.now(timezone.utc), table_count=table_count)
      - L284 except Exception as e:
        - L285 assign latency = (time.perf_counter() - start) * 1000
        - L286 return ConnectionTest(success=False, latency_ms=latency, error=str(e), tested_at=datetime.now(timezone.utc))
  - L293 def discover_schemaself:
    - L294 docstring: "Discover the database schema from DataFrames."
    - L295 assign discovery = DataFrameSchemaDiscovery(self._loader)
    - L296 return discovery.discover()
  - L298 def execute_queryself, query: str, parameters: Optional[Sequence[Any]]=None:
    - L303 docstring: "Execute a query against DataFrames and return results."
    - L304 assign start = time.perf_counter()
    - L305 with self._get_connection() as conn:
      - L306 assign conn.row_factory = sqlite_shim.Row
      - L307 assign cursor = conn.execute(query, parameters or ())
      - L308 assign rows_raw = cursor.fetchall()
      - L309 assign columns = list(rows_raw[0].keys()) if rows_raw else []
      - L310 assign rows = [list(row) for row in rows_raw]
      - L311 assign execution_time = (time.perf_counter() - start) * 1000
    - L313 return QueryResult(columns=columns, rows=rows, row_count=len(rows), query=query, execution_time_ms=execution_time)
  - L321 def stream_queryself, query: str, parameters: Optional[Sequence[Any]]=None, batch_size: int=1000:
    - L327 docstring: "Stream query results in batches from DataFrames."
    - L328 assign start = time.perf_counter()
    - L331 with self._get_connection() as conn:
      - L332 assign conn.row_factory = sqlite_shim.Row
      - L333 assign cursor = conn.execute(query, parameters or ())
      - L334 assign rows_raw = cursor.fetchall()
      - L335 assign columns = list(rows_raw[0].keys()) if rows_raw else []
      - L338 for i in range(0, len(rows_raw), batch_size):
        - L339 assign batch = rows_raw[i:i + batch_size]
        - L340 assign execution_time = (time.perf_counter() - start) * 1000
        - L341 expr (yield QueryResult(columns=columns, rows=[list(row) for row in batch], row_count=len(batch), query=query, execution_time_ms=execution_time))
  - L349 def get_table_columnsself, table_name: str:
    - L350 docstring: "Get column names from DataFrame."
    - L351 try:
      - L352 assign frame = self._loader.frame(table_name)
      - L353 return list(frame.columns)
      - L354 except Exception:
        - L355 return []
  - L357 def get_row_countself, table_name: str:
    - L358 docstring: "Get row count from DataFrame."
    - L359 try:
      - L360 assign frame = self._loader.frame(table_name)
      - L361 return len(frame)
      - L362 except Exception:
        - L363 return 0
  - L365 def closeself:
    - L366 docstring: "Close the pool."
    - L367 if self._pool:
      - L368 expr self._pool.close()
      - L369 assign self._pool = None
  - L371 def pool_statusself:
    - L372 docstring: "Get connection pool status if pooling is enabled."
    - L373 if self._pool:
      - L374 return self._pool.status()
    - L375 return {'tables_loaded': len(self._loader.table_names()), 'pooling_enabled': False}
  - L380 def __del__self:
    - L381 expr self.close()
- L385 assign SQLiteDataSource = DataFrameDataSource

## backend\adapters\extraction\__init__.py
- L1 docstring: "Document extraction adapters."
- L3 from .base import Extractor, ExtractionResult, ExtractedTable
- L4 from .pdf import PDFExtractor
- L5 from .excel import ExcelExtractor
- L7 assign __all__ = ['Extractor', 'ExtractionResult', 'ExtractedTable', 'PDFExtractor', 'ExcelExtractor']

## backend\adapters\extraction\base.py
- L1 docstring: "Base interfaces for document extraction."
- L3 from __future__ import annotations
- L5 from abc import ABC, abstractmethod
- L6 from dataclasses import dataclass, field
- L7 from pathlib import Path
- L8 from typing import Any, Dict, List, Optional, Protocol
- L12 class ExtractedTable:
  - L13 docstring: "A table extracted from a document."
  - L15 annotated assign page_number: int
  - L16 annotated assign table_index: int
  - L17 annotated assign headers: List[str]
  - L18 annotated assign rows: List[List[Any]]
  - L19 annotated assign confidence: float = 0.0
  - L20 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L23 def row_countself:
    - L24 return len(self.rows)
  - L27 def column_countself:
    - L28 return len(self.headers)
  - L30 def to_dictself:
    - L31 return {'page_number': self.page_number, 'table_index': self.table_index, 'headers': self.headers, 'rows': self.rows, 'confidence': self.confidence, 'row_count': self.row_count, 'column_count': self.column_count}
- L43 class ExtractedText:
  - L44 docstring: "Text extracted from a document."
  - L46 annotated assign page_number: int
  - L47 annotated assign content: str
  - L48 annotated assign bbox: Optional[tuple] = None
  - L49 annotated assign font_info: Optional[Dict[str, Any]] = None
- L53 class ExtractionResult:
  - L54 docstring: "Result of document extraction."
  - L56 annotated assign source_path: Path
  - L57 annotated assign page_count: int
  - L58 annotated assign tables: List[ExtractedTable] = field(default_factory=list)
  - L59 annotated assign text_blocks: List[ExtractedText] = field(default_factory=list)
  - L60 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L61 annotated assign errors: List[str] = field(default_factory=list)
  - L62 annotated assign extraction_time_ms: float = 0.0
  - L65 def successself:
    - L66 return len(self.errors) == 0
  - L69 def table_countself:
    - L70 return len(self.tables)
- L73 class Extractor(Protocol):
  - L74 docstring: "Interface for document extractors.\n\n    Extractors pull structured data from d..."
  - L79 def extractself, path: Path:
    - L80 docstring: "Extract data from a document."
    - L81 expr ...
  - L83 def extract_tablesself, path: Path:
    - L84 docstring: "Extract only tables from a document."
    - L85 expr ...
  - L87 def supportsself, path: Path:
    - L88 docstring: "Check if this extractor supports the file type."
    - L89 expr ...
- L92 class BaseExtractor(ABC):
  - L93 docstring: "Abstract base for extractors with common functionality."
  - L96 def extractself, path: Path:
    - L97 docstring: "Extract data from a document."
    - L98 pass
  - L101 def extract_tablesself, path: Path:
    - L102 docstring: "Extract only tables from a document."
    - L103 pass
  - L106 def supportsself, path: Path:
    - L107 docstring: "Check if this extractor supports the file type."
    - L108 pass
  - L110 def _validate_pathself, path: Path:
    - L111 docstring: "Validate that the path exists and is a file."
    - L112 if not path.exists():
      - L113 raise FileNotFoundError(f'File not found: {path}')
    - L114 if not path.is_file():
      - L115 raise ValueError(f'Path is not a file: {path}')

## backend\adapters\extraction\excel.py
- L1 docstring: "Excel extraction adapter."
- L3 from __future__ import annotations
- L5 import logging
- L6 import time
- L7 from pathlib import Path
- L8 from typing import Any, Dict, List
- L10 from .base import BaseExtractor, ExtractionResult, ExtractedTable
- L12 assign logger = logging.getLogger('neura.adapters.extraction.excel')
- L15 class ExcelExtractor(BaseExtractor):
  - L16 docstring: "Extract data from Excel documents."
  - L18 assign SUPPORTED_EXTENSIONS = {'.xlsx', '.xls', '.xlsm', '.xlsb'}
  - L20 def supportsself, path: Path:
    - L21 docstring: "Check if this extractor supports the file."
    - L22 return path.suffix.lower() in self.SUPPORTED_EXTENSIONS
  - L24 def extractself, path: Path:
    - L25 docstring: "Extract all data from an Excel file."
    - L26 expr self._validate_path(path)
    - L27 assign start = time.perf_counter()
    - L28 annotated assign errors: List[str] = []
    - L29 annotated assign tables: List[ExtractedTable] = []
    - L30 annotated assign metadata: Dict[str, Any] = {}
    - L32 try:
      - L33 assign tables = self.extract_tables(path)
      - L34 assign metadata = self._get_workbook_metadata(path)
      - L35 except Exception as e:
        - L36 expr errors.append(f'Excel extraction failed: {e}')
        - L37 expr logger.exception('excel_extraction_failed')
    - L39 assign extraction_time = (time.perf_counter() - start) * 1000
    - L41 return ExtractionResult(source_path=path, page_count=len(tables), tables=tables, text_blocks=[], metadata=metadata, errors=errors, extraction_time_ms=extraction_time)
  - L51 def extract_tablesself, path: Path:
    - L52 docstring: "Extract tables from each sheet in Excel file."
    - L53 expr self._validate_path(path)
    - L55 try:
      - L56 import openpyxl
      - L57 except ImportError:
        - L58 raise ImportError('openpyxl is required. Install with: pip install openpyxl')
    - L62 annotated assign tables: List[ExtractedTable] = []
    - L63 assign wb = openpyxl.load_workbook(str(path), read_only=True, data_only=True)
    - L65 for (sheet_idx, sheet_name) in enumerate(wb.sheetnames):
      - L66 assign sheet = wb[sheet_name]
      - L67 assign table = self._extract_sheet_table(sheet, sheet_idx, sheet_name)
      - L68 if table:
        - L69 expr tables.append(table)
    - L71 expr wb.close()
    - L72 return tables
  - L74 def _extract_sheet_tableself, sheet, sheet_idx: int, sheet_name: str:
    - L80 docstring: "Extract table data from a single sheet."
    - L81 annotated assign rows_data: List[List[Any]] = []
    - L83 for row in sheet.iter_rows(values_only=True):
      - L85 assign row_values = [self._cell_to_string(cell) for cell in row]
      - L87 if any((v for v in row_values)):
        - L88 expr rows_data.append(row_values)
    - L90 if not rows_data:
      - L91 return None
    - L94 assign headers = rows_data[0]
    - L95 assign rows = rows_data[1:] if len(rows_data) > 1 else []
    - L97 return ExtractedTable(page_number=sheet_idx + 1, table_index=0, headers=headers, rows=rows, confidence=0.9, metadata={'sheet_name': sheet_name})
  - L106 def _cell_to_stringself, cell: Any:
    - L107 docstring: "Convert cell value to string."
    - L108 if cell is None:
      - L109 return ''
    - L110 if isinstance(cell, (int, float)):
      - L112 if isinstance(cell, float) and cell.is_integer():
        - L113 return str(int(cell))
      - L114 return str(cell)
    - L115 return str(cell)
  - L117 def _get_workbook_metadataself, path: Path:
    - L118 docstring: "Get workbook metadata."
    - L119 try:
      - L120 import openpyxl
      - L121 assign wb = openpyxl.load_workbook(str(path), read_only=True)
      - L122 assign metadata = {'sheet_names': wb.sheetnames, 'sheet_count': len(wb.sheetnames)}
      - L126 if wb.properties:
        - L127 expr metadata.update({'title': wb.properties.title, 'creator': wb.properties.creator, 'created': str(wb.properties.created) if wb.properties.created else None, 'modified': str(wb.properties.modified) if wb.properties.modified else None})
      - L133 expr wb.close()
      - L134 return metadata
      - L135 except Exception:
        - L136 return {}

## backend\adapters\extraction\pdf.py
- L1 docstring: "PDF extraction adapter."
- L3 from __future__ import annotations
- L5 import logging
- L6 import time
- L7 from pathlib import Path
- L8 from typing import Any, Dict, List, Optional
- L10 from .base import BaseExtractor, ExtractionResult, ExtractedTable, ExtractedText
- L12 assign logger = logging.getLogger('neura.adapters.extraction.pdf')
- L15 class PDFExtractor(BaseExtractor):
  - L16 docstring: "Extract data from PDF documents.\n\n    Uses multiple extraction backends for be..."
  - L24 def __init__self, *, prefer_backend: str='pdfplumber', extract_text: bool=True:
    - L30 assign self._prefer_backend = prefer_backend
    - L31 assign self._extract_text = extract_text
  - L33 def supportsself, path: Path:
    - L34 docstring: "Check if this extractor supports the file."
    - L35 return path.suffix.lower() == '.pdf'
  - L37 def extractself, path: Path:
    - L38 docstring: "Extract all data from a PDF."
    - L39 expr self._validate_path(path)
    - L40 assign start = time.perf_counter()
    - L41 annotated assign errors: List[str] = []
    - L42 annotated assign tables: List[ExtractedTable] = []
    - L43 annotated assign text_blocks: List[ExtractedText] = []
    - L44 assign page_count = 0
    - L45 annotated assign metadata: Dict[str, Any] = {}
    - L48 try:
      - L49 assign (page_count, metadata) = self._get_pdf_info(path)
      - L50 except Exception as e:
        - L51 expr errors.append(f'Failed to read PDF info: {e}')
    - L54 try:
      - L55 assign tables = self.extract_tables(path)
      - L56 except Exception as e:
        - L57 expr errors.append(f'Table extraction failed: {e}')
    - L60 if self._extract_text:
      - L61 try:
        - L62 assign text_blocks = self._extract_text_blocks(path)
        - L63 except Exception as e:
          - L64 expr errors.append(f'Text extraction failed: {e}')
    - L66 assign extraction_time = (time.perf_counter() - start) * 1000
    - L68 return ExtractionResult(source_path=path, page_count=page_count, tables=tables, text_blocks=text_blocks, metadata=metadata, errors=errors, extraction_time_ms=extraction_time)
  - L78 def extract_tablesself, path: Path:
    - L79 docstring: "Extract tables from a PDF."
    - L80 expr self._validate_path(path)
    - L82 if self._prefer_backend == 'pdfplumber':
      - L83 return self._extract_with_pdfplumber(path)
      - L84 else:
        - L84 if self._prefer_backend == 'tabula':
          - L85 return self._extract_with_tabula(path)
          - L88 else:
            - L88 try:
              - L89 return self._extract_with_pdfplumber(path)
              - L90 except Exception:
                - L91 return self._extract_with_tabula(path)
  - L93 def _get_pdf_infoself, path: Path:
    - L94 docstring: "Get PDF page count and metadata."
    - L95 try:
      - L96 import fitz
      - L97 assign doc = fitz.open(str(path))
      - L98 assign page_count = len(doc)
      - L99 assign metadata = dict(doc.metadata) if doc.metadata else {}
      - L100 expr doc.close()
      - L101 return (page_count, metadata)
      - L102 except ImportError:
        - L103 pass
    - L105 try:
      - L106 import pdfplumber
      - L107 with pdfplumber.open(path) as pdf:
        - L108 return (len(pdf.pages), {})
      - L109 except ImportError:
        - L110 pass
    - L112 return (0, {})
  - L114 def _extract_with_pdfplumberself, path: Path:
    - L115 docstring: "Extract tables using pdfplumber."
    - L116 try:
      - L117 import pdfplumber
      - L118 except ImportError:
        - L119 raise ImportError('pdfplumber is required. Install with: pip install pdfplumber')
    - L123 annotated assign tables: List[ExtractedTable] = []
    - L125 with pdfplumber.open(path) as pdf:
      - L126 for (page_idx, page) in enumerate(pdf.pages):
        - L127 assign page_tables = page.extract_tables()
        - L128 for (table_idx, table) in enumerate(page_tables):
          - L129 if not table or len(table) < 2:
            - L130 continue
          - L132 assign headers = [str(h or '') for h in table[0]]
          - L133 assign rows = [[str(c or '') for c in row] for row in table[1:]]
          - L135 expr tables.append(ExtractedTable(page_number=page_idx + 1, table_index=table_idx, headers=headers, rows=rows, confidence=0.8))
    - L145 return tables
  - L147 def _extract_with_tabulaself, path: Path:
    - L148 docstring: "Extract tables using tabula-py."
    - L149 try:
      - L150 import tabula
      - L151 except ImportError:
        - L152 raise ImportError('tabula-py is required. Install with: pip install tabula-py')
    - L156 annotated assign tables: List[ExtractedTable] = []
    - L157 assign dfs = tabula.read_pdf(str(path), pages='all', multiple_tables=True)
    - L159 for (table_idx, df) in enumerate(dfs):
      - L160 if df.empty:
        - L161 continue
      - L163 assign headers = [str(c) for c in df.columns.tolist()]
      - L164 assign rows = df.fillna('').astype(str).values.tolist()
      - L166 expr tables.append(ExtractedTable(page_number=1, table_index=table_idx, headers=headers, rows=rows, confidence=0.7))
    - L176 return tables
  - L178 def _extract_text_blocksself, path: Path:
    - L179 docstring: "Extract text blocks from PDF."
    - L180 try:
      - L181 import fitz
      - L182 except ImportError:
        - L183 expr logger.warning('PyMuPDF not available for text extraction')
        - L184 return []
    - L186 annotated assign text_blocks: List[ExtractedText] = []
    - L187 assign doc = fitz.open(str(path))
    - L189 for (page_idx, page) in enumerate(doc):
      - L190 assign blocks = page.get_text('dict')['blocks']
      - L191 for block in blocks:
        - L192 if block.get('type') == 0:
          - L193 assign text = ''
          - L194 for line in block.get('lines', []):
            - L195 for span in line.get('spans', []):
              - L196 aug assign text Add span.get('text', '')
            - L197 aug assign text Add '\n'
          - L199 if text.strip():
            - L200 expr text_blocks.append(ExtractedText(page_number=page_idx + 1, content=text.strip(), bbox=tuple(block.get('bbox', []))))
    - L208 expr doc.close()
    - L209 return text_blocks

## backend\adapters\llm\__init__.py
- L1 docstring: "LLM adapters for AI-powered features."
- L3 from .base import LLMClient, LLMResponse, LLMMessage, LLMRole
- L4 from .openai import OpenAIClient
- L6 assign __all__ = ['LLMClient', 'LLMResponse', 'LLMMessage', 'LLMRole', 'OpenAIClient']

## backend\adapters\llm\base.py
- L1 docstring: "Base interfaces for LLM adapters."
- L3 from __future__ import annotations
- L5 from abc import ABC, abstractmethod
- L6 from dataclasses import dataclass, field
- L7 from enum import Enum
- L8 from typing import Any, Dict, List, Optional, Protocol
- L11 class LLMRole(str, Enum):
  - L12 docstring: "Role in a conversation."
  - L14 assign SYSTEM = 'system'
  - L15 assign USER = 'user'
  - L16 assign ASSISTANT = 'assistant'
- L20 class LLMMessage:
  - L21 docstring: "A message in an LLM conversation."
  - L23 annotated assign role: LLMRole
  - L24 annotated assign content: str
  - L26 def to_dictself:
    - L27 return {'role': self.role.value, 'content': self.content}
- L31 class LLMResponse:
  - L32 docstring: "Response from an LLM call."
  - L34 annotated assign content: str
  - L35 annotated assign model: str
  - L36 annotated assign usage: Dict[str, int] = field(default_factory=dict)
  - L37 annotated assign finish_reason: Optional[str] = None
  - L38 annotated assign raw_response: Optional[Any] = None
  - L41 def prompt_tokensself:
    - L42 return self.usage.get('prompt_tokens', 0)
  - L45 def completion_tokensself:
    - L46 return self.usage.get('completion_tokens', 0)
  - L49 def total_tokensself:
    - L50 return self.usage.get('total_tokens', 0)
- L53 class LLMClient(Protocol):
  - L54 docstring: "Interface for LLM clients.\n\n    Abstracts away the specific LLM provider (Open..."
  - L59 def completeself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L68 docstring: "Send a completion request."
    - L69 expr ...
  - L71 async def complete_asyncself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L80 docstring: "Send an async completion request."
    - L81 expr ...
- L84 class BaseLLMClient(ABC):
  - L85 docstring: "Abstract base for LLM clients with common functionality."
  - L87 def __init__self, *, default_model: str, max_retries: int=3, timeout_seconds: float=60.0:
    - L94 assign self._default_model = default_model
    - L95 assign self._max_retries = max_retries
    - L96 assign self._timeout = timeout_seconds
  - L99 def completeself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L108 docstring: "Send a completion request."
    - L109 pass
  - L112 async def complete_asyncself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L121 docstring: "Send an async completion request."
    - L122 pass
  - L124 def _prepare_messagesself, messages: List[LLMMessage]:
    - L125 docstring: "Convert messages to dict format."
    - L126 return [m.to_dict() for m in messages]
- L130 class PromptTemplate:
  - L131 docstring: "A reusable prompt template with variable substitution."
  - L133 annotated assign template: str
  - L134 annotated assign system_prompt: Optional[str] = None
  - L135 annotated assign variables: List[str] = field(default_factory=list)
  - L137 def renderself, **kwargs: Any:
    - L138 docstring: "Render the template with provided variables."
    - L139 assign messages = []
    - L141 if self.system_prompt:
      - L142 expr messages.append(LLMMessage(role=LLMRole.SYSTEM, content=self.system_prompt))
    - L144 assign content = self.template
    - L145 for var in self.variables:
      - L146 if var in kwargs:
        - L147 assign content = content.replace(f'{{{{{var}}}}}', str(kwargs[var]))
    - L149 expr messages.append(LLMMessage(role=LLMRole.USER, content=content))
    - L150 return messages

## backend\adapters\llm\openai.py
- L1 docstring: "OpenAI LLM adapter implementation."
- L3 from __future__ import annotations
- L5 import logging
- L6 import os
- L7 import time
- L8 from typing import Any, Dict, List, Optional
- L10 from backend.core.errors import ExternalServiceError
- L11 from .base import BaseLLMClient, LLMMessage, LLMResponse
- L13 assign logger = logging.getLogger('neura.adapters.llm.openai')
- L15 assign _FORCE_GPT5 = os.getenv('NEURA_FORCE_GPT5', 'false').lower() in {'1', 'true', 'yes'}
- L18 class OpenAIClient(BaseLLMClient):
  - L19 docstring: "OpenAI API client implementation."
  - L21 def __init__self, *, api_key: Optional[str]=None, default_model: str='gpt-5', max_retries: int=3, timeout_seconds: float=60.0, base_url: Optional[str]=None:
    - L30 expr super().__init__(default_model=default_model, max_retries=max_retries, timeout_seconds=timeout_seconds)
    - L35 assign self._api_key = api_key or os.getenv('OPENAI_API_KEY')
    - L36 assign self._base_url = base_url
    - L37 assign self._client = None
  - L39 def _get_clientself:
    - L40 docstring: "Lazy initialization of OpenAI client."
    - L41 if self._client is None:
      - L42 try:
        - L43 from openai import OpenAI
        - L44 except ImportError:
          - L45 raise ImportError('openai package is required. Install with: pip install openai')
      - L50 if not self._api_key:
        - L51 raise ValueError('OpenAI API key is required. Set OPENAI_API_KEY environment variable.')
      - L54 if not self._api_key.startswith(('sk-', 'sess-')):
        - L55 expr logger.warning("OpenAI API key may be invalid (expected 'sk-' or 'sess-' prefix)", extra={'event': 'api_key_format_warning'})
      - L60 annotated assign kwargs: Dict[str, Any] = {'api_key': self._api_key, 'timeout': self._timeout, 'max_retries': self._max_retries}
      - L65 if self._base_url:
        - L66 assign kwargs['base_url'] = self._base_url
      - L68 assign self._client = OpenAI(**kwargs)
    - L70 return self._client
  - L72 def completeself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L81 docstring: "Send a completion request to OpenAI."
    - L82 assign client = self._get_client()
    - L83 assign model_name = _force_gpt5(model or self._default_model)
    - L85 annotated assign kwargs: Dict[str, Any] = {'model': model_name, 'messages': self._prepare_messages(messages), 'temperature': temperature}
    - L91 if max_tokens:
      - L92 assign kwargs['max_tokens'] = max_tokens
    - L94 if json_mode:
      - L95 assign kwargs['response_format'] = {'type': 'json_object'}
    - L97 assign start = time.perf_counter()
    - L98 try:
      - L99 if _use_responses_model(model_name):
        - L100 assign payload = _prepare_responses_payload(kwargs)
        - L101 try:
          - L102 assign response = client.responses.create(**payload)
          - L103 except AttributeError as exc:
            - L104 raise ExternalServiceError(message='OpenAI Responses API is required for gpt-5. Upgrade the openai package to >=1.0.0.', service='openai', cause=exc)
        - L112 assign content = _response_output_text(response)
        - L113 assign usage = _response_usage(response)
        - L114 assign elapsed = (time.perf_counter() - start) * 1000
        - L116 expr logger.info('llm_completion_success', extra={'event': 'llm_completion_success', 'model': model_name, 'elapsed_ms': elapsed, 'tokens': usage.get('total_tokens', 0), 'endpoint': 'responses'})
        - L127 return LLMResponse(content=content, model=response.model if hasattr(response, 'model') else model_name, usage=usage, finish_reason='stop', raw_response=response)
      - L135 assign response = client.chat.completions.create(**kwargs)
      - L136 assign elapsed = (time.perf_counter() - start) * 1000
      - L138 expr logger.info('llm_completion_success', extra={'event': 'llm_completion_success', 'model': model_name, 'elapsed_ms': elapsed, 'tokens': response.usage.total_tokens if response.usage else 0})
      - L148 return LLMResponse(content=response.choices[0].message.content or '', model=response.model, usage={'prompt_tokens': response.usage.prompt_tokens if response.usage else 0, 'completion_tokens': response.usage.completion_tokens if response.usage else 0, 'total_tokens': response.usage.total_tokens if response.usage else 0}, finish_reason=response.choices[0].finish_reason, raw_response=response)
      - L159 except Exception as e:
        - L160 assign elapsed = (time.perf_counter() - start) * 1000
        - L161 expr logger.exception('llm_completion_failed', extra={'event': 'llm_completion_failed', 'model': model_name, 'elapsed_ms': elapsed, 'error': str(e)})
        - L170 raise ExternalServiceError(message=f'OpenAI API call failed: {e}', service='openai', cause=e)
  - L176 async def complete_asyncself, messages: List[LLMMessage], *, model: Optional[str]=None, temperature: float=0.0, max_tokens: Optional[int]=None, json_mode: bool=False:
    - L185 docstring: "Send an async completion request to OpenAI."
    - L186 try:
      - L187 from openai import AsyncOpenAI
      - L188 except ImportError:
        - L189 raise ImportError('openai package is required. Install with: pip install openai')
    - L193 annotated assign kwargs: Dict[str, Any] = {'api_key': self._api_key, 'timeout': self._timeout, 'max_retries': self._max_retries}
    - L198 if self._base_url:
      - L199 assign kwargs['base_url'] = self._base_url
    - L201 assign client = AsyncOpenAI(**kwargs)
    - L202 assign model_name = _force_gpt5(model or self._default_model)
    - L204 annotated assign request_kwargs: Dict[str, Any] = {'model': model_name, 'messages': self._prepare_messages(messages), 'temperature': temperature}
    - L210 if max_tokens:
      - L211 assign request_kwargs['max_tokens'] = max_tokens
    - L213 if json_mode:
      - L214 assign request_kwargs['response_format'] = {'type': 'json_object'}
    - L216 assign start = time.perf_counter()
    - L217 try:
      - L218 if _use_responses_model(model_name):
        - L219 assign payload = _prepare_responses_payload(request_kwargs)
        - L220 try:
          - L221 assign response = await client.responses.create(**payload)
          - L222 except AttributeError as exc:
            - L223 raise ExternalServiceError(message='OpenAI Responses API is required for gpt-5. Upgrade the openai package to >=1.0.0.', service='openai', cause=exc)
        - L231 assign content = _response_output_text(response)
        - L232 assign usage = _response_usage(response)
        - L233 assign elapsed = (time.perf_counter() - start) * 1000
        - L235 expr logger.info('llm_completion_success_async', extra={'event': 'llm_completion_success_async', 'model': model_name, 'elapsed_ms': elapsed, 'tokens': usage.get('total_tokens', 0), 'endpoint': 'responses'})
        - L246 return LLMResponse(content=content, model=response.model if hasattr(response, 'model') else model_name, usage=usage, finish_reason='stop', raw_response=response)
      - L254 assign response = await client.chat.completions.create(**request_kwargs)
      - L255 assign elapsed = (time.perf_counter() - start) * 1000
      - L257 expr logger.info('llm_completion_success_async', extra={'event': 'llm_completion_success_async', 'model': model_name, 'elapsed_ms': elapsed, 'tokens': response.usage.total_tokens if response.usage else 0})
      - L267 return LLMResponse(content=response.choices[0].message.content or '', model=response.model, usage={'prompt_tokens': response.usage.prompt_tokens if response.usage else 0, 'completion_tokens': response.usage.completion_tokens if response.usage else 0, 'total_tokens': response.usage.total_tokens if response.usage else 0}, finish_reason=response.choices[0].finish_reason, raw_response=response)
      - L278 except Exception as e:
        - L279 assign elapsed = (time.perf_counter() - start) * 1000
        - L280 expr logger.exception('llm_completion_failed_async', extra={'event': 'llm_completion_failed_async', 'model': model_name, 'elapsed_ms': elapsed, 'error': str(e)})
        - L289 raise ExternalServiceError(message=f'OpenAI API call failed: {e}', service='openai', cause=e)
- L296 def _use_responses_modelmodel_name: Optional[str]:
  - L297 assign force = os.getenv('OPENAI_USE_RESPONSES', '').lower() in {'1', 'true', 'yes'}
  - L298 return force or str(model_name or '').lower().startswith('gpt-5')
- L301 def _force_gpt5model_name: Optional[str]:
  - L302 if not _FORCE_GPT5:
    - L303 return str(model_name or 'gpt-5').strip() or 'gpt-5'
  - L304 assign normalized = str(model_name or '').strip()
  - L305 if normalized.lower().startswith('gpt-5'):
    - L306 return normalized
  - L307 if normalized:
    - L308 expr logger.warning('llm_model_overridden', extra={'event': 'llm_model_overridden', 'requested': normalized, 'forced': 'gpt-5'})
  - L312 return 'gpt-5'
- L315 def _prepare_responses_payloadrequest_kwargs: Dict[str, Any]:
  - L316 assign payload = dict(request_kwargs)
  - L317 assign messages = payload.pop('messages', [])
  - L318 assign payload['input'] = _messages_to_responses_input(messages)
  - L319 if 'max_tokens' in payload and 'max_output_tokens' not in payload:
    - L320 assign payload['max_output_tokens'] = payload.pop('max_tokens')
  - L321 return payload
- L324 def _messages_to_responses_inputmessages: List[Dict[str, Any]]:
  - L325 annotated assign converted: List[Dict[str, Any]] = []
  - L326 for message in messages:
    - L327 if not isinstance(message, dict):
      - L328 continue
    - L329 assign role = message.get('role') or 'user'
    - L330 assign content = message.get('content', '')
    - L331 if isinstance(content, list):
      - L332 annotated assign parts: List[Dict[str, Any]] = []
      - L333 for part in content:
        - L334 if isinstance(part, dict):
          - L335 assign part_type = part.get('type')
          - L336 if part_type == 'text':
            - L337 expr parts.append({'type': 'input_text', 'text': part.get('text', '')})
            - L338 continue
          - L339 if part_type == 'image_url':
            - L340 assign image_url = part.get('image_url')
            - L341 if isinstance(image_url, dict):
              - L342 assign image_url = image_url.get('url') or image_url.get('image_url')
            - L343 expr parts.append({'type': 'input_image', 'image_url': image_url})
            - L344 continue
          - L345 expr parts.append(part)
          - L347 else:
            - L347 expr parts.append({'type': 'input_text', 'text': str(part)})
      - L348 assign content = parts
    - L349 expr converted.append({'role': role, 'content': content})
  - L350 return converted
- L353 def _response_output_textresponse: Any:
  - L354 if isinstance(response, dict):
    - L355 assign output_text = response.get('output_text')
    - L356 if isinstance(output_text, str) and output_text.strip():
      - L357 return output_text
    - L358 assign output = response.get('output')
    - L360 else:
      - L360 assign output_text = getattr(response, 'output_text', None)
      - L361 if isinstance(output_text, str) and output_text.strip():
        - L362 return output_text
      - L363 assign output = getattr(response, 'output', None)
  - L365 if isinstance(output, list):
    - L366 annotated assign texts: List[str] = []
    - L367 for item in output:
      - L368 if isinstance(item, dict):
        - L369 assign item_type = item.get('type')
        - L370 assign content = item.get('content') or []
        - L372 else:
          - L372 assign item_type = getattr(item, 'type', None)
          - L373 assign content = getattr(item, 'content', None) or []
      - L374 if item_type != 'message':
        - L375 continue
      - L376 for segment in content:
        - L377 if isinstance(segment, dict):
          - L378 assign seg_type = segment.get('type')
          - L379 assign text = segment.get('text')
          - L381 else:
            - L381 assign seg_type = getattr(segment, 'type', None)
            - L382 assign text = getattr(segment, 'text', None)
        - L383 if seg_type in {'output_text', 'text'} and isinstance(text, str):
          - L384 expr texts.append(text)
    - L385 if texts:
      - L386 return '\n'.join(texts)
  - L387 return ''
- L390 def _response_usageresponse: Any:
  - L391 assign usage = response.get('usage') if isinstance(response, dict) else getattr(response, 'usage', None)
  - L392 if usage is None:
    - L393 return {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
  - L394 if isinstance(usage, dict):
    - L395 assign input_tokens = usage.get('input_tokens') or usage.get('prompt_tokens') or 0
    - L396 assign output_tokens = usage.get('output_tokens') or usage.get('completion_tokens') or 0
    - L398 else:
      - L398 assign input_tokens = getattr(usage, 'input_tokens', None)
      - L399 if input_tokens is None:
        - L400 assign input_tokens = getattr(usage, 'prompt_tokens', 0)
      - L401 assign output_tokens = getattr(usage, 'output_tokens', None)
      - L402 if output_tokens is None:
        - L403 assign output_tokens = getattr(usage, 'completion_tokens', 0)
  - L404 assign total_tokens = int(input_tokens or 0) + int(output_tokens or 0)
  - L405 return {'prompt_tokens': int(input_tokens or 0), 'completion_tokens': int(output_tokens or 0), 'total_tokens': int(total_tokens)}

## backend\adapters\persistence\__init__.py
- L1 docstring: "Persistence adapters - storage for domain entities."
- L3 from .base import Repository, UnitOfWork
- L4 from .repositories import TemplateRepository, ConnectionRepository, JobRepository, ScheduleRepository, ReportRepository
- L12 assign __all__ = ['Repository', 'UnitOfWork', 'TemplateRepository', 'ConnectionRepository', 'JobRepository', 'ScheduleRepository', 'ReportRepository']

## backend\adapters\persistence\base.py
- L1 docstring: "Base interfaces for persistence layer.\n\nThese protocols define the contract th..."
- L7 from __future__ import annotations
- L9 from abc import ABC, abstractmethod
- L10 from typing import Generic, List, Optional, Protocol, TypeVar, runtime_checkable
- L12 assign T = TypeVar('T')
- L13 assign ID = TypeVar('ID')
- L17 class Repository(Protocol[T, ID]):
  - L18 docstring: "Generic repository interface for CRUD operations.\n\n    Repositories abstract a..."
  - L24 def getself, id: ID:
    - L25 docstring: "Get entity by ID."
    - L26 expr ...
  - L28 def get_allself:
    - L29 docstring: "Get all entities."
    - L30 expr ...
  - L32 def saveself, entity: T:
    - L33 docstring: "Save entity (create or update)."
    - L34 expr ...
  - L36 def deleteself, id: ID:
    - L37 docstring: "Delete entity by ID. Returns True if deleted."
    - L38 expr ...
  - L40 def existsself, id: ID:
    - L41 docstring: "Check if entity exists."
    - L42 expr ...
- L45 class UnitOfWork(ABC):
  - L46 docstring: "Unit of Work pattern for transaction management.\n\n    Ensures all changes with..."
  - L53 def __enter__self:
    - L54 expr ...
  - L57 def __exit__self, exc_type, exc_val, exc_tb:
    - L58 expr ...
  - L61 def commitself:
    - L62 docstring: "Commit all changes."
    - L63 expr ...
  - L66 def rollbackself:
    - L67 docstring: "Rollback all changes."
    - L68 expr ...
- L71 class BaseRepository(Generic[T, ID], ABC):
  - L72 docstring: "Abstract base class for repositories with common functionality."
  - L75 def getself, id: ID:
    - L76 docstring: "Get entity by ID."
    - L77 pass
  - L80 def get_allself:
    - L81 docstring: "Get all entities."
    - L82 pass
  - L85 def saveself, entity: T:
    - L86 docstring: "Save entity (create or update)."
    - L87 pass
  - L90 def deleteself, id: ID:
    - L91 docstring: "Delete entity by ID."
    - L92 pass
  - L94 def existsself, id: ID:
    - L95 docstring: "Check if entity exists."
    - L96 return self.get(id) is not None
  - L98 def countself:
    - L99 docstring: "Count all entities."
    - L100 return len(self.get_all())

## backend\adapters\persistence\repositories.py
- L1 docstring: "Repository interfaces for each domain entity."
- L3 from __future__ import annotations
- L5 from abc import abstractmethod
- L6 from datetime import datetime
- L7 from typing import List, Optional, Protocol
- L9 from backend.domain.templates import Template
- L10 from backend.domain.connections import Connection
- L11 from backend.domain.jobs import Job, JobStatus, Schedule
- L12 from backend.domain.reports import Report
- L15 class TemplateRepository(Protocol):
  - L16 docstring: "Repository for Template entities."
  - L18 def getself, template_id: str:
    - L19 docstring: "Get template by ID."
    - L20 expr ...
  - L22 def get_allself:
    - L23 docstring: "Get all templates."
    - L24 expr ...
  - L26 def saveself, template: Template:
    - L27 docstring: "Save template."
    - L28 expr ...
  - L30 def deleteself, template_id: str:
    - L31 docstring: "Delete template."
    - L32 expr ...
  - L34 def existsself, template_id: str:
    - L35 docstring: "Check if template exists."
    - L36 expr ...
  - L38 def find_by_kindself, kind: str:
    - L39 docstring: "Find templates by kind (pdf/excel)."
    - L40 expr ...
  - L42 def find_by_nameself, name: str:
    - L43 docstring: "Find template by name."
    - L44 expr ...
- L47 class ConnectionRepository(Protocol):
  - L48 docstring: "Repository for Connection entities."
  - L50 def getself, connection_id: str:
    - L51 docstring: "Get connection by ID."
    - L52 expr ...
  - L54 def get_allself:
    - L55 docstring: "Get all connections."
    - L56 expr ...
  - L58 def saveself, connection: Connection:
    - L59 docstring: "Save connection."
    - L60 expr ...
  - L62 def deleteself, connection_id: str:
    - L63 docstring: "Delete connection."
    - L64 expr ...
  - L66 def existsself, connection_id: str:
    - L67 docstring: "Check if connection exists."
    - L68 expr ...
  - L70 def find_by_nameself, name: str:
    - L71 docstring: "Find connection by name."
    - L72 expr ...
  - L74 def get_defaultself:
    - L75 docstring: "Get the default connection."
    - L76 expr ...
- L79 class JobRepository(Protocol):
  - L80 docstring: "Repository for Job entities."
  - L82 def getself, job_id: str:
    - L83 docstring: "Get job by ID."
    - L84 expr ...
  - L86 def get_allself:
    - L87 docstring: "Get all jobs."
    - L88 expr ...
  - L90 def saveself, job: Job:
    - L91 docstring: "Save job."
    - L92 expr ...
  - L94 def deleteself, job_id: str:
    - L95 docstring: "Delete job."
    - L96 expr ...
  - L98 def find_by_statusself, status: JobStatus:
    - L99 docstring: "Find jobs by status."
    - L100 expr ...
  - L102 def find_by_templateself, template_id: str:
    - L103 docstring: "Find jobs for a template."
    - L104 expr ...
  - L106 def find_activeself:
    - L107 docstring: "Find all active (non-terminal) jobs."
    - L108 expr ...
  - L110 def find_recentself, limit: int=50:
    - L111 docstring: "Find recent jobs."
    - L112 expr ...
- L115 class ScheduleRepository(Protocol):
  - L116 docstring: "Repository for Schedule entities."
  - L118 def getself, schedule_id: str:
    - L119 docstring: "Get schedule by ID."
    - L120 expr ...
  - L122 def get_allself:
    - L123 docstring: "Get all schedules."
    - L124 expr ...
  - L126 def saveself, schedule: Schedule:
    - L127 docstring: "Save schedule."
    - L128 expr ...
  - L130 def deleteself, schedule_id: str:
    - L131 docstring: "Delete schedule."
    - L132 expr ...
  - L134 def find_activeself:
    - L135 docstring: "Find all active schedules."
    - L136 expr ...
  - L138 def find_dueself, now: Optional[datetime]=None:
    - L139 docstring: "Find schedules that are due for execution."
    - L140 expr ...
  - L142 def find_by_templateself, template_id: str:
    - L143 docstring: "Find schedules for a template."
    - L144 expr ...
- L147 class ReportRepository(Protocol):
  - L148 docstring: "Repository for Report entities (run history)."
  - L150 def getself, report_id: str:
    - L151 docstring: "Get report by ID."
    - L152 expr ...
  - L154 def get_allself:
    - L155 docstring: "Get all reports."
    - L156 expr ...
  - L158 def saveself, report: Report:
    - L159 docstring: "Save report."
    - L160 expr ...
  - L162 def deleteself, report_id: str:
    - L163 docstring: "Delete report."
    - L164 expr ...
  - L166 def find_by_templateself, template_id: str, limit: int=50:
    - L169 docstring: "Find reports for a template."
    - L170 expr ...
  - L172 def find_by_connectionself, connection_id: str, limit: int=50:
    - L175 docstring: "Find reports for a connection."
    - L176 expr ...
  - L178 def find_by_scheduleself, schedule_id: str, limit: int=50:
    - L181 docstring: "Find reports for a schedule."
    - L182 expr ...
  - L184 def find_recentself, limit: int=50:
    - L185 docstring: "Find recent reports."
    - L186 expr ...

## backend\adapters\rendering\__init__.py
- L1 docstring: "Rendering adapters for document generation."
- L3 from .base import Renderer, RenderResult, RenderContext
- L4 from .html import HTMLRenderer
- L5 from .pdf import PDFRenderer
- L6 from .docx import DOCXRenderer
- L7 from .xlsx import XLSXRenderer
- L9 assign __all__ = ['Renderer', 'RenderResult', 'RenderContext', 'HTMLRenderer', 'PDFRenderer', 'DOCXRenderer', 'XLSXRenderer']

## backend\adapters\rendering\base.py
- L1 docstring: "Base interfaces for rendering adapters."
- L3 from __future__ import annotations
- L5 from abc import ABC, abstractmethod
- L6 from dataclasses import dataclass, field
- L7 from pathlib import Path
- L8 from typing import Any, Dict, Optional, Protocol
- L10 from backend.domain.reports import OutputFormat
- L14 class RenderContext:
  - L15 docstring: "Context passed to renderers.\n\n    Contains all the data needed to render a doc..."
  - L20 annotated assign template_html: str
  - L21 annotated assign data: Dict[str, Any]
  - L22 annotated assign output_format: OutputFormat
  - L23 annotated assign output_path: Path
  - L24 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L25 annotated assign landscape: bool = False
  - L26 annotated assign font_scale: Optional[float] = None
  - L27 annotated assign page_size: str = 'A4'
  - L28 annotated assign margins: Optional[Dict[str, str]] = None
- L32 class RenderResult:
  - L33 docstring: "Result of a render operation."
  - L35 annotated assign success: bool
  - L36 annotated assign output_path: Optional[Path]
  - L37 annotated assign format: OutputFormat
  - L38 annotated assign size_bytes: int = 0
  - L39 annotated assign error: Optional[str] = None
  - L40 annotated assign warnings: list[str] = field(default_factory=list)
  - L41 annotated assign render_time_ms: float = 0.0
- L44 class Renderer(Protocol):
  - L45 docstring: "Interface for document renderers.\n\n    Each output format has its own renderer..."
  - L51 def output_formatself:
    - L52 docstring: "The format this renderer produces."
    - L53 expr ...
  - L55 def renderself, context: RenderContext:
    - L56 docstring: "Render a document from the context."
    - L57 expr ...
  - L59 def supportsself, format: OutputFormat:
    - L60 docstring: "Check if this renderer supports the format."
    - L61 expr ...
- L64 class BaseRenderer(ABC):
  - L65 docstring: "Abstract base class for renderers with common functionality."
  - L69 def output_formatself:
    - L70 docstring: "The format this renderer produces."
    - L71 pass
  - L74 def renderself, context: RenderContext:
    - L75 docstring: "Render a document from the context."
    - L76 pass
  - L78 def supportsself, format: OutputFormat:
    - L79 docstring: "Check if this renderer supports the format."
    - L80 return format == self.output_format
  - L82 def _ensure_output_dirself, path: Path:
    - L83 docstring: "Ensure output directory exists."
    - L84 expr path.parent.mkdir(parents=True, exist_ok=True)
  - L86 def _get_file_sizeself, path: Path:
    - L87 docstring: "Get file size in bytes."
    - L88 try:
      - L89 return path.stat().st_size
      - L90 except Exception:
        - L91 return 0

## backend\adapters\rendering\docx.py
- L1 docstring: "DOCX rendering adapter using python-docx and html2docx."
- L3 from __future__ import annotations
- L5 import logging
- L6 import time
- L7 from pathlib import Path
- L8 from typing import Optional
- L10 from backend.domain.reports import OutputFormat
- L11 from .base import BaseRenderer, RenderContext, RenderResult
- L13 assign logger = logging.getLogger('neura.adapters.rendering.docx')
- L16 class DOCXRenderer(BaseRenderer):
  - L17 docstring: "Renderer that produces DOCX output from HTML."
  - L19 def __init__self, *, default_font_scale: float=1.0:
    - L24 assign self._default_font_scale = default_font_scale
  - L27 def output_formatself:
    - L28 return OutputFormat.DOCX
  - L30 def renderself, context: RenderContext:
    - L31 docstring: "Render DOCX from HTML."
    - L32 assign start = time.perf_counter()
    - L34 try:
      - L35 expr self._ensure_output_dir(context.output_path)
      - L38 try:
        - L39 from docx import Document
        - L40 from docx.shared import Inches, Pt
        - L41 from docx.enum.section import WD_ORIENT
        - L42 except ImportError:
          - L43 raise ImportError('python-docx is required for DOCX rendering. Install with: pip install python-docx')
      - L49 assign doc = Document()
      - L52 if context.landscape:
        - L53 for section in doc.sections:
          - L54 assign section.orientation = WD_ORIENT.LANDSCAPE
          - L56 assign new_width = section.page_height
          - L57 assign new_height = section.page_width
          - L58 assign section.page_width = new_width
          - L59 assign section.page_height = new_height
      - L62 expr self._html_to_docx(doc, context.template_html, context.font_scale)
      - L65 expr doc.save(str(context.output_path))
      - L67 assign render_time = (time.perf_counter() - start) * 1000
      - L68 return RenderResult(success=True, output_path=context.output_path, format=OutputFormat.DOCX, size_bytes=self._get_file_size(context.output_path), render_time_ms=render_time)
      - L75 except Exception as e:
        - L76 expr logger.exception('docx_render_failed')
        - L77 return RenderResult(success=False, output_path=None, format=OutputFormat.DOCX, error=str(e), render_time_ms=(time.perf_counter() - start) * 1000)
  - L85 def _html_to_docxself, doc, html: str, font_scale: Optional[float]:
    - L91 docstring: "Convert HTML content to DOCX paragraphs.\n\n        This is a simplified impleme..."
    - L96 try:
      - L97 from html2docx import html2docx
      - L98 expr html2docx(html, doc)
      - L99 return None
      - L100 except ImportError:
        - L101 pass
    - L104 import re
    - L105 from html.parser import HTMLParser
    - L107 class SimpleHTMLParser(HTMLParser):
      - L108 def __init__self, document:
        - L109 expr super().__init__()
        - L110 assign self.doc = document
        - L111 assign self.current_para = None
        - L112 assign self.in_table = False
        - L113 assign self.table_data = []
        - L114 assign self.current_row = []
      - L116 def handle_starttagself, tag, attrs:
        - L117 if tag in ('p', 'div'):
          - L118 assign self.current_para = self.doc.add_paragraph()
          - L119 else:
            - L119 if tag == 'table':
              - L120 assign self.in_table = True
              - L121 assign self.table_data = []
              - L122 else:
                - L122 if tag == 'tr':
                  - L123 assign self.current_row = []
                  - L124 else:
                    - L124 if tag == 'br':
                      - L125 if self.current_para:
                        - L126 expr self.current_para.add_run('\n')
                      - L127 else:
                        - L127 if tag in ('h1', 'h2', 'h3'):
                          - L128 assign level = int(tag[1])
                          - L129 assign self.current_para = self.doc.add_heading(level=level)
      - L131 def handle_endtagself, tag:
        - L132 if tag == 'table' and self.table_data:
          - L133 expr self._create_table()
          - L134 assign self.in_table = False
          - L135 else:
            - L135 if tag == 'tr' and self.current_row:
              - L136 expr self.table_data.append(self.current_row)
      - L138 def handle_dataself, data:
        - L139 assign data = data.strip()
        - L140 if not data:
          - L141 return None
        - L142 if self.in_table:
          - L143 expr self.current_row.append(data)
          - L144 else:
            - L144 if self.current_para:
              - L145 expr self.current_para.add_run(data)
              - L147 else:
                - L147 assign self.current_para = self.doc.add_paragraph(data)
      - L149 def _create_tableself:
        - L150 if not self.table_data:
          - L151 return None
        - L152 assign rows = len(self.table_data)
        - L153 assign cols = max((len(row) for row in self.table_data))
        - L154 assign table = self.doc.add_table(rows=rows, cols=cols)
        - L155 assign table.style = 'Table Grid'
        - L156 for (i, row_data) in enumerate(self.table_data):
          - L157 for (j, cell_data) in enumerate(row_data):
            - L158 if j < len(table.rows[i].cells):
              - L159 assign table.rows[i].cells[j].text = str(cell_data)
    - L161 assign parser = SimpleHTMLParser(doc)
    - L162 expr parser.feed(html)
- L165 def render_docx_from_htmlhtml_path: Path, output_path: Path, *, landscape: bool=False, font_scale: Optional[float]=None:
  - L172 docstring: "Convenience function to render DOCX from HTML file."
  - L173 assign html_content = html_path.read_text(encoding='utf-8')
  - L175 assign renderer = DOCXRenderer()
  - L176 assign context = RenderContext(template_html=html_content, data={}, output_format=OutputFormat.DOCX, output_path=output_path, landscape=landscape, font_scale=font_scale)
  - L185 return renderer.render(context)

## backend\adapters\rendering\html.py
- L1 docstring: "HTML rendering adapter."
- L3 from __future__ import annotations
- L5 import logging
- L6 import re
- L7 import time
- L8 from pathlib import Path
- L9 from typing import Any, Dict, List
- L11 from backend.domain.reports import OutputFormat
- L12 from .base import BaseRenderer, RenderContext, RenderResult
- L14 assign logger = logging.getLogger('neura.adapters.rendering.html')
- L17 class HTMLRenderer(BaseRenderer):
  - L18 docstring: "Renderer that produces HTML output with token substitution."
  - L21 def output_formatself:
    - L22 return OutputFormat.HTML
  - L24 def renderself, context: RenderContext:
    - L25 docstring: "Render HTML with data substitution."
    - L26 assign start = time.perf_counter()
    - L27 annotated assign warnings: List[str] = []
    - L29 try:
      - L30 expr self._ensure_output_dir(context.output_path)
      - L33 assign html = self._substitute_tokens(context.template_html, context.data, warnings)
      - L36 expr context.output_path.write_text(html, encoding='utf-8')
      - L38 assign render_time = (time.perf_counter() - start) * 1000
      - L39 return RenderResult(success=True, output_path=context.output_path, format=OutputFormat.HTML, size_bytes=self._get_file_size(context.output_path), warnings=warnings, render_time_ms=render_time)
      - L47 except Exception as e:
        - L48 expr logger.exception('html_render_failed')
        - L49 return RenderResult(success=False, output_path=None, format=OutputFormat.HTML, error=str(e), render_time_ms=(time.perf_counter() - start) * 1000)
  - L57 def _substitute_tokensself, html: str, data: Dict[str, Any], warnings: List[str]:
    - L63 docstring: "Substitute {{token}} placeholders with data values."
    - L65 def replace_tokenmatch: re.Match:
      - L66 assign token = match.group(1).strip()
      - L67 if token in data:
        - L68 assign value = data[token]
        - L69 if value is None:
          - L70 return ''
        - L71 return str(value)
        - L73 else:
          - L73 expr warnings.append(f"Token '{token}' not found in data")
          - L74 return match.group(0)
    - L77 assign pattern = '\\{\\{([^}]+)\\}\\}'
    - L78 return re.sub(pattern, replace_token, html)
- L81 class TokenEngine:
  - L82 docstring: "Engine for processing tokens in HTML templates.\n\n    Handles:\n    - Scalar to..."
  - L91 assign SCALAR_PATTERN = re.compile('\\{\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}\\}')
  - L92 assign ROW_PATTERN = re.compile('<!--\\s*BEGIN_ROW\\s*-->(.*?)<!--\\s*END_ROW\\s*-->', re.DOTALL)
  - L96 assign CONDITIONAL_PATTERN = re.compile('<!--\\s*IF\\s+(\\w+)\\s*-->(.*?)<!--\\s*ENDIF\\s*-->', re.DOTALL)
  - L101 def __init__self:
    - L102 annotated assign self._missing_tokens: List[str] = []
  - L105 def missing_tokensself:
    - L106 return list(self._missing_tokens)
  - L108 def processself, html: str, scalars: Dict[str, Any], rows: List[Dict[str, Any]], totals: Dict[str, Any]:
    - L115 docstring: "Process all tokens in the HTML template."
    - L116 assign self._missing_tokens = []
    - L119 assign html = self._process_conditionals(html, scalars)
    - L122 assign html = self._process_rows(html, rows)
    - L125 assign html = self._process_scalars(html, {**scalars, **totals})
    - L127 return html
  - L129 def _process_scalarsself, html: str, data: Dict[str, Any]:
    - L130 docstring: "Replace scalar tokens with values."
    - L132 def replacematch: re.Match:
      - L133 assign token = match.group(1)
      - L134 if token in data:
        - L135 assign value = data[token]
        - L136 return '' if value is None else str(value)
      - L137 expr self._missing_tokens.append(token)
      - L138 return match.group(0)
    - L140 return self.SCALAR_PATTERN.sub(replace, html)
  - L142 def _process_rowsself, html: str, rows: List[Dict[str, Any]]:
    - L143 docstring: "Expand row templates for each data row."
    - L145 def expand_rowmatch: re.Match:
      - L146 assign template = match.group(1)
      - L147 assign expanded = []
      - L148 for (i, row_data) in enumerate(rows):
        - L149 assign row_html = template
        - L151 assign row_data = {**row_data, 'ROWID': i + 1, 'ROW_INDEX': i}
        - L152 assign row_html = self._process_scalars(row_html, row_data)
        - L153 expr expanded.append(row_html)
      - L154 return ''.join(expanded)
    - L156 return self.ROW_PATTERN.sub(expand_row, html)
  - L158 def _process_conditionalsself, html: str, data: Dict[str, Any]:
    - L159 docstring: "Process conditional sections."
    - L161 def evaluatematch: re.Match:
      - L162 assign condition = match.group(1)
      - L163 assign content = match.group(2)
      - L165 assign value = data.get(condition)
      - L166 if value:
        - L167 return content
      - L168 return ''
    - L170 return self.CONDITIONAL_PATTERN.sub(evaluate, html)

## backend\adapters\rendering\pdf.py
- L1 docstring: "PDF rendering adapter using Playwright."
- L3 from __future__ import annotations
- L5 import asyncio
- L6 import logging
- L7 import time
- L8 from pathlib import Path
- L9 from typing import Optional
- L11 from backend.domain.reports import OutputFormat
- L12 from .base import BaseRenderer, RenderContext, RenderResult
- L14 assign logger = logging.getLogger('neura.adapters.rendering.pdf')
- L17 class PDFRenderer(BaseRenderer):
  - L18 docstring: "Renderer that produces PDF output using Playwright."
  - L20 def __init__self, *, browser_type: str='chromium', headless: bool=True:
    - L26 assign self._browser_type = browser_type
    - L27 assign self._headless = headless
  - L30 def output_formatself:
    - L31 return OutputFormat.PDF
  - L33 def renderself, context: RenderContext:
    - L34 docstring: "Render PDF from HTML using Playwright."
    - L35 assign start = time.perf_counter()
    - L37 try:
      - L38 expr self._ensure_output_dir(context.output_path)
      - L41 expr asyncio.run(self._render_async(context))
      - L45 if not context.output_path.exists():
        - L46 return RenderResult(success=False, output_path=None, format=OutputFormat.PDF, error='PDF file was not created', render_time_ms=(time.perf_counter() - start) * 1000)
      - L54 assign render_time = (time.perf_counter() - start) * 1000
      - L55 return RenderResult(success=True, output_path=context.output_path, format=OutputFormat.PDF, size_bytes=self._get_file_size(context.output_path), render_time_ms=render_time)
      - L62 except Exception as e:
        - L63 expr logger.exception('pdf_render_failed')
        - L64 return RenderResult(success=False, output_path=None, format=OutputFormat.PDF, error=str(e), render_time_ms=(time.perf_counter() - start) * 1000)
  - L72 async def _render_asyncself, context: RenderContext:
    - L73 docstring: "Async PDF rendering with Playwright."
    - L74 try:
      - L75 from playwright.async_api import async_playwright
      - L76 except ImportError:
        - L77 raise ImportError('Playwright is required for PDF rendering. Install with: pip install playwright && playwright install chromium')
    - L82 async with async_playwright() as p:
      - L83 assign browser = await getattr(p, self._browser_type).launch(headless=self._headless)
      - L86 try:
        - L87 assign page = await browser.new_page()
        - L90 expr await page.set_content(context.template_html, wait_until='networkidle')
        - L93 assign pdf_options = {'path': str(context.output_path), 'format': context.page_size, 'print_background': True, 'landscape': context.landscape}
        - L100 if context.margins:
          - L101 assign pdf_options['margin'] = context.margins
        - L104 expr await page.pdf(**pdf_options)
        - L107 finally:
          - L107 expr await browser.close()
- L110 class PDFRendererSync:
  - L111 docstring: "Synchronous wrapper for PDF rendering.\n\n    Use this when you need to render f..."
  - L116 def __init__self, renderer: Optional[PDFRenderer]=None:
    - L117 assign self._renderer = renderer or PDFRenderer()
  - L119 def render_from_html_fileself, html_path: Path, output_path: Path, *, landscape: bool=False, page_size: str='A4':
    - L127 docstring: "Render PDF from an HTML file."
    - L128 assign html_content = html_path.read_text(encoding='utf-8')
    - L130 assign context = RenderContext(template_html=html_content, data={}, output_format=OutputFormat.PDF, output_path=output_path, landscape=landscape, page_size=page_size)
    - L139 return self._renderer.render(context)
  - L141 def render_from_html_stringself, html: str, output_path: Path, *, landscape: bool=False, page_size: str='A4':
    - L149 docstring: "Render PDF from an HTML string."
    - L150 assign context = RenderContext(template_html=html, data={}, output_format=OutputFormat.PDF, output_path=output_path, landscape=landscape, page_size=page_size)
    - L159 return self._renderer.render(context)

## backend\adapters\rendering\xlsx.py
- L1 docstring: "XLSX rendering adapter using openpyxl."
- L3 from __future__ import annotations
- L5 import logging
- L6 import re
- L7 import time
- L8 from pathlib import Path
- L9 from typing import Any, Dict, List, Optional
- L11 from backend.domain.reports import OutputFormat
- L12 from .base import BaseRenderer, RenderContext, RenderResult
- L14 assign logger = logging.getLogger('neura.adapters.rendering.xlsx')
- L17 class XLSXRenderer(BaseRenderer):
  - L18 docstring: "Renderer that produces XLSX output from HTML tables."
  - L21 def output_formatself:
    - L22 return OutputFormat.XLSX
  - L24 def renderself, context: RenderContext:
    - L25 docstring: "Render XLSX from HTML tables."
    - L26 assign start = time.perf_counter()
    - L28 try:
      - L29 expr self._ensure_output_dir(context.output_path)
      - L31 try:
        - L32 from openpyxl import Workbook
        - L33 from openpyxl.styles import Font, Alignment, Border, Side, PatternFill
        - L34 except ImportError:
          - L35 raise ImportError('openpyxl is required for XLSX rendering. Install with: pip install openpyxl')
      - L41 assign wb = Workbook()
      - L42 assign ws = wb.active
      - L43 assign ws.title = 'Report'
      - L46 assign tables = self._extract_tables_from_html(context.template_html)
      - L48 if tables:
        - L49 expr self._write_tables_to_worksheet(ws, tables)
        - L52 else:
          - L52 if context.data:
            - L53 expr self._write_data_to_worksheet(ws, context.data)
      - L56 expr self._auto_fit_columns(ws)
      - L59 expr wb.save(str(context.output_path))
      - L61 assign render_time = (time.perf_counter() - start) * 1000
      - L62 return RenderResult(success=True, output_path=context.output_path, format=OutputFormat.XLSX, size_bytes=self._get_file_size(context.output_path), render_time_ms=render_time)
      - L69 except Exception as e:
        - L70 expr logger.exception('xlsx_render_failed')
        - L71 return RenderResult(success=False, output_path=None, format=OutputFormat.XLSX, error=str(e), render_time_ms=(time.perf_counter() - start) * 1000)
  - L79 def _extract_tables_from_htmlself, html: str:
    - L80 docstring: "Extract table data from HTML."
    - L81 from html.parser import HTMLParser
    - L83 class TableParser(HTMLParser):
      - L84 def __init__self:
        - L85 expr super().__init__()
        - L86 assign self.tables = []
        - L87 assign self.current_table = []
        - L88 assign self.current_row = []
        - L89 assign self.current_cell = ''
        - L90 assign self.in_table = False
        - L91 assign self.in_cell = False
      - L93 def handle_starttagself, tag, attrs:
        - L94 if tag == 'table':
          - L95 assign self.in_table = True
          - L96 assign self.current_table = []
          - L97 else:
            - L97 if tag == 'tr' and self.in_table:
              - L98 assign self.current_row = []
              - L99 else:
                - L99 if tag in ('td', 'th') and self.in_table:
                  - L100 assign self.in_cell = True
                  - L101 assign self.current_cell = ''
      - L103 def handle_endtagself, tag:
        - L104 if tag == 'table':
          - L105 if self.current_table:
            - L106 expr self.tables.append(self.current_table)
          - L107 assign self.in_table = False
          - L108 else:
            - L108 if tag == 'tr' and self.in_table:
              - L109 if self.current_row:
                - L110 expr self.current_table.append(self.current_row)
              - L111 else:
                - L111 if tag in ('td', 'th') and self.in_table:
                  - L112 expr self.current_row.append(self.current_cell.strip())
                  - L113 assign self.in_cell = False
      - L115 def handle_dataself, data:
        - L116 if self.in_cell:
          - L117 aug assign self.current_cell Add data
    - L119 assign parser = TableParser()
    - L120 expr parser.feed(html)
    - L121 return parser.tables
  - L123 def _write_tables_to_worksheetself, ws, tables: List[List[List[str]]]:
    - L124 docstring: "Write extracted tables to worksheet."
    - L125 from openpyxl.styles import Font, Border, Side, PatternFill
    - L127 assign current_row = 1
    - L129 for (table_idx, table) in enumerate(tables):
      - L130 if table_idx > 0:
        - L131 aug assign current_row Add 2
      - L133 for (row_idx, row) in enumerate(table):
        - L134 for (col_idx, cell_value) in enumerate(row):
          - L135 assign cell = ws.cell(row=current_row, column=col_idx + 1, value=cell_value)
          - L138 if row_idx == 0:
            - L139 assign cell.font = Font(bold=True)
            - L140 assign cell.fill = PatternFill(start_color='CCCCCC', end_color='CCCCCC', fill_type='solid')
          - L147 assign thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
          - L153 assign cell.border = thin_border
        - L155 aug assign current_row Add 1
  - L157 def _write_data_to_worksheetself, ws, data: Dict[str, Any]:
    - L158 docstring: "Write raw data dictionary to worksheet."
    - L159 from openpyxl.styles import Font
    - L161 assign current_row = 1
    - L164 if 'scalars' in data:
      - L165 for (key, value) in data['scalars'].items():
        - L166 assign ws.cell(row=current_row, column=1, value=key).font = Font(bold=True)
        - L167 expr ws.cell(row=current_row, column=2, value=str(value))
        - L168 aug assign current_row Add 1
      - L169 aug assign current_row Add 1
    - L172 if 'rows' in data and data['rows']:
      - L173 assign rows = data['rows']
      - L174 if rows:
        - L176 for (col_idx, key) in enumerate(rows[0].keys()):
          - L177 assign cell = ws.cell(row=current_row, column=col_idx + 1, value=key)
          - L178 assign cell.font = Font(bold=True)
        - L179 aug assign current_row Add 1
        - L182 for row in rows:
          - L183 for (col_idx, value) in enumerate(row.values()):
            - L184 expr ws.cell(row=current_row, column=col_idx + 1, value=str(value))
          - L185 aug assign current_row Add 1
  - L187 def _auto_fit_columnsself, ws:
    - L188 docstring: "Auto-fit column widths based on content."
    - L189 for column in ws.columns:
      - L190 assign max_length = 0
      - L191 assign column_letter = column[0].column_letter
      - L192 for cell in column:
        - L193 try:
          - L194 if cell.value:
            - L195 assign max_length = max(max_length, len(str(cell.value)))
          - L196 except Exception:
            - L197 pass
      - L198 assign adjusted_width = min(max_length + 2, 50)
      - L199 assign ws.column_dimensions[column_letter].width = adjusted_width
- L202 def render_xlsx_from_htmlhtml_path: Path, output_path: Path:
  - L206 docstring: "Convenience function to render XLSX from HTML file."
  - L207 assign html_content = html_path.read_text(encoding='utf-8')
  - L209 assign renderer = XLSXRenderer()
  - L210 assign context = RenderContext(template_html=html_content, data={}, output_format=OutputFormat.XLSX, output_path=output_path)
  - L217 return renderer.render(context)

## backend\api.py
- L2 from __future__ import annotations
- L4 import logging
- L5 import os
- L6 import warnings
- L7 from contextlib import asynccontextmanager
- L8 from pathlib import Path
- L11 expr warnings.filterwarnings('ignore', message='.*on_event is deprecated.*', category=DeprecationWarning)
- L12 expr warnings.filterwarnings('ignore', message='.*Support for class-based `config` is deprecated.*', category=DeprecationWarning)
- L13 expr warnings.filterwarnings('ignore', message='.*SwigPy.*has no __module__ attribute', category=DeprecationWarning)
- L15 from fastapi import FastAPI, HTTPException, Request
- L16 from fastapi.responses import JSONResponse
- L19 from src.utils.static_files import UploadsStaticFiles
- L21 from .app.core.event_bus import EventBus, logging_middleware, metrics_middleware
- L22 from .app.core.middleware import add_middlewares
- L23 from backend.app.api.router import register_routes
- L24 from src.services.report_service import scheduler_runner as report_scheduler_runner
- L26 from .app.core.config import get_settings, log_settings
- L27 from .app.env_loader import load_env_file
- L29 from .app.services.utils import get_correlation_id
- L30 from .app.services.jobs.report_scheduler import ReportScheduler
- L33 def _configure_error_log_handlertarget_logger: logging.Logger | None=None:
  - L34 docstring: "\n    Attach a file handler that records backend errors for desktop/frontend deb..."
  - L38 assign target_logger = target_logger or logging.getLogger('neura.api')
  - L39 assign log_target = os.getenv('NEURA_ERROR_LOG')
  - L40 if log_target:
    - L41 assign log_file = Path(log_target).expanduser()
    - L43 else:
      - L43 assign backend_dir = Path(__file__).resolve().parent
      - L44 assign logs_dir = backend_dir / 'logs'
      - L45 expr logs_dir.mkdir(parents=True, exist_ok=True)
      - L46 assign log_file = logs_dir / 'backend_errors.log'
  - L48 expr log_file.parent.mkdir(parents=True, exist_ok=True)
  - L49 expr log_file.touch(exist_ok=True)
  - L51 for handler in target_logger.handlers:
    - L52 if isinstance(handler, logging.FileHandler) and getattr(handler, 'baseFilename', '') == str(log_file):
      - L53 return log_file
  - L55 try:
    - L56 assign handler = logging.FileHandler(log_file, encoding='utf-8')
    - L57 except OSError:
      - L58 return None
  - L60 expr handler.setLevel(logging.ERROR)
  - L61 expr handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s'))
  - L62 expr target_logger.addHandler(handler)
  - L63 return log_file
- L66 expr load_env_file()
- L68 assign logger = logging.getLogger('neura.api')
- L69 assign EVENT_BUS = EventBus(middlewares=[logging_middleware(logger), metrics_middleware(logger)])
- L70 assign SETTINGS = get_settings()
- L71 expr log_settings(logger, SETTINGS)
- L72 annotated assign ERROR_LOG_PATH: Path | None = None
- L73 annotated assign SCHEDULER: ReportScheduler | None = None
- L74 assign SCHEDULER_DISABLED = os.getenv('NEURA_SCHEDULER_DISABLED', 'false').lower() == 'true'
- L78 async def lifespanapp: FastAPI:
  - L79 Global
  - L80 if not ERROR_LOG_PATH:
    - L81 assign ERROR_LOG_PATH = _configure_error_log_handler(logging.getLogger())
    - L82 if ERROR_LOG_PATH:
      - L83 expr logger.info('error_log_configured', extra={'event': 'error_log_configured', 'path': str(ERROR_LOG_PATH)})
  - L85 if not SCHEDULER_DISABLED and SCHEDULER is None:
    - L86 assign poll_seconds = max(int(os.getenv('NEURA_SCHEDULER_INTERVAL', '60') or '60'), 15)
    - L87 assign SCHEDULER = ReportScheduler(_scheduler_runner, poll_seconds=poll_seconds)
  - L88 if SCHEDULER and (not SCHEDULER_DISABLED):
    - L89 expr await SCHEDULER.start()
  - L91 expr (yield)
  - L93 if SCHEDULER and (not SCHEDULER_DISABLED):
    - L94 expr await SCHEDULER.stop()
- L97 assign app = FastAPI(title=SETTINGS.api_title, version=SETTINGS.api_version, lifespan=lifespan)
- L98 expr add_middlewares(app, SETTINGS)
- L102 async def http_exception_handlerrequest: Request, exc: HTTPException:
  - L103 assign request_state = getattr(request, 'state', None)
  - L104 assign correlation_id = getattr(request_state, 'correlation_id', None) or get_correlation_id()
  - L105 assign detail = exc.detail
  - L106 if not isinstance(detail, dict) or not {'status', 'code', 'message'} <= set(detail.keys()):
    - L107 assign detail = {'status': 'error', 'code': f'http_{exc.status_code}', 'message': detail if isinstance(detail, str) else str(detail)}
  - L112 assign detail['correlation_id'] = correlation_id
  - L113 return JSONResponse(status_code=exc.status_code, content=detail)
- L117 assign APP_DIR = Path(__file__).parent.resolve()
- L118 assign UPLOAD_ROOT = SETTINGS.uploads_root
- L119 expr UPLOAD_ROOT.mkdir(parents=True, exist_ok=True)
- L120 assign EXCEL_UPLOAD_ROOT = SETTINGS.excel_uploads_root
- L121 expr EXCEL_UPLOAD_ROOT.mkdir(parents=True, exist_ok=True)
- L122 assign UPLOAD_ROOT_BASE = UPLOAD_ROOT.resolve()
- L123 assign EXCEL_UPLOAD_ROOT_BASE = EXCEL_UPLOAD_ROOT.resolve()
- L124 annotated assign _UPLOAD_KIND_BASES: dict[str, tuple[Path, str]] = {'pdf': (UPLOAD_ROOT_BASE, '/uploads'), 'excel': (EXCEL_UPLOAD_ROOT_BASE, '/excel-uploads')}
- L128 assign APP_VERSION = SETTINGS.version
- L129 assign APP_COMMIT = SETTINGS.commit
- L132 expr app.mount('/uploads', UploadsStaticFiles(directory=str(UPLOAD_ROOT)), name='uploads')
- L133 expr app.mount('/excel-uploads', UploadsStaticFiles(directory=str(EXCEL_UPLOAD_ROOT)), name='excel-uploads')
- L136 expr register_routes(app)
- L138 def _scheduler_runnerpayload: dict, kind: str, *, job_tracker: JobRunTracker | None=None:
  - L139 return report_scheduler_runner(payload, kind, job_tracker=job_tracker)

## backend\app\__init__.py
- L1 docstring: "FastAPI app placeholder."

## backend\app\api\__init__.py
- L1 docstring: "Route registration for the v4 FastAPI app."

## backend\app\api\router.py
- L1 docstring: "Centralized API Router Registration.\n\nThis module registers all API routes in ..."
- L6 from __future__ import annotations
- L8 from fastapi import FastAPI
- L10 from .routes import analytics, charts, connections, docqa, enrichment, excel, federation, health, jobs, nl2sql, recommendations, reports, schedules, state, summary, synthesis, templates
- L29 from backend.app.features.analyze.routes import router as analyze_router
- L30 from backend.app.features.analyze.routes import enhanced_analysis_routes
- L33 def register_routesapp: FastAPI:
  - L34 docstring: "Register all API routes with the FastAPI application.\n\n    Route prefixes:\n  ..."
  - L57 expr app.include_router(health.router, tags=['health'])
  - L58 expr app.include_router(connections.router, prefix='/connections', tags=['connections'])
  - L59 expr app.include_router(templates.router, prefix='/templates', tags=['templates'])
  - L60 expr app.include_router(excel.router, prefix='/excel', tags=['excel'])
  - L61 expr app.include_router(reports.router, prefix='/reports', tags=['reports'])
  - L62 expr app.include_router(jobs.router, prefix='/jobs', tags=['jobs'])
  - L63 expr app.include_router(schedules.router, prefix='/reports/schedules', tags=['schedules'])
  - L64 expr app.include_router(state.router, prefix='/state', tags=['state'])
  - L67 expr app.include_router(analyze_router, prefix='/analyze', tags=['analyze'])
  - L68 expr app.include_router(enhanced_analysis_routes.router)
  - L71 expr app.include_router(analytics.router, prefix='/analytics', tags=['analytics'])
  - L74 expr app.include_router(nl2sql.router, prefix='/nl2sql', tags=['nl2sql'])
  - L75 expr app.include_router(enrichment.router, prefix='/enrichment', tags=['enrichment'])
  - L76 expr app.include_router(federation.router, prefix='/federation', tags=['federation'])
  - L77 expr app.include_router(recommendations.router, prefix='/recommendations', tags=['recommendations'])
  - L78 expr app.include_router(charts.router, prefix='/charts', tags=['charts'])
  - L79 expr app.include_router(summary.router, prefix='/summary', tags=['summary'])
  - L80 expr app.include_router(synthesis.router, prefix='/synthesis', tags=['synthesis'])
  - L81 expr app.include_router(docqa.router, prefix='/docqa', tags=['docqa'])

## backend\app\api\routes\__init__.py
- L1 docstring: "API route modules for the NeuraReport FastAPI application.\n\nThis package conta..."
- L29 from . import analytics, charts, connections, docqa, enrichment, excel, federation, health, jobs, nl2sql, recommendations, reports, schedules, state, summary, synthesis, templates
- L49 assign __all__ = ['analytics', 'charts', 'connections', 'docqa', 'enrichment', 'excel', 'federation', 'health', 'jobs', 'nl2sql', 'recommendations', 'reports', 'schedules', 'state', 'summary', 'synthesis', 'templates']

## backend\app\api\routes\analytics.py
- L1 from __future__ import annotations
- L3 from datetime import datetime, timedelta, timezone
- L4 from typing import Any, Dict, List, Optional
- L6 from fastapi import APIRouter, Body, Depends, HTTPException, Query
- L7 from pydantic import BaseModel
- L9 from backend.app.core.config import get_settings
- L10 from backend.app.core.security import require_api_key
- L11 from backend.app.services.state import state_store
- L13 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L16 def _parse_isodate_str: Optional[str]:
  - L17 docstring: "Parse ISO date string to datetime."
  - L18 if not date_str:
    - L19 return None
  - L20 try:
    - L21 return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    - L22 except (ValueError, TypeError):
      - L23 return None
- L26 def _get_date_bucketdate_str: Optional[str], bucket: str='day':
  - L27 docstring: "Convert date string to bucket key (day, week, month)."
  - L28 assign dt = _parse_iso(date_str)
  - L29 if not dt:
    - L30 return None
  - L31 if bucket == 'day':
    - L32 return dt.strftime('%Y-%m-%d')
    - L33 else:
      - L33 if bucket == 'week':
        - L35 assign start = dt - timedelta(days=dt.weekday())
        - L36 return start.strftime('%Y-%m-%d')
        - L37 else:
          - L37 if bucket == 'month':
            - L38 return dt.strftime('%Y-%m')
  - L39 return dt.strftime('%Y-%m-%d')
- L42 def _normalize_job_statusstatus: Optional[str]:
  - L43 docstring: "Normalize job status to consistent UI-friendly values."
  - L44 assign value = (status or '').strip().lower()
  - L45 if value in {'succeeded', 'completed'}:
    - L46 return 'completed'
  - L47 if value in {'queued', 'pending'}:
    - L48 return 'pending'
  - L49 if value in {'running', 'in_progress'}:
    - L50 return 'running'
  - L51 if value in {'failed', 'error'}:
    - L52 return 'failed'
  - L53 if value in {'cancelled', 'canceled'}:
    - L54 return 'cancelled'
  - L55 return value or 'pending'
- L59 async def get_dashboard_analytics:
  - L60 docstring: "Get comprehensive dashboard analytics."
  - L63 assign connections = state_store.list_connections()
  - L64 assign templates = state_store.list_templates()
  - L65 assign jobs = state_store.list_jobs()
  - L66 assign schedules = state_store.list_schedules()
  - L69 assign now = datetime.now(timezone.utc)
  - L70 assign today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
  - L71 assign week_start = today_start - timedelta(days=today_start.weekday())
  - L72 assign month_start = today_start.replace(day=1)
  - L75 assign total_jobs = len(jobs)
  - L76 assign completed_jobs = [j for j in jobs if _normalize_job_status(j.get('status')) == 'completed']
  - L77 assign failed_jobs = [j for j in jobs if _normalize_job_status(j.get('status')) == 'failed']
  - L78 assign running_jobs = [j for j in jobs if _normalize_job_status(j.get('status')) == 'running']
  - L79 assign pending_jobs = [j for j in jobs if _normalize_job_status(j.get('status')) == 'pending']
  - L82 def count_jobs_afterjob_list: list, after: datetime:
    - L83 assign count = 0
    - L84 for j in job_list:
      - L85 assign created = _parse_iso(j.get('created_at'))
      - L86 if created and created >= after:
        - L87 aug assign count Add 1
    - L88 return count
  - L90 assign jobs_today = count_jobs_after(jobs, today_start)
  - L91 assign jobs_this_week = count_jobs_after(jobs, week_start)
  - L92 assign jobs_this_month = count_jobs_after(jobs, month_start)
  - L95 assign finished_jobs = len(completed_jobs) + len(failed_jobs)
  - L96 assign success_rate = len(completed_jobs) / finished_jobs * 100 if finished_jobs > 0 else 0
  - L99 assign pdf_templates = [t for t in templates if t.get('kind') == 'pdf']
  - L100 assign excel_templates = [t for t in templates if t.get('kind') == 'excel']
  - L101 assign approved_templates = [t for t in templates if t.get('status') == 'approved']
  - L104 assign template_usage = {}
  - L105 for job in jobs:
    - L106 assign tid = job.get('template_id')
    - L107 if tid:
      - L108 assign template_usage[tid] = template_usage.get(tid, 0) + 1
  - L110 assign top_templates = []
  - L111 for (tid, count) in sorted(template_usage.items(), key=lambda x: -x[1])[:5]:
    - L112 assign template = next((t for t in templates if t.get('id') == tid), None)
    - L113 if template:
      - L114 expr top_templates.append({'id': tid, 'name': template.get('name', tid[:12]), 'kind': template.get('kind', 'pdf'), 'runCount': count})
  - L122 assign active_connections = [c for c in connections if c.get('status') == 'connected']
  - L123 assign avg_latency = 0
  - L124 assign latencies = [c.get('lastLatencyMs') for c in connections if c.get('lastLatencyMs')]
  - L125 if latencies:
    - L126 assign avg_latency = sum(latencies) / len(latencies)
  - L129 assign active_schedules = [s for s in schedules if s.get('active')]
  - L132 assign jobs_trend = []
  - L133 for i in range(6, -1, -1):
    - L134 assign day = today_start - timedelta(days=i)
    - L135 assign day_end = day + timedelta(days=1)
    - L136 assign day_str = day.strftime('%Y-%m-%d')
    - L138 assign day_jobs = [j for j in jobs if (created := _parse_iso(j.get('created_at'))) and day <= created < day_end]
    - L142 assign day_completed = len([j for j in day_jobs if _normalize_job_status(j.get('status')) == 'completed'])
    - L143 assign day_failed = len([j for j in day_jobs if _normalize_job_status(j.get('status')) == 'failed'])
    - L145 expr jobs_trend.append({'date': day_str, 'label': day.strftime('%a'), 'total': len(day_jobs), 'completed': day_completed, 'failed': day_failed})
  - L154 assign recent_jobs = sorted(jobs, key=lambda j: j.get('created_at') or '', reverse=True)[:10]
  - L155 assign recent_activity = []
  - L156 for job in recent_jobs:
    - L157 expr recent_activity.append({'id': job.get('id'), 'type': 'job', 'action': f"Report {_normalize_job_status(job.get('status'))}", 'template': job.get('template_name') or job.get('template_id', '')[:12], 'timestamp': job.get('completed_at') or job.get('created_at'), 'status': _normalize_job_status(job.get('status'))})
  - L166 return {'summary': {'totalConnections': len(connections), 'activeConnections': len(active_connections), 'totalTemplates': len(templates), 'approvedTemplates': len(approved_templates), 'pdfTemplates': len(pdf_templates), 'excelTemplates': len(excel_templates), 'totalJobs': total_jobs, 'activeJobs': len(running_jobs) + len(pending_jobs), 'completedJobs': len(completed_jobs), 'failedJobs': len(failed_jobs), 'totalSchedules': len(schedules), 'activeSchedules': len(active_schedules)}, 'metrics': {'successRate': round(success_rate, 1), 'avgConnectionLatency': round(avg_latency, 1), 'jobsToday': jobs_today, 'jobsThisWeek': jobs_this_week, 'jobsThisMonth': jobs_this_month}, 'topTemplates': top_templates, 'jobsTrend': jobs_trend, 'recentActivity': recent_activity, 'timestamp': datetime.now(timezone.utc).isoformat()}
- L196 async def get_usage_statisticsperiod: str=Query('week', pattern='^(day|week|month)$'):
  - L199 docstring: "Get detailed usage statistics over time."
  - L201 assign jobs = state_store.list_jobs()
  - L202 assign templates = state_store.list_templates()
  - L204 assign now = datetime.now(timezone.utc)
  - L207 if period == 'day':
    - L208 assign start_date = now - timedelta(days=1)
    - L209 assign bucket = 'hour'
    - L210 else:
      - L210 if period == 'week':
        - L211 assign start_date = now - timedelta(days=7)
        - L212 assign bucket = 'day'
        - L214 else:
          - L214 assign start_date = now - timedelta(days=30)
          - L215 assign bucket = 'day'
  - L218 assign filtered_jobs = []
  - L219 for job in jobs:
    - L220 assign created = _parse_iso(job.get('created_at'))
    - L221 if created and created >= start_date:
      - L222 expr filtered_jobs.append(job)
  - L225 assign by_status = {}
  - L226 for job in filtered_jobs:
    - L227 assign status = _normalize_job_status(job.get('status'))
    - L228 assign by_status[status] = by_status.get(status, 0) + 1
  - L231 assign by_kind = {'pdf': 0, 'excel': 0}
  - L232 for job in filtered_jobs:
    - L233 assign kind = job.get('template_kind', 'pdf')
    - L234 assign by_kind[kind] = by_kind.get(kind, 0) + 1
  - L237 assign by_template = {}
  - L238 for job in filtered_jobs:
    - L239 assign tid = job.get('template_id', 'unknown')
    - L240 assign tname = job.get('template_name') or tid[:12]
    - L241 if tid not in by_template:
      - L242 assign by_template[tid] = {'name': tname, 'count': 0}
    - L243 aug assign by_template[tid]['count'] Add 1
  - L245 assign template_breakdown = sorted([{'id': k, **v} for k, v in by_template.items()], key=lambda x: -x['count'])[:10]
  - L250 return {'period': period, 'totalJobs': len(filtered_jobs), 'byStatus': by_status, 'byKind': by_kind, 'templateBreakdown': template_breakdown, 'startDate': start_date.isoformat(), 'endDate': now.isoformat()}
- L262 async def get_report_historylimit: int=Query(50, ge=1, le=200), offset: int=Query(0, ge=0), status: Optional[str]=Query(None), template_id: Optional[str]=Query(None):
  - L268 docstring: "Get report generation history with filtering."
  - L270 assign jobs = state_store.list_jobs()
  - L273 assign filtered = jobs
  - L274 if status:
    - L275 assign status_norm = _normalize_job_status(status)
    - L276 assign filtered = [j for j in filtered if _normalize_job_status(j.get('status')) == status_norm]
  - L277 if template_id:
    - L278 assign filtered = [j for j in filtered if j.get('template_id') == template_id]
  - L281 expr filtered.sort(key=lambda j: j.get('created_at') or '', reverse=True)
  - L283 assign total = len(filtered)
  - L286 assign paginated = filtered[offset:offset + limit]
  - L289 assign templates = {t.get('id'): t for t in state_store.list_templates()}
  - L291 assign history = []
  - L292 for job in paginated:
    - L293 assign tid = job.get('template_id')
    - L294 assign template = templates.get(tid, {})
    - L296 expr history.append({'id': job.get('id'), 'templateId': tid, 'templateName': job.get('template_name') or template.get('name') or (tid[:12] if tid else 'Unknown'), 'templateKind': job.get('template_kind') or template.get('kind') or 'pdf', 'connectionId': job.get('connection_id'), 'status': _normalize_job_status(job.get('status')), 'createdAt': job.get('created_at'), 'completedAt': job.get('completed_at'), 'artifacts': job.get('artifacts'), 'error': job.get('error'), 'meta': job.get('meta')})
  - L310 return {'history': history, 'total': total, 'limit': limit, 'offset': offset, 'hasMore': offset + limit < total}
- L324 async def get_activity_loglimit: int=Query(50, ge=1, le=200), offset: int=Query(0, ge=0), entity_type: Optional[str]=Query(None), action: Optional[str]=Query(None):
  - L330 docstring: "Get the activity log with optional filtering."
  - L331 assign log = state_store.get_activity_log(limit=limit, offset=offset, entity_type=entity_type, action=action)
  - L337 return {'activities': log, 'limit': limit, 'offset': offset}
- L345 async def log_activityaction: str, entity_type: str, entity_id: Optional[str]=None, entity_name: Optional[str]=None, details: Optional[Dict[str, Any]]=None:
  - L352 docstring: "Log an activity event."
  - L353 assign entry = state_store.log_activity(action=action, entity_type=entity_type, entity_id=entity_id, entity_name=entity_name, details=details)
  - L360 return {'activity': entry}
- L364 async def clear_activity_log:
  - L365 docstring: "Clear all activity log entries."
  - L366 assign count = state_store.clear_activity_log()
  - L367 return {'cleared': count}
- L375 async def get_favorites:
  - L376 docstring: "Get all favorites."
  - L377 assign favorites = state_store.get_favorites()
  - L380 assign templates = {t.get('id'): t for t in state_store.list_templates()}
  - L381 assign connections = {c.get('id'): c for c in state_store.list_connections()}
  - L383 assign enriched_templates = []
  - L384 for tid in favorites.get('templates', []):
    - L385 assign template = templates.get(tid)
    - L386 if template:
      - L387 expr enriched_templates.append({'id': tid, 'name': template.get('name'), 'kind': template.get('kind'), 'status': template.get('status')})
  - L394 assign enriched_connections = []
  - L395 for cid in favorites.get('connections', []):
    - L396 assign conn = connections.get(cid)
    - L397 if conn:
      - L398 expr enriched_connections.append({'id': cid, 'name': conn.get('name'), 'dbType': conn.get('db_type'), 'status': conn.get('status')})
  - L405 return {'templates': enriched_templates, 'connections': enriched_connections}
- L412 async def add_favoriteentity_type: str, entity_id: str:
  - L413 docstring: "Add an item to favorites."
  - L414 if entity_type not in ('templates', 'connections'):
    - L415 from fastapi import HTTPException
    - L416 raise HTTPException(status_code=400, detail='Invalid entity type')
  - L418 assign added = state_store.add_favorite(entity_type, entity_id)
  - L421 expr state_store.log_activity(action='favorite_added', entity_type=entity_type.rstrip('s'), entity_id=entity_id)
  - L427 return {'added': added, 'entityType': entity_type, 'entityId': entity_id}
- L431 async def remove_favoriteentity_type: str, entity_id: str:
  - L432 docstring: "Remove an item from favorites."
  - L433 if entity_type not in ('templates', 'connections'):
    - L434 from fastapi import HTTPException
    - L435 raise HTTPException(status_code=400, detail='Invalid entity type')
  - L437 assign removed = state_store.remove_favorite(entity_type, entity_id)
  - L440 expr state_store.log_activity(action='favorite_removed', entity_type=entity_type.rstrip('s'), entity_id=entity_id)
  - L446 return {'removed': removed, 'entityType': entity_type, 'entityId': entity_id}
- L450 async def check_favoriteentity_type: str, entity_id: str:
  - L451 docstring: "Check if an item is a favorite."
  - L452 if entity_type not in ('templates', 'connections'):
    - L453 from fastapi import HTTPException
    - L454 raise HTTPException(status_code=400, detail='Invalid entity type')
  - L456 assign is_fav = state_store.is_favorite(entity_type, entity_id)
  - L457 return {'isFavorite': is_fav, 'entityType': entity_type, 'entityId': entity_id}
- L464 class PreferenceValue(BaseModel):
  - L465 annotated assign value: Any
- L469 async def get_preferences:
  - L470 docstring: "Get user preferences."
  - L471 assign prefs = state_store.get_user_preferences()
  - L472 return {'preferences': prefs}
- L476 async def update_preferencesupdates: Dict[str, Any]:
  - L477 docstring: "Update user preferences."
  - L478 assign prefs = state_store.update_user_preferences(updates)
  - L479 return {'preferences': prefs}
- L483 async def set_preferencekey: str, payload: PreferenceValue | None=Body(default=None), value: Any=Query(default=None):
  - L488 docstring: "Set a single user preference."
  - L489 if payload is not None and payload.value is not None:
    - L490 assign pref_value = payload.value
    - L491 else:
      - L491 if value is not None:
        - L492 assign pref_value = value
        - L494 else:
          - L494 raise HTTPException(status_code=422, detail='Preference value is required.')
  - L495 assign prefs = state_store.set_user_preference(key, pref_value)
  - L496 return {'preferences': prefs}
- L504 async def export_configuration:
  - L505 docstring: "Export all configuration (templates, connections, schedules, preferences) as JSO..."
  - L506 assign connections = state_store.list_connections()
  - L507 assign templates = state_store.list_templates()
  - L508 assign schedules = state_store.list_schedules()
  - L509 assign favorites = state_store.get_favorites()
  - L510 assign preferences = state_store.get_user_preferences()
  - L513 assign safe_connections = []
  - L514 for conn in connections:
    - L515 assign safe_conn = {'id': conn.get('id'), 'name': conn.get('name'), 'db_type': conn.get('db_type'), 'summary': conn.get('summary'), 'tags': conn.get('tags')}
    - L522 expr safe_connections.append(safe_conn)
  - L524 return {'version': '1.0', 'exportedAt': datetime.now(timezone.utc).isoformat(), 'data': {'connections': safe_connections, 'templates': templates, 'schedules': schedules, 'favorites': favorites, 'preferences': preferences}}
- L542 async def global_searchq: str=Query(..., min_length=1, max_length=100), types: Optional[str]=Query(None), limit: int=Query(20, ge=1, le=100):
  - L547 docstring: "Search across templates, connections, and jobs."
  - L548 assign query = q.lower().strip()
  - L549 assign type_filter = set(types.split(',')) if types else {'templates', 'connections', 'jobs'}
  - L551 assign results = []
  - L554 if 'templates' in type_filter:
    - L555 assign templates = state_store.list_templates()
    - L556 for t in templates:
      - L557 assign name = (t.get('name') or '').lower()
      - L558 assign tid = (t.get('id') or '').lower()
      - L559 if query in name or query in tid:
        - L560 expr results.append({'type': 'template', 'id': t.get('id'), 'name': t.get('name'), 'description': f"{t.get('kind', 'pdf').upper()} Template", 'url': f'/templates', 'meta': {'kind': t.get('kind'), 'status': t.get('status')}})
  - L570 if 'connections' in type_filter:
    - L571 assign connections = state_store.list_connections()
    - L572 for c in connections:
      - L573 assign name = (c.get('name') or '').lower()
      - L574 assign cid = (c.get('id') or '').lower()
      - L575 assign summary = (c.get('summary') or '').lower()
      - L576 if query in name or query in cid or query in summary:
        - L577 expr results.append({'type': 'connection', 'id': c.get('id'), 'name': c.get('name'), 'description': c.get('summary') or c.get('db_type'), 'url': f'/connections', 'meta': {'dbType': c.get('db_type'), 'status': c.get('status')}})
  - L587 if 'jobs' in type_filter:
    - L588 assign jobs = state_store.list_jobs(limit=100)
    - L589 for j in jobs:
      - L590 assign tname = (j.get('templateName') or j.get('template_name') or '').lower()
      - L591 assign jid = (j.get('id') or '').lower()
      - L592 if query in tname or query in jid:
        - L593 expr results.append({'type': 'job', 'id': j.get('id'), 'name': j.get('templateName') or j.get('template_name') or j.get('id')[:12], 'description': f"Job - {_normalize_job_status(j.get('status'))}", 'url': f'/jobs', 'meta': {'status': _normalize_job_status(j.get('status')), 'createdAt': j.get('createdAt') or j.get('created_at')}})
  - L606 assign results = results[:limit]
  - L608 return {'query': q, 'results': results, 'total': len(results)}
- L620 async def get_notificationslimit: int=Query(50, ge=1, le=100), unread_only: bool=Query(False):
  - L624 docstring: "Get notifications list."
  - L625 assign notifications = state_store.get_notifications(limit=limit, unread_only=unread_only)
  - L626 assign unread_count = state_store.get_unread_count()
  - L627 return {'notifications': notifications, 'unreadCount': unread_count, 'total': len(notifications)}
- L635 async def get_unread_count:
  - L636 docstring: "Get count of unread notifications."
  - L637 return {'unreadCount': state_store.get_unread_count()}
- L641 async def create_notificationpayload: Dict[str, Any]:
  - L642 docstring: "Create a new notification."
  - L643 assign title = payload.get('title', 'Notification')
  - L644 assign message = payload.get('message', '')
  - L645 assign notification_type = payload.get('type', 'info')
  - L646 assign link = payload.get('link')
  - L647 assign entity_type = payload.get('entityType')
  - L648 assign entity_id = payload.get('entityId')
  - L650 assign notification = state_store.add_notification(title=title, message=message, notification_type=notification_type, link=link, entity_type=entity_type, entity_id=entity_id)
  - L658 return {'notification': notification}
- L662 async def mark_notification_readnotification_id: str:
  - L663 docstring: "Mark a notification as read."
  - L664 assign found = state_store.mark_notification_read(notification_id)
  - L665 if not found:
    - L666 from fastapi import HTTPException
    - L667 raise HTTPException(status_code=404, detail='Notification not found')
  - L668 return {'marked': True, 'notificationId': notification_id}
- L672 async def mark_all_read:
  - L673 docstring: "Mark all notifications as read."
  - L674 assign count = state_store.mark_all_notifications_read()
  - L675 return {'markedCount': count}
- L679 async def delete_notificationnotification_id: str:
  - L680 docstring: "Delete a notification."
  - L681 assign found = state_store.delete_notification(notification_id)
  - L682 if not found:
    - L683 from fastapi import HTTPException
    - L684 raise HTTPException(status_code=404, detail='Notification not found')
  - L685 return {'deleted': True, 'notificationId': notification_id}
- L689 async def clear_all_notifications:
  - L690 docstring: "Clear all notifications."
  - L691 assign count = state_store.clear_notifications()
  - L692 return {'clearedCount': count}
- L700 async def bulk_delete_templatespayload: Dict[str, Any]:
  - L701 docstring: "Delete multiple templates in bulk."
  - L702 assign template_ids = payload.get('templateIds', [])
  - L703 if not template_ids:
    - L704 from fastapi import HTTPException
    - L705 raise HTTPException(status_code=400, detail='No template IDs provided')
  - L707 assign deleted = []
  - L708 assign failed = []
  - L710 for tid in template_ids:
    - L711 try:
      - L712 expr state_store.delete_template(tid)
      - L713 expr deleted.append(tid)
      - L714 expr state_store.log_activity(action='template_deleted', entity_type='template', entity_id=tid)
      - L719 except Exception as e:
        - L720 expr failed.append({'id': tid, 'error': str(e)})
  - L722 return {'deleted': deleted, 'deletedCount': len(deleted), 'failed': failed, 'failedCount': len(failed)}
- L731 async def bulk_update_template_statuspayload: Dict[str, Any]:
  - L732 docstring: "Update status for multiple templates."
  - L733 assign template_ids = payload.get('templateIds', [])
  - L734 assign status = payload.get('status')
  - L736 if not template_ids:
    - L737 from fastapi import HTTPException
    - L738 raise HTTPException(status_code=400, detail='No template IDs provided')
  - L739 if not status:
    - L740 from fastapi import HTTPException
    - L741 raise HTTPException(status_code=400, detail='Status is required')
  - L743 assign updated = []
  - L744 assign failed = []
  - L746 for tid in template_ids:
    - L747 try:
      - L748 assign record = state_store.get_template_record(tid)
      - L749 if not record:
        - L750 expr failed.append({'id': tid, 'error': 'Template not found'})
        - L751 continue
      - L753 expr state_store.upsert_template(tid, name=record.get('name') or tid, status=status, artifacts=record.get('artifacts'), tags=record.get('tags'), connection_id=record.get('last_connection_id'), mapping_keys=record.get('mapping_keys'), template_type=record.get('kind'), description=record.get('description'))
      - L764 expr updated.append(tid)
      - L765 expr state_store.log_activity(action='template_status_updated', entity_type='template', entity_id=tid, details={'status': status})
      - L771 except Exception as e:
        - L772 expr failed.append({'id': tid, 'error': str(e)})
  - L774 return {'updated': updated, 'updatedCount': len(updated), 'failed': failed, 'failedCount': len(failed)}
- L783 async def bulk_add_tagspayload: Dict[str, Any]:
  - L784 docstring: "Add tags to multiple templates."
  - L785 assign template_ids = payload.get('templateIds', [])
  - L786 assign tags_to_add = payload.get('tags', [])
  - L788 if not template_ids:
    - L789 from fastapi import HTTPException
    - L790 raise HTTPException(status_code=400, detail='No template IDs provided')
  - L791 if not tags_to_add:
    - L792 from fastapi import HTTPException
    - L793 raise HTTPException(status_code=400, detail='No tags provided')
  - L795 assign updated = []
  - L796 assign failed = []
  - L798 for tid in template_ids:
    - L799 try:
      - L800 assign record = state_store.get_template_record(tid)
      - L801 if not record:
        - L802 expr failed.append({'id': tid, 'error': 'Template not found'})
        - L803 continue
      - L805 assign existing_tags = list(record.get('tags') or [])
      - L806 assign merged_tags = sorted(set(existing_tags + tags_to_add))
      - L808 expr state_store.upsert_template(tid, name=record.get('name') or tid, status=record.get('status') or 'draft', artifacts=record.get('artifacts'), tags=merged_tags, connection_id=record.get('last_connection_id'), mapping_keys=record.get('mapping_keys'), template_type=record.get('kind'), description=record.get('description'))
      - L819 expr updated.append(tid)
      - L820 except Exception as e:
        - L821 expr failed.append({'id': tid, 'error': str(e)})
  - L823 return {'updated': updated, 'updatedCount': len(updated), 'failed': failed, 'failedCount': len(failed)}
- L832 async def bulk_cancel_jobspayload: Dict[str, Any]:
  - L833 docstring: "Cancel multiple jobs."
  - L834 assign job_ids = payload.get('jobIds', [])
  - L835 if not job_ids:
    - L836 from fastapi import HTTPException
    - L837 raise HTTPException(status_code=400, detail='No job IDs provided')
  - L839 assign cancelled = []
  - L840 assign failed = []
  - L842 for jid in job_ids:
    - L843 try:
      - L844 assign job = state_store.get_job(jid)
      - L845 if not job:
        - L846 expr failed.append({'id': jid, 'error': 'Job not found'})
        - L847 continue
      - L849 assign status = _normalize_job_status(job.get('status'))
      - L850 if status in ('completed', 'failed', 'cancelled'):
        - L851 expr failed.append({'id': jid, 'error': f'Cannot cancel job with status: {status}'})
        - L852 continue
      - L854 expr state_store.update_job(jid, status='cancelled')
      - L855 expr cancelled.append(jid)
      - L856 expr state_store.log_activity(action='job_cancelled', entity_type='job', entity_id=jid)
      - L861 except Exception as e:
        - L862 expr failed.append({'id': jid, 'error': str(e)})
  - L864 return {'cancelled': cancelled, 'cancelledCount': len(cancelled), 'failed': failed, 'failedCount': len(failed)}
- L873 async def bulk_delete_jobspayload: Dict[str, Any]:
  - L874 docstring: "Delete multiple jobs from history."
  - L875 assign job_ids = payload.get('jobIds', [])
  - L876 if not job_ids:
    - L877 from fastapi import HTTPException
    - L878 raise HTTPException(status_code=400, detail='No job IDs provided')
  - L880 assign deleted = []
  - L881 assign failed = []
  - L883 for jid in job_ids:
    - L884 try:
      - L885 expr state_store.delete_job(jid)
      - L886 expr deleted.append(jid)
      - L887 except Exception as e:
        - L888 expr failed.append({'id': jid, 'error': str(e)})
  - L890 return {'deleted': deleted, 'deletedCount': len(deleted), 'failed': failed, 'failedCount': len(failed)}

## backend\app\api\routes\charts.py
- L1 docstring: "API routes for Auto-Chart Generation."
- L2 from __future__ import annotations
- L4 from typing import Any, Dict, List, Optional
- L5 from pydantic import BaseModel, Field
- L6 from fastapi import APIRouter, Depends, Query, Request
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.charts.service import AutoChartService
- L10 from backend.app.services.background_tasks import enqueue_background_job
- L11 from backend.app.services.state import state_store
- L13 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L16 class ChartAnalyzeRequest(BaseModel):
  - L17 annotated assign data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=100)
  - L18 annotated assign column_descriptions: Optional[Dict[str, str]] = None
  - L19 annotated assign max_suggestions: int = Field(default=3, ge=1, le=10)
- L22 class ChartGenerateRequest(BaseModel):
  - L23 annotated assign data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=1000)
  - L24 annotated assign chart_type: str
  - L25 annotated assign x_field: str
  - L26 annotated assign y_fields: List[str]
  - L27 annotated assign title: Optional[str] = None
- L30 def get_service:
  - L31 return AutoChartService()
- L35 async def analyze_for_chartspayload: ChartAnalyzeRequest, request: Request, svc: AutoChartService=Depends(get_service), background: bool=Query(False):
  - L41 docstring: "Analyze data and suggest appropriate chart visualizations."
  - L42 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L43 if not background:
    - L44 assign suggestions = svc.analyze_data_for_charts(data=payload.data, column_descriptions=payload.column_descriptions, max_suggestions=payload.max_suggestions, correlation_id=correlation_id)
    - L50 return {'status': 'ok', 'suggestions': suggestions, 'correlation_id': correlation_id}
  - L52 async def runnerjob_id: str:
    - L53 expr state_store.record_job_start(job_id)
    - L54 expr state_store.record_job_step(job_id, 'analyze', status='running', label='Analyze chart data')
    - L55 try:
      - L56 assign suggestions = svc.analyze_data_for_charts(data=payload.data, column_descriptions=payload.column_descriptions, max_suggestions=payload.max_suggestions, correlation_id=correlation_id)
      - L62 expr state_store.record_job_step(job_id, 'analyze', status='succeeded', progress=100.0)
      - L63 expr state_store.record_job_completion(job_id, status='succeeded', result={'suggestions': suggestions})
      - L68 except Exception as exc:
        - L69 expr state_store.record_job_step(job_id, 'analyze', status='failed', error=str(exc))
        - L70 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L72 assign job = await enqueue_background_job(job_type='chart_analyze', steps=[{'name': 'analyze', 'label': 'Analyze chart data'}], meta={'background': True, 'row_count': len(payload.data)}, runner=runner)
  - L78 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L82 async def generate_chart_configpayload: ChartGenerateRequest, request: Request, svc: AutoChartService=Depends(get_service), background: bool=Query(False):
  - L88 docstring: "Generate a chart configuration."
  - L89 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L90 if not background:
    - L91 assign config = svc.generate_chart_config(data=payload.data, chart_type=payload.chart_type, x_field=payload.x_field, y_fields=payload.y_fields, title=payload.title)
    - L98 return {'status': 'ok', 'chart': config, 'correlation_id': correlation_id}
  - L100 async def runnerjob_id: str:
    - L101 expr state_store.record_job_start(job_id)
    - L102 expr state_store.record_job_step(job_id, 'generate', status='running', label='Generate chart config')
    - L103 try:
      - L104 assign config = svc.generate_chart_config(data=payload.data, chart_type=payload.chart_type, x_field=payload.x_field, y_fields=payload.y_fields, title=payload.title)
      - L111 expr state_store.record_job_step(job_id, 'generate', status='succeeded', progress=100.0)
      - L112 expr state_store.record_job_completion(job_id, status='succeeded', result={'chart': config})
      - L117 except Exception as exc:
        - L118 expr state_store.record_job_step(job_id, 'generate', status='failed', error=str(exc))
        - L119 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L121 assign job = await enqueue_background_job(job_type='chart_generate', steps=[{'name': 'generate', 'label': 'Generate chart config'}], meta={'background': True, 'row_count': len(payload.data)}, runner=runner)
  - L127 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}

## backend\app\api\routes\connections.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Depends, Query, Request
- L5 from backend.app.core.security import require_api_key
- L6 from backend.app.domain.connections.repository import ConnectionRepository
- L7 from backend.app.domain.connections.schemas import ConnectionTestRequest, ConnectionUpsertRequest
- L8 from backend.app.domain.connections.service import ConnectionService
- L10 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L13 def get_service:
  - L14 return ConnectionService(ConnectionRepository())
- L18 async def test_connectionpayload: ConnectionTestRequest, request: Request, svc: ConnectionService=Depends(get_service):
  - L19 return {'status': 'ok', **svc.test(payload, getattr(request.state, 'correlation_id', None))}
- L23 async def list_connectionsrequest: Request, svc: ConnectionService=Depends(get_service):
  - L24 assign connections = svc.repo.list()
  - L25 return {'status': 'ok', 'connections': connections, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L29 async def upsert_connectionpayload: ConnectionUpsertRequest, request: Request, svc: ConnectionService=Depends(get_service):
  - L30 assign connection = svc.upsert(payload, getattr(request.state, 'correlation_id', None))
  - L31 return {'status': 'ok', 'connection': connection.dict(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L35 async def delete_connectionconnection_id: str, request: Request, svc: ConnectionService=Depends(get_service):
  - L36 expr svc.delete(connection_id)
  - L37 return {'status': 'ok', 'connection_id': connection_id, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L41 async def healthcheck_connectionconnection_id: str, request: Request, svc: ConnectionService=Depends(get_service):
  - L42 docstring: "Verify a saved connection is still accessible."
  - L43 assign result = svc.healthcheck(connection_id, getattr(request.state, 'correlation_id', None))
  - L44 return {'status': 'ok', 'connection_id': result.get('connection_id'), 'latency_ms': result.get('latency_ms'), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L53 async def connection_schemaconnection_id: str, request: Request, include_row_counts: bool=Query(True), include_foreign_keys: bool=Query(True), sample_rows: int=Query(0, ge=0, le=25):
  - L60 from src.services.connection_inspector import get_connection_schema
  - L62 assign result = get_connection_schema(connection_id, include_row_counts=include_row_counts, include_foreign_keys=include_foreign_keys, sample_rows=sample_rows)
  - L68 assign result['correlation_id'] = getattr(request.state, 'correlation_id', None)
  - L69 return result
- L73 async def connection_previewconnection_id: str, request: Request, table: str=Query(..., min_length=1), limit: int=Query(10, ge=1, le=200), offset: int=Query(0, ge=0):
  - L80 from src.services.connection_inspector import get_connection_table_preview
  - L82 assign result = get_connection_table_preview(connection_id, table=table, limit=limit, offset=offset)
  - L88 assign result['correlation_id'] = getattr(request.state, 'correlation_id', None)
  - L89 return result

## backend\app\api\routes\docqa.py
- L1 docstring: "API routes for Document Q&A Chat."
- L2 from __future__ import annotations
- L4 from typing import Optional
- L5 from pydantic import BaseModel, Field
- L6 from fastapi import APIRouter, Depends, Request, HTTPException
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.docqa.service import DocumentQAService
- L10 from backend.app.domain.docqa.schemas import AskRequest, FeedbackRequest, RegenerateRequest
- L12 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L15 class CreateSessionRequest(BaseModel):
  - L16 annotated assign name: str = Field(..., min_length=1, max_length=200)
- L19 class AddDocumentRequest(BaseModel):
  - L20 annotated assign name: str = Field(..., min_length=1, max_length=200)
  - L21 annotated assign content: str = Field(..., min_length=10, max_length=5 * 1024 * 1024)
  - L22 annotated assign page_count: Optional[int] = None
- L25 def get_service:
  - L26 return DocumentQAService()
- L30 async def create_sessionpayload: CreateSessionRequest, request: Request, svc: DocumentQAService=Depends(get_service):
  - L35 docstring: "Create a new Q&A session."
  - L36 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L37 assign session = svc.create_session(name=payload.name, correlation_id=correlation_id)
  - L41 return {'status': 'ok', 'session': session.model_dump(mode='json'), 'correlation_id': correlation_id}
- L45 async def list_sessionsrequest: Request, svc: DocumentQAService=Depends(get_service):
  - L49 docstring: "List all Q&A sessions."
  - L50 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L51 assign sessions = svc.list_sessions()
  - L52 return {'status': 'ok', 'sessions': [s.model_dump(mode='json') for s in sessions], 'correlation_id': correlation_id}
- L60 async def get_sessionsession_id: str, request: Request, svc: DocumentQAService=Depends(get_service):
  - L65 docstring: "Get a Q&A session by ID."
  - L66 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L67 assign session = svc.get_session(session_id)
  - L69 if not session:
    - L70 raise HTTPException(status_code=404, detail='Session not found')
  - L72 return {'status': 'ok', 'session': session.model_dump(mode='json'), 'correlation_id': correlation_id}
- L76 async def delete_sessionsession_id: str, request: Request, svc: DocumentQAService=Depends(get_service):
  - L81 docstring: "Delete a Q&A session."
  - L82 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L83 assign success = svc.delete_session(session_id)
  - L85 if not success:
    - L86 raise HTTPException(status_code=404, detail='Session not found')
  - L88 return {'status': 'ok', 'deleted': True, 'correlation_id': correlation_id}
- L92 async def add_documentsession_id: str, payload: AddDocumentRequest, request: Request, svc: DocumentQAService=Depends(get_service):
  - L98 docstring: "Add a document to a Q&A session."
  - L99 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L100 assign document = svc.add_document(session_id=session_id, name=payload.name, content=payload.content, page_count=payload.page_count, correlation_id=correlation_id)
  - L108 if not document:
    - L109 raise HTTPException(status_code=404, detail='Session not found')
  - L111 return {'status': 'ok', 'document': document.model_dump(mode='json'), 'correlation_id': correlation_id}
- L115 async def remove_documentsession_id: str, document_id: str, request: Request, svc: DocumentQAService=Depends(get_service):
  - L121 docstring: "Remove a document from a session."
  - L122 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L123 assign success = svc.remove_document(session_id, document_id)
  - L125 if not success:
    - L126 raise HTTPException(status_code=404, detail='Session or document not found')
  - L128 return {'status': 'ok', 'removed': True, 'correlation_id': correlation_id}
- L132 async def ask_questionsession_id: str, payload: AskRequest, request: Request, svc: DocumentQAService=Depends(get_service):
  - L138 docstring: "Ask a question about the documents in a session."
  - L139 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L141 assign session = svc.get_session(session_id)
  - L142 if not session:
    - L143 raise HTTPException(status_code=404, detail='Session not found')
  - L145 assign response = svc.ask(session_id, payload, correlation_id)
  - L147 if not response:
    - L148 raise HTTPException(status_code=500, detail='Failed to process question')
  - L150 return {'status': 'ok', 'response': response.model_dump(mode='json'), 'correlation_id': correlation_id}
- L158 async def submit_feedbacksession_id: str, message_id: str, payload: FeedbackRequest, request: Request, svc: DocumentQAService=Depends(get_service):
  - L165 docstring: "Submit feedback for a chat message."
  - L166 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L168 assign session = svc.get_session(session_id)
  - L169 if not session:
    - L170 raise HTTPException(status_code=404, detail='Session not found')
  - L172 assign message = svc.submit_feedback(session_id, message_id, payload, correlation_id)
  - L179 if not message:
    - L180 raise HTTPException(status_code=404, detail='Message not found')
  - L182 return {'status': 'ok', 'message': message.model_dump(mode='json'), 'correlation_id': correlation_id}
- L190 async def regenerate_responsesession_id: str, message_id: str, payload: RegenerateRequest, request: Request, svc: DocumentQAService=Depends(get_service):
  - L197 docstring: "Regenerate a response for a message."
  - L198 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L200 assign session = svc.get_session(session_id)
  - L201 if not session:
    - L202 raise HTTPException(status_code=404, detail='Session not found')
  - L204 try:
    - L205 assign response = svc.regenerate_response(session_id, message_id, payload, correlation_id)
    - L211 except RuntimeError as exc:
      - L212 raise HTTPException(status_code=500, detail=str(exc))
  - L214 if not response:
    - L215 raise HTTPException(status_code=404, detail='Message not found')
  - L217 return {'status': 'ok', 'response': response.model_dump(mode='json'), 'correlation_id': correlation_id}
- L225 async def get_chat_historysession_id: str, request: Request, limit: int=50, svc: DocumentQAService=Depends(get_service):
  - L231 docstring: "Get chat history for a session."
  - L232 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L234 assign session = svc.get_session(session_id)
  - L235 if not session:
    - L236 raise HTTPException(status_code=404, detail='Session not found')
  - L238 assign messages = svc.get_chat_history(session_id, limit)
  - L240 return {'status': 'ok', 'messages': [m.model_dump(mode='json') for m in messages], 'count': len(messages), 'correlation_id': correlation_id}
- L249 async def clear_chat_historysession_id: str, request: Request, svc: DocumentQAService=Depends(get_service):
  - L254 docstring: "Clear chat history for a session."
  - L255 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L257 assign success = svc.clear_history(session_id)
  - L258 if not success:
    - L259 raise HTTPException(status_code=404, detail='Session not found')
  - L261 return {'status': 'ok', 'cleared': True, 'correlation_id': correlation_id}

## backend\app\api\routes\enrichment.py
- L1 docstring: "API routes for Data Enrichment feature."
- L2 from __future__ import annotations
- L4 from typing import Any, Dict, List, Optional
- L6 from fastapi import APIRouter, Depends, Query, Request
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.enrichment.schemas import EnrichmentSourceCreate, EnrichmentSourceType, SimpleEnrichmentRequest, SimplePreviewRequest
- L15 from backend.app.domain.enrichment.service import EnrichmentService
- L17 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L20 def get_service:
  - L21 return EnrichmentService()
- L25 assign AVAILABLE_SOURCES = [{'id': 'company', 'name': 'Company Information', 'type': EnrichmentSourceType.COMPANY_INFO.value, 'description': 'Enrich with company details (industry, size, revenue)', 'required_fields': ['company_name'], 'output_fields': ['industry', 'company_size', 'estimated_revenue', 'founded_year']}, {'id': 'address', 'name': 'Address Standardization', 'type': EnrichmentSourceType.ADDRESS.value, 'description': 'Standardize and validate addresses', 'required_fields': ['address'], 'output_fields': ['formatted_address', 'city', 'state', 'postal_code', 'country']}, {'id': 'exchange', 'name': 'Currency Exchange', 'type': EnrichmentSourceType.EXCHANGE_RATE.value, 'description': 'Convert currencies to target currency', 'required_fields': ['amount', 'currency'], 'output_fields': ['converted_amount', 'exchange_rate', 'target_currency']}]
- L54 async def list_available_sourcesrequest: Request, svc: EnrichmentService=Depends(get_service):
  - L58 docstring: "List available enrichment source types."
  - L59 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L60 assign custom_sources = [source.dict() for source in svc.list_sources()]
  - L61 return {'status': 'ok', 'sources': [*AVAILABLE_SOURCES, *custom_sources], 'correlation_id': correlation_id}
- L69 async def list_source_typesrequest: Request, svc: EnrichmentService=Depends(get_service):
  - L73 docstring: "List available enrichment source types (legacy endpoint)."
  - L74 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L75 assign source_types = svc.get_available_source_types()
  - L76 return {'status': 'ok', 'source_types': source_types, 'correlation_id': correlation_id}
- L84 async def enrich_datapayload: SimpleEnrichmentRequest, request: Request, svc: EnrichmentService=Depends(get_service):
  - L89 docstring: "Enrich data with additional information using selected sources."
  - L90 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L91 assign result = await svc.simple_enrich(data=payload.data, sources=payload.sources, options=payload.options, correlation_id=correlation_id)
  - L97 return {'status': 'ok', 'enriched_data': result['enriched_data'], 'total_rows': result['total_rows'], 'enriched_rows': result['enriched_rows'], 'processing_time_ms': result['processing_time_ms'], 'correlation_id': correlation_id}
- L108 async def preview_enrichmentpayload: SimplePreviewRequest, request: Request, svc: EnrichmentService=Depends(get_service):
  - L113 docstring: "Preview enrichment results on a sample."
  - L114 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L115 assign result = await svc.simple_preview(data=payload.data, sources=payload.sources, sample_size=payload.sample_size, correlation_id=correlation_id)
  - L121 return {'status': 'ok', 'preview': result['preview'], 'total_rows': result['total_rows'], 'enriched_rows': result['enriched_rows'], 'processing_time_ms': result['processing_time_ms'], 'correlation_id': correlation_id}
- L132 async def create_sourcepayload: EnrichmentSourceCreate, request: Request, svc: EnrichmentService=Depends(get_service):
  - L137 docstring: "Create a custom enrichment source."
  - L138 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L139 assign source = svc.create_source(payload, correlation_id)
  - L140 return {'status': 'ok', 'source': source.dict(), 'correlation_id': correlation_id}
- L148 async def get_sourcesource_id: str, request: Request, svc: EnrichmentService=Depends(get_service):
  - L153 docstring: "Get an enrichment source by ID."
  - L154 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L156 for source in AVAILABLE_SOURCES:
    - L157 if source['id'] == source_id:
      - L158 return {'status': 'ok', 'source': source, 'correlation_id': correlation_id}
  - L164 assign source = svc.get_source(source_id)
  - L165 if not source:
    - L166 return {'status': 'error', 'code': 'not_found', 'message': 'Source not found', 'correlation_id': correlation_id}
  - L172 return {'status': 'ok', 'source': source.dict(), 'correlation_id': correlation_id}
- L180 async def delete_sourcesource_id: str, request: Request, svc: EnrichmentService=Depends(get_service):
  - L185 docstring: "Delete a custom enrichment source."
  - L186 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L187 assign deleted = svc.delete_source(source_id)
  - L188 return {'status': 'ok' if deleted else 'error', 'deleted': deleted, 'source_id': source_id, 'correlation_id': correlation_id}
- L197 async def get_cache_statsrequest: Request, svc: EnrichmentService=Depends(get_service):
  - L201 docstring: "Get enrichment cache statistics."
  - L202 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L203 assign stats = svc.get_cache_stats()
  - L204 return {'status': 'ok', 'stats': stats, 'correlation_id': correlation_id}
- L212 async def clear_cacherequest: Request, source_id: Optional[str]=Query(None, max_length=64), svc: EnrichmentService=Depends(get_service):
  - L217 docstring: "Clear enrichment cache."
  - L218 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L219 assign cleared = svc.clear_cache(source_id)
  - L220 return {'status': 'ok', 'cleared_entries': cleared, 'source_id': source_id, 'correlation_id': correlation_id}

## backend\app\api\routes\excel.py
- L1 docstring: "Excel Template API Routes.\n\nThis module contains endpoints for Excel template ..."
- L9 from __future__ import annotations
- L11 import contextlib
- L12 import logging
- L13 import os
- L14 import tempfile
- L15 from pathlib import Path
- L16 from types import SimpleNamespace
- L17 from typing import Optional
- L19 from fastapi import APIRouter, Depends, File, Form, HTTPException, Query, Request, UploadFile
- L21 from backend.app.core.security import require_api_key
- L22 from backend.app.features.generate.schemas.charts import ChartSuggestPayload, SavedChartCreatePayload, SavedChartUpdatePayload
- L27 from backend.app.features.generate.schemas.reports import RunPayload, DiscoverPayload
- L28 from backend.app.features.generate.services.chart_suggestions_service import suggest_charts as suggest_charts_service
- L29 from backend.app.features.generate.services.saved_charts_service import create_saved_chart as create_saved_chart_service, delete_saved_chart as delete_saved_chart_service, list_saved_charts as list_saved_charts_service, update_saved_chart as update_saved_chart_service
- L35 from backend.app.services.background_tasks import enqueue_background_job, iter_ndjson_events_async, run_event_stream_async
- L40 from backend.app.services.contract.ContractBuilderV2 import load_contract_v2
- L41 from backend.app.services.prompts.llm_prompts_charts import CHART_SUGGEST_PROMPT_VERSION, build_chart_suggestions_prompt
- L45 from backend.app.services.reports.discovery_excel import discover_batches_and_counts as discover_batches_and_counts_excel
- L46 from backend.app.services.reports.discovery_metrics import build_batch_field_catalog_and_stats, build_batch_metrics
- L50 from backend.app.services.state import state_store
- L51 from backend.app.services.templates.TemplateVerify import get_openai_client
- L52 from backend.app.services.utils import call_chat_completion, get_correlation_id, strip_code_fences
- L54 from src.services.template_service import verify_excel, generator_assets
- L55 from src.services.mapping.approve import run_mapping_approve
- L56 from src.services.mapping.corrections import run_corrections_preview
- L57 from src.services.mapping.key_options import mapping_key_options as mapping_key_options_service
- L58 from src.services.mapping.preview import run_mapping_preview
- L59 from src.services.file_service import artifact_head_response, artifact_manifest_response
- L60 from src.services.report_service import queue_report_job, run_report as run_report_service
- L61 from src.schemas.template_schema import CorrectionsPreviewPayload, GeneratorAssetsPayload, MappingPayload
- L62 from src.utils.connection_utils import db_path_from_payload_or_default
- L63 from src.utils.schedule_utils import clean_key_values
- L64 from src.utils.template_utils import normalize_template_id, template_dir
- L66 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L69 def _correlationrequest: Request:
  - L70 return getattr(request.state, 'correlation_id', None)
- L73 def _request_with_correlationcorrelation_id: str | None:
  - L74 return SimpleNamespace(state=SimpleNamespace(correlation_id=correlation_id))
- L77 def _wrappayload: dict, correlation_id: str | None:
  - L78 assign payload = dict(payload)
  - L79 if correlation_id is not None:
    - L80 assign payload['correlation_id'] = correlation_id
  - L81 return payload
- L84 def _ensure_template_existstemplate_id: str:
  - L85 assign normalized = normalize_template_id(template_id)
  - L86 assign record = state_store.get_template_record(normalized)
  - L87 if not record:
    - L88 raise HTTPException(status_code=404, detail='template_not_found')
  - L89 return (normalized, record)
- L92 async def _persist_uploadfile: UploadFile, suffix: str:
  - L93 assign filename = Path(file.filename or f'upload{suffix}').name
  - L94 assign tmp = tempfile.NamedTemporaryFile(prefix='nr-upload-', suffix=suffix, delete=False)
  - L95 try:
    - L96 with tmp:
      - L97 expr file.file.seek(0)
      - L98 while True:
        - L99 assign chunk = file.file.read(1024 * 1024)
        - L100 if not chunk:
          - L101 break
        - L102 expr tmp.write(chunk)
    - L104 finally:
      - L104 with contextlib.suppress(Exception):
        - L105 expr await file.close()
  - L106 return (Path(tmp.name), filename)
- L114 async def verify_excel_routerequest: Request, file: UploadFile=File(...), connection_id: str | None=Form(None), background: bool=Query(False):
  - L120 docstring: "Verify and process an Excel template."
  - L121 if not background:
    - L122 return verify_excel(file=file, request=request, connection_id=connection_id)
  - L124 assign (upload_path, filename) = await _persist_upload(file, suffix='.xlsx')
  - L125 assign correlation_id = _correlation(request)
  - L126 assign template_name = Path(filename).stem or filename
  - L128 async def runnerjob_id: str:
    - L129 assign upload = UploadFile(filename=filename, file=upload_path.open('rb'))
    - L130 try:
      - L131 assign response = verify_excel(file=upload, request=_request_with_correlation(correlation_id), connection_id=connection_id)
      - L136 expr await run_event_stream_async(job_id, iter_ndjson_events_async(response.body_iterator))
      - L138 finally:
        - L138 with contextlib.suppress(Exception):
          - L139 expr await upload.close()
        - L140 with contextlib.suppress(FileNotFoundError):
          - L141 expr upload_path.unlink(missing_ok=True)
  - L143 assign job = await enqueue_background_job(job_type='verify_excel', connection_id=connection_id, template_name=template_name, template_kind='excel', meta={'filename': filename, 'background': True}, runner=runner)
  - L151 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L159 async def mapping_preview_exceltemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False:
  - L160 docstring: "Preview mapping for an Excel template."
  - L161 return await run_mapping_preview(template_id, connection_id, request, force_refresh, kind='excel')
- L165 async def mapping_approve_exceltemplate_id: str, payload: MappingPayload, request: Request:
  - L166 docstring: "Approve mapping for an Excel template."
  - L167 return await run_mapping_approve(template_id, payload, request, kind='excel')
- L171 def mapping_corrections_preview_exceltemplate_id: str, payload: CorrectionsPreviewPayload, request: Request:
  - L172 docstring: "Preview corrections for Excel template mapping."
  - L173 return run_corrections_preview(template_id, payload, request, kind='excel')
- L181 def generator_assets_excel_routetemplate_id: str, payload: GeneratorAssetsPayload, request: Request:
  - L182 docstring: "Generate assets for an Excel template."
  - L183 return generator_assets(template_id, payload, request, kind='excel')
- L191 def mapping_key_options_exceltemplate_id: str, request: Request, connection_id: str | None=None, tokens: str | None=None, limit: int=500, start_date: str | None=None, end_date: str | None=None, debug: bool=False:
  - L201 docstring: "Get available key options for Excel template filtering."
  - L202 return mapping_key_options_service(template_id=template_id, request=request, connection_id=connection_id, tokens=tokens, limit=limit, start_date=start_date, end_date=end_date, kind='excel', debug=debug)
- L220 def get_artifact_manifest_exceltemplate_id: str, request: Request:
  - L221 docstring: "Get the artifact manifest for an Excel template."
  - L222 assign data = artifact_manifest_response(template_id, kind='excel')
  - L223 return _wrap(data, _correlation(request))
- L227 def get_artifact_head_exceltemplate_id: str, request: Request, name: str:
  - L228 docstring: "Get the head (preview) of a specific artifact."
  - L229 assign data = artifact_head_response(template_id, name, kind='excel')
  - L230 return _wrap(data, _correlation(request))
- L238 def suggest_charts_excel_routetemplate_id: str, payload: ChartSuggestPayload, request: Request:
  - L239 docstring: "Get chart suggestions for an Excel template."
  - L240 assign correlation_id = _correlation(request) or get_correlation_id()
  - L241 assign logger = logging.getLogger('neura.api')
  - L242 return suggest_charts_service(template_id, payload, kind='excel', correlation_id=correlation_id, template_dir_fn=lambda tpl: template_dir(tpl, kind='excel'), db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_fn=discover_batches_and_counts_excel, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_metrics_fn=build_batch_metrics, build_prompt_fn=build_chart_suggestions_prompt, call_chat_completion_fn=lambda **kwargs: call_chat_completion(get_openai_client(), **kwargs, description=CHART_SUGGEST_PROMPT_VERSION), model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini'), strip_code_fences_fn=strip_code_fences, logger=logger)
- L265 def list_saved_charts_excel_routetemplate_id: str, request: Request:
  - L266 docstring: "List saved charts for an Excel template."
  - L267 assign payload = list_saved_charts_service(template_id, _ensure_template_exists)
  - L268 return _wrap(payload, _correlation(request))
- L272 def create_saved_chart_excel_routetemplate_id: str, payload: SavedChartCreatePayload, request: Request:
  - L277 docstring: "Create a saved chart for an Excel template."
  - L278 assign chart = create_saved_chart_service(template_id, payload, ensure_template_exists=_ensure_template_exists, normalize_template_id=normalize_template_id)
  - L284 assign chart_payload = chart.model_dump(mode='json') if hasattr(chart, 'model_dump') else chart
  - L285 return _wrap(chart_payload, _correlation(request))
- L289 def update_saved_chart_excel_routetemplate_id: str, chart_id: str, payload: SavedChartUpdatePayload, request: Request:
  - L295 docstring: "Update a saved chart for an Excel template."
  - L296 assign chart = update_saved_chart_service(template_id, chart_id, payload, _ensure_template_exists)
  - L297 assign chart_payload = chart.model_dump(mode='json') if hasattr(chart, 'model_dump') else chart
  - L298 return _wrap(chart_payload, _correlation(request))
- L302 def delete_saved_chart_excel_routetemplate_id: str, chart_id: str, request: Request:
  - L307 docstring: "Delete a saved chart for an Excel template."
  - L308 assign payload = delete_saved_chart_service(template_id, chart_id, _ensure_template_exists)
  - L309 return _wrap(payload, _correlation(request))
- L317 def run_report_excelpayload: RunPayload, request: Request:
  - L318 docstring: "Run an Excel report synchronously."
  - L319 return run_report_service(payload, request, kind='excel')
- L323 async def enqueue_report_job_excelpayload: RunPayload | list[RunPayload], request: Request:
  - L324 docstring: "Queue an Excel report job for async generation."
  - L325 return await queue_report_job(payload, request, kind='excel')
- L333 def discover_reports_excelpayload: DiscoverPayload, request: Request:
  - L334 docstring: "Discover available batches for Excel report generation."
  - L335 from backend.app.features.generate.services.discovery_service import discover_reports as discover_reports_service
  - L336 from backend.app.services.utils.artifacts import load_manifest
  - L337 from src.utils.template_utils import manifest_endpoint
  - L339 assign logger = logging.getLogger('neura.api')
  - L340 return discover_reports_service(payload, kind='excel', template_dir_fn=lambda tpl: template_dir(tpl, kind='excel'), db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_fn=discover_batches_and_counts_excel, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_batch_metrics_fn=build_batch_metrics, load_manifest_fn=load_manifest, manifest_endpoint_fn=lambda tpl: manifest_endpoint(tpl, kind='excel'), logger=logger)

## backend\app\api\routes\federation.py
- L1 docstring: "API routes for Cross-Database Federation feature."
- L2 from __future__ import annotations
- L4 from fastapi import APIRouter, Depends, Request
- L6 from backend.app.core.security import require_api_key
- L7 from backend.app.domain.federation.schemas import VirtualSchemaCreate, SuggestJoinsRequest, FederatedQueryRequest
- L8 from backend.app.domain.federation.service import FederationService
- L10 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L13 def get_service:
  - L14 return FederationService()
- L18 async def create_virtual_schemapayload: VirtualSchemaCreate, request: Request, svc: FederationService=Depends(get_service):
  - L23 docstring: "Create a virtual schema spanning multiple databases."
  - L24 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L25 assign schema = svc.create_virtual_schema(payload, correlation_id)
  - L26 return {'status': 'ok', 'schema': schema.dict(), 'correlation_id': correlation_id}
- L30 async def list_virtual_schemasrequest: Request, svc: FederationService=Depends(get_service):
  - L34 docstring: "List all virtual schemas."
  - L35 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L36 assign schemas = svc.list_virtual_schemas()
  - L37 return {'status': 'ok', 'schemas': [s.dict() for s in schemas], 'correlation_id': correlation_id}
- L41 async def get_virtual_schemaschema_id: str, request: Request, svc: FederationService=Depends(get_service):
  - L46 docstring: "Get a virtual schema by ID."
  - L47 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L48 assign schema = svc.get_virtual_schema(schema_id)
  - L49 if not schema:
    - L50 return {'status': 'error', 'code': 'not_found', 'correlation_id': correlation_id}
  - L51 return {'status': 'ok', 'schema': schema.dict(), 'correlation_id': correlation_id}
- L55 async def delete_virtual_schemaschema_id: str, request: Request, svc: FederationService=Depends(get_service):
  - L60 docstring: "Delete a virtual schema."
  - L61 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L62 assign deleted = svc.delete_virtual_schema(schema_id)
  - L63 return {'status': 'ok' if deleted else 'error', 'deleted': deleted, 'correlation_id': correlation_id}
- L67 async def suggest_joinspayload: SuggestJoinsRequest, request: Request, svc: FederationService=Depends(get_service):
  - L72 docstring: "Get AI-suggested joins between tables in different connections."
  - L73 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L74 assign suggestions = svc.suggest_joins(payload.connection_ids, correlation_id)
  - L75 return {'status': 'ok', 'suggestions': [s.dict() for s in suggestions], 'correlation_id': correlation_id}
- L79 async def execute_federated_querypayload: FederatedQueryRequest, request: Request, svc: FederationService=Depends(get_service):
  - L84 docstring: "Execute a federated query across multiple databases."
  - L85 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L86 assign result = svc.execute_query(payload, correlation_id)
  - L87 return {'status': 'ok', 'result': result, 'correlation_id': correlation_id}

## backend\app\api\routes\health.py
- L1 from __future__ import annotations
- L3 import os
- L4 import time
- L5 from datetime import datetime, timezone
- L6 from pathlib import Path
- L7 from typing import Any, Dict, Optional
- L9 from fastapi import APIRouter, Request
- L11 from backend.app.core.config import get_settings
- L12 from backend.app.core.middleware import limiter
- L13 from backend.app.features.analyze.services.document_analysis_service import _ANALYSIS_CACHE
- L14 from backend.app.services.utils.mailer import MAILER_CONFIG, refresh_mailer_config
- L16 assign router = APIRouter()
- L19 def _check_directory_accesspath: Path:
  - L20 docstring: "Check if a directory is accessible for read/write operations."
  - L21 try:
    - L22 if not path.exists():
      - L23 return {'status': 'warning', 'message': 'Directory does not exist', 'path': str(path)}
    - L24 if not path.is_dir():
      - L25 return {'status': 'error', 'message': 'Path is not a directory', 'path': str(path)}
    - L27 expr list(path.iterdir())
    - L29 assign test_file = path / f'.health_check_{os.getpid()}'
    - L30 try:
      - L31 expr test_file.write_text('health check')
      - L32 expr test_file.unlink()
      - L33 return {'status': 'healthy', 'path': str(path), 'writable': True}
      - L34 except (OSError, PermissionError):
        - L35 return {'status': 'warning', 'path': str(path), 'writable': False, 'message': 'Read-only access'}
    - L36 except Exception as e:
      - L37 return {'status': 'error', 'message': str(e), 'path': str(path)}
- L40 def _check_openai_connection:
  - L41 docstring: "Check if OpenAI API is configured and accessible."
  - L42 assign settings = get_settings()
  - L43 if not settings.openai_api_key:
    - L44 return {'status': 'not_configured', 'message': 'OPENAI_API_KEY not set'}
  - L46 try:
    - L47 from backend.app.services.templates.TemplateVerify import get_openai_client
    - L48 assign client = get_openai_client()
    - L50 if client is not None:
      - L51 return {'status': 'configured', 'message': 'OpenAI client initialized', 'key_prefix': settings.openai_api_key[:8] + '...' if settings.openai_api_key else None}
    - L56 return {'status': 'error', 'message': 'Failed to initialize OpenAI client'}
    - L57 except Exception as e:
      - L58 return {'status': 'error', 'message': str(e)}
- L61 def _get_memory_usage:
  - L62 docstring: "Get current process memory usage."
  - L63 try:
    - L64 import resource
    - L65 assign usage = resource.getrusage(resource.RUSAGE_SELF)
    - L66 return {'max_rss_mb': usage.ru_maxrss / 1024 if hasattr(usage, 'ru_maxrss') else None, 'status': 'healthy'}
    - L70 except ImportError:
      - L72 try:
        - L73 import psutil
        - L74 assign process = psutil.Process(os.getpid())
        - L75 assign mem_info = process.memory_info()
        - L76 return {'rss_mb': mem_info.rss / 1024 / 1024, 'vms_mb': mem_info.vms / 1024 / 1024, 'status': 'healthy'}
        - L81 except ImportError:
          - L82 return {'status': 'unknown', 'message': 'Memory stats not available'}
- L87 async def healthrequest: Request:
  - L88 docstring: "Basic health check - fast, for load balancer probes."
  - L89 return {'status': 'ok', 'timestamp': datetime.now(timezone.utc).isoformat(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L98 async def healthz:
  - L99 docstring: "Kubernetes-style liveness probe."
  - L100 return {'status': 'ok'}
- L105 async def ready:
  - L106 docstring: "Kubernetes-style readiness probe - checks if app can serve requests."
  - L107 assign settings = get_settings()
  - L108 assign checks = {}
  - L109 assign overall_status = 'ready'
  - L112 assign uploads_check = _check_directory_access(settings.uploads_dir)
  - L113 assign checks['uploads_dir'] = uploads_check
  - L114 if uploads_check['status'] == 'error':
    - L115 assign overall_status = 'not_ready'
  - L118 assign state_check = _check_directory_access(settings.state_dir)
  - L119 assign checks['state_dir'] = state_check
  - L120 if state_check['status'] == 'error':
    - L121 assign overall_status = 'not_ready'
  - L123 return {'status': overall_status, 'checks': checks}
- L131 async def readyz:
  - L132 docstring: "Compatibility alias for readiness probe."
  - L133 return await ready()
- L138 async def token_usagerequest: Request:
  - L139 docstring: "Get LLM token usage statistics."
  - L140 try:
    - L141 from backend.app.services.llm.client import get_global_usage_stats
    - L142 assign stats = get_global_usage_stats()
    - L143 return {'status': 'ok', 'usage': stats, 'correlation_id': getattr(request.state, 'correlation_id', None)}
    - L148 except Exception as e:
      - L149 return {'status': 'error', 'message': str(e), 'usage': {'total_input_tokens': 0, 'total_output_tokens': 0, 'total_tokens': 0, 'estimated_cost_usd': 0.0, 'request_count': 0}, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L165 async def health_detailedrequest: Request:
  - L166 docstring: "Comprehensive health check with all dependencies."
  - L167 assign settings = get_settings()
  - L168 assign started = time.time()
  - L170 annotated assign checks: Dict[str, Any] = {}
  - L171 annotated assign issues: list[str] = []
  - L174 assign checks['uploads_dir'] = _check_directory_access(settings.uploads_dir)
  - L175 if checks['uploads_dir']['status'] == 'error':
    - L176 expr issues.append('Uploads directory not accessible')
  - L178 assign checks['excel_uploads_dir'] = _check_directory_access(settings.excel_uploads_dir)
  - L180 assign checks['state_dir'] = _check_directory_access(settings.state_dir)
  - L181 if checks['state_dir']['status'] == 'error':
    - L182 expr issues.append('State directory not accessible')
  - L185 assign checks['openai'] = _check_openai_connection()
  - L186 if checks['openai']['status'] == 'error':
    - L187 expr issues.append('OpenAI API error')
  - L190 assign checks['analysis_cache'] = {'status': 'healthy', 'current_size': _ANALYSIS_CACHE.size(), 'max_size': _ANALYSIS_CACHE.max_items, 'ttl_seconds': _ANALYSIS_CACHE.ttl_seconds}
  - L198 assign checks['memory'] = _get_memory_usage()
  - L201 assign checks['configuration'] = {'api_key_configured': settings.api_key is not None, 'rate_limiting_enabled': settings.rate_limit_enabled, 'rate_limit': f'{settings.rate_limit_requests}/{settings.rate_limit_window_seconds}s', 'request_timeout': settings.request_timeout_seconds, 'max_upload_size_mb': settings.max_upload_bytes / 1024 / 1024, 'max_zip_entries': settings.max_zip_entries, 'max_zip_uncompressed_mb': settings.max_zip_uncompressed_bytes / 1024 / 1024, 'template_import_max_concurrency': settings.template_import_max_concurrency, 'analysis_max_concurrency': settings.analysis_max_concurrency, 'debug_mode': settings.debug_mode}
  - L214 assign elapsed_ms = int((time.time() - started) * 1000)
  - L216 assign overall_status = 'healthy' if not issues else 'degraded'
  - L218 return {'status': overall_status, 'version': settings.api_version, 'timestamp': datetime.now(timezone.utc).isoformat(), 'response_time_ms': elapsed_ms, 'checks': checks, 'issues': issues if issues else None, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L229 def _check_email_config:
  - L230 docstring: "Check email/SMTP configuration status."
  - L231 assign config = MAILER_CONFIG
  - L232 annotated assign result: Dict[str, Any] = {'enabled': config.enabled, 'host_configured': bool(config.host), 'sender_configured': bool(config.sender), 'auth_configured': bool(config.username and config.password), 'use_tls': config.use_tls, 'port': config.port}
  - L241 if not config.enabled:
    - L242 assign result['status'] = 'not_configured'
    - L243 assign missing = []
    - L244 if not config.host:
      - L245 expr missing.append('NEURA_MAIL_HOST')
    - L246 if not config.sender:
      - L247 expr missing.append('NEURA_MAIL_SENDER')
    - L248 assign result['missing_env_vars'] = missing
    - L249 assign result['message'] = f"Email disabled. Set {', '.join(missing)} to enable."
    - L251 else:
      - L251 assign result['status'] = 'configured'
      - L252 assign result['host'] = config.host
      - L254 if config.sender:
        - L255 assign parts = config.sender.split('@')
        - L256 if len(parts) == 2:
          - L257 assign masked = parts[0][:3] + '***@' + parts[1]
          - L258 assign result['sender_masked'] = masked
          - L260 else:
            - L260 assign result['sender_masked'] = config.sender[:5] + '***'
  - L262 return result
- L265 def _test_smtp_connection:
  - L266 docstring: "Attempt to connect to SMTP server (without sending email)."
  - L267 assign config = MAILER_CONFIG
  - L268 if not config.enabled or not config.host:
    - L269 return {'status': 'skipped', 'reason': 'email_not_configured'}
  - L271 import smtplib
  - L272 import ssl
  - L274 try:
    - L275 if config.use_tls:
      - L276 with smtplib.SMTP(config.host, config.port, timeout=10) as client:
        - L277 expr client.ehlo()
        - L278 assign context = ssl.create_default_context()
        - L279 expr client.starttls(context=context)
        - L280 expr client.ehlo()
        - L281 if config.username and config.password:
          - L282 expr client.login(config.username, config.password)
        - L283 return {'status': 'connected', 'message': 'SMTP connection successful'}
      - L285 else:
        - L285 with smtplib.SMTP(config.host, config.port, timeout=10) as client:
          - L286 expr client.ehlo()
          - L287 if config.username and config.password:
            - L288 expr client.login(config.username, config.password)
          - L289 return {'status': 'connected', 'message': 'SMTP connection successful'}
    - L290 except smtplib.SMTPAuthenticationError as e:
      - L291 return {'status': 'auth_failed', 'error': str(e), 'message': 'SMTP authentication failed'}
    - L292 except smtplib.SMTPConnectError as e:
      - L293 return {'status': 'connection_failed', 'error': str(e), 'message': 'Could not connect to SMTP server'}
    - L294 except Exception as e:
      - L295 return {'status': 'error', 'error': str(e), 'message': 'SMTP connection test failed'}
- L300 async def email_healthrequest: Request:
  - L301 docstring: "Check email/SMTP configuration and optionally test connection."
  - L302 assign config_status = _check_email_config()
  - L303 return {'status': 'ok' if config_status.get('status') == 'configured' else 'warning', 'email': config_status, 'timestamp': datetime.now(timezone.utc).isoformat(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L313 async def email_connection_testrequest: Request:
  - L314 docstring: "Test SMTP connection (without sending an email)."
  - L315 assign config_status = _check_email_config()
  - L316 assign connection_test = _test_smtp_connection()
  - L318 assign overall_status = 'ok'
  - L319 if config_status.get('status') != 'configured':
    - L320 assign overall_status = 'warning'
    - L321 else:
      - L321 if connection_test.get('status') != 'connected':
        - L322 assign overall_status = 'error'
  - L324 return {'status': overall_status, 'email': config_status, 'connection_test': connection_test, 'timestamp': datetime.now(timezone.utc).isoformat(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L335 async def refresh_email_configrequest: Request:
  - L336 docstring: "Refresh email configuration from environment variables."
  - L337 expr refresh_mailer_config()
  - L338 assign config_status = _check_email_config()
  - L339 return {'status': 'ok', 'message': 'Email configuration refreshed', 'email': config_status, 'timestamp': datetime.now(timezone.utc).isoformat(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L350 async def scheduler_healthrequest: Request:
  - L351 docstring: "Check scheduler status with detailed information."
  - L352 assign scheduler_disabled = os.getenv('NEURA_SCHEDULER_DISABLED', 'false').lower() == 'true'
  - L353 assign poll_interval = int(os.getenv('NEURA_SCHEDULER_INTERVAL', '60') or '60')
  - L356 assign scheduler_running = False
  - L357 annotated assign inflight_jobs: list[str] = []
  - L358 assign scheduler_instance = None
  - L360 try:
    - L361 import backend.api as api_module
    - L362 assign scheduler_instance = getattr(api_module, 'SCHEDULER', None)
    - L363 if scheduler_instance is not None:
      - L364 assign scheduler_running = scheduler_instance._task is not None and (not scheduler_instance._task.done())
      - L365 assign inflight_jobs = list(scheduler_instance._inflight)
    - L366 except Exception:
      - L367 pass
  - L370 assign schedules_info = {'total': 0, 'active': 0, 'next_run': None}
  - L371 try:
    - L372 from backend.app.services.state import state_store
    - L373 assign schedules = state_store.list_schedules()
    - L374 assign schedules_info['total'] = len(schedules)
    - L375 assign schedules_info['active'] = sum((1 for s in schedules if s.get('active', True)))
    - L378 assign now = datetime.now(timezone.utc)
    - L379 assign next_runs = []
    - L380 for s in schedules:
      - L381 if s.get('active', True) and s.get('next_run_at'):
        - L382 try:
          - L383 assign next_run = datetime.fromisoformat(s['next_run_at'].replace('Z', '+00:00'))
          - L384 expr next_runs.append((next_run, s.get('name', s.get('id'))))
          - L385 except Exception:
            - L386 pass
    - L388 if next_runs:
      - L389 expr next_runs.sort(key=lambda x: x[0])
      - L390 assign (next_run_time, next_run_name) = next_runs[0]
      - L391 assign schedules_info['next_run'] = {'schedule_name': next_run_name, 'next_run_at': next_run_time.isoformat(), 'in_seconds': max(0, int((next_run_time - now).total_seconds()))}
    - L396 except Exception:
      - L397 pass
  - L399 assign status = 'ok'
  - L400 assign message = None
  - L401 if scheduler_disabled:
    - L402 assign status = 'disabled'
    - L403 assign message = 'Scheduler is disabled via NEURA_SCHEDULER_DISABLED environment variable'
    - L404 else:
      - L404 if not scheduler_running:
        - L405 assign status = 'warning'
        - L406 assign message = 'Scheduler is enabled but not currently running'
  - L408 return {'status': status, 'message': message, 'scheduler': {'enabled': not scheduler_disabled, 'running': scheduler_running, 'poll_interval_seconds': poll_interval, 'inflight_jobs': inflight_jobs}, 'schedules': schedules_info, 'timestamp': datetime.now(timezone.utc).isoformat(), 'correlation_id': getattr(request.state, 'correlation_id', None)}

## backend\app\api\routes\jobs.py
- L1 docstring: "Jobs API Routes.\n\nThis module contains endpoints for job management:\n- List j..."
- L9 from __future__ import annotations
- L11 from typing import Any, Dict, List, Optional
- L13 from fastapi import APIRouter, Depends, HTTPException, Query, Request
- L15 from backend.app.core.security import require_api_key
- L16 from src.services.scheduler_service import get_job, list_active_jobs, list_jobs, cancel_job
- L18 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L21 def _normalize_job_statusstatus: Optional[str]:
  - L22 docstring: "Normalize job status to consistent UI-friendly values.\n\n    Maps backend statu..."
  - L27 assign value = (status or '').strip().lower()
  - L28 if value in {'succeeded', 'success', 'done'}:
    - L29 return 'completed'
  - L30 if value in {'queued'}:
    - L31 return 'pending'
  - L32 if value in {'in_progress', 'started'}:
    - L33 return 'running'
  - L34 if value in {'error'}:
    - L35 return 'failed'
  - L36 if value in {'canceled'}:
    - L37 return 'cancelled'
  - L39 if value in {'pending', 'running', 'completed', 'failed', 'cancelled', 'cancelling'}:
    - L40 return value
  - L41 return value or 'pending'
- L44 def _normalize_jobjob: Optional[Dict[str, Any]]:
  - L45 docstring: "Normalize a job record for consistent API responses."
  - L46 if not job:
    - L47 return job
  - L48 assign normalized = dict(job)
  - L49 if 'status' in normalized:
    - L50 assign normalized['status'] = _normalize_job_status(normalized['status'])
  - L51 if 'state' in normalized and 'status' not in normalized:
    - L52 assign normalized['status'] = _normalize_job_status(normalized['state'])
  - L53 return normalized
- L56 def _correlationrequest: Request:
  - L57 return getattr(request.state, 'correlation_id', None)
- L61 def list_jobs_routerequest: Request, status: Optional[List[str]]=Query(None), job_type: Optional[List[str]]=Query(None, alias='type'), limit: int=Query(50, ge=1, le=200), active_only: bool=Query(False):
  - L68 docstring: "List jobs with optional filtering by status and type."
  - L69 assign jobs = list_jobs(status, job_type, limit, active_only)
  - L70 assign normalized_jobs = [_normalize_job(job) for job in jobs] if jobs else []
  - L71 return {'jobs': normalized_jobs, 'correlation_id': _correlation(request)}
- L75 def list_active_jobs_routerequest: Request, limit: int=Query(20, ge=1, le=200):
  - L76 docstring: "List only active (non-completed) jobs."
  - L77 assign jobs = list_active_jobs(limit)
  - L78 assign normalized_jobs = [_normalize_job(job) for job in jobs] if jobs else []
  - L79 return {'jobs': normalized_jobs, 'correlation_id': _correlation(request)}
- L83 def get_job_routejob_id: str, request: Request:
  - L84 docstring: "Get details for a specific job."
  - L85 assign job = get_job(job_id)
  - L86 return {'job': _normalize_job(job), 'correlation_id': _correlation(request)}
- L90 def cancel_job_routejob_id: str, request: Request, force: bool=Query(False):
  - L91 docstring: "Cancel a running job."
  - L92 assign job = cancel_job(job_id, force=force)
  - L93 return {'job': _normalize_job(job), 'correlation_id': _correlation(request)}
- L97 async def retry_job_routejob_id: str, request: Request:
  - L98 docstring: "Retry a failed job by re-queuing it with the same parameters.\n\n    Only jobs w..."
  - L102 from src.services.report_service import queue_report_job
  - L103 from backend.app.features.generate.schemas.reports import RunPayload
  - L105 assign original_job = get_job(job_id)
  - L106 if not original_job:
    - L107 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'job_not_found', 'message': 'Job not found'})
  - L112 assign normalized_status = _normalize_job_status(original_job.get('status'))
  - L113 if normalized_status != 'failed':
    - L114 raise HTTPException(status_code=400, detail={'status': 'error', 'code': 'invalid_job_status', 'message': f'Only failed jobs can be retried. Current status: {normalized_status}'})
  - L124 assign meta = original_job.get('meta') or original_job.get('metadata') or {}
  - L125 assign template_id = original_job.get('template_id') or meta.get('template_id')
  - L126 assign connection_id = original_job.get('connection_id') or meta.get('connection_id')
  - L127 assign start_date = meta.get('start_date') or original_job.get('start_date')
  - L128 assign end_date = meta.get('end_date') or original_job.get('end_date')
  - L129 assign key_values = meta.get('key_values') or original_job.get('key_values')
  - L130 assign batch_ids = meta.get('batch_ids') or original_job.get('batch_ids')
  - L131 assign docx = meta.get('docx', False)
  - L132 assign xlsx = meta.get('xlsx', False)
  - L133 assign template_name = original_job.get('template_name') or meta.get('template_name')
  - L134 assign kind = original_job.get('template_kind') or meta.get('kind') or 'pdf'
  - L136 if not template_id:
    - L137 raise HTTPException(status_code=400, detail={'status': 'error', 'code': 'missing_template_id', 'message': 'Cannot retry job: missing template_id'})
  - L147 assign payload = RunPayload(template_id=template_id, connection_id=connection_id, start_date=start_date, end_date=end_date, key_values=key_values, batch_ids=batch_ids, docx=docx, xlsx=xlsx, template_name=template_name)
  - L160 assign result = await queue_report_job(payload, request, kind=kind)
  - L162 return {'status': 'ok', 'message': 'Job retry queued successfully', 'original_job_id': job_id, 'new_job': result, 'correlation_id': _correlation(request)}

## backend\app\api\routes\nl2sql.py
- L1 docstring: "API routes for Natural Language to SQL feature."
- L2 from __future__ import annotations
- L4 from typing import List, Optional
- L6 from fastapi import APIRouter, Depends, Query, Request
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.nl2sql.schemas import NL2SQLGenerateRequest, NL2SQLExecuteRequest, NL2SQLSaveRequest
- L14 from backend.app.domain.nl2sql.service import NL2SQLService
- L16 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L19 def get_service:
  - L20 return NL2SQLService()
- L24 async def generate_sqlpayload: NL2SQLGenerateRequest, request: Request, svc: NL2SQLService=Depends(get_service):
  - L29 docstring: "Generate SQL from a natural language question."
  - L30 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L31 assign result = svc.generate_sql(payload, correlation_id)
  - L32 return {'status': 'ok', 'sql': result.sql, 'explanation': result.explanation, 'confidence': result.confidence, 'warnings': result.warnings, 'original_question': result.original_question, 'correlation_id': correlation_id}
- L44 async def execute_querypayload: NL2SQLExecuteRequest, request: Request, svc: NL2SQLService=Depends(get_service):
  - L49 docstring: "Execute a SQL query and return results."
  - L50 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L51 assign result = svc.execute_query(payload, correlation_id)
  - L52 return {'status': 'ok', 'columns': result.columns, 'rows': result.rows, 'row_count': result.row_count, 'total_count': result.total_count, 'execution_time_ms': result.execution_time_ms, 'truncated': result.truncated, 'correlation_id': correlation_id}
- L65 async def explain_queryrequest: Request, sql: str=Query(..., min_length=1, max_length=10000), svc: NL2SQLService=Depends(get_service):
  - L70 docstring: "Get a natural language explanation of a SQL query."
  - L71 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L72 assign explanation = svc.explain_query(sql, correlation_id)
  - L73 return {'status': 'ok', 'explanation': explanation, 'correlation_id': correlation_id}
- L81 async def save_querypayload: NL2SQLSaveRequest, request: Request, svc: NL2SQLService=Depends(get_service):
  - L86 docstring: "Save a query as a reusable data source."
  - L87 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L88 assign saved = svc.save_query(payload, correlation_id)
  - L89 return {'status': 'ok', 'query': saved.dict(), 'correlation_id': correlation_id}
- L97 async def list_saved_queriesrequest: Request, connection_id: Optional[str]=Query(None, max_length=64), tags: Optional[List[str]]=Query(None), svc: NL2SQLService=Depends(get_service):
  - L103 docstring: "List saved queries."
  - L104 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L105 assign queries = svc.list_saved_queries(connection_id=connection_id, tags=tags)
  - L106 return {'status': 'ok', 'queries': [q.dict() for q in queries], 'correlation_id': correlation_id}
- L114 async def get_saved_queryquery_id: str, request: Request, svc: NL2SQLService=Depends(get_service):
  - L119 docstring: "Get a saved query by ID."
  - L120 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L121 assign query = svc.get_saved_query(query_id)
  - L122 if not query:
    - L123 return {'status': 'error', 'code': 'not_found', 'message': 'Query not found', 'correlation_id': correlation_id}
  - L129 return {'status': 'ok', 'query': query.dict(), 'correlation_id': correlation_id}
- L137 async def delete_saved_queryquery_id: str, request: Request, svc: NL2SQLService=Depends(get_service):
  - L142 docstring: "Delete a saved query."
  - L143 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L144 assign deleted = svc.delete_saved_query(query_id)
  - L145 return {'status': 'ok' if deleted else 'error', 'deleted': deleted, 'query_id': query_id, 'correlation_id': correlation_id}
- L154 async def get_query_historyrequest: Request, connection_id: Optional[str]=Query(None, max_length=64), limit: int=Query(50, ge=1, le=200), svc: NL2SQLService=Depends(get_service):
  - L160 docstring: "Get query history."
  - L161 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L162 assign history = svc.get_query_history(connection_id=connection_id, limit=limit)
  - L163 return {'status': 'ok', 'history': [h.dict() for h in history], 'correlation_id': correlation_id}
- L171 async def delete_query_history_entryentry_id: str, request: Request, svc: NL2SQLService=Depends(get_service):
  - L176 docstring: "Delete a query history entry."
  - L177 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L178 assign deleted = svc.delete_query_history_entry(entry_id)
  - L179 return {'status': 'ok' if deleted else 'error', 'deleted': deleted, 'entry_id': entry_id, 'correlation_id': correlation_id}

## backend\app\api\routes\recommendations.py
- L1 docstring: "API routes for Template Recommendations."
- L2 from __future__ import annotations
- L4 from typing import List, Optional
- L5 from pydantic import BaseModel, Field
- L6 from fastapi import APIRouter, Depends, Query, Request
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.recommendations.service import RecommendationService
- L10 from backend.app.services.state import store as state_store_module
- L11 from backend.app.services.background_tasks import enqueue_background_job
- L12 from backend.app.services.state import state_store
- L14 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L17 class TemplateRecommendRequest(BaseModel):
  - L18 docstring: "Request payload for template recommendations (frontend format)."
  - L19 annotated assign data_description: Optional[str] = Field(None, max_length=1000)
  - L20 annotated assign data_columns: Optional[List[str]] = Field(None, max_items=100)
  - L21 annotated assign industry: Optional[str] = Field(None, max_length=100)
  - L22 annotated assign output_format: Optional[str] = Field(None, max_length=50)
- L25 def get_service:
  - L26 return RecommendationService()
- L29 def _state_store:
  - L30 return state_store_module.state_store
- L34 async def recommend_templates_postpayload: TemplateRecommendRequest, request: Request, svc: RecommendationService=Depends(get_service), background: bool=Query(False):
  - L40 docstring: "Get template recommendations based on data description and columns."
  - L41 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L44 assign context_parts = []
  - L45 if payload.data_description:
    - L46 expr context_parts.append(f'Data description: {payload.data_description}')
  - L47 if payload.data_columns:
    - L48 expr context_parts.append(f"Data columns: {', '.join(payload.data_columns)}")
  - L49 if payload.industry:
    - L50 expr context_parts.append(f'Industry: {payload.industry}')
  - L51 if payload.output_format:
    - L52 expr context_parts.append(f'Output format: {payload.output_format}')
  - L54 assign context = ' | '.join(context_parts) if context_parts else None
  - L56 if not background:
    - L57 assign recommendations = svc.recommend_templates(context=context, limit=5, correlation_id=correlation_id)
    - L62 return {'status': 'ok', 'recommendations': recommendations, 'correlation_id': correlation_id}
  - L64 async def runnerjob_id: str:
    - L65 expr state_store.record_job_start(job_id)
    - L66 expr state_store.record_job_step(job_id, 'recommend', status='running', label='Generate recommendations')
    - L67 try:
      - L68 assign recommendations = svc.recommend_templates(context=context, limit=5, correlation_id=correlation_id)
      - L73 expr state_store.record_job_step(job_id, 'recommend', status='succeeded', progress=100.0)
      - L74 expr state_store.record_job_completion(job_id, status='succeeded', result={'recommendations': recommendations})
      - L79 except Exception as exc:
        - L80 expr state_store.record_job_step(job_id, 'recommend', status='failed', error=str(exc))
        - L81 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L83 assign job = await enqueue_background_job(job_type='recommend_templates', steps=[{'name': 'recommend', 'label': 'Generate recommendations'}], meta={'background': True, 'context': context}, runner=runner)
  - L89 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L93 async def recommend_templates_getrequest: Request, connection_id: Optional[str]=Query(None), context: Optional[str]=Query(None, max_length=500), limit: int=Query(5, ge=1, le=20), svc: RecommendationService=Depends(get_service), background: bool=Query(False):
  - L101 docstring: "Get template recommendations based on context (query params)."
  - L102 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L103 if not background:
    - L104 assign recommendations = svc.recommend_templates(connection_id=connection_id, context=context, limit=limit, correlation_id=correlation_id)
    - L110 return {'status': 'ok', 'recommendations': recommendations, 'correlation_id': correlation_id}
  - L112 async def runnerjob_id: str:
    - L113 expr state_store.record_job_start(job_id)
    - L114 expr state_store.record_job_step(job_id, 'recommend', status='running', label='Generate recommendations')
    - L115 try:
      - L116 assign recommendations = svc.recommend_templates(connection_id=connection_id, context=context, limit=limit, correlation_id=correlation_id)
      - L122 expr state_store.record_job_step(job_id, 'recommend', status='succeeded', progress=100.0)
      - L123 expr state_store.record_job_completion(job_id, status='succeeded', result={'recommendations': recommendations})
      - L128 except Exception as exc:
        - L129 expr state_store.record_job_step(job_id, 'recommend', status='failed', error=str(exc))
        - L130 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L132 assign job = await enqueue_background_job(job_type='recommend_templates', steps=[{'name': 'recommend', 'label': 'Generate recommendations'}], meta={'background': True, 'context': context, 'connection_id': connection_id}, runner=runner)
  - L138 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L142 async def get_template_catalogrequest: Request:
  - L145 docstring: "Get template catalog for browsing."
  - L146 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L148 assign store = _state_store()
  - L149 assign templates = store._read_state().get('templates', {})
  - L152 assign catalog = []
  - L153 for (tid, t) in templates.items():
    - L154 if t.get('status') == 'approved':
      - L155 expr catalog.append({'id': t.get('id'), 'name': t.get('name'), 'kind': t.get('kind'), 'description': t.get('description', ''), 'tags': t.get('tags', []), 'created_at': t.get('created_at')})
  - L165 expr catalog.sort(key=lambda x: x.get('name', '').lower())
  - L167 return {'status': 'ok', 'catalog': catalog, 'total': len(catalog), 'correlation_id': correlation_id}
- L171 async def get_similar_templatestemplate_id: str, request: Request, limit: int=Query(3, ge=1, le=10), svc: RecommendationService=Depends(get_service):
  - L177 docstring: "Get templates similar to a given template."
  - L178 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L179 assign similar = svc.get_similar_templates(template_id, limit)
  - L180 return {'status': 'ok', 'similar': similar, 'correlation_id': correlation_id}

## backend\app\api\routes\reports.py
- L1 docstring: "Reports API Routes.\n\nThis module contains endpoints for report generation and ..."
- L8 from __future__ import annotations
- L10 from typing import Optional
- L12 from fastapi import APIRouter, Depends, HTTPException, Request
- L14 from backend.app.core.security import require_api_key
- L15 from backend.app.features.generate.schemas.reports import RunPayload, DiscoverPayload
- L16 from src.services.report_service import queue_report_job, run_report as run_report_service, list_report_runs as list_report_runs_service, get_report_run as get_report_run_service
- L23 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L26 def _correlationrequest: Request:
  - L27 return getattr(request.state, 'correlation_id', None)
- L35 def run_reportpayload: RunPayload, request: Request:
  - L36 docstring: "Run a PDF report synchronously."
  - L37 return run_report_service(payload, request, kind='pdf')
- L41 async def enqueue_report_jobpayload: RunPayload | list[RunPayload], request: Request:
  - L42 docstring: "Queue a PDF report job for async generation."
  - L43 return await queue_report_job(payload, request, kind='pdf')
- L51 def discover_reportspayload: DiscoverPayload, request: Request:
  - L52 docstring: "Discover available batches for report generation."
  - L53 from backend.app.features.generate.services.discovery_service import discover_reports as discover_reports_service
  - L54 from src.utils.template_utils import template_dir
  - L55 from src.utils.connection_utils import db_path_from_payload_or_default
  - L56 from backend.app.services.contract.ContractBuilderV2 import load_contract_v2
  - L57 from src.utils.schedule_utils import clean_key_values
  - L58 from backend.app.services.reports.discovery import discover_batches_and_counts
  - L59 from backend.app.services.reports.discovery_metrics import build_batch_field_catalog_and_stats, build_batch_metrics
  - L60 from backend.app.services.utils.artifacts import load_manifest
  - L61 from src.utils.template_utils import manifest_endpoint
  - L62 import logging
  - L64 assign logger = logging.getLogger('neura.api')
  - L65 return discover_reports_service(payload, kind='pdf', template_dir_fn=lambda tpl: template_dir(tpl, kind='pdf'), db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_fn=discover_batches_and_counts, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_batch_metrics_fn=build_batch_metrics, load_manifest_fn=load_manifest, manifest_endpoint_fn=lambda tpl: manifest_endpoint(tpl, kind='pdf'), logger=logger)
- L86 def list_report_runs_routerequest: Request, template_id: Optional[str]=None, connection_id: Optional[str]=None, schedule_id: Optional[str]=None, limit: int=50:
  - L93 docstring: "List report generation runs with optional filtering."
  - L94 assign runs = list_report_runs_service(template_id=template_id, connection_id=connection_id, schedule_id=schedule_id, limit=limit)
  - L100 return {'runs': runs, 'correlation_id': _correlation(request)}
- L104 def get_report_run_routerun_id: str, request: Request:
  - L105 docstring: "Get a specific report run by ID."
  - L106 assign run = get_report_run_service(run_id)
  - L107 if not run:
    - L108 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'run_not_found', 'message': 'Run not found.'})
  - L112 return {'run': run, 'correlation_id': _correlation(request)}

## backend\app\api\routes\schedules.py
- L1 docstring: "Schedules API Routes.\n\nThis module contains endpoints for report scheduling:\n..."
- L8 from __future__ import annotations
- L10 import logging
- L11 from datetime import datetime, timezone
- L13 from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Request
- L15 from backend.app.core.security import require_api_key
- L16 from backend.app.services.state import state_store
- L17 from src.schemas.report_schema import ScheduleCreatePayload, ScheduleUpdatePayload
- L18 from src.services.scheduler_service import create_schedule, delete_schedule, get_schedule, list_schedules, update_schedule
- L26 assign logger = logging.getLogger('neura.schedules')
- L28 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L31 def _correlationrequest: Request:
  - L32 return getattr(request.state, 'correlation_id', None)
- L36 def list_report_schedulesrequest: Request:
  - L37 docstring: "List all report schedules."
  - L38 return {'schedules': list_schedules(), 'correlation_id': _correlation(request)}
- L42 def create_report_schedulepayload: ScheduleCreatePayload, request: Request:
  - L43 docstring: "Create a new report schedule."
  - L44 assign schedule = create_schedule(payload)
  - L45 return {'schedule': schedule, 'correlation_id': _correlation(request)}
- L49 def get_report_scheduleschedule_id: str, request: Request:
  - L50 docstring: "Get a specific schedule by ID."
  - L51 assign schedule = get_schedule(schedule_id)
  - L52 if not schedule:
    - L53 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L57 return {'schedule': schedule, 'correlation_id': _correlation(request)}
- L61 def update_report_scheduleschedule_id: str, payload: ScheduleUpdatePayload, request: Request:
  - L62 docstring: "Update an existing report schedule."
  - L63 assign schedule = update_schedule(schedule_id, payload)
  - L64 return {'schedule': schedule, 'correlation_id': _correlation(request)}
- L68 def delete_report_scheduleschedule_id: str, request: Request:
  - L69 docstring: "Delete a report schedule."
  - L70 assign removed = delete_schedule(schedule_id)
  - L71 if not removed:
    - L72 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L76 return {'status': 'ok', 'schedule_id': schedule_id, 'correlation_id': _correlation(request)}
- L80 async def trigger_scheduleschedule_id: str, background_tasks: BackgroundTasks, request: Request:
  - L81 docstring: "\n    Manually trigger a scheduled report to run immediately.\n\n    This create..."
  - L87 assign correlation_id = _correlation(request) or f'manual-trigger-{schedule_id}'
  - L90 assign schedule = get_schedule(schedule_id)
  - L91 if not schedule:
    - L92 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L98 from backend.app.features.generate.schemas.reports import RunPayload
  - L99 from src.services.report_service import JobRunTracker, _build_job_steps, _step_progress_from_steps, scheduler_runner
  - L107 assign payload = {'template_id': schedule.get('template_id'), 'connection_id': schedule.get('connection_id'), 'start_date': schedule.get('start_date'), 'end_date': schedule.get('end_date'), 'batch_ids': schedule.get('batch_ids') or None, 'key_values': schedule.get('key_values') or None, 'docx': bool(schedule.get('docx')), 'xlsx': bool(schedule.get('xlsx')), 'email_recipients': schedule.get('email_recipients') or None, 'email_subject': schedule.get('email_subject') or f"[Manual Trigger] {schedule.get('name') or schedule.get('template_id')}", 'email_message': schedule.get('email_message') or f"Manually triggered run for schedule '{schedule.get('name')}'.", 'schedule_id': schedule_id, 'schedule_name': schedule.get('name')}
  - L122 assign kind = schedule.get('template_kind') or 'pdf'
  - L125 try:
    - L126 assign run_payload = RunPayload(**payload)
    - L127 except Exception as exc:
      - L128 raise HTTPException(status_code=400, detail={'status': 'error', 'code': 'invalid_schedule_payload', 'message': f'Schedule has invalid configuration: {exc}'})
  - L138 assign steps = _build_job_steps(run_payload, kind=kind)
  - L139 assign meta = {'start_date': payload.get('start_date'), 'end_date': payload.get('end_date'), 'schedule_id': schedule_id, 'schedule_name': schedule.get('name'), 'manual_trigger': True, 'docx': bool(payload.get('docx')), 'xlsx': bool(payload.get('xlsx'))}
  - L148 assign job_record = state_store.create_job(job_type='run_report', template_id=run_payload.template_id, connection_id=run_payload.connection_id, template_name=schedule.get('template_name') or run_payload.template_id, template_kind=kind, schedule_id=schedule_id, correlation_id=correlation_id, steps=steps, meta=meta)
  - L160 assign job_id = job_record.get('id')
  - L161 assign step_progress = _step_progress_from_steps(steps)
  - L162 assign job_tracker = JobRunTracker(job_id, correlation_id=correlation_id, step_progress=step_progress)
  - L164 def run_scheduled_report:
    - L165 docstring: "Background task to run the scheduled report."
    - L166 import asyncio
    - L167 assign started = datetime.now(timezone.utc)
    - L168 try:
      - L169 expr job_tracker.start()
      - L170 assign result = scheduler_runner(payload, kind, job_tracker=job_tracker)
      - L171 assign finished = datetime.now(timezone.utc)
      - L174 assign artifacts = {'html_url': result.get('html_url'), 'pdf_url': result.get('pdf_url'), 'docx_url': result.get('docx_url'), 'xlsx_url': result.get('xlsx_url')}
      - L180 expr state_store.record_schedule_run(schedule_id, started_at=started.isoformat(), finished_at=finished.isoformat(), status='success', next_run_at=None, error=None, artifacts=artifacts)
      - L189 expr job_tracker.succeed(result)
      - L190 expr logger.info('manual_trigger_completed', extra={'event': 'manual_trigger_completed', 'schedule_id': schedule_id, 'job_id': job_id, 'correlation_id': correlation_id})
      - L199 except Exception as exc:
        - L200 assign finished = datetime.now(timezone.utc)
        - L201 expr state_store.record_schedule_run(schedule_id, started_at=started.isoformat(), finished_at=finished.isoformat(), status='failed', next_run_at=None, error=str(exc), artifacts=None)
        - L210 expr job_tracker.fail(str(exc))
        - L211 expr logger.exception('manual_trigger_failed', extra={'event': 'manual_trigger_failed', 'schedule_id': schedule_id, 'job_id': job_id, 'correlation_id': correlation_id, 'error': str(exc)})
  - L223 expr background_tasks.add_task(run_scheduled_report)
  - L225 expr logger.info('manual_trigger_queued', extra={'event': 'manual_trigger_queued', 'schedule_id': schedule_id, 'job_id': job_id, 'correlation_id': correlation_id})
  - L235 return {'status': 'triggered', 'message': 'Schedule triggered for immediate execution', 'schedule_id': schedule_id, 'job_id': job_id, 'correlation_id': correlation_id}
- L245 def pause_scheduleschedule_id: str, request: Request:
  - L246 docstring: "Pause a schedule (set active to false)."
  - L247 assign schedule = get_schedule(schedule_id)
  - L248 if not schedule:
    - L249 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L254 assign updated = update_schedule(schedule_id, ScheduleUpdatePayload(active=False))
  - L255 return {'status': 'ok', 'message': 'Schedule paused', 'schedule': updated, 'correlation_id': _correlation(request)}
- L264 def resume_scheduleschedule_id: str, request: Request:
  - L265 docstring: "Resume a paused schedule (set active to true)."
  - L266 assign schedule = get_schedule(schedule_id)
  - L267 if not schedule:
    - L268 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L273 assign updated = update_schedule(schedule_id, ScheduleUpdatePayload(active=True))
  - L274 return {'status': 'ok', 'message': 'Schedule resumed', 'schedule': updated, 'correlation_id': _correlation(request)}

## backend\app\api\routes\state.py
- L1 docstring: "State Management API Routes.\n\nThis module contains endpoints for application s..."
- L7 from __future__ import annotations
- L9 from fastapi import APIRouter, Depends, Request
- L10 from pydantic import BaseModel
- L11 from typing import Optional
- L13 from backend.app.core.security import require_api_key
- L14 from backend.app.services.state import state_store as state_store_module
- L15 from src.services.template_service import bootstrap_state
- L17 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L20 class LastUsedPayload(BaseModel):
  - L21 annotated assign connection_id: Optional[str] = None
  - L22 annotated assign template_id: Optional[str] = None
- L25 def _correlationrequest: Request:
  - L26 return getattr(request.state, 'correlation_id', None)
- L29 def _state_store:
  - L30 return state_store_module.state_store
- L34 def bootstrap_state_routerequest: Request:
  - L35 docstring: "Get bootstrap state for app initialization.\n\n    Returns connections, template..."
  - L40 return bootstrap_state(request)
- L44 def set_last_used_routepayload: LastUsedPayload, request: Request:
  - L45 docstring: "Record the last-used connection and template IDs for session persistence."
  - L46 assign last_used = _state_store().set_last_used(connection_id=payload.connection_id, template_id=payload.template_id)
  - L50 return {'status': 'ok', 'last_used': last_used, 'correlation_id': _correlation(request)}

## backend\app\api\routes\summary.py
- L1 docstring: "API routes for Executive Summary Generation."
- L2 from __future__ import annotations
- L4 from typing import List, Optional
- L5 from pydantic import BaseModel, Field
- L6 from fastapi import APIRouter, Depends, Query, Request
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.summary.service import SummaryService
- L10 from backend.app.services.background_tasks import enqueue_background_job
- L11 from backend.app.services.state import state_store
- L13 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L16 class SummaryRequest(BaseModel):
  - L17 annotated assign content: str = Field(..., min_length=10, max_length=50000)
  - L18 annotated assign tone: str = Field(default='formal', pattern='^(formal|conversational|technical)$')
  - L19 annotated assign max_sentences: int = Field(default=5, ge=2, le=15)
  - L20 annotated assign focus_areas: Optional[List[str]] = Field(None, max_items=5)
- L23 def get_service:
  - L24 return SummaryService()
- L28 async def generate_summarypayload: SummaryRequest, request: Request, svc: SummaryService=Depends(get_service), background: bool=Query(False):
  - L34 docstring: "Generate an executive summary from content."
  - L35 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L36 if not background:
    - L37 assign summary = svc.generate_summary(content=payload.content, tone=payload.tone, max_sentences=payload.max_sentences, focus_areas=payload.focus_areas, correlation_id=correlation_id)
    - L44 return {'status': 'ok', 'summary': summary, 'correlation_id': correlation_id}
  - L46 async def runnerjob_id: str:
    - L47 expr state_store.record_job_start(job_id)
    - L48 expr state_store.record_job_step(job_id, 'generate', status='running', label='Generate summary')
    - L49 try:
      - L50 assign summary = svc.generate_summary(content=payload.content, tone=payload.tone, max_sentences=payload.max_sentences, focus_areas=payload.focus_areas, correlation_id=correlation_id)
      - L57 expr state_store.record_job_step(job_id, 'generate', status='succeeded', progress=100.0)
      - L58 expr state_store.record_job_completion(job_id, status='succeeded', result={'summary': summary})
      - L63 except Exception as exc:
        - L64 expr state_store.record_job_step(job_id, 'generate', status='failed', error=str(exc))
        - L65 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L67 assign job = await enqueue_background_job(job_type='summary_generate', steps=[{'name': 'generate', 'label': 'Generate summary'}], meta={'background': True, 'content_length': len(payload.content)}, runner=runner)
  - L73 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L77 async def get_report_summaryreport_id: str, request: Request, svc: SummaryService=Depends(get_service), background: bool=Query(False):
  - L83 docstring: "Generate summary for a specific report."
  - L84 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L85 if not background:
    - L86 assign summary = svc.generate_report_summary(report_id, correlation_id)
    - L87 return {'status': 'ok', 'summary': summary, 'correlation_id': correlation_id}
  - L89 async def runnerjob_id: str:
    - L90 expr state_store.record_job_start(job_id)
    - L91 expr state_store.record_job_step(job_id, 'generate', status='running', label='Generate report summary')
    - L92 try:
      - L93 assign summary = svc.generate_report_summary(report_id, correlation_id)
      - L94 expr state_store.record_job_step(job_id, 'generate', status='succeeded', progress=100.0)
      - L95 expr state_store.record_job_completion(job_id, status='succeeded', result={'summary': summary, 'report_id': report_id})
      - L100 except Exception as exc:
        - L101 expr state_store.record_job_step(job_id, 'generate', status='failed', error=str(exc))
        - L102 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L104 assign job = await enqueue_background_job(job_type='summary_report', steps=[{'name': 'generate', 'label': 'Generate report summary'}], meta={'background': True, 'report_id': report_id}, runner=runner)
  - L110 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}

## backend\app\api\routes\synthesis.py
- L1 docstring: "API routes for Multi-Document Synthesis."
- L2 from __future__ import annotations
- L4 from typing import List, Optional
- L5 from pydantic import BaseModel, Field
- L6 from fastapi import APIRouter, Depends, Request, HTTPException
- L8 from backend.app.core.security import require_api_key
- L9 from backend.app.domain.synthesis.service import DocumentSynthesisService
- L10 from backend.app.domain.synthesis.schemas import DocumentType, SynthesisRequest
- L12 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L15 class CreateSessionRequest(BaseModel):
  - L16 annotated assign name: str = Field(..., min_length=1, max_length=200)
- L19 class AddDocumentRequest(BaseModel):
  - L20 annotated assign name: str = Field(..., min_length=1, max_length=200)
  - L21 annotated assign content: str = Field(..., min_length=10, max_length=5 * 1024 * 1024)
  - L22 annotated assign doc_type: DocumentType = Field(default=DocumentType.TEXT)
  - L23 annotated assign metadata: Optional[dict] = None
- L26 def get_service:
  - L27 return DocumentSynthesisService()
- L31 async def create_sessionpayload: CreateSessionRequest, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L36 docstring: "Create a new synthesis session."
  - L37 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L38 assign session = svc.create_session(name=payload.name, correlation_id=correlation_id)
  - L42 return {'status': 'ok', 'session': session.model_dump(mode='json'), 'correlation_id': correlation_id}
- L46 async def list_sessionsrequest: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L50 docstring: "List all synthesis sessions."
  - L51 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L52 assign sessions = svc.list_sessions()
  - L53 return {'status': 'ok', 'sessions': [s.model_dump(mode='json') for s in sessions], 'correlation_id': correlation_id}
- L61 async def get_sessionsession_id: str, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L66 docstring: "Get a synthesis session by ID."
  - L67 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L68 assign session = svc.get_session(session_id)
  - L70 if not session:
    - L71 raise HTTPException(status_code=404, detail='Session not found')
  - L73 return {'status': 'ok', 'session': session.model_dump(mode='json'), 'correlation_id': correlation_id}
- L77 async def delete_sessionsession_id: str, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L82 docstring: "Delete a synthesis session."
  - L83 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L84 assign success = svc.delete_session(session_id)
  - L86 if not success:
    - L87 raise HTTPException(status_code=404, detail='Session not found')
  - L89 return {'status': 'ok', 'deleted': True, 'correlation_id': correlation_id}
- L93 async def add_documentsession_id: str, payload: AddDocumentRequest, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L99 docstring: "Add a document to a synthesis session."
  - L100 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L101 assign document = svc.add_document(session_id=session_id, name=payload.name, content=payload.content, doc_type=payload.doc_type, metadata=payload.metadata, correlation_id=correlation_id)
  - L110 if not document:
    - L111 raise HTTPException(status_code=404, detail='Session not found')
  - L113 return {'status': 'ok', 'document': document.model_dump(mode='json'), 'correlation_id': correlation_id}
- L117 async def remove_documentsession_id: str, document_id: str, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L123 docstring: "Remove a document from a session."
  - L124 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L125 assign success = svc.remove_document(session_id, document_id)
  - L127 if not success:
    - L128 raise HTTPException(status_code=404, detail='Session or document not found')
  - L130 return {'status': 'ok', 'removed': True, 'correlation_id': correlation_id}
- L134 async def find_inconsistenciessession_id: str, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L139 docstring: "Find inconsistencies between documents in a session."
  - L140 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L142 assign session = svc.get_session(session_id)
  - L143 if not session:
    - L144 raise HTTPException(status_code=404, detail='Session not found')
  - L146 assign inconsistencies = svc.find_inconsistencies(session_id, correlation_id)
  - L148 return {'status': 'ok', 'inconsistencies': [i.model_dump(mode='json') for i in inconsistencies], 'count': len(inconsistencies), 'correlation_id': correlation_id}
- L157 async def synthesize_documentssession_id: str, payload: SynthesisRequest, request: Request, svc: DocumentSynthesisService=Depends(get_service):
  - L163 docstring: "Synthesize information from all documents in a session."
  - L164 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L166 assign session = svc.get_session(session_id)
  - L167 if not session:
    - L168 raise HTTPException(status_code=404, detail='Session not found')
  - L170 if not session.documents:
    - L171 raise HTTPException(status_code=400, detail='No documents in session')
  - L173 assign result = svc.synthesize(session_id, payload, correlation_id)
  - L175 if not result:
    - L176 raise HTTPException(status_code=500, detail='Synthesis failed')
  - L178 return {'status': 'ok', 'result': result.model_dump(mode='json'), 'correlation_id': correlation_id}

## backend\app\api\routes\templates.py
- L1 docstring: "Consolidated Templates API Routes.\n\nThis module contains all template-related ..."
- L13 from __future__ import annotations
- L15 import contextlib
- L16 import logging
- L17 import os
- L18 import tempfile
- L19 from pathlib import Path
- L20 from types import SimpleNamespace
- L21 from typing import Optional
- L23 from fastapi import APIRouter, Depends, File, Form, HTTPException, Query, Request, UploadFile
- L24 from fastapi.responses import FileResponse
- L26 from backend.app.core.config import get_settings
- L27 from backend.app.core.security import require_api_key
- L28 from backend.app.core.validation import is_safe_name, validate_file_extension
- L29 from backend.app.domain.templates.schemas import TemplateImportResult
- L30 from backend.app.domain.templates.service import TemplateService
- L31 from backend.app.features.generate.schemas.charts import ChartSuggestPayload, SavedChartCreatePayload, SavedChartUpdatePayload
- L36 from backend.app.features.generate.services.chart_suggestions_service import suggest_charts as suggest_charts_service
- L37 from backend.app.features.generate.services.saved_charts_service import create_saved_chart as create_saved_chart_service, delete_saved_chart as delete_saved_chart_service, list_saved_charts as list_saved_charts_service, update_saved_chart as update_saved_chart_service
- L43 from backend.app.services.contract.ContractBuilderV2 import load_contract_v2
- L44 from backend.app.services.background_tasks import enqueue_background_job, iter_ndjson_events_async, run_event_stream_async
- L49 from backend.app.services.prompts.llm_prompts_charts import CHART_SUGGEST_PROMPT_VERSION, build_chart_suggestions_prompt
- L53 from backend.app.services.reports.discovery import discover_batches_and_counts
- L54 from backend.app.services.reports.discovery_metrics import build_batch_field_catalog_and_stats, build_batch_metrics
- L58 from backend.app.services.state import state_store
- L59 from backend.app.services.templates.TemplateVerify import get_openai_client
- L60 from backend.app.services.utils import call_chat_completion, get_correlation_id, strip_code_fences
- L63 from src.services.template_service import get_template_html, edit_template_ai, edit_template_manual, chat_template_edit, apply_chat_template_edit, undo_last_template_edit, verify_excel, verify_template, list_templates, templates_catalog, recommend_templates, delete_template, update_template_metadata, generator_assets
- L79 from src.services.mapping.approve import run_mapping_approve
- L80 from src.services.mapping.corrections import run_corrections_preview
- L81 from src.services.mapping.key_options import mapping_key_options as mapping_key_options_service
- L82 from src.services.mapping.preview import run_mapping_preview
- L83 from src.services.file_service import artifact_head_response, artifact_manifest_response
- L84 from src.schemas.template_schema import CorrectionsPreviewPayload, GeneratorAssetsPayload, MappingPayload, TemplateAiEditPayload, TemplateChatPayload, TemplateManualEditPayload, TemplateRecommendPayload, TemplateRecommendResponse, TemplateUpdatePayload
- L95 from src.utils.connection_utils import db_path_from_payload_or_default
- L96 from src.utils.schedule_utils import clean_key_values
- L97 from src.utils.template_utils import normalize_template_id, template_dir
- L99 assign router = APIRouter(dependencies=[Depends(require_api_key)])
- L101 assign ALLOWED_EXTENSIONS = ['.zip']
- L102 assign MAX_FILENAME_LENGTH = 255
- L105 def _correlationrequest: Request:
  - L106 return getattr(request.state, 'correlation_id', None)
- L109 def _request_with_correlationcorrelation_id: str | None:
  - L110 return SimpleNamespace(state=SimpleNamespace(correlation_id=correlation_id))
- L113 def _wrappayload: dict, correlation_id: str | None:
  - L114 assign payload = dict(payload)
  - L115 if correlation_id is not None:
    - L116 assign payload['correlation_id'] = correlation_id
  - L117 return payload
- L120 def _ensure_template_existstemplate_id: str:
  - L121 assign normalized = normalize_template_id(template_id)
  - L122 assign record = state_store.get_template_record(normalized)
  - L123 if not record:
    - L124 raise HTTPException(status_code=404, detail='template_not_found')
  - L125 return (normalized, record)
- L128 def get_servicesettings=Depends(get_settings):
  - L129 return TemplateService(uploads_root=settings.uploads_dir, excel_uploads_root=settings.excel_uploads_dir, max_bytes=settings.max_upload_bytes, max_zip_entries=settings.max_zip_entries, max_zip_uncompressed_bytes=settings.max_zip_uncompressed_bytes, max_concurrency=settings.template_import_max_concurrency)
- L139 def validate_upload_filefile: UploadFile, max_bytes: int:
  - L140 if not file.filename:
    - L141 raise HTTPException(status_code=400, detail='Filename is required')
  - L142 if len(file.filename) > MAX_FILENAME_LENGTH:
    - L143 raise HTTPException(status_code=400, detail=f'Filename too long (max {MAX_FILENAME_LENGTH} characters)')
  - L144 assign (is_valid, error) = validate_file_extension(file.filename, ALLOWED_EXTENSIONS)
  - L145 if not is_valid:
    - L146 raise HTTPException(status_code=400, detail=error)
  - L147 if file.content_type and file.content_type not in ('application/zip', 'application/x-zip-compressed', 'application/octet-stream'):
    - L152 raise HTTPException(status_code=400, detail='Please upload a valid ZIP file. The file you selected does not appear to be a ZIP archive.')
- L158 async def _persist_uploadfile: UploadFile, suffix: str:
  - L159 assign filename = Path(file.filename or f'upload{suffix}').name
  - L160 assign tmp = tempfile.NamedTemporaryFile(prefix='nr-upload-', suffix=suffix, delete=False)
  - L161 try:
    - L162 with tmp:
      - L163 expr file.file.seek(0)
      - L164 while True:
        - L165 assign chunk = file.file.read(1024 * 1024)
        - L166 if not chunk:
          - L167 break
        - L168 expr tmp.write(chunk)
    - L170 finally:
      - L170 with contextlib.suppress(Exception):
        - L171 expr await file.close()
  - L172 return (Path(tmp.name), filename)
- L180 def list_templates_routerequest: Request, status: Optional[str]=None:
  - L181 docstring: "List all templates with optional status filter."
  - L182 return list_templates(status, request)
- L186 def templates_catalog_routerequest: Request:
  - L187 docstring: "Get template catalog for browsing."
  - L188 return templates_catalog(request)
- L196 def delete_template_routetemplate_id: str, request: Request:
  - L197 docstring: "Delete a template."
  - L198 return delete_template(template_id, request)
- L202 def update_template_metadata_routetemplate_id: str, payload: TemplateUpdatePayload, request: Request:
  - L203 docstring: "Update template metadata (name, description, etc.)."
  - L204 return update_template_metadata(template_id, payload, request)
- L212 async def verify_template_routerequest: Request, file: UploadFile=File(...), connection_id: Optional[str]=Form(None), refine_iters: int=Form(0), background: bool=Query(False):
  - L219 docstring: "Verify and process a PDF template."
  - L220 if not background:
    - L221 return verify_template(file=file, connection_id=connection_id, refine_iters=refine_iters, request=request)
  - L223 assign (upload_path, filename) = await _persist_upload(file, suffix='.pdf')
  - L224 assign correlation_id = _correlation(request)
  - L225 assign template_name = Path(filename).stem or filename
  - L227 async def runnerjob_id: str:
    - L228 assign upload = UploadFile(filename=filename, file=upload_path.open('rb'))
    - L229 try:
      - L230 assign response = verify_template(file=upload, connection_id=connection_id, refine_iters=refine_iters, request=_request_with_correlation(correlation_id))
      - L236 expr await run_event_stream_async(job_id, iter_ndjson_events_async(response.body_iterator))
      - L238 finally:
        - L238 with contextlib.suppress(Exception):
          - L239 expr await upload.close()
        - L240 with contextlib.suppress(FileNotFoundError):
          - L241 expr upload_path.unlink(missing_ok=True)
  - L243 assign job = await enqueue_background_job(job_type='verify_template', connection_id=connection_id, template_name=template_name, template_kind='pdf', meta={'filename': filename, 'background': True, 'refine_iters': refine_iters}, runner=runner)
  - L251 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L259 async def import_template_ziprequest: Request, file: UploadFile=File(...), name: str | None=Form(None, max_length=100), service: TemplateService=Depends(get_service), settings=Depends(get_settings):
  - L266 docstring: "Import a template from a zip file."
  - L267 expr validate_upload_file(file, settings.max_upload_bytes)
  - L268 if name is not None and (not is_safe_name(name)):
    - L269 raise HTTPException(status_code=400, detail='Template name contains invalid characters')
  - L270 assign correlation_id = _correlation(request)
  - L271 return await service.import_zip(file, name, correlation_id)
- L275 async def export_template_ziptemplate_id: str, request: Request, service: TemplateService=Depends(get_service):
  - L280 docstring: "Export a template as a zip file for sharing or backup."
  - L281 assign correlation_id = _correlation(request)
  - L282 assign result = await service.export_zip(template_id, correlation_id)
  - L283 return FileResponse(path=result['zip_path'], filename=result['filename'], media_type='application/zip', background=None)
- L292 async def duplicate_templatetemplate_id: str, request: Request, name: str | None=Form(None, max_length=100), service: TemplateService=Depends(get_service):
  - L298 docstring: "Duplicate a template to create a new copy."
  - L299 if name is not None and (not is_safe_name(name)):
    - L300 raise HTTPException(status_code=400, detail='Template name contains invalid characters')
  - L301 assign correlation_id = _correlation(request)
  - L302 return await service.duplicate(template_id, name, correlation_id)
- L310 async def update_template_tagstemplate_id: str, payload: dict, service: TemplateService=Depends(get_service):
  - L315 docstring: "Update tags for a template."
  - L316 assign tags = payload.get('tags', [])
  - L317 if not isinstance(tags, list):
    - L318 raise HTTPException(status_code=400, detail='Tags must be an array of strings')
  - L319 for tag in tags:
    - L320 if not isinstance(tag, str) or len(tag) > 50:
      - L321 raise HTTPException(status_code=400, detail='Each tag must be a string under 50 characters')
  - L322 return await service.update_tags(template_id, tags)
- L326 async def get_all_tagsservice: TemplateService=Depends(get_service):
  - L327 docstring: "Get all unique tags across all templates."
  - L328 return await service.get_all_tags()
- L336 async def recommend_templates_routepayload: TemplateRecommendPayload, request: Request, background: bool=Query(False):
  - L341 docstring: "Get AI-powered template recommendations based on user requirements."
  - L342 if not background:
    - L343 return recommend_templates(payload, request)
  - L345 assign correlation_id = _correlation(request)
  - L347 async def runnerjob_id: str:
    - L348 expr state_store.record_job_start(job_id)
    - L349 expr state_store.record_job_step(job_id, 'recommend', status='running', label='Generate recommendations')
    - L350 try:
      - L351 assign response = recommend_templates(payload, _request_with_correlation(correlation_id))
      - L352 if hasattr(response, 'model_dump'):
        - L353 assign result_payload = response.model_dump(mode='json')
        - L354 else:
          - L354 if hasattr(response, 'dict'):
            - L355 assign result_payload = response.dict()
            - L357 else:
              - L357 assign result_payload = response
      - L358 assign result_data = result_payload.get('recommendations') if isinstance(result_payload, dict) else result_payload
      - L363 expr state_store.record_job_step(job_id, 'recommend', status='succeeded', progress=100.0)
      - L364 expr state_store.record_job_completion(job_id, status='succeeded', result={'recommendations': result_data})
      - L369 except Exception as exc:
        - L370 expr state_store.record_job_step(job_id, 'recommend', status='failed', error=str(exc))
        - L371 expr state_store.record_job_completion(job_id, status='failed', error=str(exc))
  - L373 assign job = await enqueue_background_job(job_type='recommend_templates', steps=[{'name': 'recommend', 'label': 'Generate recommendations'}], meta={'background': True, 'requirement': payload.requirement}, runner=runner)
  - L379 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L387 def get_template_html_routetemplate_id: str, request: Request:
  - L388 docstring: "Get the current HTML content of a template."
  - L389 return get_template_html(template_id, request)
- L393 def edit_template_manual_routetemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L394 docstring: "Save manual HTML edits to a template."
  - L395 return edit_template_manual(template_id, payload, request)
- L399 def edit_template_ai_routetemplate_id: str, payload: TemplateAiEditPayload, request: Request:
  - L400 docstring: "Apply AI-powered edits to a template based on instructions."
  - L401 return edit_template_ai(template_id, payload, request)
- L405 def undo_last_edit_routetemplate_id: str, request: Request:
  - L406 docstring: "Undo the last edit made to a template."
  - L407 return undo_last_template_edit(template_id, request)
- L411 def chat_template_edit_routetemplate_id: str, payload: TemplateChatPayload, request: Request:
  - L412 docstring: "Conversational template editing endpoint."
  - L413 return chat_template_edit(template_id, payload, request)
- L417 def apply_chat_template_edit_routetemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L418 docstring: "Apply the HTML changes from a chat conversation."
  - L419 return apply_chat_template_edit(template_id, payload.html, request)
- L427 async def mapping_previewtemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False:
  - L428 docstring: "Preview mapping for a PDF template."
  - L429 return await run_mapping_preview(template_id, connection_id, request, force_refresh, kind='pdf')
- L433 async def mapping_approvetemplate_id: str, payload: MappingPayload, request: Request:
  - L434 docstring: "Approve mapping for a PDF template."
  - L435 return await run_mapping_approve(template_id, payload, request, kind='pdf')
- L439 def mapping_corrections_previewtemplate_id: str, payload: CorrectionsPreviewPayload, request: Request:
  - L440 docstring: "Preview corrections for PDF template mapping."
  - L441 return run_corrections_preview(template_id, payload, request, kind='pdf')
- L449 def generator_assets_routetemplate_id: str, payload: GeneratorAssetsPayload, request: Request:
  - L450 docstring: "Generate assets for a PDF template."
  - L451 return generator_assets(template_id, payload, request, kind='pdf')
- L459 def mapping_key_optionstemplate_id: str, request: Request, connection_id: str | None=None, tokens: str | None=None, limit: int=500, start_date: str | None=None, end_date: str | None=None, debug: bool=False:
  - L469 docstring: "Get available key options for template filtering."
  - L470 return mapping_key_options_service(template_id=template_id, request=request, connection_id=connection_id, tokens=tokens, limit=limit, start_date=start_date, end_date=end_date, kind='pdf', debug=debug)
- L488 def get_artifact_manifesttemplate_id: str, request: Request:
  - L489 docstring: "Get the artifact manifest for a template."
  - L490 assign data = artifact_manifest_response(template_id, kind='pdf')
  - L491 return _wrap(data, _correlation(request))
- L495 def get_artifact_headtemplate_id: str, request: Request, name: str:
  - L496 docstring: "Get the head (preview) of a specific artifact."
  - L497 assign data = artifact_head_response(template_id, name, kind='pdf')
  - L498 return _wrap(data, _correlation(request))
- L506 def suggest_charts_routetemplate_id: str, payload: ChartSuggestPayload, request: Request:
  - L507 docstring: "Get chart suggestions for a template."
  - L508 assign correlation_id = _correlation(request) or get_correlation_id()
  - L509 assign logger = logging.getLogger('neura.api')
  - L510 return suggest_charts_service(template_id, payload, kind='pdf', correlation_id=correlation_id, template_dir_fn=lambda tpl: template_dir(tpl, kind='pdf'), db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_fn=discover_batches_and_counts, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_metrics_fn=build_batch_metrics, build_prompt_fn=build_chart_suggestions_prompt, call_chat_completion_fn=lambda **kwargs: call_chat_completion(get_openai_client(), **kwargs, description=CHART_SUGGEST_PROMPT_VERSION), model=os.getenv('OPENAI_MODEL', 'gpt-4o-mini'), strip_code_fences_fn=strip_code_fences, logger=logger)
- L533 def list_saved_charts_routetemplate_id: str, request: Request:
  - L534 docstring: "List saved charts for a template."
  - L535 assign payload = list_saved_charts_service(template_id, _ensure_template_exists)
  - L536 return _wrap(payload, _correlation(request))
- L540 def create_saved_chart_routetemplate_id: str, payload: SavedChartCreatePayload, request: Request:
  - L545 docstring: "Create a saved chart for a template."
  - L546 assign chart = create_saved_chart_service(template_id, payload, ensure_template_exists=_ensure_template_exists, normalize_template_id=normalize_template_id)
  - L552 assign chart_payload = chart.model_dump(mode='json') if hasattr(chart, 'model_dump') else chart
  - L553 return _wrap(chart_payload, _correlation(request))
- L557 def update_saved_chart_routetemplate_id: str, chart_id: str, payload: SavedChartUpdatePayload, request: Request:
  - L563 docstring: "Update a saved chart."
  - L564 assign chart = update_saved_chart_service(template_id, chart_id, payload, _ensure_template_exists)
  - L565 assign chart_payload = chart.model_dump(mode='json') if hasattr(chart, 'model_dump') else chart
  - L566 return _wrap(chart_payload, _correlation(request))
- L570 def delete_saved_chart_routetemplate_id: str, chart_id: str, request: Request:
  - L575 docstring: "Delete a saved chart."
  - L576 assign payload = delete_saved_chart_service(template_id, chart_id, _ensure_template_exists)
  - L577 return _wrap(payload, _correlation(request))

## backend\app\core\__init__.py
- L1 docstring: "\nCore application wiring (settings, middleware, errors, security) for the v4 Fa..."

## backend\app\core\config.py
- L1 from __future__ import annotations
- L3 from functools import lru_cache
- L4 import json
- L5 import logging
- L6 import os
- L7 from pathlib import Path
- L8 from typing import Any, List, Optional
- L10 try:
  - L12 from pydantic_settings import BaseSettings, SettingsConfigDict
  - L14 assign _V2_SETTINGS = True
  - L15 except ImportError:
    - L16 from pydantic import BaseSettings
    - L18 assign SettingsConfigDict = None
    - L19 assign _V2_SETTINGS = False
- L20 from pydantic import Field
- L23 assign logger = logging.getLogger('neura.config')
- L26 def _default_uploads_root:
  - L27 return Path(__file__).resolve().parents[2] / 'uploads'
- L30 def _default_excel_uploads_root:
  - L31 return Path(__file__).resolve().parents[2] / 'uploads_excel'
- L34 def _default_state_dir:
  - L35 return Path(__file__).resolve().parents[2] / 'state'
- L38 def _load_version_info:
  - L39 assign version_path = Path(__file__).resolve().parents[1] / 'version.json'
  - L40 if not version_path.exists():
    - L41 return {'version': 'dev', 'commit': 'unknown'}
  - L42 try:
    - L43 return json.loads(version_path.read_text(encoding='utf-8'))
    - L44 except Exception as exc:
      - L45 expr logger.warning('version_info_load_failed', extra={'event': 'version_info_load_failed', 'error': str(exc)})
      - L46 return {'version': 'dev', 'commit': 'unknown'}
- L49 class Settings(BaseSettings):
  - L50 annotated assign api_title: str = 'NeuraReport API'
  - L51 annotated assign api_version: str = '4.0'
  - L52 annotated assign cors_origins: List[str] = Field(default_factory=lambda: ['http://localhost:3000'])
  - L53 annotated assign api_key: Optional[str] = Field(default=None, env='NEURA_API_KEY')
  - L55 annotated assign uploads_dir: Path = Field(default_factory=_default_uploads_root, env='UPLOAD_ROOT')
  - L56 annotated assign excel_uploads_dir: Path = Field(default_factory=_default_excel_uploads_root, env='EXCEL_UPLOAD_ROOT')
  - L57 annotated assign state_dir: Path = Field(default_factory=_default_state_dir, env='NEURA_STATE_DIR')
  - L59 annotated assign max_upload_bytes: int = 50 * 1024 * 1024
  - L60 annotated assign max_verify_pdf_bytes: int = 50 * 1024 * 1024
  - L61 annotated assign max_zip_entries: int = 2000
  - L62 annotated assign max_zip_uncompressed_bytes: int = 200 * 1024 * 1024
  - L64 annotated assign job_workers: int = 4
  - L65 annotated assign job_queue_size: int = 32
  - L66 annotated assign template_import_max_concurrency: int = 4
  - L68 annotated assign openai_api_key: Optional[str] = Field(default=None, env='OPENAI_API_KEY')
  - L69 annotated assign openai_model: str = Field(default='gpt-5', env='OPENAI_MODEL')
  - L71 annotated assign artifact_warn_bytes: int = Field(default=5 * 1024 * 1024, env='ARTIFACT_WARN_BYTES')
  - L72 annotated assign artifact_warn_render_ms: int = Field(default=2000, env='ARTIFACT_WARN_RENDER_MS')
  - L73 annotated assign version: str = Field(default='dev', env='NEURA_VERSION')
  - L74 annotated assign commit: str = Field(default='unknown', env='NEURA_COMMIT')
  - L77 annotated assign rate_limit_enabled: bool = Field(default=True, env='NEURA_RATE_LIMIT_ENABLED')
  - L78 annotated assign rate_limit_requests: int = Field(default=100, env='NEURA_RATE_LIMIT_REQUESTS')
  - L79 annotated assign rate_limit_window_seconds: int = Field(default=60, env='NEURA_RATE_LIMIT_WINDOW_SECONDS')
  - L80 annotated assign rate_limit_burst: int = Field(default=20, env='NEURA_RATE_LIMIT_BURST')
  - L83 annotated assign trusted_hosts: List[str] = Field(default_factory=lambda: ['localhost', '127.0.0.1'])
  - L84 annotated assign allowed_hosts_all: bool = True
  - L87 annotated assign request_timeout_seconds: int = 300
  - L90 annotated assign analysis_cache_max_items: int = 100
  - L91 annotated assign analysis_cache_ttl_seconds: int = 3600
  - L92 annotated assign analysis_max_concurrency: int = 4
  - L95 annotated assign debug_mode: bool = Field(default=False, env='NEURA_DEBUG')
  - L98 def uploads_rootself:
    - L99 return self.uploads_dir
  - L102 def excel_uploads_rootself:
    - L103 return self.excel_uploads_dir
  - L105 if _V2_SETTINGS:
    - L107 assign model_config = SettingsConfigDict(env_file='.env', extra='ignore')
    - L109 else:
      - L109 class Config:
        - L110 assign env_file = '.env'
        - L111 assign extra = 'ignore'
- L114 def _apply_runtime_defaultssettings: Settings:
  - L115 if isinstance(settings.openai_api_key, str) and (not settings.openai_api_key.strip()):
    - L116 assign settings.openai_api_key = None
  - L118 assign allow_missing = os.getenv('NEURA_ALLOW_MISSING_OPENAI', 'false').lower() == 'true'
  - L119 if not settings.openai_api_key and (not allow_missing):
    - L120 raise RuntimeError('OPENAI_API_KEY is required. Set NEURA_ALLOW_MISSING_OPENAI=true to bypass for tests (not for production).')
  - L124 assign force_gpt5 = os.getenv('NEURA_FORCE_GPT5', 'true').lower() in {'1', 'true', 'yes'}
  - L125 if force_gpt5 and (not str(settings.openai_model or '').lower().startswith('gpt-5')):
    - L126 expr logger.warning('openai_model_overridden', extra={'event': 'openai_model_overridden', 'requested': settings.openai_model, 'forced': 'gpt-5'})
    - L134 assign settings.openai_model = 'gpt-5'
  - L136 if not os.getenv('NEURA_VERSION') or not os.getenv('NEURA_COMMIT'):
    - L137 assign version_info = _load_version_info()
    - L138 if not os.getenv('NEURA_VERSION'):
      - L139 assign settings.version = str(version_info.get('version', settings.version))
    - L140 if not os.getenv('NEURA_COMMIT'):
      - L141 assign settings.commit = str(version_info.get('commit', settings.commit))
  - L143 expr settings.uploads_dir.mkdir(parents=True, exist_ok=True)
  - L144 expr settings.excel_uploads_dir.mkdir(parents=True, exist_ok=True)
  - L145 expr settings.state_dir.mkdir(parents=True, exist_ok=True)
  - L146 return settings
- L150 def get_settings:
  - L151 return _apply_runtime_defaults(Settings())
- L154 def log_settingstarget_logger: logging.Logger, settings: Settings:
  - L155 assign key_preview = f'...{settings.openai_api_key[-4:]}' if settings.openai_api_key else '(missing)'
  - L156 expr target_logger.info('app_config', extra={'event': 'app_config', 'version': settings.version, 'commit': settings.commit, 'openai_model': settings.openai_model, 'openai_key': key_preview, 'uploads_root': str(settings.uploads_root), 'excel_uploads_root': str(settings.excel_uploads_root), 'artifact_warn_bytes': settings.artifact_warn_bytes, 'artifact_warn_render_ms': settings.artifact_warn_render_ms})

## backend\app\core\errors.py
- L1 from __future__ import annotations
- L3 import logging
- L5 from fastapi import FastAPI, Request
- L6 from fastapi.responses import JSONResponse
- L8 assign logger = logging.getLogger('neura.api.errors')
- L11 class AppError(Exception):
  - L12 def __init__self, *, code: str, message: str, status_code: int=400, detail: str | None=None:
    - L13 assign self.code = code
    - L14 assign self.message = message
    - L15 assign self.status_code = status_code
    - L16 assign self.detail = detail
    - L17 expr super().__init__(message)
- L20 async def app_error_handlerrequest: Request, exc: AppError:
  - L21 assign correlation_id = getattr(getattr(request, 'state', None), 'correlation_id', None)
  - L22 assign body = {'status': 'error', 'code': exc.code, 'message': exc.message}
  - L23 if exc.detail:
    - L24 assign body['detail'] = exc.detail
  - L25 if correlation_id:
    - L26 assign body['correlation_id'] = correlation_id
  - L27 return JSONResponse(status_code=exc.status_code, content=body)
- L30 async def http_error_handlerrequest: Request, exc:
  - L31 assign correlation_id = getattr(getattr(request, 'state', None), 'correlation_id', None)
  - L32 assign detail = exc.detail if hasattr(exc, 'detail') else str(exc)
  - L33 assign status_code = exc.status_code if hasattr(exc, 'status_code') else 500
  - L34 assign body = {'status': 'error', 'code': f'http_{status_code}', 'message': detail}
  - L35 if correlation_id:
    - L36 assign body['correlation_id'] = correlation_id
  - L37 return JSONResponse(status_code=status_code, content=body)
- L40 async def generic_error_handlerrequest: Request, exc: Exception:
  - L41 docstring: "Handle any unhandled exceptions with proper logging."
  - L42 assign correlation_id = getattr(getattr(request, 'state', None), 'correlation_id', None)
  - L45 expr logger.exception('unhandled_exception', extra={'event': 'unhandled_exception', 'path': request.url.path, 'method': request.method, 'exception_type': type(exc).__name__, 'exception_message': str(exc), 'correlation_id': correlation_id})
  - L58 assign body = {'status': 'error', 'code': 'internal_error', 'message': 'An unexpected error occurred. Please try again later.'}
  - L63 if correlation_id:
    - L64 assign body['correlation_id'] = correlation_id
  - L66 return JSONResponse(status_code=500, content=body)
- L69 def add_exception_handlersapp: FastAPI:
  - L70 from fastapi import HTTPException
  - L72 expr app.add_exception_handler(AppError, app_error_handler)
  - L73 expr app.add_exception_handler(HTTPException, http_error_handler)
  - L74 expr app.add_exception_handler(Exception, generic_error_handler)

## backend\app\core\event_bus.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import time
- L5 from dataclasses import dataclass, field
- L6 from typing import Any, Awaitable, Callable, Dict, List, Optional, Protocol
- L8 from .result import _maybe_await
- L12 class Event:
  - L13 annotated assign name: str
  - L14 annotated assign payload: Dict[str, Any] = field(default_factory=dict)
  - L15 annotated assign correlation_id: Optional[str] = None
  - L16 annotated assign timestamp: float = field(default_factory=lambda: time.time())
- L19 class EventHandler(Protocol):
  - L20 def __call__self, event: Event:
    - L20 expr ...
- L23 class EventMiddleware(Protocol):
  - L24 def __call__self, event: Event, call_next: Callable[[Event], Awaitable[None]]:
    - L24 expr ...
- L27 class EventBus:
  - L28 def __init__self, *, middlewares: Optional[List[EventMiddleware]]=None:
    - L29 annotated assign self._handlers: Dict[str, List[EventHandler]] = {}
    - L30 assign self._middlewares = list(middlewares or [])
  - L32 def subscribeself, event_name: str, handler: EventHandler:
    - L33 expr self._handlers.setdefault(event_name, []).append(handler)
  - L35 async def publishself, event: Event:
    - L36 async def _dispatchev: Event:
      - L37 assign handlers = list(self._handlers.get(ev.name, []))
      - L38 for handler in handlers:
        - L39 expr await _maybe_await(handler(ev))
    - L41 async def _run_middlewareindex: int, ev: Event:
      - L42 if index >= len(self._middlewares):
        - L43 expr await _dispatch(ev)
        - L44 return None
      - L45 assign middleware = self._middlewares[index]
      - L46 expr await middleware(ev, lambda e=ev: _run_middleware(index + 1, e))
    - L48 expr await _run_middleware(0, event)
- L51 class NullEventBus(EventBus):
  - L52 async def publishself, event: Event:
    - L53 return None
  - L55 def subscribeself, event_name: str, handler: EventHandler:
    - L56 return None
- L59 def logging_middlewarelogger: logging.Logger:
  - L60 async def _middlewareevent: Event, call_next: Callable[[Event], Awaitable[None]]:
    - L61 expr logger.info('event_bus_publish', extra={'event': event.name, 'payload_keys': list(event.payload.keys()), 'correlation_id': event.correlation_id, 'ts': event.timestamp})
    - L70 expr await call_next(event)
  - L72 return _middleware
- L75 def metrics_middlewarelogger: logging.Logger:
  - L76 async def _middlewareevent: Event, call_next: Callable[[Event], Awaitable[None]]:
    - L77 assign started = time.time()
    - L78 try:
      - L79 expr await call_next(event)
      - L81 finally:
        - L81 assign elapsed_ms = int((time.time() - started) * 1000)
        - L82 expr logger.info('event_bus_metric', extra={'event': event.name, 'elapsed_ms': elapsed_ms, 'correlation_id': event.correlation_id})
  - L91 return _middleware

## backend\app\core\middleware.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import logging
- L5 import time
- L6 import uuid
- L8 from fastapi import FastAPI, Request
- L9 from fastapi.middleware.cors import CORSMiddleware
- L10 from fastapi.responses import JSONResponse
- L11 from slowapi import Limiter, _rate_limit_exceeded_handler
- L12 from slowapi.errors import RateLimitExceeded
- L13 from slowapi.middleware import SlowAPIMiddleware
- L14 from starlette.middleware.base import BaseHTTPMiddleware
- L15 from starlette.middleware.trustedhost import TrustedHostMiddleware
- L17 from ..services.utils.context import set_correlation_id
- L18 from .config import Settings
- L20 assign logger = logging.getLogger('neura.api')
- L22 def _get_client_keyrequest: Request:
  - L23 docstring: "Get unique client identifier for rate limiting."
  - L24 assign api_key = request.headers.get('x-api-key')
  - L25 if api_key:
    - L26 return f'key:{api_key[:16]}'
  - L27 assign forwarded = request.headers.get('x-forwarded-for')
  - L28 if forwarded:
    - L29 return forwarded.split(',')[0].strip()
  - L30 assign real_ip = request.headers.get('x-real-ip')
  - L31 if real_ip:
    - L32 return real_ip
  - L33 return request.client.host if request.client else 'unknown'
- L36 def _format_rate_limitrequests: int, window_seconds: int:
  - L37 if window_seconds <= 1:
    - L38 return f'{requests}/second'
  - L39 if window_seconds == 60:
    - L40 return f'{requests}/minute'
  - L41 if window_seconds == 3600:
    - L42 return f'{requests}/hour'
  - L43 if window_seconds == 86400:
    - L44 return f'{requests}/day'
  - L45 return f'{requests}/{window_seconds} second'
- L48 def _build_default_limitssettings: Settings:
  - L49 annotated assign limits: list[str] = []
  - L50 if settings.rate_limit_requests > 0 and settings.rate_limit_window_seconds > 0:
    - L51 expr limits.append(_format_rate_limit(settings.rate_limit_requests, settings.rate_limit_window_seconds))
  - L52 if settings.rate_limit_burst > 0:
    - L53 expr limits.append(f'{settings.rate_limit_burst}/second')
  - L54 return limits
- L57 assign limiter = Limiter(key_func=_get_client_key, default_limits=[], headers_enabled=True)
- L60 def _configure_limitersettings: Settings:
  - L61 assign limiter.default_limits = _build_default_limits(settings)
- L64 class SecurityHeadersMiddleware(BaseHTTPMiddleware):
  - L65 docstring: "Middleware to add security headers to all responses."
  - L67 async def dispatchself, request: Request, call_next:
    - L68 assign response = await call_next(request)
    - L71 assign response.headers['X-Frame-Options'] = 'DENY'
    - L74 assign response.headers['X-Content-Type-Options'] = 'nosniff'
    - L77 assign response.headers['X-XSS-Protection'] = '1; mode=block'
    - L80 assign response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
    - L83 assign response.headers['Content-Security-Policy'] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data: blob:; font-src 'self'; connect-src 'self'"
    - L93 assign response.headers['Permissions-Policy'] = 'geolocation=(), microphone=(), camera=()'
    - L97 return response
- L100 class RequestTimeoutMiddleware(BaseHTTPMiddleware):
  - L101 docstring: "Middleware to enforce request timeout."
  - L103 def __init__self, app, timeout_seconds: int=300:
    - L104 expr super().__init__(app)
    - L105 assign self.timeout_seconds = timeout_seconds
  - L107 async def dispatchself, request: Request, call_next:
    - L109 assign timeout = self.timeout_seconds
    - L110 if '/stream' in request.url.path or '/upload' in request.url.path:
      - L111 assign timeout = timeout * 2
    - L113 try:
      - L114 assign response = await asyncio.wait_for(call_next(request), timeout=timeout)
      - L118 return response
      - L119 except asyncio.TimeoutError:
        - L120 expr logger.error('request_timeout', extra={'event': 'request_timeout', 'path': request.url.path, 'method': request.method, 'timeout': timeout})
        - L129 return JSONResponse(status_code=504, content={'detail': 'Request timed out. Please try again with a smaller payload or simpler request.', 'timeout_seconds': timeout})
- L138 class CorrelationIdMiddleware(BaseHTTPMiddleware):
  - L139 docstring: "Middleware to handle correlation IDs and request logging."
  - L141 async def dispatchself, request: Request, call_next:
    - L142 assign correlation_id = request.headers.get('x-correlation-id') or uuid.uuid4().hex
    - L143 expr set_correlation_id(correlation_id)
    - L144 assign request.state.correlation_id = correlation_id
    - L145 assign started = time.time()
    - L147 expr logger.info('request_start', extra={'event': 'request_start', 'path': request.url.path, 'method': request.method, 'correlation_id': correlation_id})
    - L157 try:
      - L158 assign response = await call_next(request)
      - L159 except Exception:
        - L160 assign elapsed = int((time.time() - started) * 1000)
        - L161 expr logger.exception('request_error', extra={'event': 'request_error', 'path': request.url.path, 'method': request.method, 'elapsed_ms': elapsed, 'correlation_id': correlation_id})
        - L171 expr set_correlation_id(None)
        - L172 raise
    - L174 assign elapsed = int((time.time() - started) * 1000)
    - L175 expr logger.info('request_complete', extra={'event': 'request_complete', 'path': request.url.path, 'method': request.method, 'status': response.status_code, 'elapsed_ms': elapsed, 'correlation_id': correlation_id})
    - L186 assign response.headers['X-Correlation-ID'] = correlation_id
    - L187 expr response.headers.setdefault('X-Content-Type-Options', 'nosniff')
    - L188 assign content_type = response.headers.get('Content-Type', '')
    - L189 if content_type.startswith(('application/json', 'text/html', 'application/x-ndjson')):
      - L190 expr response.headers.setdefault('Cache-Control', 'no-store')
    - L191 expr set_correlation_id(None)
    - L192 return response
- L195 def add_middlewaresapp: FastAPI, settings: Settings:
  - L196 docstring: "Configure all application middlewares."
  - L199 assign cors_origins = settings.cors_origins
  - L200 if settings.debug_mode:
    - L202 assign cors_origins = ['*']
  - L204 expr app.add_middleware(CORSMiddleware, allow_origins=cors_origins, allow_methods=['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'OPTIONS'], allow_headers=['Content-Type', 'Authorization', 'X-API-Key', 'X-Correlation-ID', 'Accept'], allow_credentials=True, expose_headers=['X-Correlation-ID', 'X-RateLimit-Remaining', 'X-RateLimit-Limit'])
  - L220 if settings.allowed_hosts_all:
    - L221 assign allowed_hosts = ['*']
    - L223 else:
      - L223 assign allowed_hosts = settings.trusted_hosts
  - L224 expr app.add_middleware(TrustedHostMiddleware, allowed_hosts=allowed_hosts)
  - L227 expr app.add_middleware(SecurityHeadersMiddleware)
  - L230 expr app.add_middleware(RequestTimeoutMiddleware, timeout_seconds=settings.request_timeout_seconds)
  - L236 if settings.rate_limit_enabled:
    - L237 expr _configure_limiter(settings)
    - L238 assign app.state.limiter = limiter
    - L239 expr app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
    - L240 expr app.add_middleware(SlowAPIMiddleware)
  - L243 expr app.add_middleware(CorrelationIdMiddleware)

## backend\app\core\pipeline.py
- L1 from __future__ import annotations
- L3 import logging
- L4 from dataclasses import dataclass
- L5 from typing import Awaitable, Callable, Generic, List, Optional, Protocol, TypeVar
- L7 from .event_bus import Event, EventBus, NullEventBus
- L8 from .result import Result, err, ok, _maybe_await
- L10 assign Ctx = TypeVar('Ctx')
- L11 assign ErrType = TypeVar('ErrType')
- L14 class PipelineStepFn(Protocol[Ctx, ErrType]):
  - L15 def __call__self, ctx: Ctx:
    - L15 expr ...
- L18 assign GuardFn = Callable[[Ctx], bool]
- L22 class PipelineStep(Generic[Ctx, ErrType]):
  - L23 annotated assign name: str
  - L24 annotated assign fn: PipelineStepFn[Ctx, ErrType]
  - L25 annotated assign guard: GuardFn[Ctx] = lambda ctx: True
- L28 class PipelineRunner(Generic[Ctx, ErrType]):
  - L29 def __init__self, steps: List[PipelineStep[Ctx, ErrType]], *, bus: Optional[EventBus]=None, logger: Optional[logging.Logger]=None, correlation_id: Optional[str]=None:
    - L37 assign self.steps = steps
    - L38 assign self.bus = bus or NullEventBus()
    - L39 assign self.logger = logger or logging.getLogger('neura.pipeline')
    - L40 assign self.correlation_id = correlation_id
  - L42 async def runself, ctx: Ctx:
    - L43 assign current = ok(ctx)
    - L44 for step in self.steps:
      - L45 if not step.guard(ctx):
        - L46 continue
      - L47 expr await self._emit(f'pipeline.{step.name}.start', {'ctx_type': type(ctx).__name__})
      - L48 try:
        - L49 assign current = await _maybe_await(step.fn(current.unwrap()))
        - L50 except Exception as exc:
          - L51 expr self.logger.exception('pipeline_step_failed', extra={'event': 'pipeline_step_failed', 'step': step.name, 'correlation_id': self.correlation_id})
          - L59 return err(exc)
      - L61 if current.is_err:
        - L62 expr await self._emit(f'pipeline.{step.name}.error', {'ctx_type': type(ctx).__name__, 'error': str(current.unwrap_err())})
        - L69 return current
      - L71 assign ctx = current.unwrap()
      - L72 expr await self._emit(f'pipeline.{step.name}.ok', {'ctx_type': type(ctx).__name__})
    - L74 expr await self._emit('pipeline.complete', {'ctx_type': type(ctx).__name__})
    - L75 return current
  - L77 async def _emitself, name: str, payload: dict:
    - L78 expr await self.bus.publish(Event(name=name, payload=payload, correlation_id=self.correlation_id))

## backend\app\core\result.py
- L1 from __future__ import annotations
- L3 from dataclasses import dataclass
- L4 from typing import Callable, Generic, Optional, TypeVar, Union, Awaitable
- L6 assign T = TypeVar('T')
- L7 assign E = TypeVar('E')
- L8 assign U = TypeVar('U')
- L11 def _maybe_awaitvalue: Union[Awaitable[T], T]:
  - L12 if hasattr(value, '__await__'):
    - L13 return value
  - L15 async def _wrap:
    - L16 return value
  - L18 return _wrap()
- L22 class Result(Generic[T, E]):
  - L23 annotated assign value: Optional[T] = None
  - L24 annotated assign error: Optional[E] = None
  - L27 def is_okself:
    - L28 return self.error is None
  - L31 def is_errself:
    - L32 return self.error is not None
  - L34 def unwrapself:
    - L35 if self.error is not None:
      - L36 raise RuntimeError(f'Tried to unwrap Err result: {self.error}')
    - L37 return self.value
  - L39 def unwrap_errself:
    - L40 if self.error is None:
      - L41 raise RuntimeError('Tried to unwrap_err on Ok result')
    - L42 return self.error
  - L44 def mapself, fn: Callable[[T], U]:
    - L45 if self.is_err:
      - L46 return Result(error=self.error)
    - L47 return ok(fn(self.value))
  - L49 def bindself, fn: Callable[[T], 'Result[U, E]']:
    - L50 if self.is_err:
      - L51 return Result(error=self.error)
    - L52 return fn(self.value)
  - L54 async def bind_asyncself, fn: Callable[[T], Awaitable['Result[U, E]']]:
    - L55 if self.is_err:
      - L56 return Result(error=self.error)
    - L57 return await fn(self.value)
  - L59 def map_errself, fn: Callable[[E], U]:
    - L60 if self.is_ok:
      - L61 return Result(value=self.value)
    - L62 return err(fn(self.error))
  - L64 def unwrap_orself, default: T:
    - L65 return self.value if self.error is None else default
  - L67 def tapself, fn: Callable[[T], None]:
    - L68 if self.is_ok:
      - L69 expr fn(self.value)
    - L70 return self
  - L72 async def tap_asyncself, fn: Callable[[T], Awaitable[None]]:
    - L73 if self.is_ok:
      - L74 expr await _maybe_await(fn(self.value))
    - L75 return self
- L78 def okvalue: T:
  - L79 return Result(value=value, error=None)
- L82 def errerror: E:
  - L83 return Result(value=None, error=error)

## backend\app\core\security.py
- L1 from __future__ import annotations
- L3 import hmac
- L4 import secrets
- L6 from fastapi import Depends, Header
- L8 from .config import get_settings
- L9 from .errors import AppError
- L12 def constant_time_comparea: str | None, b: str | None:
  - L13 docstring: "\n    Compare two strings in constant time to prevent timing attacks.\n    Retur..."
  - L17 if a is None or b is None:
    - L18 return False
  - L21 return hmac.compare_digest(a.encode('utf-8'), b.encode('utf-8'))
- L24 async def require_api_keyx_api_key: str | None=Header(None), settings=Depends(get_settings):
  - L25 docstring: "\n    Lightweight API key gate. If NEURA_API_KEY is unset, the dependency is a n..."
  - L29 if not settings.api_key:
    - L30 return None
  - L31 if not constant_time_compare(x_api_key, settings.api_key):
    - L32 raise AppError(code='unauthorized', message='Invalid API key', status_code=401)

## backend\app\core\strategies.py
- L1 from __future__ import annotations
- L3 from typing import Callable, Dict, Generic, Optional, TypeVar
- L5 assign S = TypeVar('S')
- L8 class StrategyRegistry(Generic[S]):
  - L9 def __init__self, *, default_factory: Optional[Callable[[], S]]=None:
    - L10 annotated assign self._registry: Dict[str, S] = {}
    - L11 assign self._default_factory = default_factory
  - L13 def registerself, name: str, strategy: S:
    - L14 assign self._registry[name] = strategy
  - L16 def getself, name: str:
    - L17 return self._registry.get(name)
  - L19 def resolveself, name: str:
    - L20 if name in self._registry:
      - L21 return self._registry[name]
    - L22 if self._default_factory:
      - L23 return self._default_factory()
    - L24 raise KeyError(f"No strategy registered for '{name}'")

## backend\app\core\validation.py
- L1 docstring: "Input validation and sanitization utilities.\n\nProvides comprehensive validatio..."
- L13 from __future__ import annotations
- L15 import re
- L16 import unicodedata
- L17 from dataclasses import dataclass
- L18 from datetime import datetime
- L19 from pathlib import Path
- L20 from typing import Any, Callable, List, Optional, Tuple, TypeVar, Union
- L21 from urllib.parse import urlparse
- L23 assign T = TypeVar('T')
- L31 class ValidationResult:
  - L32 docstring: "Result of a validation operation."
  - L33 annotated assign valid: bool
  - L34 annotated assign value: Any = None
  - L35 annotated assign error: Optional[str] = None
  - L36 annotated assign field: Optional[str] = None
  - L39 def successvalue: Any=None:
    - L40 return ValidationResult(valid=True, value=value)
  - L43 def failureerror: str, field: Optional[str]=None:
    - L44 return ValidationResult(valid=False, error=error, field=field)
- L52 assign SAFE_ID_PATTERN = re.compile('^[a-zA-Z0-9][a-zA-Z0-9_-]{0,62}$')
- L53 assign SAFE_NAME_PATTERN = re.compile('^[\\w\\s\\-\\.()]{1,100}$', re.UNICODE)
- L54 assign SAFE_FILENAME_PATTERN = re.compile('^[\\w\\-\\.()]+$', re.UNICODE)
- L55 assign EMAIL_PATTERN = re.compile('^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')
- L58 assign UUID_PATTERN = re.compile('^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
- L62 assign SLUG_PATTERN = re.compile('^[a-z0-9]+(?:-[a-z0-9]+)*$')
- L65 assign DANGEROUS_PATH_PATTERNS = ['\\.\\.', '^/', '^[A-Za-z]:', '~', '\\$', '%', '\\x00']
- L76 assign SQL_INJECTION_PATTERNS = [';\\s*--', ';\\s*drop\\s', ';\\s*delete\\s', ';\\s*update\\s', ';\\s*insert\\s', 'union\\s+select', 'or\\s+1\\s*=\\s*1', "'\\s*or\\s*'"]
- L88 assign XSS_PATTERNS = ['<script', 'javascript:', 'on\\w+\\s*=', '<iframe', '<object', '<embed']
- L98 def is_safe_idvalue: str:
  - L99 docstring: "Check if a value is safe to use as an ID (alphanumeric with dashes/underscores)."
  - L100 if not value or not isinstance(value, str):
    - L101 return False
  - L102 return bool(SAFE_ID_PATTERN.match(value))
- L105 def is_safe_namevalue: str:
  - L106 docstring: "Check if a value is safe to use as a display name."
  - L107 if not value or not isinstance(value, str):
    - L108 return False
  - L109 return bool(SAFE_NAME_PATTERN.match(value)) and len(value) <= 100
- L112 def is_safe_filenamevalue: str:
  - L113 docstring: "Check if a value is safe to use as a filename."
  - L114 if not value or not isinstance(value, str):
    - L115 return False
  - L116 if not SAFE_FILENAME_PATTERN.match(value):
    - L117 return False
  - L119 for pattern in DANGEROUS_PATH_PATTERNS:
    - L120 if re.search(pattern, value):
      - L121 return False
  - L122 return True
- L125 def sanitize_idvalue: str:
  - L126 docstring: "Sanitize a string to be safe for use as an ID."
  - L127 if not value:
    - L128 return ''
  - L130 assign sanitized = re.sub('[^a-zA-Z0-9_-]', '', value)
  - L132 assign sanitized = re.sub('^[^a-zA-Z0-9]+', '', sanitized)
  - L133 return sanitized[:63]
- L136 def sanitize_filenamevalue: str:
  - L137 docstring: "Sanitize a string to be safe for use as a filename."
  - L138 if not value:
    - L139 return ''
  - L141 assign sanitized = re.sub('[/\\\\:*?\\"<>|]', '', value)
  - L143 assign sanitized = sanitized.replace('..', '')
  - L145 assign sanitized = sanitized.strip('. ')
  - L147 if not sanitized:
    - L148 return 'unnamed'
  - L149 return sanitized[:255]
- L152 def validate_path_safetypath: str | Path:
  - L153 docstring: "\n    Validate that a path is safe (no traversal attacks, etc).\n    Returns (is..."
  - L157 assign path_str = str(path)
  - L160 for pattern in DANGEROUS_PATH_PATTERNS:
    - L161 if re.search(pattern, path_str):
      - L162 return (False, f'Path contains disallowed pattern: {pattern}')
  - L165 if '\x00' in path_str:
    - L166 return (False, 'Path contains null byte')
  - L168 return (True, None)
- L171 def validate_file_extensionfilename: str, allowed_extensions: list[str]:
  - L172 docstring: "\n    Validate that a file has an allowed extension.\n    Returns (is_valid, err..."
  - L176 if not filename:
    - L177 return (False, 'Filename is required')
  - L179 assign ext = Path(filename).suffix.lower()
  - L180 if not ext:
    - L181 return (False, 'File must have an extension')
  - L184 assign allowed = [e.lower() if e.startswith('.') else f'.{e.lower()}' for e in allowed_extensions]
  - L186 if ext not in allowed:
    - L187 return (False, f"Invalid file type '{ext}'. Allowed: {', '.join(allowed)}")
  - L189 return (True, None)
- L192 def sanitize_sql_identifiervalue: str:
  - L193 docstring: "\n    Sanitize a SQL identifier (table/column name).\n    Note: This is for disp..."
  - L198 if not value:
    - L199 return ''
  - L201 assign sanitized = re.sub('[^a-zA-Z0-9_]', '', value)
  - L202 return sanitized[:128]
- L205 def validate_json_string_lengthvalue: str, max_length: int=10000:
  - L206 docstring: "Validate that a JSON string field is not too long."
  - L207 if not value:
    - L208 return (True, None)
  - L209 if len(value) > max_length:
    - L210 return (False, f'Value too long (max {max_length} characters)')
  - L211 return (True, None)
- L218 def is_valid_emailvalue: str:
  - L219 docstring: "Check if a string is a valid email address."
  - L220 if not value or not isinstance(value, str):
    - L221 return False
  - L222 return bool(EMAIL_PATTERN.match(value.strip()))
- L225 def is_valid_uuidvalue: str:
  - L226 docstring: "Check if a string is a valid UUID."
  - L227 if not value or not isinstance(value, str):
    - L228 return False
  - L229 return bool(UUID_PATTERN.match(value.strip()))
- L232 def is_valid_slugvalue: str:
  - L233 docstring: "Check if a string is a valid URL slug."
  - L234 if not value or not isinstance(value, str):
    - L235 return False
  - L236 return bool(SLUG_PATTERN.match(value.strip()))
- L239 def is_valid_urlvalue: str, require_https: bool=False:
  - L240 docstring: "Check if a string is a valid URL."
  - L241 if not value or not isinstance(value, str):
    - L242 return False
  - L244 try:
    - L245 assign parsed = urlparse(value.strip())
    - L246 if not parsed.scheme or not parsed.netloc:
      - L247 return False
    - L248 if require_https and parsed.scheme != 'https':
      - L249 return False
    - L250 if parsed.scheme not in ('http', 'https'):
      - L251 return False
    - L252 return True
    - L253 except Exception:
      - L254 return False
- L257 def contains_sql_injectionvalue: str:
  - L258 docstring: "Check if a string contains potential SQL injection patterns."
  - L259 if not value:
    - L260 return False
  - L262 assign lower_value = value.lower()
  - L263 for pattern in SQL_INJECTION_PATTERNS:
    - L264 if re.search(pattern, lower_value, re.IGNORECASE):
      - L265 return True
  - L266 return False
- L269 def contains_xssvalue: str:
  - L270 docstring: "Check if a string contains potential XSS patterns."
  - L271 if not value:
    - L272 return False
  - L274 assign lower_value = value.lower()
  - L275 for pattern in XSS_PATTERNS:
    - L276 if re.search(pattern, lower_value, re.IGNORECASE):
      - L277 return True
  - L278 return False
- L281 def sanitize_htmlvalue: str:
  - L282 docstring: "Remove potentially dangerous HTML content."
  - L283 if not value:
    - L284 return ''
  - L287 assign result = re.sub('<script[^>]*>.*?</script>', '', value, flags=re.IGNORECASE | re.DOTALL)
  - L290 assign result = re.sub('\\s*on\\w+\\s*=\\s*[\\"\'][^\\"\']*[\\"\']', '', result, flags=re.IGNORECASE)
  - L293 assign result = re.sub('javascript:', '', result, flags=re.IGNORECASE)
  - L295 return result
- L298 def normalize_stringvalue: str:
  - L299 docstring: "Normalize a string by removing control characters and normalizing unicode."
  - L300 if not value:
    - L301 return ''
  - L304 assign normalized = unicodedata.normalize('NFC', value)
  - L307 assign normalized = ''.join((char for char in normalized if char == '\n' or char == '\t' or (not unicodedata.category(char).startswith('C'))))
  - L312 return normalized.strip()
- L315 def validate_numeric_rangevalue: Union[int, float], min_value: Optional[Union[int, float]]=None, max_value: Optional[Union[int, float]]=None, field_name: str='value':
  - L321 docstring: "Validate that a numeric value is within a range."
  - L322 if min_value is not None and value < min_value:
    - L323 return (False, f'{field_name} must be at least {min_value}')
  - L324 if max_value is not None and value > max_value:
    - L325 return (False, f'{field_name} must be at most {max_value}')
  - L326 return (True, None)
- L329 def validate_date_stringvalue: str, formats: Optional[List[str]]=None:
  - L333 docstring: "\n    Validate that a string is a valid date.\n    Returns (is_valid, parsed_dat..."
  - L337 if not value:
    - L338 return (False, None)
  - L340 assign formats = formats or ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d %H:%M:%S']
  - L350 for fmt in formats:
    - L351 try:
      - L352 assign parsed = datetime.strptime(value.strip(), fmt)
      - L353 return (True, parsed)
      - L354 except ValueError:
        - L355 continue
  - L357 return (False, None)
- L360 def validate_required_fieldsdata: dict, required_fields: List[str]:
  - L364 docstring: "\n    Validate that all required fields are present and non-empty.\n    Returns ..."
  - L368 assign missing = []
  - L369 for field in required_fields:
    - L370 assign value = data.get(field)
    - L371 if value is None or (isinstance(value, str) and (not value.strip())):
      - L372 expr missing.append(field)
  - L374 return (len(missing) == 0, missing)
- L377 def validate_field_typevalue: Any, expected_type: type, field_name: str='value':
  - L382 docstring: "Validate that a value is of the expected type."
  - L383 if not isinstance(value, expected_type):
    - L384 return (False, f'{field_name} must be of type {expected_type.__name__}')
  - L385 return (True, None)
- L388 def truncate_stringvalue: str, max_length: int, suffix: str='...':
  - L389 docstring: "Truncate a string to max length, adding suffix if truncated."
  - L390 if not value or len(value) <= max_length:
    - L391 return value
  - L392 return value[:max_length - len(suffix)] + suffix
- L395 def generate_safe_idvalue: str, max_length: int=63:
  - L396 docstring: "Generate a safe ID from a string."
  - L397 if not value:
    - L398 return ''
  - L401 assign safe = normalize_string(value).lower()
  - L404 assign safe = re.sub('[^a-z0-9]+', '-', safe)
  - L407 assign safe = safe.strip('-')
  - L410 if safe and (not safe[0].isalnum()):
    - L411 assign safe = 'x' + safe
  - L413 return safe[:max_length]
- L420 class Validator:
  - L421 docstring: "\n    Chainable validator for building complex validation rules.\n\n    Usage:\n..."
  - L430 def __init__self, value: Any, field_name: str='value':
    - L431 assign self._value = value
    - L432 assign self._field = field_name
    - L433 annotated assign self._errors: List[str] = []
    - L434 assign self._stop_on_first_error = False
  - L436 def stop_on_first_errorself:
    - L437 docstring: "Stop validation after first error."
    - L438 assign self._stop_on_first_error = True
    - L439 return self
  - L441 def _add_errorself, error: str:
    - L442 expr self._errors.append(error)
  - L444 def _should_continueself:
    - L445 return not self._stop_on_first_error or not self._errors
  - L447 def requiredself, message: Optional[str]=None:
    - L448 docstring: "Validate that the value is not None or empty."
    - L449 if not self._should_continue():
      - L450 return self
    - L452 if self._value is None:
      - L453 expr self._add_error(message or f'{self._field} is required')
      - L454 else:
        - L454 if isinstance(self._value, str) and (not self._value.strip()):
          - L455 expr self._add_error(message or f'{self._field} cannot be empty')
    - L457 return self
  - L459 def min_lengthself, length: int, message: Optional[str]=None:
    - L460 docstring: "Validate minimum length."
    - L461 if not self._should_continue():
      - L462 return self
    - L464 if isinstance(self._value, (str, list, dict)) and len(self._value) < length:
      - L465 expr self._add_error(message or f'{self._field} must be at least {length} characters')
    - L467 return self
  - L469 def max_lengthself, length: int, message: Optional[str]=None:
    - L470 docstring: "Validate maximum length."
    - L471 if not self._should_continue():
      - L472 return self
    - L474 if isinstance(self._value, (str, list, dict)) and len(self._value) > length:
      - L475 expr self._add_error(message or f'{self._field} must be at most {length} characters')
    - L477 return self
  - L479 def patternself, regex: str, message: Optional[str]=None:
    - L480 docstring: "Validate against a regex pattern."
    - L481 if not self._should_continue():
      - L482 return self
    - L484 if isinstance(self._value, str) and (not re.match(regex, self._value)):
      - L485 expr self._add_error(message or f'{self._field} has invalid format')
    - L487 return self
  - L489 def emailself, message: Optional[str]=None:
    - L490 docstring: "Validate as email address."
    - L491 if not self._should_continue():
      - L492 return self
    - L494 if isinstance(self._value, str) and (not is_valid_email(self._value)):
      - L495 expr self._add_error(message or f'{self._field} must be a valid email address')
    - L497 return self
  - L499 def urlself, require_https: bool=False, message: Optional[str]=None:
    - L500 docstring: "Validate as URL."
    - L501 if not self._should_continue():
      - L502 return self
    - L504 if isinstance(self._value, str) and (not is_valid_url(self._value, require_https)):
      - L505 expr self._add_error(message or f'{self._field} must be a valid URL')
    - L507 return self
  - L509 def safe_idself, message: Optional[str]=None:
    - L510 docstring: "Validate as safe ID."
    - L511 if not self._should_continue():
      - L512 return self
    - L514 if isinstance(self._value, str) and (not is_safe_id(self._value)):
      - L515 expr self._add_error(message or f'{self._field} contains invalid characters')
    - L517 return self
  - L519 def no_sql_injectionself, message: Optional[str]=None:
    - L520 docstring: "Validate against SQL injection patterns."
    - L521 if not self._should_continue():
      - L522 return self
    - L524 if isinstance(self._value, str) and contains_sql_injection(self._value):
      - L525 expr self._add_error(message or f'{self._field} contains potentially dangerous content')
    - L527 return self
  - L529 def no_xssself, message: Optional[str]=None:
    - L530 docstring: "Validate against XSS patterns."
    - L531 if not self._should_continue():
      - L532 return self
    - L534 if isinstance(self._value, str) and contains_xss(self._value):
      - L535 expr self._add_error(message or f'{self._field} contains potentially dangerous content')
    - L537 return self
  - L539 def customself, validator: Callable[[Any], bool], message: str:
    - L540 docstring: "Apply a custom validation function."
    - L541 if not self._should_continue():
      - L542 return self
    - L544 if not validator(self._value):
      - L545 expr self._add_error(message)
    - L547 return self
  - L549 def validateself:
    - L550 docstring: "Execute validation and return result."
    - L551 if self._errors:
      - L552 return ValidationResult.failure(error='; '.join(self._errors), field=self._field)
    - L556 return ValidationResult.success(value=self._value)

## backend\app\domain\__init__.py
- L1 docstring: "Domain layer for v4 services (schemas, repositories, services)."

## backend\app\domain\charts\__init__.py
- L1 docstring: "Auto-Chart Generation domain module."
- L2 from .service import AutoChartService
- L4 assign __all__ = ['AutoChartService']

## backend\app\domain\charts\service.py
- L1 docstring: "Service for automatic chart generation using AI."
- L2 from __future__ import annotations
- L4 import logging
- L5 from typing import Any, Dict, List, Optional
- L7 from backend.app.services.llm.client import get_llm_client
- L9 assign logger = logging.getLogger('neura.domain.charts')
- L12 class AutoChartService:
  - L13 docstring: "Service for automatic chart generation."
  - L15 def __init__self:
    - L16 assign self._llm_client = None
  - L18 def _get_llm_clientself:
    - L19 if self._llm_client is None:
      - L20 assign self._llm_client = get_llm_client()
    - L21 return self._llm_client
  - L23 def analyze_data_for_chartsself, data: List[Dict[str, Any]], column_descriptions: Optional[Dict[str, str]]=None, max_suggestions: int=3, correlation_id: Optional[str]=None:
    - L30 docstring: "\n        Analyze data and suggest appropriate chart visualizations.\n\n        ..."
    - L42 expr logger.info('Analyzing data for chart suggestions', extra={'correlation_id': correlation_id})
    - L44 if not data:
      - L45 return []
    - L48 assign columns = list(data[0].keys()) if data else []
    - L49 assign column_stats = {}
    - L51 for col in columns:
      - L52 assign values = [row.get(col) for row in data if row.get(col) is not None]
      - L53 if not values:
        - L54 continue
      - L57 assign sample = values[0]
      - L58 if isinstance(sample, (int, float)):
        - L59 assign col_type = 'numeric'
        - L60 else:
          - L60 if isinstance(sample, bool):
            - L61 assign col_type = 'boolean'
            - L63 else:
              - L63 assign unique_ratio = len(set((str(v) for v in values))) / len(values)
              - L64 assign col_type = 'categorical' if unique_ratio < 0.5 else 'text'
      - L66 assign column_stats[col] = {'type': col_type, 'unique_count': len(set((str(v) for v in values))), 'sample_values': [str(v) for v in values[:3]]}
    - L73 assign prompt = f"""Analyze this data structure and suggest appropriate chart visualizations.\n\nCOLUMNS:\n{column_stats}\n\n{(f'COLUMN DESCRIPTIONS: {column_descriptions}' if column_descriptions else '')}\n\nSuggest up to {max_suggestions} charts. For each chart, provide:\n- type: "bar", "line", "pie", or "scatter"\n- title: Descriptive chart title\n- xField: Column for X-axis\n- yFields: Array of columns for Y-axis\n- description: Why this visualization is useful\n\nReturn a JSON array:\n[\n  {{\n    "type": "bar",\n    "title": "Chart Title",\n    "xField": "column_name",\n    "yFields": ["value_column"],\n    "description": "Shows distribution of values"\n  }}\n]\n\nReturn ONLY the JSON array."""
    - L100 try:
      - L101 assign client = self._get_llm_client()
      - L102 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='chart_suggestions', temperature=0.5)
      - L108 import json
      - L109 import re
      - L110 assign content = response['choices'][0]['message']['content']
      - L111 assign json_match = re.search('\\[[\\s\\S]*\\]', content)
      - L112 if json_match:
        - L113 return json.loads(json_match.group())[:max_suggestions]
      - L115 except Exception as exc:
        - L116 expr logger.error(f'Chart suggestion failed: {exc}')
    - L119 assign numeric_cols = [c for c, s in column_stats.items() if s['type'] == 'numeric']
    - L120 assign categorical_cols = [c for c, s in column_stats.items() if s['type'] == 'categorical']
    - L122 assign suggestions = []
    - L123 if numeric_cols and categorical_cols:
      - L124 expr suggestions.append({'type': 'bar', 'title': f'{numeric_cols[0]} by {categorical_cols[0]}', 'xField': categorical_cols[0], 'yFields': [numeric_cols[0]], 'description': 'Bar chart showing values by category'})
    - L132 return suggestions
  - L134 def generate_chart_configself, data: List[Dict[str, Any]], chart_type: str, x_field: str, y_fields: List[str], title: Optional[str]=None:
    - L142 docstring: "Generate a complete chart configuration."
    - L143 return {'type': chart_type, 'title': title or f"{', '.join(y_fields)} by {x_field}", 'xField': x_field, 'yFields': y_fields, 'data': data, 'config': {'responsive': True, 'maintainAspectRatio': True}}

## backend\app\domain\connections\__init__.py
- L1 docstring: "Connections domain objects."

## backend\app\domain\connections\repository.py
- L1 from __future__ import annotations
- L3 from pathlib import Path
- L5 from backend.app.services.connections.db_connection import resolve_db_path, save_connection, verify_sqlite
- L6 from backend.app.services.state import state_store
- L9 class ConnectionRepository:
  - L10 def resolve_pathself, *, connection_id: str | None, db_url: str | None, db_path: str | None:
    - L11 return resolve_db_path(connection_id=connection_id, db_url=db_url, db_path=db_path)
  - L13 def verifyself, path: Path:
    - L14 expr verify_sqlite(path)
  - L16 def saveself, payload: dict:
    - L17 return save_connection(payload)
  - L19 def upsertself, **kwargs:
    - L20 return state_store.upsert_connection(**kwargs)
  - L22 def listself:
    - L23 return state_store.list_connections()
  - L25 def deleteself, connection_id: str:
    - L26 return state_store.delete_connection(connection_id)
  - L28 def record_pingself, connection_id: str, status: str, detail: str | None, latency_ms: float | None:
    - L29 expr state_store.record_connection_ping(connection_id, status=status, detail=detail, latency_ms=latency_ms)

## backend\app\domain\connections\schemas.py
- L1 from __future__ import annotations
- L3 from pathlib import Path
- L4 from typing import Optional
- L6 from pydantic import BaseModel, Field, validator
- L8 from backend.app.core.validation import is_safe_id, is_safe_name, sanitize_id, sanitize_filename
- L11 class ConnectionTestRequest(BaseModel):
  - L12 annotated assign db_url: Optional[str] = Field(None, max_length=1000)
  - L13 annotated assign database: Optional[str] = Field(None, max_length=500)
  - L14 annotated assign db_type: str = Field(default='sqlite', max_length=50)
  - L17 def enforce_sqlitecls, value: str:
    - L18 if (value or '').lower() != 'sqlite':
      - L19 raise ValueError('Only sqlite is supported in this build')
    - L20 return value
  - L23 def validate_database_pathcls, value: Optional[str]:
    - L24 if value is None:
      - L25 return None
    - L27 if '..' in value:
      - L28 raise ValueError('Path traversal not allowed')
    - L29 return value
- L32 class ConnectionUpsertRequest(ConnectionTestRequest):
  - L33 annotated assign id: Optional[str] = Field(None, max_length=64)
  - L34 annotated assign name: Optional[str] = Field(None, max_length=100)
  - L35 annotated assign status: Optional[str] = Field(None, max_length=50)
  - L36 annotated assign latency_ms: Optional[float] = Field(None, ge=0, le=1000000)
  - L37 annotated assign tags: Optional[list[str]] = Field(None, max_items=20)
  - L40 def validate_idcls, value: Optional[str]:
    - L41 if value is None:
      - L42 return None
    - L43 if not is_safe_id(value):
      - L44 raise ValueError('ID must be alphanumeric with dashes/underscores only')
    - L45 return value
  - L48 def validate_namecls, value: Optional[str]:
    - L49 if value is None:
      - L50 return None
    - L51 if not is_safe_name(value):
      - L52 raise ValueError('Name contains invalid characters')
    - L53 return value
  - L56 def validate_tagcls, value: str:
    - L57 if len(value) > 50:
      - L58 raise ValueError('Tag must be 50 characters or less')
    - L59 return value.strip()
- L62 class ConnectionResponse(BaseModel):
  - L63 annotated assign id: str
  - L64 annotated assign name: str
  - L65 annotated assign db_type: str
  - L66 annotated assign database_path: Path
  - L67 annotated assign status: str
  - L68 annotated assign latency_ms: Optional[float] = None

## backend\app\domain\connections\service.py
- L1 from __future__ import annotations
- L3 import time
- L4 from pathlib import Path
- L6 from .repository import ConnectionRepository
- L7 from .schemas import ConnectionResponse, ConnectionTestRequest, ConnectionUpsertRequest
- L8 from backend.app.core.errors import AppError
- L11 class ConnectionService:
  - L12 def __init__self, repo: ConnectionRepository:
    - L13 assign self.repo = repo
  - L15 def _resolve_and_verifyself, *, connection_id: str | None, db_url: str | None, db_path: str | None, verify: bool=True:
    - L23 try:
      - L24 assign path = self.repo.resolve_path(connection_id=connection_id, db_url=db_url, db_path=db_path)
      - L25 if verify:
        - L26 expr self.repo.verify(path)
      - L27 return path
      - L28 except Exception as exc:
        - L29 raise AppError(code='invalid_database', message='Invalid or unreachable database', detail=str(exc), status_code=400)
  - L36 def testself, payload: ConnectionTestRequest, correlation_id: str | None=None:
    - L37 assign started = time.time()
    - L38 assign db_path = self._resolve_and_verify(connection_id=None, db_url=payload.db_url, db_path=payload.database)
    - L39 assign latency_ms = int((time.time() - started) * 1000)
    - L40 assign resolved = Path(db_path).resolve()
    - L41 assign cfg = {'db_type': 'sqlite', 'database': str(resolved), 'db_url': payload.db_url, 'name': resolved.name, 'status': 'connected', 'latency_ms': latency_ms}
    - L49 assign connection_id = self.repo.save(cfg)
    - L50 expr self.repo.record_ping(connection_id, status='connected', detail='Connected', latency_ms=latency_ms)
    - L51 return {'ok': True, 'details': f'Connected ({resolved.name})', 'latency_ms': latency_ms, 'connection_id': connection_id, 'normalized': {'db_type': 'sqlite', 'database': str(resolved)}, 'correlation_id': correlation_id}
  - L60 def upsertself, payload: ConnectionUpsertRequest, correlation_id: str | None=None:
    - L61 assign db_path = self._resolve_and_verify(connection_id=payload.id, db_url=payload.db_url, db_path=payload.database, verify=False)
    - L67 assign record = self.repo.upsert(conn_id=payload.id, name=payload.name or Path(db_path).name, db_type='sqlite', database_path=str(db_path), secret_payload={'db_url': payload.db_url, 'database': str(db_path)}, status=payload.status, latency_ms=payload.latency_ms, tags=payload.tags)
    - L77 if payload.status:
      - L78 expr self.repo.record_ping(record['id'], status=payload.status, detail=None, latency_ms=payload.latency_ms)
    - L79 return ConnectionResponse(id=record['id'], name=record['name'], db_type=record['db_type'], database_path=Path(record['database_path']), status=record.get('status') or 'unknown', latency_ms=record.get('latency_ms'))
  - L88 def deleteself, connection_id: str:
    - L89 if not self.repo.delete(connection_id):
      - L90 raise AppError(code='connection_not_found', message='Connection not found', status_code=404)
  - L92 def healthcheckself, connection_id: str, correlation_id: str | None=None:
    - L93 docstring: "Verify a saved connection is still accessible and record the ping."
    - L94 assign connections = self.repo.list()
    - L95 assign conn = next((c for c in connections if c.get('id') == connection_id), None)
    - L96 if not conn:
      - L97 raise AppError(code='connection_not_found', message='Connection not found', status_code=404)
    - L99 assign db_path = conn.get('database_path')
    - L100 if not db_path:
      - L101 raise AppError(code='invalid_connection', message='Connection has no database path', status_code=400)
    - L107 assign started = time.time()
    - L108 try:
      - L109 expr self.repo.verify(Path(db_path))
      - L110 except Exception as exc:
        - L111 assign latency_ms = int((time.time() - started) * 1000)
        - L112 expr self.repo.record_ping(connection_id, status='error', detail=str(exc), latency_ms=latency_ms)
        - L113 raise AppError(code='connection_failed', message='Database connection failed', detail=str(exc), status_code=503)
    - L120 assign latency_ms = int((time.time() - started) * 1000)
    - L121 expr self.repo.record_ping(connection_id, status='connected', detail='Health check passed', latency_ms=latency_ms)
    - L122 return {'status': 'connected', 'latency_ms': latency_ms, 'connection_id': connection_id, 'correlation_id': correlation_id}

## backend\app\domain\docqa\__init__.py
- L1 docstring: "Document Q&A Chat domain module."
- L2 from .schemas import DocQASession, ChatMessage, Citation, AskRequest, AskResponse
- L9 from .service import DocumentQAService
- L11 assign __all__ = ['DocQASession', 'ChatMessage', 'Citation', 'AskRequest', 'AskResponse', 'DocumentQAService']

## backend\app\domain\docqa\schemas.py
- L1 docstring: "Schemas for Document Q&A Chat."
- L2 from __future__ import annotations
- L4 from datetime import datetime
- L5 from typing import Any, Dict, List, Optional
- L6 from pydantic import BaseModel, Field
- L7 from enum import Enum
- L10 class MessageRole(str, Enum):
  - L11 assign USER = 'user'
  - L12 assign ASSISTANT = 'assistant'
  - L13 assign SYSTEM = 'system'
- L16 class FeedbackType(str, Enum):
  - L17 assign HELPFUL = 'helpful'
  - L18 assign NOT_HELPFUL = 'not_helpful'
- L21 class MessageFeedback(BaseModel):
  - L22 docstring: "Feedback on a message."
  - L24 annotated assign feedback_type: FeedbackType
  - L25 annotated assign timestamp: datetime = Field(default_factory=datetime.utcnow)
  - L26 annotated assign comment: Optional[str] = None
- L29 class Citation(BaseModel):
  - L30 docstring: "A citation to a document source."
  - L32 annotated assign document_id: str
  - L33 annotated assign document_name: str
  - L34 annotated assign page_number: Optional[int] = None
  - L35 annotated assign section: Optional[str] = None
  - L36 annotated assign quote: str
  - L37 annotated assign relevance_score: float = Field(default=1.0, ge=0.0, le=1.0)
- L40 class ChatMessage(BaseModel):
  - L41 docstring: "A message in the Q&A chat."
  - L43 annotated assign id: str
  - L44 annotated assign role: MessageRole
  - L45 annotated assign content: str
  - L46 annotated assign citations: List[Citation] = Field(default_factory=list)
  - L47 annotated assign timestamp: datetime = Field(default_factory=datetime.utcnow)
  - L48 annotated assign metadata: Dict[str, Any] = Field(default_factory=dict)
  - L49 annotated assign feedback: Optional[MessageFeedback] = None
- L52 class DocumentReference(BaseModel):
  - L53 docstring: "A document added to a Q&A session."
  - L55 annotated assign id: str
  - L56 annotated assign name: str
  - L57 annotated assign content_preview: str
  - L58 annotated assign full_content: str
  - L59 annotated assign page_count: Optional[int] = None
  - L60 annotated assign added_at: datetime = Field(default_factory=datetime.utcnow)
- L63 class DocQASession(BaseModel):
  - L64 docstring: "A Document Q&A chat session."
  - L66 annotated assign id: str
  - L67 annotated assign name: str
  - L68 annotated assign documents: List[DocumentReference] = Field(default_factory=list)
  - L69 annotated assign messages: List[ChatMessage] = Field(default_factory=list)
  - L70 annotated assign context_window: int = Field(default=10)
  - L71 annotated assign created_at: datetime = Field(default_factory=datetime.utcnow)
  - L72 annotated assign updated_at: datetime = Field(default_factory=datetime.utcnow)
- L75 class AskRequest(BaseModel):
  - L76 docstring: "Request to ask a question."
  - L78 annotated assign question: str = Field(..., min_length=3, max_length=2000)
  - L79 annotated assign include_citations: bool = Field(default=True)
  - L80 annotated assign max_response_length: int = Field(default=2000, ge=100, le=10000)
- L83 class AskResponse(BaseModel):
  - L84 docstring: "Response to a question."
  - L86 annotated assign message: ChatMessage
  - L87 annotated assign processing_time_ms: int
  - L88 annotated assign tokens_used: Optional[int] = None
- L91 class FeedbackRequest(BaseModel):
  - L92 docstring: "Request to submit feedback on a message."
  - L94 annotated assign feedback_type: FeedbackType
  - L95 annotated assign comment: Optional[str] = None
- L98 class RegenerateRequest(BaseModel):
  - L99 docstring: "Request to regenerate a response."
  - L101 annotated assign include_citations: bool = Field(default=True)
  - L102 annotated assign max_response_length: int = Field(default=2000, ge=100, le=10000)

## backend\app\domain\docqa\service.py
- L1 docstring: "Service for Document Q&A Chat using AI."
- L2 from __future__ import annotations
- L4 import json
- L5 import logging
- L6 import re
- L7 import time
- L8 import uuid
- L9 from datetime import datetime
- L10 from typing import Any, Dict, List, Optional
- L12 from backend.app.services.llm.client import get_llm_client
- L13 from backend.app.services.state import store as state_store_module
- L15 from .schemas import AskRequest, AskResponse, ChatMessage, Citation, DocQASession, DocumentReference, FeedbackRequest, FeedbackType, MessageFeedback, MessageRole, RegenerateRequest
- L29 assign logger = logging.getLogger('neura.domain.docqa')
- L32 def _state_store:
  - L33 return state_store_module.state_store
- L36 class DocumentQAService:
  - L37 docstring: "Service for document-based Q&A chat."
  - L39 def __init__self:
    - L40 assign self._llm_client = None
  - L42 def _get_llm_clientself:
    - L43 if self._llm_client is None:
      - L44 assign self._llm_client = get_llm_client()
    - L45 return self._llm_client
  - L47 def create_sessionself, name: str, correlation_id: Optional[str]=None:
    - L52 docstring: "Create a new Q&A session."
    - L53 expr logger.info('Creating DocQA session', extra={'correlation_id': correlation_id})
    - L55 assign session = DocQASession(id=str(uuid.uuid4()), name=name, created_at=datetime.utcnow(), updated_at=datetime.utcnow())
    - L62 assign store = _state_store()
    - L63 assign state = store._read_state()
    - L64 assign sessions = state.get('docqa_sessions', {})
    - L65 assign sessions[session.id] = session.model_dump(mode='json')
    - L66 expr store._write_state({**state, 'docqa_sessions': sessions})
    - L68 return session
  - L70 def get_sessionself, session_id: str:
    - L71 docstring: "Get a Q&A session by ID."
    - L72 assign store = _state_store()
    - L73 assign state = store._read_state()
    - L74 assign sessions = state.get('docqa_sessions', {})
    - L75 assign session_data = sessions.get(session_id)
    - L77 if session_data:
      - L78 return DocQASession(**session_data)
    - L79 return None
  - L81 def list_sessionsself:
    - L82 docstring: "List all Q&A sessions."
    - L83 assign store = _state_store()
    - L84 assign state = store._read_state()
    - L85 assign sessions = state.get('docqa_sessions', {})
    - L86 return [DocQASession(**data) for data in sessions.values()]
  - L88 def add_documentself, session_id: str, name: str, content: str, page_count: Optional[int]=None, correlation_id: Optional[str]=None:
    - L96 docstring: "Add a document to a Q&A session."
    - L97 expr logger.info(f'Adding document to DocQA session {session_id}', extra={'correlation_id': correlation_id})
    - L102 assign session = self.get_session(session_id)
    - L103 if not session:
      - L104 return None
    - L106 assign document = DocumentReference(id=str(uuid.uuid4()), name=name, content_preview=content[:500] + '...' if len(content) > 500 else content, full_content=content[:100000], page_count=page_count, added_at=datetime.utcnow())
    - L115 expr session.documents.append(document)
    - L116 assign session.updated_at = datetime.utcnow()
    - L118 assign store = _state_store()
    - L119 assign state = store._read_state()
    - L120 assign sessions = state.get('docqa_sessions', {})
    - L121 assign sessions[session_id] = session.model_dump(mode='json')
    - L122 expr store._write_state({**state, 'docqa_sessions': sessions})
    - L124 return document
  - L126 def remove_documentself, session_id: str, document_id: str:
    - L127 docstring: "Remove a document from a session."
    - L128 assign session = self.get_session(session_id)
    - L129 if not session:
      - L130 return False
    - L132 assign session.documents = [d for d in session.documents if d.id != document_id]
    - L133 assign session.updated_at = datetime.utcnow()
    - L135 assign store = _state_store()
    - L136 assign state = store._read_state()
    - L137 assign sessions = state.get('docqa_sessions', {})
    - L138 assign sessions[session_id] = session.model_dump(mode='json')
    - L139 expr store._write_state({**state, 'docqa_sessions': sessions})
    - L141 return True
  - L143 def askself, session_id: str, request: AskRequest, correlation_id: Optional[str]=None:
    - L149 docstring: "Ask a question about the documents in a session."
    - L150 assign start_time = time.time()
    - L151 expr logger.info(f'Processing question in session {session_id}', extra={'correlation_id': correlation_id})
    - L156 assign session = self.get_session(session_id)
    - L157 if not session:
      - L158 return None
    - L160 if not session.documents:
      - L162 assign no_docs_message = ChatMessage(id=str(uuid.uuid4()), role=MessageRole.ASSISTANT, content='No documents have been added to this session yet. Please add documents first.', citations=[], timestamp=datetime.utcnow())
      - L169 return AskResponse(message=no_docs_message, processing_time_ms=int((time.time() - start_time) * 1000))
    - L175 assign user_message = ChatMessage(id=str(uuid.uuid4()), role=MessageRole.USER, content=request.question, timestamp=datetime.utcnow())
    - L181 expr session.messages.append(user_message)
    - L184 assign doc_context = []
    - L185 for doc in session.documents:
      - L186 expr doc_context.append(f'[Document: {doc.name} (ID: {doc.id})]\n{doc.full_content[:15000]}')
    - L189 assign history_messages = session.messages[-(session.context_window * 2):-1]
    - L190 assign history_text = ''
    - L191 if history_messages:
      - L192 assign history_parts = []
      - L193 for msg in history_messages:
        - L194 assign role = 'User' if msg.role == MessageRole.USER else 'Assistant'
        - L195 expr history_parts.append(f'{role}: {msg.content}')
      - L196 assign history_text = f'\nPREVIOUS CONVERSATION:\n' + '\n'.join(history_parts)
    - L198 assign citation_instruction = ''
    - L199 if request.include_citations:
      - L200 assign citation_instruction = '\nFor each statement, provide citations in this format:\nInclude a "citations" array in your response with:\n- document_id: The document ID\n- document_name: The document name\n- quote: The relevant quote from the document\n- relevance_score: How relevant (0-1)\n'
    - L209 assign prompt = f'You are a helpful document Q&A assistant. Answer questions based ONLY on the provided documents.\n\nDOCUMENTS:\n{chr(10).join(doc_context)}\n{history_text}\n\nCURRENT QUESTION: {request.question}\n\nINSTRUCTIONS:\n1. Answer based ONLY on information in the documents\n2. If the information is not in the documents, say so clearly\n3. Be concise but thorough (max {request.max_response_length} characters)\n4. Reference specific documents when appropriate\n{citation_instruction}\n\nReturn a JSON object:\n{{\n  "answer": "Your comprehensive answer here",\n  "citations": [\n    {{\n      "document_id": "doc_id",\n      "document_name": "doc_name",\n      "quote": "relevant quote",\n      "relevance_score": 0.9\n    }}\n  ],\n  "confidence": 0.9,\n  "follow_up_questions": ["Suggested follow-up 1", "Suggested follow-up 2"]\n}}\n\nReturn ONLY the JSON object.'
    - L241 try:
      - L242 assign client = self._get_llm_client()
      - L243 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='docqa_answer', temperature=0.2)
      - L249 assign content = response['choices'][0]['message']['content']
      - L250 assign json_match = re.search('\\{[\\s\\S]*\\}', content)
      - L252 if json_match:
        - L253 assign response_data = json.loads(json_match.group())
        - L256 assign citations = []
        - L257 for cit in response_data.get('citations', []):
          - L258 expr citations.append(Citation(document_id=cit.get('document_id', ''), document_name=cit.get('document_name', ''), quote=cit.get('quote', ''), relevance_score=cit.get('relevance_score', 1.0)))
        - L266 assign assistant_message = ChatMessage(id=str(uuid.uuid4()), role=MessageRole.ASSISTANT, content=response_data.get('answer', "I couldn't generate an answer."), citations=citations, timestamp=datetime.utcnow(), metadata={'confidence': response_data.get('confidence', 0.8), 'follow_up_questions': response_data.get('follow_up_questions', [])})
        - L279 expr session.messages.append(assistant_message)
        - L280 assign session.updated_at = datetime.utcnow()
        - L282 assign store = _state_store()
        - L283 assign state = store._read_state()
        - L284 assign sessions = state.get('docqa_sessions', {})
        - L285 assign sessions[session_id] = session.model_dump(mode='json')
        - L286 expr store._write_state({**state, 'docqa_sessions': sessions})
        - L288 assign processing_time = int((time.time() - start_time) * 1000)
        - L290 return AskResponse(message=assistant_message, processing_time_ms=processing_time, tokens_used=response.get('usage', {}).get('total_tokens'))
      - L296 except Exception as exc:
        - L297 expr logger.error(f'DocQA failed: {exc}')
        - L299 assign error_message = ChatMessage(id=str(uuid.uuid4()), role=MessageRole.ASSISTANT, content=f'I encountered an error processing your question. Please try again.', timestamp=datetime.utcnow())
        - L306 return AskResponse(message=error_message, processing_time_ms=int((time.time() - start_time) * 1000))
    - L311 return None
  - L313 def submit_feedbackself, session_id: str, message_id: str, request: FeedbackRequest, correlation_id: Optional[str]=None:
    - L320 docstring: "Submit feedback for a message."
    - L321 expr logger.info('docqa_feedback_recorded', extra={'correlation_id': correlation_id, 'session_id': session_id, 'message_id': message_id, 'feedback_type': request.feedback_type})
    - L331 assign session = self.get_session(session_id)
    - L332 if not session:
      - L333 return None
    - L335 assign target = next((msg for msg in session.messages if msg.id == message_id), None)
    - L336 if not target:
      - L337 return None
    - L339 assign feedback = MessageFeedback(feedback_type=request.feedback_type, timestamp=datetime.utcnow(), comment=request.comment)
    - L344 assign target.feedback = feedback
    - L345 assign meta = dict(target.metadata or {})
    - L346 assign meta['feedback'] = feedback.model_dump(mode='json')
    - L347 assign target.metadata = meta
    - L348 assign session.updated_at = datetime.utcnow()
    - L350 assign store = _state_store()
    - L351 assign state = store._read_state()
    - L352 assign sessions = state.get('docqa_sessions', {})
    - L353 assign sessions[session_id] = session.model_dump(mode='json')
    - L354 expr store._write_state({**state, 'docqa_sessions': sessions})
    - L356 return target
  - L358 def regenerate_responseself, session_id: str, message_id: str, request: RegenerateRequest, correlation_id: Optional[str]=None:
    - L365 docstring: "Regenerate the assistant response for a given message."
    - L366 assign start_time = time.time()
    - L368 assign session = self.get_session(session_id)
    - L369 if not session:
      - L370 return None
    - L372 assign message_index = None
    - L373 for (idx, msg) in enumerate(session.messages):
      - L374 if msg.id == message_id:
        - L375 assign message_index = idx
        - L376 break
    - L377 if message_index is None:
      - L378 return None
    - L380 assign question_message = None
    - L381 assign question_index = None
    - L382 for idx in range(message_index - 1, -1, -1):
      - L383 assign candidate = session.messages[idx]
      - L384 if candidate.role == MessageRole.USER:
        - L385 assign question_message = candidate
        - L386 assign question_index = idx
        - L387 break
    - L388 if not question_message:
      - L389 return None
    - L391 assign question = question_message.content
    - L393 if not session.documents:
      - L394 return None
    - L396 assign doc_context = []
    - L397 for doc in session.documents:
      - L398 expr doc_context.append(f'[Document: {doc.name} (ID: {doc.id})]\n{doc.full_content[:15000]}')
    - L400 assign history_window = session.context_window * 2
    - L401 assign history_start = max(0, (question_index or 0) - history_window)
    - L402 assign history_messages = session.messages[history_start:question_index]
    - L403 assign history_text = ''
    - L404 if history_messages:
      - L405 assign history_parts = []
      - L406 for msg in history_messages:
        - L407 assign role = 'User' if msg.role == MessageRole.USER else 'Assistant'
        - L408 expr history_parts.append(f'{role}: {msg.content}')
      - L409 assign history_text = '\nPREVIOUS CONVERSATION:\n' + '\n'.join(history_parts)
    - L411 assign citation_instruction = ''
    - L412 if request.include_citations:
      - L413 assign citation_instruction = '\nFor each statement, provide citations in this format:\nInclude a "citations" array in your response with:\n- document_id: The document ID\n- document_name: The document name\n- quote: The relevant quote from the document\n- relevance_score: How relevant (0-1)\n'
    - L422 assign prompt = f'You are a helpful document Q&A assistant. Answer questions based ONLY on the provided documents.\n\nDOCUMENTS:\n{chr(10).join(doc_context)}\n{history_text}\n\nCURRENT QUESTION: {question}\n\nINSTRUCTIONS:\n1. Answer based ONLY on information in the documents\n2. If the information is not in the documents, say so clearly\n3. Be concise but thorough (max {request.max_response_length} characters)\n4. Reference specific documents when appropriate\n5. Provide a DIFFERENT perspective or additional details compared to previous answers\n{citation_instruction}\n\nReturn a JSON object:\n{{\n  "answer": "Your comprehensive answer here",\n  "citations": [\n    {{\n      "document_id": "doc_id",\n      "document_name": "doc_name",\n      "quote": "relevant quote",\n      "relevance_score": 0.9\n    }}\n  ],\n  "confidence": 0.9,\n  "follow_up_questions": ["Suggested follow-up 1", "Suggested follow-up 2"]\n}}\n\nReturn ONLY the JSON object.'
    - L455 try:
      - L456 assign client = self._get_llm_client()
      - L457 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='docqa_regenerate', temperature=0.5)
      - L463 assign content = response['choices'][0]['message']['content']
      - L464 assign json_match = re.search('\\{[\\s\\S]*\\}', content)
      - L466 if not json_match:
        - L467 raise ValueError('No JSON payload returned from LLM')
      - L469 assign response_data = json.loads(json_match.group())
      - L471 assign citations = []
      - L472 for cit in response_data.get('citations', []):
        - L473 expr citations.append(Citation(document_id=cit.get('document_id', ''), document_name=cit.get('document_name', ''), quote=cit.get('quote', ''), relevance_score=cit.get('relevance_score', 1.0)))
      - L480 assign assistant_message = ChatMessage(id=message_id, role=MessageRole.ASSISTANT, content=response_data.get('answer', "I couldn't generate an answer."), citations=citations, timestamp=datetime.utcnow(), metadata={'confidence': response_data.get('confidence', 0.8), 'follow_up_questions': response_data.get('follow_up_questions', []), 'regenerated': True}, feedback=None)
      - L494 assign session.messages[message_index] = assistant_message
      - L495 assign session.updated_at = datetime.utcnow()
      - L497 assign store = _state_store()
      - L498 assign state = store._read_state()
      - L499 assign sessions = state.get('docqa_sessions', {})
      - L500 assign sessions[session_id] = session.model_dump(mode='json')
      - L501 expr store._write_state({**state, 'docqa_sessions': sessions})
      - L503 assign processing_time = int((time.time() - start_time) * 1000)
      - L505 return AskResponse(message=assistant_message, processing_time_ms=processing_time, tokens_used=response.get('usage', {}).get('total_tokens'))
      - L511 except Exception as exc:
        - L512 expr logger.error(f'DocQA regenerate failed: {exc}')
        - L513 raise RuntimeError('Failed to regenerate response')
  - L515 def get_chat_historyself, session_id: str, limit: int=50:
    - L520 docstring: "Get chat history for a session."
    - L521 assign session = self.get_session(session_id)
    - L522 if not session:
      - L523 return []
    - L525 return session.messages[-limit:] if limit else session.messages
  - L527 def clear_historyself, session_id: str:
    - L528 docstring: "Clear chat history for a session."
    - L529 assign session = self.get_session(session_id)
    - L530 if not session:
      - L531 return False
    - L533 assign session.messages = []
    - L534 assign session.updated_at = datetime.utcnow()
    - L536 assign store = _state_store()
    - L537 assign state = store._read_state()
    - L538 assign sessions = state.get('docqa_sessions', {})
    - L539 assign sessions[session_id] = session.model_dump(mode='json')
    - L540 expr store._write_state({**state, 'docqa_sessions': sessions})
    - L542 return True
  - L544 def delete_sessionself, session_id: str:
    - L545 docstring: "Delete a Q&A session."
    - L546 assign store = _state_store()
    - L547 assign state = store._read_state()
    - L548 assign sessions = state.get('docqa_sessions', {})
    - L550 if session_id in sessions:
      - L551 delete sessions[session_id]
      - L552 expr store._write_state({**state, 'docqa_sessions': sessions})
      - L553 return True
    - L555 return False

## backend\app\domain\enrichment\__init__.py
- L1 docstring: "Data Enrichment domain module."
- L2 from .schemas import EnrichmentSource, EnrichmentRequest, EnrichmentResult, EnrichmentConfig
- L8 from .service import EnrichmentService
- L10 assign __all__ = ['EnrichmentSource', 'EnrichmentRequest', 'EnrichmentResult', 'EnrichmentConfig', 'EnrichmentService']

## backend\app\domain\enrichment\cache.py
- L1 docstring: "Caching layer for data enrichment."
- L2 from __future__ import annotations
- L4 import hashlib
- L5 import json
- L6 import logging
- L7 import time
- L8 from datetime import datetime, timezone
- L9 from typing import Any, Dict, Optional
- L11 assign logger = logging.getLogger('neura.domain.enrichment.cache')
- L14 def _now_iso:
  - L15 return datetime.now(timezone.utc).replace(microsecond=0).isoformat()
- L18 def _compute_cache_keysource_id: str, lookup_value: Any:
  - L19 docstring: "Compute a cache key from source and lookup value."
  - L20 assign value_str = json.dumps(lookup_value, sort_keys=True, default=str)
  - L21 assign content = f'{source_id}:{value_str}'
  - L22 return hashlib.sha256(content.encode()).hexdigest()[:16]
- L25 class EnrichmentCache:
  - L26 docstring: "In-memory cache with TTL support for enrichment data."
  - L28 def __init__self, state_store:
    - L29 assign self._store = state_store
    - L30 assign self._hits = 0
    - L31 assign self._misses = 0
  - L33 def getself, source_id: str, lookup_value: Any, max_age_hours: int=24:
    - L39 docstring: "\n        Get cached enrichment result.\n\n        Args:\n            source_id:..."
    - L50 assign cache_key = _compute_cache_key(source_id, lookup_value)
    - L52 try:
      - L53 assign cache = self._store._read_state().get('enrichment_cache', {})
      - L54 assign entry = cache.get(cache_key)
      - L56 if not entry:
        - L57 aug assign self._misses Add 1
        - L58 return None
      - L61 assign cached_at = entry.get('cached_at')
      - L62 if cached_at:
        - L63 assign cache_time = datetime.fromisoformat(cached_at.replace('Z', '+00:00'))
        - L64 assign now = datetime.now(timezone.utc)
        - L65 assign age_hours = (now - cache_time).total_seconds() / 3600
        - L67 if age_hours > max_age_hours:
          - L68 expr logger.debug(f'Cache expired for key {cache_key}')
          - L69 aug assign self._misses Add 1
          - L70 return None
      - L72 aug assign self._hits Add 1
      - L73 return entry.get('data')
      - L75 except Exception as exc:
        - L76 expr logger.warning(f'Cache read error: {exc}')
        - L77 aug assign self._misses Add 1
        - L78 return None
  - L80 def setself, source_id: str, lookup_value: Any, data: Dict[str, Any], ttl_hours: int=24:
    - L87 docstring: "\n        Cache enrichment result.\n\n        Args:\n            source_id: ID o..."
    - L96 assign cache_key = _compute_cache_key(source_id, lookup_value)
    - L98 try:
      - L99 with self._store._lock:
        - L100 assign state = self._store._read_state()
        - L101 assign cache = state.setdefault('enrichment_cache', {})
        - L103 assign cache[cache_key] = {'source_id': source_id, 'lookup_value': lookup_value, 'data': data, 'ttl_hours': ttl_hours, 'cached_at': _now_iso()}
        - L112 if len(cache) > 1000:
          - L113 assign sorted_entries = sorted(cache.items(), key=lambda x: x[1].get('cached_at', ''), reverse=True)
          - L118 assign state['enrichment_cache'] = dict(sorted_entries[:1000])
        - L120 expr self._store._write_state(state)
      - L122 except Exception as exc:
        - L123 expr logger.warning(f'Cache write error: {exc}')
  - L125 def invalidateself, source_id: Optional[str]=None:
    - L126 docstring: "\n        Invalidate cache entries.\n\n        Args:\n            source_id: If ..."
    - L136 try:
      - L137 with self._store._lock:
        - L138 assign state = self._store._read_state()
        - L139 assign cache = state.get('enrichment_cache', {})
        - L141 if source_id is None:
          - L142 assign count = len(cache)
          - L143 assign state['enrichment_cache'] = {}
          - L145 else:
            - L145 assign original_count = len(cache)
            - L146 assign cache = {k: v for k, v in cache.items() if v.get('source_id') != source_id}
            - L147 assign count = original_count - len(cache)
            - L148 assign state['enrichment_cache'] = cache
        - L150 expr self._store._write_state(state)
        - L151 return count
      - L153 except Exception as exc:
        - L154 expr logger.warning(f'Cache invalidation error: {exc}')
        - L155 return 0
  - L157 def get_statsself:
    - L158 docstring: "Get cache statistics."
    - L159 try:
      - L160 assign cache = self._store._read_state().get('enrichment_cache', {})
      - L162 assign now = datetime.now(timezone.utc)
      - L163 assign expired_count = 0
      - L164 assign sources = {}
      - L165 assign size_bytes = 0
      - L167 for entry in cache.values():
        - L168 assign source_id = entry.get('source_id', 'unknown')
        - L169 assign sources[source_id] = sources.get(source_id, 0) + 1
        - L172 try:
          - L173 aug assign size_bytes Add len(json.dumps(entry, default=str))
          - L174 except Exception:
            - L175 aug assign size_bytes Add 100
        - L177 assign cached_at = entry.get('cached_at')
        - L178 assign ttl_hours = entry.get('ttl_hours', 24)
        - L179 if cached_at:
          - L180 assign cache_time = datetime.fromisoformat(cached_at.replace('Z', '+00:00'))
          - L181 assign age_hours = (now - cache_time).total_seconds() / 3600
          - L182 if age_hours > ttl_hours:
            - L183 aug assign expired_count Add 1
      - L186 assign total_requests = self._hits + self._misses
      - L187 assign hit_rate = self._hits / total_requests if total_requests > 0 else 0.0
      - L189 return {'total_entries': len(cache), 'expired_entries': expired_count, 'entries_by_source': sources, 'hits': self._hits, 'misses': self._misses, 'hit_rate': hit_rate, 'size_bytes': size_bytes}
      - L199 except Exception as exc:
        - L200 expr logger.warning(f'Cache stats error: {exc}')
        - L201 return {'error': str(exc), 'hits': 0, 'misses': 0, 'hit_rate': 0.0, 'size_bytes': 0}

## backend\app\domain\enrichment\schemas.py
- L1 docstring: "Schemas for Data Enrichment feature."
- L2 from __future__ import annotations
- L4 from datetime import datetime
- L5 from enum import Enum
- L6 from typing import Any, Dict, List, Optional
- L8 from pydantic import BaseModel, Field, validator
- L10 from backend.app.core.validation import is_safe_id, is_safe_name
- L13 class EnrichmentSourceType(str, Enum):
  - L14 docstring: "Types of enrichment sources."
  - L15 assign COMPANY_INFO = 'company_info'
  - L16 assign ADDRESS = 'address'
  - L17 assign EXCHANGE_RATE = 'exchange_rate'
  - L18 assign CUSTOM = 'custom'
- L21 class EnrichmentSource(BaseModel):
  - L22 docstring: "Configuration for an enrichment source."
  - L23 annotated assign id: str
  - L24 annotated assign name: str
  - L25 annotated assign type: EnrichmentSourceType
  - L26 annotated assign description: Optional[str] = None
  - L27 annotated assign enabled: bool = True
  - L28 annotated assign config: Dict[str, Any] = Field(default_factory=dict)
  - L29 annotated assign cache_ttl_hours: int = Field(default=24, ge=1, le=720)
  - L30 annotated assign created_at: str
  - L31 annotated assign updated_at: str
- L34 class EnrichmentSourceCreate(BaseModel):
  - L35 docstring: "Request to create an enrichment source."
  - L36 annotated assign name: str = Field(..., min_length=1, max_length=100)
  - L37 annotated assign type: EnrichmentSourceType
  - L38 annotated assign description: Optional[str] = Field(None, max_length=500)
  - L39 annotated assign config: Dict[str, Any] = Field(default_factory=dict)
  - L40 annotated assign cache_ttl_hours: int = Field(default=24, ge=1, le=720)
  - L43 def validate_namecls, value: str:
    - L44 if not is_safe_name(value):
      - L45 raise ValueError('Name contains invalid characters')
    - L46 return value.strip()
- L49 class EnrichmentFieldMapping(BaseModel):
  - L50 docstring: "Mapping of source field to enrichment lookup."
  - L51 annotated assign source_field: str = Field(..., min_length=1, max_length=128)
  - L52 annotated assign enrichment_source_id: str = Field(..., min_length=1, max_length=64)
  - L53 annotated assign target_fields: List[str] = Field(..., min_items=1, max_items=20)
  - L54 annotated assign lookup_key: Optional[str] = None
- L57 class EnrichmentRequest(BaseModel):
  - L58 docstring: "Request to enrich data."
  - L59 annotated assign data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=1000)
  - L60 annotated assign mappings: List[EnrichmentFieldMapping] = Field(..., min_items=1, max_items=20)
  - L61 annotated assign use_cache: bool = Field(default=True)
- L64 class EnrichedField(BaseModel):
  - L65 docstring: "A single enriched field."
  - L66 annotated assign field: str
  - L67 annotated assign original_value: Any
  - L68 annotated assign enriched_value: Any
  - L69 annotated assign confidence: float = Field(default=1.0, ge=0.0, le=1.0)
  - L70 annotated assign source: str
  - L71 annotated assign cached: bool = False
- L74 class EnrichmentResult(BaseModel):
  - L75 docstring: "Result of enrichment for a single row."
  - L76 annotated assign row_index: int
  - L77 annotated assign enriched_fields: List[EnrichedField]
  - L78 annotated assign errors: List[str] = Field(default_factory=list)
- L81 class EnrichmentResponse(BaseModel):
  - L82 docstring: "Response from enrichment operation."
  - L83 annotated assign total_rows: int
  - L84 annotated assign enriched_rows: int
  - L85 annotated assign results: List[EnrichmentResult]
  - L86 annotated assign cache_hits: int = 0
  - L87 annotated assign cache_misses: int = 0
  - L88 annotated assign processing_time_ms: int
- L91 class EnrichmentPreviewRequest(BaseModel):
  - L92 docstring: "Request to preview enrichment without persisting."
  - L93 annotated assign sample_data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=10)
  - L94 annotated assign mappings: List[EnrichmentFieldMapping] = Field(..., min_items=1, max_items=20)
- L97 class EnrichmentConfig(BaseModel):
  - L98 docstring: "Global enrichment configuration."
  - L99 annotated assign default_cache_ttl_hours: int = Field(default=24, ge=1, le=720)
  - L100 annotated assign max_batch_size: int = Field(default=100, ge=1, le=1000)
  - L101 annotated assign rate_limit_per_minute: int = Field(default=60, ge=1, le=1000)
- L105 class SimpleEnrichmentRequest(BaseModel):
  - L106 docstring: "Simplified request to enrich data (frontend-compatible)."
  - L107 annotated assign data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=1000)
  - L108 annotated assign sources: List[str] = Field(..., min_items=1, max_items=10)
  - L109 annotated assign options: Dict[str, Any] = Field(default_factory=dict)
- L112 class SimplePreviewRequest(BaseModel):
  - L113 docstring: "Simplified preview request (frontend-compatible)."
  - L114 annotated assign data: List[Dict[str, Any]] = Field(..., min_items=1, max_items=100)
  - L115 annotated assign sources: List[str] = Field(..., min_items=1, max_items=10)
  - L116 annotated assign sample_size: int = Field(default=5, ge=1, le=10)

## backend\app\domain\enrichment\service.py
- L1 docstring: "Service layer for Data Enrichment feature."
- L2 from __future__ import annotations
- L4 import asyncio
- L5 import logging
- L6 import time
- L7 import uuid
- L8 from datetime import datetime, timezone
- L9 from typing import Any, Dict, List, Optional, Type
- L11 from backend.app.core.errors import AppError
- L12 from backend.app.services.state import store as state_store_module
- L14 from .cache import EnrichmentCache
- L15 from .schemas import EnrichmentSource, EnrichmentSourceCreate, EnrichmentSourceType, EnrichmentRequest, EnrichmentResult, EnrichedField, EnrichmentResponse, EnrichmentFieldMapping
- L25 from .sources.base import EnrichmentSourceBase
- L26 from .sources.company import CompanyInfoSource
- L27 from .sources.address import AddressSource
- L28 from .sources.exchange import ExchangeRateSource
- L30 assign logger = logging.getLogger('neura.domain.enrichment')
- L33 def _now_iso:
  - L34 return datetime.now(timezone.utc).replace(microsecond=0).isoformat()
- L37 def _state_store:
  - L38 return state_store_module.state_store
- L42 annotated assign SOURCE_REGISTRY: Dict[EnrichmentSourceType, Type[EnrichmentSourceBase]] = {EnrichmentSourceType.COMPANY_INFO: CompanyInfoSource, EnrichmentSourceType.ADDRESS: AddressSource, EnrichmentSourceType.EXCHANGE_RATE: ExchangeRateSource}
- L49 class EnrichmentService:
  - L50 docstring: "Service for data enrichment operations."
  - L52 def __init__self:
    - L53 assign self._cache = EnrichmentCache(_state_store())
    - L54 annotated assign self._source_instances: Dict[str, EnrichmentSourceBase] = {}
  - L56 def _get_source_instanceself, source: EnrichmentSource:
    - L57 docstring: "Get or create a source instance."
    - L58 if source.id in self._source_instances:
      - L59 return self._source_instances[source.id]
    - L61 assign source_class = SOURCE_REGISTRY.get(source.type)
    - L62 if not source_class:
      - L63 raise AppError(code='unknown_source_type', message=f'Unknown enrichment source type: {source.type}', status_code=400)
    - L69 assign instance = source_class(source.config)
    - L70 assign self._source_instances[source.id] = instance
    - L71 return instance
  - L73 def create_sourceself, request: EnrichmentSourceCreate, correlation_id: Optional[str]=None:
    - L78 docstring: "Create a new enrichment source."
    - L79 expr logger.info(f'Creating enrichment source: {request.name}', extra={'correlation_id': correlation_id})
    - L81 assign source_id = str(uuid.uuid4())[:8]
    - L82 assign now = _now_iso()
    - L84 assign source = EnrichmentSource(id=source_id, name=request.name, type=request.type, description=request.description, config=request.config, cache_ttl_hours=request.cache_ttl_hours, created_at=now, updated_at=now)
    - L96 assign store = _state_store()
    - L97 with store._lock:
      - L98 assign state = store._read_state()
      - L99 assign state.setdefault('enrichment_sources', {})[source_id] = source.dict()
      - L100 expr store._write_state(state)
    - L102 return source
  - L104 def list_sourcesself:
    - L105 docstring: "List all enrichment sources."
    - L106 assign store = _state_store()
    - L107 assign sources = store._read_state().get('enrichment_sources', {})
    - L108 return [EnrichmentSource(**s) for s in sources.values()]
  - L110 def get_sourceself, source_id: str:
    - L111 docstring: "Get an enrichment source by ID."
    - L112 assign store = _state_store()
    - L113 assign source = store._read_state().get('enrichment_sources', {}).get(source_id)
    - L114 return EnrichmentSource(**source) if source else None
  - L116 def delete_sourceself, source_id: str:
    - L117 docstring: "Delete an enrichment source."
    - L118 assign store = _state_store()
    - L119 with store._lock:
      - L120 assign state = store._read_state()
      - L121 assign sources = state.get('enrichment_sources', {})
      - L122 if source_id not in sources:
        - L123 return False
      - L124 delete sources[source_id]
      - L125 expr store._write_state(state)
    - L128 expr self._source_instances.pop(source_id, None)
    - L131 expr self._cache.invalidate(source_id)
    - L133 return True
  - L135 async def enrichself, request: EnrichmentRequest, correlation_id: Optional[str]=None:
    - L140 docstring: "\n        Enrich data with additional information.\n\n        Args:\n           ..."
    - L150 expr logger.info(f'Enriching {len(request.data)} rows with {len(request.mappings)} mappings', extra={'correlation_id': correlation_id})
    - L155 assign started = time.time()
    - L156 annotated assign results: List[EnrichmentResult] = []
    - L157 assign cache_hits = 0
    - L158 assign cache_misses = 0
    - L159 assign enriched_count = 0
    - L162 annotated assign sources: Dict[str, EnrichmentSource] = {}
    - L163 for mapping in request.mappings:
      - L164 if mapping.enrichment_source_id not in sources:
        - L165 assign source = self.get_source(mapping.enrichment_source_id)
        - L166 if not source:
          - L167 raise AppError(code='source_not_found', message=f'Enrichment source not found: {mapping.enrichment_source_id}', status_code=404)
        - L172 if not source.enabled:
          - L173 raise AppError(code='source_disabled', message=f'Enrichment source is disabled: {source.name}', status_code=400)
        - L178 assign sources[mapping.enrichment_source_id] = source
    - L181 for (row_index, row) in enumerate(request.data):
      - L182 annotated assign enriched_fields: List[EnrichedField] = []
      - L183 annotated assign errors: List[str] = []
      - L185 for mapping in request.mappings:
        - L186 assign source = sources[mapping.enrichment_source_id]
        - L187 assign source_instance = self._get_source_instance(source)
        - L190 assign lookup_value = row.get(mapping.source_field)
        - L191 if lookup_value is None:
          - L192 continue
        - L195 assign cached_data = None
        - L196 if request.use_cache:
          - L197 assign cached_data = self._cache.get(source.id, lookup_value, max_age_hours=source.cache_ttl_hours)
        - L203 if cached_data:
          - L204 aug assign cache_hits Add 1
          - L205 assign enrichment_data = cached_data
          - L206 assign from_cache = True
          - L208 else:
            - L208 aug assign cache_misses Add 1
            - L209 try:
              - L210 assign enrichment_data = await source_instance.lookup(lookup_value)
              - L211 if enrichment_data and request.use_cache:
                - L212 expr self._cache.set(source.id, lookup_value, enrichment_data, ttl_hours=source.cache_ttl_hours)
              - L218 except Exception as exc:
                - L219 expr errors.append(f'Lookup failed for {mapping.source_field}: {exc}')
                - L220 assign enrichment_data = None
            - L221 assign from_cache = False
        - L223 if enrichment_data:
          - L224 assign confidence = source_instance.get_confidence(enrichment_data)
          - L225 for target_field in mapping.target_fields:
            - L226 if target_field in enrichment_data:
              - L227 expr enriched_fields.append(EnrichedField(field=target_field, original_value=lookup_value, enriched_value=enrichment_data[target_field], confidence=confidence, source=source.name, cached=from_cache))
      - L238 if enriched_fields:
        - L239 aug assign enriched_count Add 1
      - L241 expr results.append(EnrichmentResult(row_index=row_index, enriched_fields=enriched_fields, errors=errors))
    - L249 assign processing_time_ms = int((time.time() - started) * 1000)
    - L251 return EnrichmentResponse(total_rows=len(request.data), enriched_rows=enriched_count, results=results, cache_hits=cache_hits, cache_misses=cache_misses, processing_time_ms=processing_time_ms)
  - L260 async def preview_enrichmentself, sample_data: List[Dict[str, Any]], mappings: List[EnrichmentFieldMapping], correlation_id: Optional[str]=None:
    - L266 docstring: "\n        Preview enrichment without caching results.\n\n        Args:\n        ..."
    - L277 assign request = EnrichmentRequest(data=sample_data, mappings=mappings, use_cache=False)
    - L282 return await self.enrich(request, correlation_id)
  - L284 def get_cache_statsself:
    - L285 docstring: "Get cache statistics."
    - L286 return self._cache.get_stats()
  - L288 def clear_cacheself, source_id: Optional[str]=None:
    - L289 docstring: "Clear enrichment cache."
    - L290 return self._cache.invalidate(source_id)
  - L292 def get_available_source_typesself:
    - L293 docstring: "Get list of available source types with their supported fields."
    - L294 assign result = []
    - L295 for (source_type, source_class) in SOURCE_REGISTRY.items():
      - L296 assign instance = source_class({})
      - L297 expr result.append({'type': source_type.value, 'name': source_type.value.replace('_', ' ').title(), 'supported_fields': instance.get_supported_fields()})
    - L302 return result
  - L304 async def simple_enrichself, data: List[Dict[str, Any]], sources: List[str], options: Dict[str, Any], correlation_id: Optional[str]=None:
    - L311 docstring: "\n        Simplified enrichment for frontend.\n\n        Args:\n            data..."
    - L323 expr logger.info(f'Simple enrichment: {len(data)} rows with sources {sources}', extra={'correlation_id': correlation_id})
    - L328 assign started = time.time()
    - L329 assign enriched_data = []
    - L330 assign enriched_count = 0
    - L333 assign source_type_map = {'company': EnrichmentSourceType.COMPANY_INFO, 'company_info': EnrichmentSourceType.COMPANY_INFO, 'address': EnrichmentSourceType.ADDRESS, 'exchange': EnrichmentSourceType.EXCHANGE_RATE, 'exchange_rate': EnrichmentSourceType.EXCHANGE_RATE}
    - L341 for row in data:
      - L342 assign enriched_row = dict(row)
      - L343 assign row_enriched = False
      - L345 for source_id in sources:
        - L346 assign source_type = source_type_map.get(source_id)
        - L347 if not source_type:
          - L348 continue
        - L350 assign source_class = SOURCE_REGISTRY.get(source_type)
        - L351 if not source_class:
          - L352 continue
        - L355 assign config = dict(options) if options else {}
        - L356 assign source_instance = source_class(config)
        - L359 assign lookup_value = None
        - L360 if source_type == EnrichmentSourceType.COMPANY_INFO:
          - L361 assign lookup_value = row.get('company_name') or row.get('company')
          - L362 else:
            - L362 if source_type == EnrichmentSourceType.ADDRESS:
              - L363 assign lookup_value = row.get('address')
              - L364 else:
                - L364 if source_type == EnrichmentSourceType.EXCHANGE_RATE:
                  - L365 assign lookup_value = row.get('amount')
                  - L366 if lookup_value is not None:
                    - L368 assign from_currency = row.get('currency') or row.get('from_currency') or 'USD'
                    - L369 assign target_currency = config.get('target_currency', 'USD')
                    - L370 assign lookup_value = {'amount': lookup_value, 'from_currency': from_currency, 'to_currency': target_currency}
        - L376 if lookup_value is None:
          - L377 continue
        - L379 try:
          - L381 assign enrichment_result = await source_instance.lookup(lookup_value)
          - L382 if enrichment_result:
            - L383 expr enriched_row.update(enrichment_result)
            - L384 assign row_enriched = True
          - L385 except Exception as exc:
            - L386 expr logger.warning(f'Enrichment lookup failed: {exc}')
      - L388 expr enriched_data.append(enriched_row)
      - L389 if row_enriched:
        - L390 aug assign enriched_count Add 1
    - L392 assign processing_time_ms = int((time.time() - started) * 1000)
    - L394 return {'enriched_data': enriched_data, 'total_rows': len(data), 'enriched_rows': enriched_count, 'processing_time_ms': processing_time_ms}
  - L401 async def simple_previewself, data: List[Dict[str, Any]], sources: List[str], sample_size: int=5, correlation_id: Optional[str]=None:
    - L408 docstring: "\n        Preview enrichment on a sample of data.\n\n        Args:\n            ..."
    - L421 assign sample_data = data[:sample_size]
    - L423 assign result = await self.simple_enrich(data=sample_data, sources=sources, options={}, correlation_id=correlation_id)
    - L430 return {'preview': result['enriched_data'], 'total_rows': len(data), 'enriched_rows': result['enriched_rows'], 'processing_time_ms': result['processing_time_ms']}

## backend\app\domain\enrichment\sources\__init__.py
- L1 docstring: "Enrichment sources module."
- L2 from .base import EnrichmentSourceBase
- L3 from .company import CompanyInfoSource
- L4 from .address import AddressSource
- L5 from .exchange import ExchangeRateSource
- L7 assign __all__ = ['EnrichmentSourceBase', 'CompanyInfoSource', 'AddressSource', 'ExchangeRateSource']

## backend\app\domain\enrichment\sources\address.py
- L1 docstring: "Address normalization and enrichment source."
- L2 from __future__ import annotations
- L4 import logging
- L5 from typing import Any, Dict, List, Optional
- L7 from .base import EnrichmentSourceBase
- L9 assign logger = logging.getLogger('neura.domain.enrichment.address')
- L12 class AddressSource(EnrichmentSourceBase):
  - L13 docstring: "\n    Enrichment source for address normalization and geocoding.\n\n    Uses LLM..."
  - L19 assign source_type = 'address'
  - L20 assign supported_fields = ['street_address', 'city', 'state_province', 'postal_code', 'country', 'country_code', 'formatted_address', 'address_type']
  - L31 def __init__self, config: Dict[str, Any]:
    - L32 expr super().__init__(config)
    - L33 assign self._llm_client = None
  - L35 def _get_llm_clientself:
    - L36 docstring: "Get or create LLM client."
    - L37 if self._llm_client is None:
      - L38 from backend.app.services.llm.client import get_llm_client
      - L39 assign self._llm_client = get_llm_client()
    - L40 return self._llm_client
  - L42 async def lookupself, value: Any:
    - L43 docstring: "\n        Parse and normalize an address.\n\n        Args:\n            value: R..."
    - L52 if not value or not isinstance(value, str):
      - L53 return None
    - L55 assign address = value.strip()
    - L56 if not address:
      - L57 return None
    - L59 try:
      - L60 assign client = self._get_llm_client()
      - L62 assign prompt = f'Parse and normalize this address into components:\n\nAddress: "{address}"\n\nReturn a JSON object with the following fields (use null for unknown/missing):\n{{\n  "street_address": "123 Main St, Suite 100",\n  "city": "City name",\n  "state_province": "State or province name",\n  "postal_code": "12345",\n  "country": "Country name",\n  "country_code": "US",\n  "formatted_address": "Complete formatted address",\n  "address_type": "residential|commercial|po_box|unknown"\n}}\n\nReturn ONLY the JSON object, no other text.'
      - L80 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='address_enrichment', temperature=0.0)
      - L86 assign content = response['choices'][0]['message']['content']
      - L89 import json
      - L90 import re
      - L92 assign json_match = re.search('\\{[\\s\\S]*\\}', content)
      - L93 if json_match:
        - L94 assign result = json.loads(json_match.group())
        - L95 return result
      - L97 return None
      - L99 except Exception as exc:
        - L101 assign exc_str = str(exc).lower()
        - L102 assign is_critical = any((indicator in exc_str for indicator in ['authentication', 'api_key', 'invalid_api_key', 'unauthorized', 'quota', 'rate_limit', 'insufficient_quota']))
        - L107 if is_critical:
          - L108 expr logger.error(f"Address lookup critical error for '{address[:50]}...': {exc}", exc_info=True, extra={'event': 'address_enrichment_critical_error', 'address_preview': address[:50]})
          - L113 raise
        - L116 expr logger.warning(f"Address lookup failed for '{address[:50]}...': {exc}", exc_info=True, extra={'event': 'address_enrichment_failed', 'address_preview': address[:50], 'error_type': type(exc).__name__})
        - L121 return None
  - L123 def get_supported_fieldsself:
    - L124 return self.supported_fields
  - L126 def get_confidenceself, result: Dict[str, Any]:
    - L127 docstring: "Calculate confidence based on how many fields are populated."
    - L128 if not result:
      - L129 return 0.0
    - L132 assign weights = {'city': 0.2, 'country': 0.2, 'postal_code': 0.15, 'street_address': 0.15, 'state_province': 0.1, 'country_code': 0.1, 'formatted_address': 0.1}
    - L142 assign score = 0.0
    - L143 for (field, weight) in weights.items():
      - L144 if result.get(field):
        - L145 aug assign score Add weight
    - L147 return min(score, 1.0)

## backend\app\domain\enrichment\sources\base.py
- L1 docstring: "Base class for enrichment sources."
- L2 from __future__ import annotations
- L4 from abc import ABC, abstractmethod
- L5 from typing import Any, Dict, List, Optional
- L8 class EnrichmentSourceBase(ABC):
  - L9 docstring: "Abstract base class for enrichment sources."
  - L11 annotated assign source_type: str = 'base'
  - L12 annotated assign supported_fields: List[str] = []
  - L14 def __init__self, config: Dict[str, Any]:
    - L15 docstring: "\n        Initialize the enrichment source.\n\n        Args:\n            config..."
    - L21 assign self.config = config
  - L24 async def lookupself, value: Any:
    - L25 docstring: "\n        Look up enrichment data for a value.\n\n        Args:\n            val..."
    - L34 pass
  - L37 def get_supported_fieldsself:
    - L38 docstring: "\n        Get list of fields this source can provide.\n\n        Returns:\n     ..."
    - L44 pass
  - L46 def validate_configself:
    - L47 docstring: "\n        Validate the source configuration.\n\n        Returns:\n            Tr..."
    - L53 return True
  - L55 def get_confidenceself, result: Dict[str, Any]:
    - L56 docstring: "\n        Calculate confidence score for a result.\n\n        Args:\n           ..."
    - L65 return 1.0

## backend\app\domain\enrichment\sources\company.py
- L1 docstring: "Company information enrichment source using LLM."
- L2 from __future__ import annotations
- L4 import logging
- L5 from typing import Any, Dict, List, Optional
- L7 from .base import EnrichmentSourceBase
- L9 assign logger = logging.getLogger('neura.domain.enrichment.company')
- L12 class CompanyInfoSource(EnrichmentSourceBase):
  - L13 docstring: "\n    Enrichment source for company information.\n\n    Uses LLM to lookup compa..."
  - L19 assign source_type = 'company_info'
  - L20 assign supported_fields = ['industry', 'sector', 'company_size', 'founded_year', 'headquarters_city', 'headquarters_country', 'website', 'description']
  - L31 def __init__self, config: Dict[str, Any]:
    - L32 expr super().__init__(config)
    - L33 assign self._llm_client = None
  - L35 def _get_llm_clientself:
    - L36 docstring: "Get or create LLM client."
    - L37 if self._llm_client is None:
      - L38 from backend.app.services.llm.client import get_llm_client
      - L39 assign self._llm_client = get_llm_client()
    - L40 return self._llm_client
  - L42 async def lookupself, value: Any:
    - L43 docstring: "\n        Look up company information.\n\n        Args:\n            value: Comp..."
    - L52 if not value or not isinstance(value, str):
      - L53 return None
    - L55 assign company_name = value.strip()
    - L56 if not company_name:
      - L57 return None
    - L59 try:
      - L60 assign client = self._get_llm_client()
      - L62 assign prompt = f'Provide information about the company "{company_name}".\n\nReturn a JSON object with the following fields (use null for unknown values):\n{{\n  "industry": "Primary industry/sector",\n  "sector": "Business sector",\n  "company_size": "small/medium/large/enterprise",\n  "founded_year": 1900,\n  "headquarters_city": "City name",\n  "headquarters_country": "Country name",\n  "website": "https://example.com",\n  "description": "Brief company description"\n}}\n\nReturn ONLY the JSON object, no other text.'
      - L78 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='company_enrichment', temperature=0.0)
      - L84 assign content = response['choices'][0]['message']['content']
      - L87 import json
      - L88 import re
      - L91 assign json_match = re.search('\\{[\\s\\S]*\\}', content)
      - L92 if json_match:
        - L93 assign result = json.loads(json_match.group())
        - L94 return result
      - L96 return None
      - L98 except Exception as exc:
        - L100 assign exc_str = str(exc).lower()
        - L101 assign is_critical = any((indicator in exc_str for indicator in ['authentication', 'api_key', 'invalid_api_key', 'unauthorized', 'quota', 'rate_limit', 'insufficient_quota']))
        - L106 if is_critical:
          - L107 expr logger.error(f"Company lookup critical error for '{company_name}': {exc}", exc_info=True, extra={'event': 'company_enrichment_critical_error', 'company_name': company_name})
          - L112 raise
        - L115 expr logger.warning(f"Company lookup failed for '{company_name}': {exc}", exc_info=True, extra={'event': 'company_enrichment_failed', 'company_name': company_name, 'error_type': type(exc).__name__})
        - L120 return None
  - L122 def get_supported_fieldsself:
    - L123 return self.supported_fields
  - L125 def get_confidenceself, result: Dict[str, Any]:
    - L126 docstring: "Calculate confidence based on how many fields are populated."
    - L127 if not result:
      - L128 return 0.0
    - L130 assign populated = sum((1 for v in result.values() if v is not None))
    - L131 return min(populated / len(self.supported_fields), 1.0)

## backend\app\domain\enrichment\sources\exchange.py
- L1 docstring: "Currency exchange rate enrichment source with live API support."
- L2 from __future__ import annotations
- L4 import logging
- L5 import os
- L6 import time
- L7 from datetime import datetime, timezone
- L8 from typing import Any, Dict, List, Optional
- L10 from .base import EnrichmentSourceBase
- L12 assign logger = logging.getLogger('neura.domain.enrichment.exchange')
- L16 assign FALLBACK_RATES_USD = {'EUR': 0.92, 'GBP': 0.79, 'JPY': 149.5, 'CAD': 1.36, 'AUD': 1.53, 'CHF': 0.88, 'CNY': 7.24, 'INR': 83.12, 'KRW': 1325.0, 'SGD': 1.34, 'HKD': 7.82, 'TWD': 31.5, 'THB': 34.5, 'MYR': 4.45, 'IDR': 15800.0, 'PHP': 56.2, 'VND': 24500.0, 'PKR': 278.0, 'BDT': 110.0, 'NOK': 10.65, 'SEK': 10.42, 'DKK': 6.87, 'PLN': 4.02, 'CZK': 23.2, 'HUF': 365.0, 'RON': 4.6, 'BGN': 1.8, 'HRK': 6.95, 'UAH': 41.5, 'RUB': 92.5, 'TRY': 32.15, 'MXN': 17.15, 'BRL': 4.97, 'ARS': 875.0, 'CLP': 950.0, 'COP': 4050.0, 'PEN': 3.72, 'ILS': 3.7, 'AED': 3.67, 'SAR': 3.75, 'QAR': 3.64, 'KWD': 0.31, 'BHD': 0.38, 'OMR': 0.38, 'EGP': 30.9, 'ZAR': 18.75, 'NGN': 1580.0, 'KES': 153.0, 'NZD': 1.64, 'FJD': 2.27}
- L76 assign EXCHANGE_RATE_API_KEY = os.getenv('EXCHANGE_RATE_API_KEY', '')
- L81 assign EXCHANGE_API_URLS = ['https://api.exchangerate.host/latest', 'https://api.frankfurter.app/latest']
- L87 annotated assign _RATES_CACHE: Dict[str, Any] = {}
- L88 assign _CACHE_TTL_SECONDS = 6 * 60 * 60
- L91 class ExchangeRateSource(EnrichmentSourceBase):
  - L92 docstring: "\n    Enrichment source for currency exchange rates.\n\n    Converts amounts bet..."
  - L99 assign source_type = 'exchange_rate'
  - L100 assign supported_fields = ['converted_amount', 'exchange_rate', 'source_currency', 'target_currency', 'rate_date', 'rate_source']
  - L109 def __init__self, config: Dict[str, Any]:
    - L110 expr super().__init__(config)
    - L111 assign self.base_currency = config.get('base_currency', 'USD')
    - L112 assign self.target_currency = config.get('target_currency', 'USD')
    - L113 assign self._use_live_rates = config.get('use_live_rates', True)
  - L115 async def _fetch_live_ratesself, base: str='USD':
    - L116 docstring: "\n        Fetch live exchange rates from API.\n\n        Args:\n            base..."
    - L125 Global
    - L127 assign cache_key = f'rates_{base}'
    - L128 assign cached = _RATES_CACHE.get(cache_key)
    - L129 if cached:
      - L130 assign cache_time = cached.get('_timestamp', 0)
      - L131 if time.time() - cache_time < _CACHE_TTL_SECONDS:
        - L132 assign rates = {k: v for k, v in cached.items() if k != '_timestamp'}
        - L133 return rates
    - L136 try:
      - L137 import httpx
      - L138 return await self._fetch_with_httpx(base, cache_key)
      - L139 except ImportError:
        - L140 pass
    - L143 try:
      - L144 import aiohttp
      - L145 return await self._fetch_with_aiohttp(base, cache_key)
      - L146 except ImportError:
        - L147 pass
    - L150 try:
      - L151 import requests
      - L152 return self._fetch_with_requests_sync(base, cache_key)
      - L153 except ImportError:
        - L154 expr logger.warning('No HTTP library available for live rates')
        - L155 return None
  - L157 async def _fetch_with_httpxself, base: str, cache_key: str:
    - L158 docstring: "Fetch rates using httpx (async)."
    - L159 import httpx
    - L160 Global
    - L162 async with httpx.AsyncClient(timeout=10.0) as client:
      - L163 for api_url in EXCHANGE_API_URLS:
        - L164 try:
          - L165 assign params = {'base': base} if 'frankfurter' in api_url else {'from': base}
          - L166 assign response = await client.get(api_url, params=params)
          - L167 if response.status_code == 200:
            - L168 assign data = response.json()
            - L169 assign rates = data.get('rates', {})
            - L170 if rates:
              - L171 assign _RATES_CACHE[cache_key] = {**rates, '_timestamp': time.time()}
              - L175 expr logger.info(f'Fetched live exchange rates from {api_url}', extra={'currencies': len(rates), 'base': base})
              - L179 return rates
          - L180 except Exception as exc:
            - L181 expr logger.debug(f'API {api_url} failed with httpx: {exc}')
            - L182 continue
    - L184 expr logger.warning('All exchange rate APIs failed (httpx), using fallback rates')
    - L185 return None
  - L187 async def _fetch_with_aiohttpself, base: str, cache_key: str:
    - L188 docstring: "Fetch rates using aiohttp."
    - L189 import aiohttp
    - L190 Global
    - L192 for api_url in EXCHANGE_API_URLS:
      - L193 try:
        - L194 assign params = {'base': base} if 'frankfurter' in api_url else {'from': base}
        - L195 async with aiohttp.ClientSession() as session:
          - L196 async with session.get(api_url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as response:
            - L201 if response.status == 200:
              - L202 assign data = await response.json()
              - L203 assign rates = data.get('rates', {})
              - L204 if rates:
                - L205 assign _RATES_CACHE[cache_key] = {**rates, '_timestamp': time.time()}
                - L209 expr logger.info(f'Fetched live exchange rates from {api_url}', extra={'currencies': len(rates), 'base': base})
                - L213 return rates
        - L214 except Exception as exc:
          - L215 expr logger.debug(f'API {api_url} failed with aiohttp: {exc}')
          - L216 continue
    - L218 expr logger.warning('All exchange rate APIs failed (aiohttp), using fallback rates')
    - L219 return None
  - L221 def _fetch_with_requests_syncself, base: str, cache_key: str:
    - L222 docstring: "Fetch rates using requests (sync, last resort)."
    - L223 import requests
    - L224 Global
    - L226 for api_url in EXCHANGE_API_URLS:
      - L227 try:
        - L228 assign params = {'base': base} if 'frankfurter' in api_url else {'from': base}
        - L229 assign response = requests.get(api_url, params=params, timeout=10)
        - L230 if response.status_code == 200:
          - L231 assign data = response.json()
          - L232 assign rates = data.get('rates', {})
          - L233 if rates:
            - L234 assign _RATES_CACHE[cache_key] = {**rates, '_timestamp': time.time()}
            - L238 expr logger.info(f'Fetched live exchange rates from {api_url} (sync)', extra={'currencies': len(rates), 'base': base})
            - L242 return rates
        - L243 except Exception as exc:
          - L244 expr logger.debug(f'API {api_url} failed with requests: {exc}')
          - L245 continue
    - L247 expr logger.warning('All exchange rate APIs failed (requests), using fallback rates')
    - L248 return None
  - L250 async def lookupself, value: Any:
    - L251 docstring: "\n        Convert currency amount.\n\n        Args:\n            value: Can be:\..."
    - L263 if value is None:
      - L264 return None
    - L266 try:
      - L267 annotated assign amount: float
      - L268 annotated assign from_currency: str
      - L269 annotated assign to_currency: str
      - L271 if isinstance(value, dict):
        - L272 assign amount = float(value.get('amount', 0))
        - L273 assign from_currency = value.get('from_currency', self.base_currency).upper()
        - L274 assign to_currency = value.get('to_currency', self.target_currency).upper()
        - L275 else:
          - L275 if isinstance(value, (int, float)):
            - L276 assign amount = float(value)
            - L277 assign from_currency = self.base_currency
            - L278 assign to_currency = self.target_currency
            - L279 else:
              - L279 if isinstance(value, str):
                - L281 if '|' in value:
                  - L282 assign parts = value.strip().split('|')
                  - L283 assign amount = float(parts[0].replace(',', ''))
                  - L284 assign from_currency = parts[1].upper() if len(parts) > 1 else self.base_currency
                  - L286 else:
                    - L286 assign parts = value.strip().split()
                    - L287 if len(parts) == 2:
                      - L288 if parts[0].replace('.', '').replace(',', '').replace('-', '').isdigit():
                        - L289 assign amount = float(parts[0].replace(',', ''))
                        - L290 assign from_currency = parts[1].upper()
                        - L292 else:
                          - L292 assign from_currency = parts[0].upper()
                          - L293 assign amount = float(parts[1].replace(',', ''))
                      - L295 else:
                        - L295 assign amount = float(value.replace(',', ''))
                        - L296 assign from_currency = self.base_currency
                - L297 assign to_currency = self.target_currency
                - L299 else:
                  - L299 return None
      - L302 assign (rate, source) = await self._get_rate_async(from_currency, to_currency)
      - L303 if rate is None:
        - L304 return None
      - L306 assign converted = amount * rate
      - L308 return {'converted_amount': round(converted, 2), 'exchange_rate': round(rate, 6), 'source_currency': from_currency, 'target_currency': to_currency, 'rate_date': datetime.now(timezone.utc).strftime('%Y-%m-%d'), 'rate_source': source}
      - L317 except Exception as exc:
        - L318 expr logger.warning(f"Exchange rate lookup failed for '{value}': {exc}")
        - L319 return None
  - L321 async def _get_rate_asyncself, from_currency: str, to_currency: str:
    - L324 docstring: "\n        Get exchange rate between two currencies asynchronously.\n\n        Re..."
    - L330 if from_currency == to_currency:
      - L331 return (1.0, 'identity')
    - L334 if self._use_live_rates:
      - L335 assign live_rates = await self._fetch_live_rates(from_currency)
      - L336 if live_rates:
        - L337 assign rate = live_rates.get(to_currency)
        - L338 if rate:
          - L339 return (float(rate), 'live')
        - L341 assign inverse_rates = await self._fetch_live_rates(to_currency)
        - L342 if inverse_rates:
          - L343 assign inverse_rate = inverse_rates.get(from_currency)
          - L344 if inverse_rate:
            - L345 return (1.0 / float(inverse_rate), 'live')
    - L348 return (self._get_fallback_rate(from_currency, to_currency), 'fallback')
  - L350 def _get_fallback_rateself, from_currency: str, to_currency: str:
    - L353 docstring: "Get exchange rate from fallback hardcoded rates."
    - L354 if from_currency == to_currency:
      - L355 return 1.0
    - L357 try:
      - L358 if from_currency == 'USD':
        - L359 assign rate = FALLBACK_RATES_USD.get(to_currency)
        - L360 if rate:
          - L361 return rate
        - L362 else:
          - L362 if to_currency == 'USD':
            - L363 assign rate = FALLBACK_RATES_USD.get(from_currency)
            - L364 if rate:
              - L365 return 1.0 / rate
            - L368 else:
              - L368 assign from_usd = FALLBACK_RATES_USD.get(from_currency)
              - L369 assign to_usd = FALLBACK_RATES_USD.get(to_currency)
              - L370 if from_usd and to_usd:
                - L371 return to_usd / from_usd
      - L373 expr logger.warning(f'No fallback rate found for {from_currency} -> {to_currency}')
      - L374 return None
      - L376 except Exception as exc:
        - L377 expr logger.error(f'Fallback rate calculation error: {exc}')
        - L378 return None
  - L380 def get_supported_fieldsself:
    - L381 return self.supported_fields
  - L383 def get_confidenceself, result: Dict[str, Any]:
    - L384 docstring: "\n        Exchange rates confidence based on data source.\n\n        Live rates ..."
    - L389 if not result or not result.get('exchange_rate'):
      - L390 return 0.0
    - L392 assign rate_source = result.get('rate_source', 'fallback')
    - L393 if rate_source == 'live':
      - L394 return 0.99
      - L395 else:
        - L395 if rate_source == 'identity':
          - L396 return 1.0
          - L398 else:
            - L398 return 0.85
  - L400 def validate_configself:
    - L401 docstring: "Validate source configuration."
    - L403 return True
- L406 def clear_exchange_rate_cache:
  - L407 docstring: "Clear the in-memory exchange rate cache."
  - L408 Global
  - L409 expr _RATES_CACHE.clear()
  - L410 expr logger.info('Exchange rate cache cleared')
- L413 def get_exchange_rate_cache_status:
  - L414 docstring: "Get current exchange rate cache status for monitoring."
  - L415 assign cache_info = {}
  - L416 for (key, value) in _RATES_CACHE.items():
    - L417 if isinstance(value, dict) and '_timestamp' in value:
      - L418 assign cache_info[key] = {'currencies': len(value) - 1, 'age_seconds': int(time.time() - value.get('_timestamp', 0)), 'is_stale': time.time() - value.get('_timestamp', 0) > _CACHE_TTL_SECONDS}
  - L423 return {'cache_ttl_seconds': _CACHE_TTL_SECONDS, 'cached_bases': list(cache_info.keys()), 'cache_details': cache_info, 'fallback_currencies': len(FALLBACK_RATES_USD)}
- L431 def get_supported_currencies:
  - L432 docstring: "Get list of all supported currency codes."
  - L433 return ['USD'] + sorted(FALLBACK_RATES_USD.keys())
- L436 async def convert_currencyamount: float, from_currency: str, to_currency: str, use_live_rates: bool=True:
  - L442 docstring: "\n    Convenience function to convert currency amounts.\n\n    Args:\n        am..."
  - L454 assign source = ExchangeRateSource({'base_currency': from_currency.upper(), 'target_currency': to_currency.upper(), 'use_live_rates': use_live_rates})
  - L459 return await source.lookup(amount)

## backend\app\domain\errors.py
- L1 from __future__ import annotations
- L3 from dataclasses import dataclass
- L4 from typing import Optional
- L6 from backend.app.core.errors import AppError
- L10 class DomainError(AppError):
  - L11 docstring: "\n    Base domain error to keep HTTP concerns at the edge while providing typed ..."
  - L15 annotated assign code: str
  - L16 annotated assign message: str
  - L17 annotated assign status_code: int = 400
  - L18 annotated assign detail: Optional[str] = None
  - L20 def __post_init__self:
    - L21 expr super().__init__(code=self.code, message=self.message, status_code=self.status_code, detail=self.detail)

## backend\app\domain\federation\__init__.py
- L1 docstring: "Cross-Database Federation domain module."
- L2 from .schemas import VirtualSchema, JoinSuggestion, FederatedQueryRequest
- L7 from .service import FederationService
- L9 assign __all__ = ['VirtualSchema', 'JoinSuggestion', 'FederatedQueryRequest', 'FederationService']

## backend\app\domain\federation\schemas.py
- L1 docstring: "Schemas for Cross-Database Federation feature."
- L2 from __future__ import annotations
- L4 from typing import Any, Dict, List, Optional
- L5 from pydantic import BaseModel, Field
- L8 class TableReference(BaseModel):
  - L9 docstring: "Reference to a table in a connection."
  - L10 annotated assign connection_id: str
  - L11 annotated assign table_name: str
  - L12 annotated assign alias: Optional[str] = None
- L15 class JoinCondition(BaseModel):
  - L16 docstring: "A join condition between two tables."
  - L17 annotated assign left_table: str
  - L18 annotated assign left_column: str
  - L19 annotated assign right_table: str
  - L20 annotated assign right_column: str
  - L21 annotated assign join_type: str = 'INNER'
- L24 class JoinSuggestion(BaseModel):
  - L25 docstring: "AI-suggested join between tables."
  - L26 annotated assign left_connection_id: str
  - L27 annotated assign left_table: str
  - L28 annotated assign left_column: str
  - L29 annotated assign right_connection_id: str
  - L30 annotated assign right_table: str
  - L31 annotated assign right_column: str
  - L32 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L33 annotated assign reason: str
  - L34 annotated assign suggested_join_type: str = 'INNER'
- L37 class VirtualSchema(BaseModel):
  - L38 docstring: "A virtual schema spanning multiple databases."
  - L39 annotated assign id: str
  - L40 annotated assign name: str
  - L41 annotated assign description: Optional[str] = None
  - L42 annotated assign connections: List[str]
  - L43 annotated assign tables: List[TableReference]
  - L44 annotated assign joins: List[JoinCondition]
  - L45 annotated assign created_at: str
  - L46 annotated assign updated_at: str
- L49 class VirtualSchemaCreate(BaseModel):
  - L50 docstring: "Request to create a virtual schema."
  - L51 annotated assign name: str = Field(..., min_length=1, max_length=100)
  - L52 annotated assign description: Optional[str] = Field(None, max_length=500)
  - L53 annotated assign connection_ids: List[str] = Field(..., min_items=1, max_items=10)
- L56 class SuggestJoinsRequest(BaseModel):
  - L57 docstring: "Request to suggest joins between connections."
  - L58 annotated assign connection_ids: List[str] = Field(..., min_items=2, max_items=10)
- L61 class FederatedQueryRequest(BaseModel):
  - L62 docstring: "Request to execute a federated query."
  - L63 annotated assign virtual_schema_id: str
  - L64 annotated assign sql: str = Field(..., min_length=1, max_length=10000)
  - L65 annotated assign limit: int = Field(default=100, ge=1, le=1000)

## backend\app\domain\federation\service.py
- L1 docstring: "Service layer for Cross-Database Federation feature."
- L2 from __future__ import annotations
- L4 import logging
- L5 import uuid
- L6 from datetime import datetime, timezone
- L7 from typing import Any, Dict, List, Optional
- L9 from backend.app.core.errors import AppError
- L10 from backend.app.services.state import store as state_store_module
- L11 from backend.app.services.llm.client import get_llm_client
- L13 from .schemas import VirtualSchema, VirtualSchemaCreate, JoinSuggestion, TableReference, JoinCondition, FederatedQueryRequest
- L22 assign logger = logging.getLogger('neura.domain.federation')
- L25 def _now_iso:
  - L26 return datetime.now(timezone.utc).replace(microsecond=0).isoformat()
- L29 def _state_store:
  - L30 return state_store_module.state_store
- L33 class FederationService:
  - L34 docstring: "Service for cross-database federation operations."
  - L36 def __init__self:
    - L37 assign self._llm_client = None
  - L39 def _get_llm_clientself:
    - L40 if self._llm_client is None:
      - L41 assign self._llm_client = get_llm_client()
    - L42 return self._llm_client
  - L44 def _get_connection_schemaself, connection_id: str:
    - L45 docstring: "Get schema for a connection."
    - L46 from src.services.connection_inspector import get_connection_schema
    - L47 return get_connection_schema(connection_id, include_row_counts=False, sample_rows=3)
  - L49 def create_virtual_schemaself, request: VirtualSchemaCreate, correlation_id: Optional[str]=None:
    - L54 docstring: "Create a new virtual schema."
    - L55 expr logger.info(f'Creating virtual schema: {request.name}', extra={'correlation_id': correlation_id})
    - L57 assign schema_id = str(uuid.uuid4())[:8]
    - L58 assign now = _now_iso()
    - L61 annotated assign tables: List[TableReference] = []
    - L62 for conn_id in request.connection_ids:
      - L63 try:
        - L64 assign schema = self._get_connection_schema(conn_id)
        - L65 for table in schema.get('tables', []):
          - L66 expr tables.append(TableReference(connection_id=conn_id, table_name=table['name'], alias=f"{conn_id[:4]}_{table['name']}"))
        - L71 except Exception as exc:
          - L72 expr logger.warning(f'Failed to get schema for {conn_id}: {exc}')
    - L74 assign virtual_schema = VirtualSchema(id=schema_id, name=request.name, description=request.description, connections=request.connection_ids, tables=tables, joins=[], created_at=now, updated_at=now)
    - L86 assign store = _state_store()
    - L87 with store._lock:
      - L88 assign state = store._read_state()
      - L89 assign state.setdefault('virtual_schemas', {})[schema_id] = virtual_schema.dict()
      - L90 expr store._write_state(state)
    - L92 return virtual_schema
  - L94 def suggest_joinsself, connection_ids: List[str], correlation_id: Optional[str]=None:
    - L99 docstring: "Suggest joins between tables in different connections using AI."
    - L100 expr logger.info(f'Suggesting joins for {len(connection_ids)} connections', extra={'correlation_id': correlation_id})
    - L103 assign schemas = {}
    - L104 for conn_id in connection_ids:
      - L105 try:
        - L106 assign schemas[conn_id] = self._get_connection_schema(conn_id)
        - L107 except Exception as exc:
          - L108 expr logger.warning(f'Failed to get schema for {conn_id}: {exc}')
    - L110 if len(schemas) < 2:
      - L111 return []
    - L114 assign schema_desc = []
    - L115 for (conn_id, schema) in schemas.items():
      - L116 assign tables_desc = []
      - L117 for table in schema.get('tables', []):
        - L118 assign cols = [f"{c['name']} ({c.get('type', 'TEXT')})" for c in table.get('columns', [])]
        - L119 expr tables_desc.append(f"  - {table['name']}: {', '.join(cols)}")
      - L120 expr schema_desc.append(f'Connection {conn_id}:\n' + '\n'.join(tables_desc))
    - L122 assign prompt = f"""Analyze these database schemas and suggest joins between tables from different connections.\n\n{chr(10).join(schema_desc)}\n\nFor each potential join, consider:\n1. Column names that might match (like 'customer_id', 'user_id', 'id')\n2. Data types compatibility\n3. Business logic relationships\n\nReturn a JSON array of join suggestions:\n[\n  {{\n    "left_connection_id": "conn1",\n    "left_table": "table1",\n    "left_column": "column1",\n    "right_connection_id": "conn2",\n    "right_table": "table2",\n    "right_column": "column2",\n    "confidence": 0.9,\n    "reason": "Both columns appear to be customer identifiers"\n  }}\n]\n\nReturn ONLY the JSON array."""
    - L147 try:
      - L148 assign client = self._get_llm_client()
      - L149 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='join_suggestion', temperature=0.0)
      - L155 import json
      - L156 import re
      - L157 assign content = response['choices'][0]['message']['content']
      - L158 assign json_match = re.search('\\[[\\s\\S]*\\]', content)
      - L159 if json_match:
        - L160 assign suggestions_data = json.loads(json_match.group())
        - L161 return [JoinSuggestion(**s) for s in suggestions_data]
      - L163 except Exception as exc:
        - L164 expr logger.error(f'Join suggestion failed: {exc}')
    - L166 return []
  - L168 def list_virtual_schemasself:
    - L169 docstring: "List all virtual schemas."
    - L170 assign store = _state_store()
    - L171 assign schemas = store._read_state().get('virtual_schemas', {})
    - L172 return [VirtualSchema(**s) for s in schemas.values()]
  - L174 def get_virtual_schemaself, schema_id: str:
    - L175 docstring: "Get a virtual schema by ID."
    - L176 assign store = _state_store()
    - L177 assign schema = store._read_state().get('virtual_schemas', {}).get(schema_id)
    - L178 return VirtualSchema(**schema) if schema else None
  - L180 def delete_virtual_schemaself, schema_id: str:
    - L181 docstring: "Delete a virtual schema."
    - L182 assign store = _state_store()
    - L183 with store._lock:
      - L184 assign state = store._read_state()
      - L185 assign schemas = state.get('virtual_schemas', {})
      - L186 if schema_id not in schemas:
        - L187 return False
      - L188 delete schemas[schema_id]
      - L189 expr store._write_state(state)
    - L190 return True
  - L192 def _extract_table_namesself, sql: str:
    - L193 docstring: "Extract table names from SQL query using improved parsing.\n\n        Handles:\n..."
    - L203 import re
    - L206 assign sql_normalized = ' '.join(sql.split())
    - L210 assign ident = '(?:(?:[a-zA-Z_][a-zA-Z0-9_]*(?:\\.[a-zA-Z_][a-zA-Z0-9_]*)?)|"[^"]+"(?:\\."[^"]+")?|`[^`]+`(?:\\.`[^`]+`)?|\\[[^\\]]+\\](?:\\.\\[[^\\]]+\\])?)'
    - L218 assign table_list_pattern = f'{ident}(?:\\s+(?:AS\\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?(?:\\s*,\\s*{ident}(?:\\s+(?:AS\\s+)?[a-zA-Z_][a-zA-Z0-9_]*)?)*'
    - L221 assign patterns = [f'\\bFROM\\s+({table_list_pattern})', f'\\b(?:LEFT|RIGHT|INNER|OUTER|CROSS|FULL)?\\s*JOIN\\s+({ident})', f'\\bJOIN\\s+({ident})', f'\\bINTO\\s+({ident})', f'\\bUPDATE\\s+({ident})']
    - L230 assign cte_pattern = f'\\bWITH\\s+({ident})\\s+AS\\s*\\('
    - L231 assign cte_names = set()
    - L232 for match in re.findall(cte_pattern, sql_normalized, re.IGNORECASE):
      - L233 expr cte_names.add(self._clean_identifier(match).lower())
    - L235 assign tables = set()
    - L236 assign sql_keywords = {'SELECT', 'WHERE', 'ON', 'AND', 'OR', 'NOT', 'IN', 'EXISTS', 'HAVING', 'GROUP', 'ORDER', 'BY', 'LIMIT', 'OFFSET', 'UNION', 'INTERSECT', 'EXCEPT', 'AS', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'NULL', 'TRUE', 'FALSE', 'IS', 'LIKE', 'BETWEEN', 'ALL', 'ANY', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'CROSS', 'FULL', 'NATURAL'}
    - L244 for pattern in patterns:
      - L245 assign matches = re.findall(pattern, sql_normalized, re.IGNORECASE)
      - L246 for match in matches:
        - L249 assign parts = re.split('\\s*,\\s*', match)
        - L250 for part in parts:
          - L252 assign tokens = part.strip().split()
          - L253 if tokens:
            - L254 assign table_ref = tokens[0]
            - L256 if table_ref.upper() == 'AS' and len(tokens) > 1:
              - L257 continue
            - L259 assign table_name = self._clean_identifier(table_ref)
            - L262 if table_name.upper() not in sql_keywords and table_name.lower() not in cte_names:
              - L263 expr tables.add(table_name.lower())
    - L265 return list(tables)
  - L267 def _clean_identifierself, ident: str:
    - L268 docstring: "Remove quotes from identifier and extract table name from schema.table."
    - L270 assign cleaned = ident.strip()
    - L271 if cleaned.startswith('"') and cleaned.endswith('"'):
      - L272 assign cleaned = cleaned[1:-1]
      - L273 else:
        - L273 if cleaned.startswith('`') and cleaned.endswith('`'):
          - L274 assign cleaned = cleaned[1:-1]
          - L275 else:
            - L275 if cleaned.startswith('[') and cleaned.endswith(']'):
              - L276 assign cleaned = cleaned[1:-1]
    - L279 if '.' in cleaned:
      - L281 assign parts = cleaned.split('.')
      - L282 assign cleaned = parts[-1]
      - L284 if cleaned.startswith('"') and cleaned.endswith('"'):
        - L285 assign cleaned = cleaned[1:-1]
        - L286 else:
          - L286 if cleaned.startswith('`') and cleaned.endswith('`'):
            - L287 assign cleaned = cleaned[1:-1]
            - L288 else:
              - L288 if cleaned.startswith('[') and cleaned.endswith(']'):
                - L289 assign cleaned = cleaned[1:-1]
    - L291 return cleaned
  - L293 def _map_tables_to_connectionsself, table_names: List[str], schema: VirtualSchema:
    - L296 docstring: "Map table names to their respective connections."
    - L297 annotated assign connection_tables: Dict[str, List[str]] = {}
    - L299 for table_name in table_names:
      - L300 for table_ref in schema.tables:
        - L302 if table_ref.table_name.lower() == table_name.lower() or table_ref.alias.lower() == table_name.lower():
          - L306 assign conn_id = table_ref.connection_id
          - L307 if conn_id not in connection_tables:
            - L308 assign connection_tables[conn_id] = []
          - L309 expr connection_tables[conn_id].append(table_ref.table_name)
          - L310 break
    - L312 return connection_tables
  - L314 def _execute_on_connectionself, connection_id: str, sql: str, limit: Optional[int]=None:
    - L320 docstring: "Execute SQL on a specific connection."
    - L321 from backend.app.services.connections.db_connection import execute_query
    - L322 return execute_query(connection_id=connection_id, sql=sql, limit=limit)
  - L324 def _merge_resultsself, results: List[Dict[str, Any]], join_keys: Optional[List[str]]=None:
    - L329 docstring: "Merge results from multiple connections."
    - L330 if not results:
      - L331 return {'columns': [], 'rows': [], 'row_count': 0}
    - L333 if len(results) == 1:
      - L334 return results[0]
    - L337 if not join_keys:
      - L338 assign all_columns = []
      - L339 assign seen_cols = set()
      - L340 for r in results:
        - L341 for col in r.get('columns', []):
          - L342 if col not in seen_cols:
            - L343 expr all_columns.append(col)
            - L344 expr seen_cols.add(col)
      - L346 assign all_rows = []
      - L347 for r in results:
        - L348 for row in r.get('rows', []):
          - L350 assign extended_row = []
          - L351 for col in all_columns:
            - L352 try:
              - L353 assign idx = r.get('columns', []).index(col)
              - L354 expr extended_row.append(row[idx])
              - L355 except (ValueError, IndexError):
                - L356 expr extended_row.append(None)
          - L357 expr all_rows.append(extended_row)
      - L359 return {'columns': all_columns, 'rows': all_rows, 'row_count': len(all_rows), 'merge_type': 'union'}
    - L367 if len(results) == 2:
      - L368 assign left = results[0]
      - L369 assign right = results[1]
      - L371 assign left_cols = left.get('columns', [])
      - L372 assign right_cols = right.get('columns', [])
      - L375 assign left_key_idx = None
      - L376 assign right_key_idx = None
      - L377 for key in join_keys:
        - L378 if key in left_cols:
          - L379 assign left_key_idx = left_cols.index(key)
        - L380 if key in right_cols:
          - L381 assign right_key_idx = right_cols.index(key)
      - L383 if left_key_idx is None or right_key_idx is None:
        - L385 return self._merge_results(results, None)
      - L388 annotated assign right_lookup: Dict[Any, List[List[Any]]] = {}
      - L389 for row in right.get('rows', []):
        - L390 assign key_val = row[right_key_idx]
        - L391 if key_val not in right_lookup:
          - L392 assign right_lookup[key_val] = []
        - L393 expr right_lookup[key_val].append(row)
      - L396 assign merged_cols = list(left_cols) + [c for i, c in enumerate(right_cols) if i != right_key_idx]
      - L401 assign merged_rows = []
      - L402 for left_row in left.get('rows', []):
        - L403 assign key_val = left_row[left_key_idx]
        - L404 if key_val in right_lookup:
          - L405 for right_row in right_lookup[key_val]:
            - L406 assign new_row = list(left_row) + [v for i, v in enumerate(right_row) if i != right_key_idx]
            - L409 expr merged_rows.append(new_row)
      - L411 return {'columns': merged_cols, 'rows': merged_rows, 'row_count': len(merged_rows), 'merge_type': 'join', 'join_key': join_keys[0]}
    - L419 return self._merge_results(results, None)
  - L421 def execute_queryself, request: FederatedQueryRequest, correlation_id: Optional[str]=None:
    - L426 docstring: "Execute a federated query across multiple databases.\n\n        This method:\n  ..."
    - L434 expr logger.info(f'Executing federated query on schema {request.virtual_schema_id}', extra={'correlation_id': correlation_id})
    - L440 assign schema = self.get_virtual_schema(request.virtual_schema_id)
    - L441 if not schema:
      - L442 raise AppError(code='schema_not_found', message=f'Virtual schema {request.virtual_schema_id} not found', status_code=404)
    - L448 if not schema.connections:
      - L449 raise AppError(code='no_connections', message='Virtual schema has no connections', status_code=400)
    - L455 try:
      - L457 assign table_names = self._extract_table_names(request.sql)
      - L458 expr logger.debug(f'Extracted tables from SQL: {table_names}')
      - L461 assign connection_tables = self._map_tables_to_connections(table_names, schema)
      - L462 expr logger.debug(f'Table-to-connection mapping: {connection_tables}')
      - L465 if len(connection_tables) <= 1:
        - L466 assign target_connection = list(connection_tables.keys())[0] if connection_tables else schema.connections[0]
        - L471 assign result = self._execute_on_connection(connection_id=target_connection, sql=request.sql, limit=request.limit)
        - L476 return {'columns': result.get('columns', []), 'rows': result.get('rows', []), 'row_count': len(result.get('rows', [])), 'schema_id': request.virtual_schema_id, 'executed_on': [target_connection], 'routing': 'single'}
      - L486 expr logger.info(f'Federated query spans {len(connection_tables)} connections', extra={'connections': list(connection_tables.keys())})
      - L492 assign join_keys = []
      - L493 for join in schema.joins:
        - L494 if hasattr(join, 'conditions'):
          - L495 for cond in join.conditions:
            - L496 expr join_keys.extend([cond.left_column, cond.right_column])
      - L499 assign results = []
      - L500 assign executed_on = []
      - L501 for (conn_id, tables) in connection_tables.items():
        - L505 try:
          - L508 assign result = self._execute_on_connection(connection_id=conn_id, sql=request.sql, limit=request.limit)
          - L513 expr results.append(result)
          - L514 expr executed_on.append(conn_id)
          - L515 except Exception as exc:
            - L516 expr logger.warning(f'Query on {conn_id} failed: {exc}')
            - L518 for table in tables:
              - L519 try:
                - L520 assign simple_sql = f'SELECT * FROM {table}'
                - L521 if request.limit:
                  - L522 aug assign simple_sql Add f' LIMIT {request.limit}'
                - L523 assign result = self._execute_on_connection(connection_id=conn_id, sql=simple_sql, limit=request.limit)
                - L528 expr results.append(result)
                - L529 expr executed_on.append(conn_id)
                - L530 except Exception as inner_exc:
                  - L531 expr logger.warning(f'Simple query on {conn_id}.{table} failed: {inner_exc}')
      - L533 if not results:
        - L534 raise AppError(code='query_failed', message='Could not execute query on any connection', status_code=500)
      - L541 assign merged = self._merge_results(results, join_keys if join_keys else None)
      - L543 return {'columns': merged.get('columns', []), 'rows': merged.get('rows', [])[:request.limit] if request.limit else merged.get('rows', []), 'row_count': len(merged.get('rows', [])), 'schema_id': request.virtual_schema_id, 'executed_on': executed_on, 'routing': 'federated', 'merge_type': merged.get('merge_type', 'unknown')}
      - L553 except AppError:
        - L554 raise
      - L555 except Exception as exc:
        - L556 expr logger.error(f'Federated query failed: {exc}')
        - L557 raise AppError(code='query_failed', message=f'Query execution failed: {str(exc)}', status_code=500)

## backend\app\domain\nl2sql\__init__.py
- L1 docstring: "Natural Language to SQL domain module."
- L2 from .schemas import NL2SQLGenerateRequest, NL2SQLExecuteRequest, NL2SQLSaveRequest, NL2SQLResult, SavedQuery
- L9 from .service import NL2SQLService
- L11 assign __all__ = ['NL2SQLGenerateRequest', 'NL2SQLExecuteRequest', 'NL2SQLSaveRequest', 'NL2SQLResult', 'SavedQuery', 'NL2SQLService']

## backend\app\domain\nl2sql\schemas.py
- L1 docstring: "Schemas for Natural Language to SQL feature."
- L2 from __future__ import annotations
- L4 from datetime import datetime
- L5 from typing import Any, Dict, List, Optional
- L7 from pydantic import BaseModel, Field, validator
- L9 from backend.app.core.validation import is_safe_id, is_safe_name
- L12 class NL2SQLGenerateRequest(BaseModel):
  - L13 docstring: "Request to generate SQL from natural language."
  - L14 annotated assign question: str = Field(..., min_length=3, max_length=2000)
  - L15 annotated assign connection_id: str = Field(..., min_length=1, max_length=64)
  - L16 annotated assign tables: Optional[List[str]] = Field(None, max_items=50)
  - L17 annotated assign context: Optional[str] = Field(None, max_length=1000)
  - L20 def validate_connection_idcls, value: str:
    - L21 if not is_safe_id(value):
      - L22 raise ValueError('Connection ID must be alphanumeric with dashes/underscores only')
    - L23 return value
  - L26 def validate_table_namecls, value: str:
    - L27 if not value or len(value) > 128:
      - L28 raise ValueError('Table name must be 1-128 characters')
    - L29 return value.strip()
- L32 class NL2SQLExecuteRequest(BaseModel):
  - L33 docstring: "Request to execute a SQL query."
  - L34 annotated assign sql: str = Field(..., min_length=1, max_length=10000)
  - L35 annotated assign connection_id: str = Field(..., min_length=1, max_length=64)
  - L36 annotated assign limit: int = Field(default=100, ge=1, le=1000)
  - L37 annotated assign offset: int = Field(default=0, ge=0)
  - L40 def validate_connection_idcls, value: str:
    - L41 if not is_safe_id(value):
      - L42 raise ValueError('Connection ID must be alphanumeric with dashes/underscores only')
    - L43 return value
- L46 class NL2SQLSaveRequest(BaseModel):
  - L47 docstring: "Request to save a query as a reusable data source."
  - L48 annotated assign name: str = Field(..., min_length=1, max_length=100)
  - L49 annotated assign description: Optional[str] = Field(None, max_length=500)
  - L50 annotated assign sql: str = Field(..., min_length=1, max_length=10000)
  - L51 annotated assign connection_id: str = Field(..., min_length=1, max_length=64)
  - L52 annotated assign original_question: Optional[str] = Field(None, max_length=2000)
  - L53 annotated assign tags: Optional[List[str]] = Field(None, max_items=20)
  - L56 def validate_namecls, value: str:
    - L57 if not is_safe_name(value):
      - L58 raise ValueError('Name contains invalid characters')
    - L59 return value.strip()
  - L62 def validate_connection_idcls, value: str:
    - L63 if not is_safe_id(value):
      - L64 raise ValueError('Connection ID must be alphanumeric with dashes/underscores only')
    - L65 return value
  - L68 def validate_tagcls, value: str:
    - L69 if len(value) > 50:
      - L70 raise ValueError('Tag must be 50 characters or less')
    - L71 return value.strip()
- L74 class NL2SQLResult(BaseModel):
  - L75 docstring: "Result from SQL generation."
  - L76 annotated assign sql: str
  - L77 annotated assign explanation: str
  - L78 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L79 annotated assign warnings: List[str] = Field(default_factory=list)
  - L80 annotated assign original_question: str
- L83 class QueryExecutionResult(BaseModel):
  - L84 docstring: "Result from query execution."
  - L85 annotated assign columns: List[str]
  - L86 annotated assign rows: List[Dict[str, Any]]
  - L87 annotated assign row_count: int
  - L88 annotated assign total_count: Optional[int] = None
  - L89 annotated assign execution_time_ms: int
  - L90 annotated assign truncated: bool = False
- L93 class SavedQuery(BaseModel):
  - L94 docstring: "A saved SQL query."
  - L95 annotated assign id: str
  - L96 annotated assign name: str
  - L97 annotated assign description: Optional[str] = None
  - L98 annotated assign sql: str
  - L99 annotated assign connection_id: str
  - L100 annotated assign original_question: Optional[str] = None
  - L101 annotated assign tags: List[str] = Field(default_factory=list)
  - L102 annotated assign created_at: str
  - L103 annotated assign updated_at: str
  - L104 annotated assign last_run_at: Optional[str] = None
  - L105 annotated assign run_count: int = 0
- L108 class QueryHistoryEntry(BaseModel):
  - L109 docstring: "An entry in the query history."
  - L110 annotated assign id: str
  - L111 annotated assign question: str
  - L112 annotated assign sql: str
  - L113 annotated assign connection_id: str
  - L114 annotated assign confidence: float
  - L115 annotated assign success: bool
  - L116 annotated assign error: Optional[str] = None
  - L117 annotated assign execution_time_ms: Optional[int] = None
  - L118 annotated assign row_count: Optional[int] = None
  - L119 annotated assign created_at: str

## backend\app\domain\nl2sql\service.py
- L1 docstring: "Service layer for Natural Language to SQL feature."
- L2 from __future__ import annotations
- L4 import logging
- L5 import time
- L6 import uuid
- L7 from datetime import datetime, timezone
- L8 from pathlib import Path
- L9 from typing import Any, Dict, List, Optional
- L11 from backend.app.core.errors import AppError
- L12 from backend.app.services.connections.db_connection import resolve_db_path, verify_sqlite
- L13 from backend.app.services.dataframes import sqlite_shim, ensure_connection_loaded
- L14 from backend.app.services.llm.client import get_llm_client
- L15 from backend.app.services.llm.text_to_sql import TextToSQL, TableSchema
- L16 from backend.app.services.state import store as state_store_module
- L18 from .schemas import NL2SQLGenerateRequest, NL2SQLExecuteRequest, NL2SQLSaveRequest, NL2SQLResult, QueryExecutionResult, SavedQuery, QueryHistoryEntry
- L28 assign logger = logging.getLogger('neura.domain.nl2sql')
- L31 def _now_iso:
  - L32 return datetime.now(timezone.utc).replace(microsecond=0).isoformat()
- L35 def _state_store:
  - L36 return state_store_module.state_store
- L39 def _quote_identifiername: str:
  - L40 return name.replace('"', '""')
- L43 def _coerce_valuevalue: Any:
  - L44 docstring: "Convert bytes and other non-JSON types to serializable formats."
  - L45 if isinstance(value, (bytes, bytearray, memoryview)):
    - L46 return bytes(value).hex()
  - L47 return value
- L50 class NL2SQLService:
  - L51 docstring: "Service for natural language to SQL operations."
  - L53 def __init__self:
    - L54 annotated assign self._text_to_sql: Optional[TextToSQL] = None
  - L56 def _get_text_to_sqlself:
    - L57 docstring: "Get or create TextToSQL instance."
    - L58 if self._text_to_sql is None:
      - L59 assign client = get_llm_client()
      - L60 assign self._text_to_sql = TextToSQL(client=client, dialect='sqlite')
    - L61 return self._text_to_sql
  - L63 def _resolve_connectionself, connection_id: str:
    - L64 docstring: "Resolve and verify a database connection."
    - L65 try:
      - L66 assign db_path = resolve_db_path(connection_id=connection_id, db_url=None, db_path=None)
      - L67 expr verify_sqlite(db_path)
      - L68 return db_path
      - L69 except Exception as exc:
        - L70 raise AppError(code='connection_invalid', message='Invalid or unreachable database connection', detail=str(exc), status_code=400)
  - L77 def _get_schema_for_connectionself, db_path: Path, tables: Optional[List[str]]=None:
    - L78 docstring: "Get database schema for SQL generation context using DataFrames."
    - L79 from backend.app.services.dataframes.sqlite_loader import get_loader
    - L81 assign loader = get_loader(db_path)
    - L82 assign schema = {}
    - L84 assign table_names = tables if tables else loader.table_names()
    - L86 for table_name in table_names:
      - L87 if tables and table_name not in tables:
        - L88 continue
      - L90 assign columns = []
      - L91 for col in loader.pragma_table_info(table_name):
        - L92 expr columns.append({'name': col.get('name'), 'type': col.get('type', 'TEXT'), 'description': ''})
      - L99 assign sample_values = {}
      - L100 try:
        - L101 assign frame = loader.frame(table_name)
        - L102 if not frame.empty:
          - L103 assign sample_rows = frame.head(3)
          - L104 for col in columns:
            - L105 assign col_name = col['name']
            - L106 if col_name in sample_rows.columns:
              - L107 assign values = [_coerce_value(v) for v in sample_rows[col_name].tolist()]
              - L108 if values:
                - L109 assign sample_values[col_name] = values[:3]
        - L110 except Exception:
          - L111 pass
      - L113 assign schema[table_name] = {'columns': columns, 'foreign_keys': loader.foreign_keys(table_name), 'sample_values': sample_values}
    - L119 return schema
  - L121 def generate_sqlself, request: NL2SQLGenerateRequest, correlation_id: Optional[str]=None:
    - L126 docstring: "Generate SQL from a natural language question."
    - L127 expr logger.info(f'Generating SQL for question: {request.question[:100]}...', extra={'correlation_id': correlation_id})
    - L130 assign db_path = self._resolve_connection(request.connection_id)
    - L133 assign schema = self._get_schema_for_connection(db_path, request.tables)
    - L134 if not schema:
      - L135 raise AppError(code='no_tables', message='No tables found in the database', status_code=400)
    - L142 assign t2sql = self._get_text_to_sql()
    - L143 expr t2sql._schemas.clear()
    - L144 expr t2sql.add_schemas_from_catalog(schema)
    - L147 try:
      - L148 assign result = t2sql.generate_sql(question=request.question, tables=request.tables, context=request.context)
      - L153 except Exception as exc:
        - L154 expr logger.error(f'SQL generation failed: {exc}', extra={'correlation_id': correlation_id})
        - L155 raise AppError(code='generation_failed', message='Failed to generate SQL query', detail=str(exc), status_code=500)
    - L163 expr self._record_history(question=request.question, sql=result.sql, connection_id=request.connection_id, confidence=result.confidence, success=True)
    - L171 return NL2SQLResult(sql=result.sql, explanation=result.explanation, confidence=result.confidence, warnings=result.warnings, original_question=request.question)
  - L179 def execute_queryself, request: NL2SQLExecuteRequest, correlation_id: Optional[str]=None:
    - L184 docstring: "Execute a SQL query and return results using DataFrames."
    - L185 expr logger.info(f'Executing SQL query on connection {request.connection_id}', extra={'correlation_id': correlation_id})
    - L188 assign db_path = self._resolve_connection(request.connection_id)
    - L191 expr ensure_connection_loaded(request.connection_id, db_path)
    - L194 assign sql_upper = request.sql.upper().strip()
    - L195 if not sql_upper.startswith('SELECT') and (not sql_upper.startswith('WITH')):
      - L196 raise AppError(code='invalid_query', message='Only SELECT queries are allowed', status_code=400)
    - L203 assign dangerous_patterns = ['DROP ', 'DELETE ', 'INSERT ', 'UPDATE ', 'ALTER ', 'CREATE ', 'TRUNCATE ']
    - L204 for pattern in dangerous_patterns:
      - L205 if pattern in sql_upper:
        - L206 raise AppError(code='dangerous_query', message=f'Query contains prohibited operation: {pattern.strip()}', status_code=400)
    - L213 assign started = time.time()
    - L214 try:
      - L215 with sqlite_shim.connect(str(db_path)) as con:
        - L216 assign con.row_factory = sqlite_shim.Row
        - L219 assign count_sql = f'SELECT COUNT(*) as cnt FROM ({request.sql}) AS subq'
        - L220 try:
          - L221 assign total_count = con.execute(count_sql).fetchone()['cnt']
          - L222 except Exception:
            - L223 assign total_count = None
        - L226 assign limited_sql = f'{request.sql} LIMIT {request.limit} OFFSET {request.offset}'
        - L227 assign cur = con.execute(limited_sql)
        - L228 assign rows_raw = cur.fetchall()
        - L230 assign columns = list(rows_raw[0].keys()) if rows_raw else []
        - L231 assign rows = [{col: _coerce_value(row[col]) for col in columns} for row in rows_raw]
      - L233 except sqlite_shim.OperationalError as exc:
        - L234 assign execution_time_ms = int((time.time() - started) * 1000)
        - L235 expr logger.error(f'Query execution failed: {exc}', extra={'correlation_id': correlation_id})
        - L238 expr self._update_history_execution(sql=request.sql, connection_id=request.connection_id, success=False, error=str(exc), execution_time_ms=execution_time_ms)
        - L246 raise AppError(code='execution_failed', message='Failed to execute SQL query', detail=str(exc), status_code=400)
    - L253 assign execution_time_ms = int((time.time() - started) * 1000)
    - L256 expr self._update_history_execution(sql=request.sql, connection_id=request.connection_id, success=True, execution_time_ms=execution_time_ms, row_count=len(rows))
    - L264 return QueryExecutionResult(columns=columns, rows=rows, row_count=len(rows), total_count=total_count, execution_time_ms=execution_time_ms, truncated=total_count is not None and total_count > request.limit)
  - L273 def explain_queryself, sql: str, correlation_id: Optional[str]=None:
    - L278 docstring: "Get a natural language explanation of a SQL query."
    - L279 assign t2sql = self._get_text_to_sql()
    - L280 return t2sql.explain_sql(sql)
  - L282 def save_queryself, request: NL2SQLSaveRequest, correlation_id: Optional[str]=None:
    - L287 docstring: "Save a query as a reusable data source."
    - L288 expr logger.info(f'Saving query: {request.name}', extra={'correlation_id': correlation_id})
    - L291 expr self._resolve_connection(request.connection_id)
    - L293 assign query_id = str(uuid.uuid4())[:8]
    - L294 assign now = _now_iso()
    - L296 assign saved_query = SavedQuery(id=query_id, name=request.name, description=request.description, sql=request.sql, connection_id=request.connection_id, original_question=request.original_question, tags=request.tags or [], created_at=now, updated_at=now, run_count=0)
    - L310 assign store = _state_store()
    - L311 expr store.save_query(saved_query.dict())
    - L313 return saved_query
  - L315 def list_saved_queriesself, connection_id: Optional[str]=None, tags: Optional[List[str]]=None:
    - L320 docstring: "List saved queries, optionally filtered."
    - L321 assign store = _state_store()
    - L322 assign queries = store.list_saved_queries()
    - L324 if connection_id:
      - L325 assign queries = [q for q in queries if q.get('connection_id') == connection_id]
    - L327 if tags:
      - L328 assign tag_set = set(tags)
      - L329 assign queries = [q for q in queries if tag_set.intersection(set(q.get('tags', [])))]
    - L331 return [SavedQuery(**q) for q in queries]
  - L333 def get_saved_queryself, query_id: str:
    - L334 docstring: "Get a saved query by ID."
    - L335 assign store = _state_store()
    - L336 assign query = store.get_saved_query(query_id)
    - L337 return SavedQuery(**query) if query else None
  - L339 def delete_saved_queryself, query_id: str:
    - L340 docstring: "Delete a saved query."
    - L341 assign store = _state_store()
    - L342 return store.delete_saved_query(query_id)
  - L344 def get_query_historyself, connection_id: Optional[str]=None, limit: int=50:
    - L349 docstring: "Get query history."
    - L350 assign store = _state_store()
    - L351 assign history = store.get_query_history(limit=limit)
    - L353 if connection_id:
      - L354 assign history = [h for h in history if h.get('connection_id') == connection_id]
    - L356 return [QueryHistoryEntry(**h) for h in history]
  - L358 def delete_query_history_entryself, entry_id: str:
    - L359 docstring: "Delete a query history entry by ID."
    - L360 assign store = _state_store()
    - L361 return store.delete_query_history_entry(entry_id)
  - L363 def _record_historyself, question: str, sql: str, connection_id: str, confidence: float, success: bool, error: Optional[str]=None:
    - L372 docstring: "Record a query generation in history."
    - L373 assign store = _state_store()
    - L374 assign entry = {'id': str(uuid.uuid4())[:8], 'question': question, 'sql': sql, 'connection_id': connection_id, 'confidence': confidence, 'success': success, 'error': error, 'created_at': _now_iso()}
    - L384 expr store.add_query_history(entry)
  - L386 def _update_history_executionself, sql: str, connection_id: str, success: bool, error: Optional[str]=None, execution_time_ms: Optional[int]=None, row_count: Optional[int]=None:
    - L395 docstring: "Update history with execution results."
    - L397 pass

## backend\app\domain\recommendations\__init__.py
- L1 docstring: "Template Recommendations domain module."
- L2 from .service import RecommendationService
- L4 assign __all__ = ['RecommendationService']

## backend\app\domain\recommendations\service.py
- L1 docstring: "Service for template recommendations using AI."
- L2 from __future__ import annotations
- L4 import logging
- L5 from typing import Any, Dict, List, Optional
- L7 from backend.app.services.llm.client import get_llm_client
- L8 from backend.app.services.state import store as state_store_module
- L10 assign logger = logging.getLogger('neura.domain.recommendations')
- L13 def _state_store:
  - L14 return state_store_module.state_store
- L17 class RecommendationService:
  - L18 docstring: "Service for template recommendations."
  - L20 def __init__self:
    - L21 assign self._llm_client = None
  - L23 def _get_llm_clientself:
    - L24 if self._llm_client is None:
      - L25 assign self._llm_client = get_llm_client()
    - L26 return self._llm_client
  - L28 def recommend_templatesself, connection_id: Optional[str]=None, schema_info: Optional[Dict[str, Any]]=None, context: Optional[str]=None, limit: int=5, correlation_id: Optional[str]=None:
    - L36 docstring: "\n        Recommend templates based on connection schema and context.\n\n       ..."
    - L49 expr logger.info('Generating template recommendations', extra={'correlation_id': correlation_id})
    - L52 assign store = _state_store()
    - L53 assign templates = store._read_state().get('templates', {})
    - L54 assign approved = [t for t in templates.values() if t.get('status') == 'approved']
    - L56 if not approved:
      - L57 return []
    - L60 if connection_id and (not schema_info):
      - L61 try:
        - L62 from src.services.connection_inspector import get_connection_schema
        - L63 assign schema_info = get_connection_schema(connection_id, include_row_counts=False)
        - L64 except Exception:
          - L65 pass
    - L68 assign template_catalog = []
    - L69 for t in approved:
      - L70 expr template_catalog.append({'id': t.get('id'), 'name': t.get('name'), 'kind': t.get('kind'), 'tags': t.get('tags', [])})
    - L77 assign prompt = f"Recommend templates from this catalog based on the user's needs.\n\nTEMPLATE CATALOG:\n{template_catalog}\n\n"
    - L83 if schema_info:
      - L84 assign tables = [t['name'] for t in schema_info.get('tables', [])]
      - L85 aug assign prompt Add f"DATABASE TABLES: {', '.join(tables)}\n\n"
    - L87 if context:
      - L88 aug assign prompt Add f'USER CONTEXT: {context}\n\n'
    - L90 aug assign prompt Add f'Return a JSON array of the top {limit} recommended templates:\n[\n  {{\n    "template_id": "id",\n    "score": 0.95,\n    "reason": "Why this template matches"\n  }}\n]\n\nReturn ONLY the JSON array.'
    - L101 try:
      - L102 assign client = self._get_llm_client()
      - L103 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='template_recommendations', temperature=0.3)
      - L109 import json
      - L110 import re
      - L111 assign content = response['choices'][0]['message']['content']
      - L112 assign json_match = re.search('\\[[\\s\\S]*\\]', content)
      - L113 if json_match:
        - L114 assign recommendations = json.loads(json_match.group())
        - L116 for rec in recommendations:
          - L117 assign tid = rec.get('template_id')
          - L118 if tid in templates:
            - L119 assign rec['template'] = templates[tid]
        - L120 return recommendations[:limit]
      - L122 except Exception as exc:
        - L123 expr logger.error(f'Recommendation generation failed: {exc}')
    - L126 assign sorted_templates = sorted(approved, key=lambda t: t.get('created_at', ''), reverse=True)
    - L127 return [{'template_id': t['id'], 'template': t, 'score': 0.5, 'reason': 'Recently created'} for t in sorted_templates[:limit]]
  - L129 def get_similar_templatesself, template_id: str, limit: int=3:
    - L130 docstring: "Get templates similar to a given template."
    - L131 assign store = _state_store()
    - L132 assign templates = store._read_state().get('templates', {})
    - L133 assign target = templates.get(template_id)
    - L135 if not target:
      - L136 return []
    - L139 assign target_tags = set(target.get('tags', []))
    - L140 assign similar = []
    - L142 for (tid, t) in templates.items():
      - L143 if tid == template_id or t.get('status') != 'approved':
        - L144 continue
      - L145 assign t_tags = set(t.get('tags', []))
      - L146 assign overlap = len(target_tags & t_tags)
      - L147 if overlap > 0:
        - L148 expr similar.append({'template': t, 'score': overlap / max(len(target_tags), 1)})
    - L150 expr similar.sort(key=lambda x: x['score'], reverse=True)
    - L151 return similar[:limit]

## backend\app\domain\reports\__init__.py
- L1 docstring: "Report domain strategies and helpers."

## backend\app\domain\reports\strategies.py
- L1 from __future__ import annotations
- L3 import logging
- L4 from dataclasses import dataclass
- L5 import importlib
- L6 from pathlib import Path
- L7 from typing import Optional
- L9 from backend.app.core.strategies import StrategyRegistry
- L10 from backend.app.services.reports.docx_export import html_file_to_docx, pdf_file_to_docx
- L11 from backend.app.services.reports.xlsx_export import html_file_to_xlsx
- L12 from backend.app.services.utils.mailer import send_report_email
- L14 assign logger = logging.getLogger('neura.reports.strategies')
- L18 class RenderArtifacts:
  - L19 annotated assign docx_path: Optional[Path]
  - L20 annotated assign xlsx_path: Optional[Path]
- L23 class RenderStrategy:
  - L24 def render_docxself, html_path: Path, pdf_path: Optional[Path], dest_tmp: Path, *, landscape: bool, font_scale: Optional[float]:
    - L25 if pdf_path and pdf_path.exists():
      - L26 try:
        - L27 assign pdf_result = pdf_file_to_docx(pdf_path, dest_tmp)
        - L28 except Exception:
          - L29 expr logger.exception('docx_pdf_convert_failed')
        - L31 else:
          - L31 if pdf_result:
            - L32 return pdf_result
    - L33 try:
      - L34 assign api_mod = importlib.import_module('backend.api')
      - L35 assign html_to_docx = getattr(api_mod, 'html_file_to_docx', html_file_to_docx)
      - L36 except Exception:
        - L37 assign html_to_docx = html_file_to_docx
    - L38 return html_to_docx(html_path, dest_tmp, landscape=landscape, body_font_scale=font_scale)
  - L40 def render_xlsxself, html_path: Path, dest_tmp: Path:
    - L41 try:
      - L42 assign api_mod = importlib.import_module('backend.api')
      - L43 assign html_to_xlsx = getattr(api_mod, 'html_file_to_xlsx', html_file_to_xlsx)
      - L44 except Exception:
        - L45 assign html_to_xlsx = html_file_to_xlsx
    - L46 return html_to_xlsx(html_path, dest_tmp)
- L49 class NotificationStrategy:
  - L50 def sendself, *, recipients: list[str], subject: str, body: str, attachments: list[Path]:
    - L51 return send_report_email(to_addresses=recipients, subject=subject, body=body, attachments=attachments)
- L59 def build_render_strategy_registry:
  - L60 annotated assign registry: StrategyRegistry[RenderStrategy] = StrategyRegistry(default_factory=RenderStrategy)
  - L61 expr registry.register('pdf', RenderStrategy())
  - L62 expr registry.register('excel', RenderStrategy())
  - L63 return registry
- L66 def build_notification_strategy_registry:
  - L67 annotated assign registry: StrategyRegistry[NotificationStrategy] = StrategyRegistry(default_factory=NotificationStrategy)
  - L68 expr registry.register('email', NotificationStrategy())
  - L69 return registry

## backend\app\domain\summary\__init__.py
- L1 docstring: "Executive Summary domain module."
- L2 from .service import SummaryService
- L4 assign __all__ = ['SummaryService']

## backend\app\domain\summary\service.py
- L1 docstring: "Service for executive summary generation using AI."
- L2 from __future__ import annotations
- L4 import logging
- L5 from typing import Any, Dict, List, Optional
- L7 from backend.app.services.llm.client import get_llm_client
- L8 from backend.app.services.state import store as state_store_module
- L10 assign logger = logging.getLogger('neura.domain.summary')
- L13 def _state_store:
  - L14 return state_store_module.state_store
- L17 class SummaryService:
  - L18 docstring: "Service for executive summary generation."
  - L20 def __init__self:
    - L21 assign self._llm_client = None
  - L23 def _get_llm_clientself:
    - L24 if self._llm_client is None:
      - L25 assign self._llm_client = get_llm_client()
    - L26 return self._llm_client
  - L28 def generate_summaryself, content: str, tone: str='formal', max_sentences: int=5, focus_areas: Optional[List[str]]=None, correlation_id: Optional[str]=None:
    - L36 docstring: "\n        Generate an executive summary from content.\n\n        Args:\n        ..."
    - L49 expr logger.info('Generating executive summary', extra={'correlation_id': correlation_id})
    - L51 assign prompt = f"""Generate an executive summary of the following content.\n\nCONTENT:\n{content[:8000]}  # Limit content length\n\nREQUIREMENTS:\n- Tone: {tone}\n- Maximum sentences: {max_sentences}\n- Style: Professional, data-driven\n{(f"- Focus on: {', '.join(focus_areas)}" if focus_areas else '')}\n\nReturn a JSON object:\n{{\n  "executive_summary": "2-3 sentence overview",\n  "key_findings": ["finding 1", "finding 2", "finding 3"],\n  "metrics": [\n    {{"name": "metric_name", "value": "123", "unit": "USD", "trend": "up"}}\n  ],\n  "recommendations": ["action 1", "action 2"],\n  "confidence": 0.9\n}}\n\nReturn ONLY the JSON object."""
    - L75 try:
      - L76 assign client = self._get_llm_client()
      - L77 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='executive_summary', temperature=0.3)
      - L83 import json
      - L84 import re
      - L85 assign content_response = response['choices'][0]['message']['content']
      - L86 assign json_match = re.search('\\{[\\s\\S]*\\}', content_response)
      - L87 if json_match:
        - L88 return json.loads(json_match.group())
      - L90 except Exception as exc:
        - L91 expr logger.error(f'Summary generation failed: {exc}')
    - L93 return {'executive_summary': 'Summary generation failed', 'key_findings': [], 'metrics': [], 'recommendations': [], 'confidence': 0.0, 'error': str(exc) if 'exc' in dir() else 'Unknown error'}
  - L102 def generate_report_summaryself, report_id: str, correlation_id: Optional[str]=None:
    - L107 docstring: "Generate summary for a specific report."
    - L109 assign store = _state_store()
    - L110 assign runs = store._read_state().get('runs', {})
    - L111 assign report = runs.get(report_id)
    - L113 if not report:
      - L114 return {'error': 'Report not found'}
    - L117 assign content = str(report)
    - L118 return self.generate_summary(content, correlation_id=correlation_id)

## backend\app\domain\synthesis\__init__.py
- L1 docstring: "Multi-Document Synthesis domain module."
- L2 from .schemas import SynthesisSession, SynthesisDocument, SynthesisRequest, SynthesisResult, Inconsistency
- L9 from .service import DocumentSynthesisService
- L11 assign __all__ = ['SynthesisSession', 'SynthesisDocument', 'SynthesisRequest', 'SynthesisResult', 'Inconsistency', 'DocumentSynthesisService']

## backend\app\domain\synthesis\schemas.py
- L1 docstring: "Schemas for Multi-Document Synthesis."
- L2 from __future__ import annotations
- L4 from datetime import datetime
- L5 from typing import Any, Dict, List, Optional
- L6 from pydantic import BaseModel, Field
- L7 from enum import Enum
- L10 class DocumentType(str, Enum):
  - L11 assign PDF = 'pdf'
  - L12 assign EXCEL = 'excel'
  - L13 assign WORD = 'word'
  - L14 assign TEXT = 'text'
  - L15 assign JSON = 'json'
- L18 class SynthesisDocument(BaseModel):
  - L19 docstring: "A document added to a synthesis session."
  - L21 annotated assign id: str
  - L22 annotated assign name: str
  - L23 annotated assign doc_type: DocumentType
  - L24 annotated assign content_hash: str
  - L25 annotated assign extracted_text: Optional[str] = None
  - L26 annotated assign metadata: Dict[str, Any] = Field(default_factory=dict)
  - L27 annotated assign added_at: datetime = Field(default_factory=datetime.utcnow)
- L30 class Inconsistency(BaseModel):
  - L31 docstring: "An inconsistency found between documents."
  - L33 annotated assign id: str
  - L34 annotated assign description: str
  - L35 annotated assign severity: str = Field(default='medium', pattern='^(low|medium|high|critical)$')
  - L36 annotated assign documents_involved: List[str]
  - L37 annotated assign field_or_topic: str
  - L38 annotated assign values: Dict[str, Any]
  - L39 annotated assign suggested_resolution: Optional[str] = None
- L42 class SynthesisSession(BaseModel):
  - L43 docstring: "A synthesis session containing multiple documents."
  - L45 annotated assign id: str
  - L46 annotated assign name: str
  - L47 annotated assign documents: List[SynthesisDocument] = Field(default_factory=list)
  - L48 annotated assign inconsistencies: List[Inconsistency] = Field(default_factory=list)
  - L49 annotated assign synthesis_result: Optional[Dict[str, Any]] = None
  - L50 annotated assign created_at: datetime = Field(default_factory=datetime.utcnow)
  - L51 annotated assign updated_at: datetime = Field(default_factory=datetime.utcnow)
  - L52 annotated assign status: str = Field(default='active', pattern='^(active|processing|completed|error)$')
- L55 class SynthesisRequest(BaseModel):
  - L56 docstring: "Request to synthesize documents in a session."
  - L58 annotated assign focus_topics: Optional[List[str]] = Field(None, max_items=10)
  - L59 annotated assign output_format: str = Field(default='structured', pattern='^(structured|narrative|comparison)$')
  - L60 annotated assign include_sources: bool = Field(default=True)
  - L61 annotated assign max_length: int = Field(default=5000, ge=500, le=20000)
- L64 class SynthesisResult(BaseModel):
  - L65 docstring: "Result of document synthesis."
  - L67 annotated assign session_id: str
  - L68 annotated assign synthesis: Dict[str, Any]
  - L69 annotated assign inconsistencies: List[Inconsistency]
  - L70 annotated assign source_references: List[Dict[str, Any]]
  - L71 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L72 annotated assign generated_at: datetime = Field(default_factory=datetime.utcnow)

## backend\app\domain\synthesis\service.py
- L1 docstring: "Service for Multi-Document Synthesis using AI."
- L2 from __future__ import annotations
- L4 import hashlib
- L5 import json
- L6 import logging
- L7 import re
- L8 import uuid
- L9 from datetime import datetime
- L10 from typing import Any, Dict, List, Optional
- L12 from backend.app.services.llm.client import get_llm_client
- L13 from backend.app.services.state import store as state_store_module
- L15 from .schemas import DocumentType, Inconsistency, SynthesisDocument, SynthesisRequest, SynthesisResult, SynthesisSession
- L24 assign logger = logging.getLogger('neura.domain.synthesis')
- L27 def _state_store:
  - L28 return state_store_module.state_store
- L31 class DocumentSynthesisService:
  - L32 docstring: "Service for synthesizing information from multiple documents."
  - L34 def __init__self:
    - L35 assign self._llm_client = None
  - L37 def _get_llm_clientself:
    - L38 if self._llm_client is None:
      - L39 assign self._llm_client = get_llm_client()
    - L40 return self._llm_client
  - L42 def create_sessionself, name: str, correlation_id: Optional[str]=None:
    - L47 docstring: "Create a new synthesis session."
    - L48 expr logger.info('Creating synthesis session', extra={'correlation_id': correlation_id})
    - L50 assign session = SynthesisSession(id=str(uuid.uuid4()), name=name, created_at=datetime.utcnow(), updated_at=datetime.utcnow())
    - L57 assign store = _state_store()
    - L58 assign state = store._read_state()
    - L59 assign sessions = state.get('synthesis_sessions', {})
    - L60 assign sessions[session.id] = session.model_dump(mode='json')
    - L61 expr store._write_state({**state, 'synthesis_sessions': sessions})
    - L63 return session
  - L65 def get_sessionself, session_id: str:
    - L66 docstring: "Get a synthesis session by ID."
    - L67 assign store = _state_store()
    - L68 assign state = store._read_state()
    - L69 assign sessions = state.get('synthesis_sessions', {})
    - L70 assign session_data = sessions.get(session_id)
    - L72 if session_data:
      - L73 return SynthesisSession(**session_data)
    - L74 return None
  - L76 def list_sessionsself:
    - L77 docstring: "List all synthesis sessions."
    - L78 assign store = _state_store()
    - L79 assign state = store._read_state()
    - L80 assign sessions = state.get('synthesis_sessions', {})
    - L81 return [SynthesisSession(**data) for data in sessions.values()]
  - L83 def add_documentself, session_id: str, name: str, content: str, doc_type: DocumentType, metadata: Optional[Dict[str, Any]]=None, correlation_id: Optional[str]=None:
    - L92 docstring: "Add a document to a synthesis session."
    - L93 expr logger.info(f'Adding document to session {session_id}', extra={'correlation_id': correlation_id})
    - L98 assign session = self.get_session(session_id)
    - L99 if not session:
      - L100 return None
    - L102 assign content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    - L104 assign document = SynthesisDocument(id=str(uuid.uuid4()), name=name, doc_type=doc_type, content_hash=content_hash, extracted_text=content[:50000], metadata=metadata or {}, added_at=datetime.utcnow())
    - L114 expr session.documents.append(document)
    - L115 assign session.updated_at = datetime.utcnow()
    - L117 assign store = _state_store()
    - L118 assign state = store._read_state()
    - L119 assign sessions = state.get('synthesis_sessions', {})
    - L120 assign sessions[session_id] = session.model_dump(mode='json')
    - L121 expr store._write_state({**state, 'synthesis_sessions': sessions})
    - L123 return document
  - L125 def remove_documentself, session_id: str, document_id: str:
    - L126 docstring: "Remove a document from a session."
    - L127 assign session = self.get_session(session_id)
    - L128 if not session:
      - L129 return False
    - L131 assign session.documents = [d for d in session.documents if d.id != document_id]
    - L132 assign session.updated_at = datetime.utcnow()
    - L134 assign store = _state_store()
    - L135 assign state = store._read_state()
    - L136 assign sessions = state.get('synthesis_sessions', {})
    - L137 assign sessions[session_id] = session.model_dump(mode='json')
    - L138 expr store._write_state({**state, 'synthesis_sessions': sessions})
    - L140 return True
  - L142 def find_inconsistenciesself, session_id: str, correlation_id: Optional[str]=None:
    - L147 docstring: "Find inconsistencies between documents in a session."
    - L148 expr logger.info(f'Finding inconsistencies in session {session_id}', extra={'correlation_id': correlation_id})
    - L153 assign session = self.get_session(session_id)
    - L154 if not session or len(session.documents) < 2:
      - L155 return []
    - L158 assign doc_summaries = []
    - L159 for doc in session.documents:
      - L160 expr doc_summaries.append({'id': doc.id, 'name': doc.name, 'content': doc.extracted_text[:5000] if doc.extracted_text else ''})
    - L166 assign prompt = f"""Analyze these documents for inconsistencies, contradictions, or conflicting information.\n\nDOCUMENTS:\n{json.dumps(doc_summaries, indent=2)}\n\nFind any:\n1. Numerical discrepancies (different values for the same metric)\n2. Date/time conflicts\n3. Contradictory statements\n4. Conflicting facts or claims\n5. Missing information in one doc that's present in another\n\nReturn a JSON array of inconsistencies:\n[\n  {{\n    "description": "Brief description of the inconsistency",\n    "severity": "low|medium|high|critical",\n    "documents_involved": ["doc_id_1", "doc_id_2"],\n    "field_or_topic": "The field or topic with inconsistency",\n    "values": {{"doc_id_1": "value1", "doc_id_2": "value2"}},\n    "suggested_resolution": "How to resolve this"\n  }}\n]\n\nReturn ONLY the JSON array. Return [] if no inconsistencies found."""
    - L192 try:
      - L193 assign client = self._get_llm_client()
      - L194 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='find_inconsistencies', temperature=0.2)
      - L200 assign content = response['choices'][0]['message']['content']
      - L201 assign json_match = re.search('\\[[\\s\\S]*\\]', content)
      - L203 if json_match:
        - L204 assign inconsistencies_data = json.loads(json_match.group())
        - L205 assign inconsistencies = []
        - L207 for (i, item) in enumerate(inconsistencies_data):
          - L208 expr inconsistencies.append(Inconsistency(id=str(uuid.uuid4()), description=item.get('description', ''), severity=item.get('severity', 'medium'), documents_involved=item.get('documents_involved', []), field_or_topic=item.get('field_or_topic', ''), values=item.get('values', {}), suggested_resolution=item.get('suggested_resolution')))
        - L219 assign session.inconsistencies = inconsistencies
        - L220 assign session.updated_at = datetime.utcnow()
        - L222 assign store = _state_store()
        - L223 assign state = store._read_state()
        - L224 assign sessions = state.get('synthesis_sessions', {})
        - L225 assign sessions[session_id] = session.model_dump(mode='json')
        - L226 expr store._write_state({**state, 'synthesis_sessions': sessions})
        - L228 return inconsistencies
      - L230 except Exception as exc:
        - L231 expr logger.error(f'Inconsistency detection failed: {exc}')
    - L233 return []
  - L235 def synthesizeself, session_id: str, request: SynthesisRequest, correlation_id: Optional[str]=None:
    - L241 docstring: "Synthesize information from all documents in a session."
    - L242 expr logger.info(f'Synthesizing documents in session {session_id}', extra={'correlation_id': correlation_id})
    - L247 assign session = self.get_session(session_id)
    - L248 if not session or not session.documents:
      - L249 return None
    - L251 assign session.status = 'processing'
    - L254 assign doc_contents = []
    - L255 for doc in session.documents:
      - L256 expr doc_contents.append({'id': doc.id, 'name': doc.name, 'type': doc.doc_type.value, 'content': doc.extracted_text[:8000] if doc.extracted_text else ''})
    - L263 assign focus_str = ''
    - L264 if request.focus_topics:
      - L265 assign focus_str = f"\nFOCUS TOPICS: {', '.join(request.focus_topics)}"
    - L267 assign format_instructions = {'structured': 'Return a structured JSON with sections, key_points, and summary', 'narrative': 'Return a cohesive narrative summary combining all information', 'comparison': 'Return a comparison table/matrix of key points across documents'}
    - L273 assign prompt = f"""Synthesize information from these documents into a comprehensive summary.\n\nDOCUMENTS:\n{json.dumps(doc_contents, indent=2)}\n{focus_str}\n\nOUTPUT FORMAT: {request.output_format}\n{format_instructions.get(request.output_format, '')}\n\nMAX LENGTH: {request.max_length} characters\n\nRequirements:\n1. Combine information intelligently, avoiding redundancy\n2. Highlight key insights and findings\n3. Note any patterns or trends across documents\n4. {('Include source references for each point' if request.include_sources else 'Do not include source references')}\n\nReturn a JSON object:\n{{\n  "title": "Synthesis title",\n  "executive_summary": "2-3 sentence overview",\n  "sections": [\n    {{\n      "heading": "Section heading",\n      "content": "Section content",\n      "sources": ["doc_id_1", "doc_id_2"]\n    }}\n  ],\n  "key_insights": ["insight 1", "insight 2"],\n  "cross_references": [\n    {{"topic": "topic", "documents": ["doc1", "doc2"], "finding": "what was found"}}\n  ],\n  "confidence": 0.9\n}}\n\nReturn ONLY the JSON object."""
    - L310 try:
      - L311 assign client = self._get_llm_client()
      - L312 assign response = client.complete(messages=[{'role': 'user', 'content': prompt}], description='document_synthesis', temperature=0.3)
      - L318 assign content = response['choices'][0]['message']['content']
      - L319 assign json_match = re.search('\\{[\\s\\S]*\\}', content)
      - L321 if json_match:
        - L322 assign synthesis_data = json.loads(json_match.group())
        - L325 assign source_refs = []
        - L326 for doc in session.documents:
          - L327 expr source_refs.append({'document_id': doc.id, 'document_name': doc.name, 'document_type': doc.doc_type.value})
        - L333 assign result = SynthesisResult(session_id=session_id, synthesis=synthesis_data, inconsistencies=session.inconsistencies, source_references=source_refs, confidence=synthesis_data.get('confidence', 0.8), generated_at=datetime.utcnow())
        - L343 assign session.synthesis_result = synthesis_data
        - L344 assign session.status = 'completed'
        - L345 assign session.updated_at = datetime.utcnow()
        - L347 assign store = _state_store()
        - L348 assign state = store._read_state()
        - L349 assign sessions = state.get('synthesis_sessions', {})
        - L350 assign sessions[session_id] = session.model_dump(mode='json')
        - L351 expr store._write_state({**state, 'synthesis_sessions': sessions})
        - L353 return result
      - L355 except Exception as exc:
        - L356 expr logger.error(f'Synthesis failed: {exc}')
        - L357 assign session.status = 'error'
    - L359 return None
  - L361 def delete_sessionself, session_id: str:
    - L362 docstring: "Delete a synthesis session."
    - L363 assign store = _state_store()
    - L364 assign state = store._read_state()
    - L365 assign sessions = state.get('synthesis_sessions', {})
    - L367 if session_id in sessions:
      - L368 delete sessions[session_id]
      - L369 expr store._write_state({**state, 'synthesis_sessions': sessions})
      - L370 return True
    - L372 return False

## backend\app\domain\templates\__init__.py
- L1 docstring: "Template import/management domain objects."

## backend\app\domain\templates\errors.py
- L1 from __future__ import annotations
- L3 from typing import Optional
- L5 from backend.app.domain.errors import DomainError
- L8 class TemplateImportError(DomainError):
  - L9 def __init__self, *, code: str, message: str, status_code: int=400, detail: Optional[str]=None:
    - L10 expr super().__init__(code=code, message=message, status_code=status_code, detail=detail)
- L13 class TemplateZipInvalidError(TemplateImportError):
  - L14 def __init__self, detail: Optional[str]=None:
    - L15 expr super().__init__(code='invalid_zip', message='Invalid zip file', detail=detail, status_code=400)
- L18 class TemplateLockedError(TemplateImportError):
  - L19 def __init__self:
    - L20 expr super().__init__(code='template_locked', message='Template is busy', status_code=409)
- L23 class TemplateTooLargeError(TemplateImportError):
  - L24 def __init__self, max_bytes: int:
    - L25 expr super().__init__(code='upload_too_large', message=f'Upload exceeds limit of {max_bytes} bytes', status_code=413)
- L32 class TemplateExtractionError(TemplateImportError):
  - L33 def __init__self, detail: Optional[str]=None:
    - L34 expr super().__init__(code='import_failed', message='Failed to extract zip', detail=detail, status_code=400)

## backend\app\domain\templates\schemas.py
- L1 from __future__ import annotations
- L3 from typing import Optional
- L5 from pydantic import BaseModel
- L8 class TemplateImportResult(BaseModel):
  - L9 annotated assign template_id: str
  - L10 annotated assign name: str
  - L11 annotated assign kind: str
  - L12 annotated assign artifacts: dict
  - L13 annotated assign correlation_id: Optional[str] = None

## backend\app\domain\templates\service.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import contextlib
- L5 import logging
- L6 import tempfile
- L7 import zipfile
- L8 from dataclasses import dataclass, field, replace
- L9 from pathlib import Path
- L10 from typing import Optional
- L12 from fastapi import UploadFile
- L14 from backend.app.core.event_bus import Event, EventBus, logging_middleware, metrics_middleware
- L15 from backend.app.core.pipeline import PipelineRunner, PipelineStep
- L16 from backend.app.core.result import Result, err, ok
- L17 from backend.app.core.strategies import StrategyRegistry
- L18 from backend.app.domain.templates.errors import TemplateExtractionError, TemplateImportError, TemplateLockedError, TemplateTooLargeError, TemplateZipInvalidError
- L25 from backend.app.domain.templates.strategies import TemplateKindStrategy, build_template_kind_registry
- L26 from backend.app.core.validation import sanitize_filename
- L27 from backend.app.services.state import state_store
- L28 from backend.app.services.utils import TemplateLockError, acquire_template_lock
- L29 from backend.app.services.utils.artifacts import load_manifest
- L30 from backend.app.services.utils.zip_tools import detect_zip_root, extract_zip_to_dir
- L34 class TemplateImportContext:
  - L35 annotated assign upload: UploadFile
  - L36 annotated assign display_name: Optional[str]
  - L37 annotated assign correlation_id: Optional[str]
  - L38 annotated assign tmp_path: Optional[Path] = None
  - L39 annotated assign root: Optional[str] = None
  - L40 annotated assign contains_excel: bool = False
  - L41 annotated assign kind: Optional[str] = None
  - L42 annotated assign template_id: Optional[str] = None
  - L43 annotated assign template_dir: Optional[Path] = None
  - L44 annotated assign name: Optional[str] = None
  - L45 annotated assign artifacts: dict = field(default_factory=dict)
  - L46 annotated assign manifest: dict = field(default_factory=dict)
- L49 def _create_temp_path*, suffix: str:
  - L50 assign handle = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
  - L51 assign tmp_path = Path(handle.name)
  - L52 expr handle.close()
  - L53 return tmp_path
- L56 class TemplateService:
  - L57 def __init__self, uploads_root: Path, excel_uploads_root: Path, max_bytes: int, *, max_zip_entries: int | None=None, max_zip_uncompressed_bytes: int | None=None, max_concurrency: int=4, event_bus: Optional[EventBus]=None, kind_registry: Optional[StrategyRegistry[TemplateKindStrategy] | dict[str, TemplateKindStrategy]]=None:
    - L69 assign self.uploads_root = uploads_root
    - L70 assign self.excel_uploads_root = excel_uploads_root
    - L71 assign self.max_bytes = max_bytes
    - L72 assign self.max_zip_entries = max_zip_entries
    - L73 assign self.max_zip_uncompressed_bytes = max_zip_uncompressed_bytes
    - L74 assign self._semaphore = asyncio.Semaphore(max(1, int(max_concurrency or 1)))
    - L75 assign self.logger = logging.getLogger('neura.templates')
    - L76 assign self.event_bus = event_bus or EventBus(middlewares=[logging_middleware(logging.getLogger('neura.events')), metrics_middleware(logging.getLogger('neura.events'))])
    - L79 if isinstance(kind_registry, StrategyRegistry):
      - L80 assign self.kind_registry = kind_registry
      - L82 else:
        - L82 assign self.kind_registry = build_template_kind_registry(self.uploads_root, self.excel_uploads_root)
        - L83 if kind_registry:
          - L84 for (key, strategy) in kind_registry.items():
            - L85 expr self.kind_registry.register(key, strategy)
  - L87 def _normalize_display_nameself, display_name: Optional[str], root_name: Optional[str], upload_name: Optional[str]:
    - L93 assign raw = (display_name or '').strip() or (root_name or '').strip() or (upload_name or '').strip() or 'template'
    - L94 assign base = Path(raw).name
    - L95 assign base = Path(base).stem or base
    - L96 assign safe = sanitize_filename(base)
    - L97 if len(safe) > 100:
      - L98 assign safe = safe[:100].rstrip()
    - L99 return safe or 'template'
  - L101 async def _write_uploadself, upload, dest: Path:
    - L102 assign size = 0
    - L103 expr dest.parent.mkdir(parents=True, exist_ok=True)
    - L104 try:
      - L105 with dest.open('wb') as fh:
        - L106 while True:
          - L107 assign chunk = await upload.read(1024 * 1024)
          - L108 if not chunk:
            - L109 break
          - L110 aug assign size Add len(chunk)
          - L111 if size > self.max_bytes:
            - L112 raise TemplateTooLargeError(self.max_bytes)
          - L113 expr fh.write(chunk)
      - L114 except Exception:
        - L115 with contextlib.suppress(Exception):
          - L116 expr dest.unlink(missing_ok=True)
        - L117 raise
      - L119 finally:
        - L119 with contextlib.suppress(Exception):
          - L120 expr await upload.seek(0)
    - L121 return size
  - L123 async def import_zipself, upload: UploadFile, display_name: Optional[str], correlation_id: Optional[str]:
    - L129 assign ctx = TemplateImportContext(upload=upload, display_name=display_name, correlation_id=correlation_id)
    - L130 annotated assign tmp_paths: list[Path] = []
    - L132 async def _writectx: TemplateImportContext:
      - L133 assign tmp_path = _create_temp_path(suffix='.zip')
      - L134 try:
        - L135 expr await self._write_upload(ctx.upload, tmp_path)
        - L136 except TemplateImportError as exc:
          - L137 return err(exc)
        - L138 except Exception as exc:
          - L139 return err(TemplateImportError(code='upload_failed', message='Upload failed', detail=str(exc)))
      - L140 expr tmp_paths.append(tmp_path)
      - L141 return ok(replace(ctx, tmp_path=tmp_path))
    - L143 def _inspectctx: TemplateImportContext:
      - L144 if not ctx.tmp_path:
        - L145 return err(TemplateImportError(code='upload_missing', message='Temporary upload path missing'))
      - L146 try:
        - L147 with zipfile.ZipFile(ctx.tmp_path, 'r') as zf:
          - L148 assign members = list(zf.infolist())
          - L149 assign file_members = [member for member in members if not member.is_dir()]
          - L150 if self.max_zip_entries is not None and len(file_members) > self.max_zip_entries:
            - L151 return err(TemplateImportError(code='zip_too_many_files', message='Zip contains too many files', detail=f'max_entries={self.max_zip_entries}'))
          - L158 if self.max_zip_uncompressed_bytes is not None:
            - L159 assign total_uncompressed = sum((member.file_size for member in file_members))
            - L160 if total_uncompressed > self.max_zip_uncompressed_bytes:
              - L161 return err(TemplateImportError(code='zip_too_large', message='Zip expands beyond allowed size', detail=f'max_uncompressed_bytes={self.max_zip_uncompressed_bytes}'))
          - L168 assign root = detect_zip_root((m.filename for m in members))
          - L169 assign contains_excel = any((Path(m.filename).name.lower() == 'source.xlsx' for m in members))
        - L170 except Exception as exc:
          - L171 return err(TemplateZipInvalidError(detail=str(exc)))
      - L172 assign kind = 'excel' if contains_excel else 'pdf'
      - L173 assign name = self._normalize_display_name(display_name, root, upload.filename)
      - L174 return ok(replace(ctx, root=root, contains_excel=contains_excel, kind=kind, name=name))
    - L184 def _allocatectx: TemplateImportContext:
      - L185 if not ctx.kind:
        - L186 return err(TemplateImportError(code='kind_missing', message='Unable to infer template kind'))
      - L187 assign strategy = self.kind_registry.resolve(ctx.kind)
      - L188 assign template_id = strategy.generate_id(ctx.name)
      - L189 assign tdir = strategy.ensure_target_dir(template_id)
      - L190 return ok(replace(ctx, template_id=template_id, template_dir=tdir))
    - L192 def _extractctx: TemplateImportContext:
      - L193 if not ctx.template_dir or not ctx.template_id or (not ctx.tmp_path):
        - L194 return err(TemplateImportError(code='missing_context', message='Template import context incomplete'))
      - L195 try:
        - L196 assign lock_ctx = acquire_template_lock(ctx.template_dir, 'import_zip', ctx.correlation_id)
        - L197 except TemplateLockError:
          - L198 return err(TemplateLockedError())
      - L200 with lock_ctx:
        - L201 try:
          - L202 expr extract_zip_to_dir(ctx.tmp_path, ctx.template_dir, strip_root=True, max_entries=self.max_zip_entries, max_uncompressed_bytes=self.max_zip_uncompressed_bytes)
          - L209 except Exception as exc:
            - L210 with contextlib.suppress(Exception):
              - L211 for path in ctx.template_dir.rglob('*'):
                - L212 if path.is_file():
                  - L213 expr path.unlink()
            - L214 return err(TemplateExtractionError(detail=str(exc)))
        - L216 assign manifest = load_manifest(ctx.template_dir) or {}
        - L217 assign artifacts = manifest.get('artifacts') or {}
        - L218 assign template_name = ctx.name or f'Template {ctx.template_id[:6]}'
        - L219 assign status = 'approved' if (ctx.template_dir / 'contract.json').exists() else 'draft'
        - L220 expr state_store.upsert_template(ctx.template_id, name=template_name, status=status, artifacts=artifacts, connection_id=None, mapping_keys=[], template_type=ctx.kind)
      - L230 return ok(replace(ctx, artifacts=artifacts, manifest=manifest, name=template_name))
    - L239 async def _emit_completectx: TemplateImportContext:
      - L242 expr await self.event_bus.publish(Event(name='template.imported', payload={'template_id': ctx.template_id, 'kind': ctx.kind, 'artifacts': list((ctx.artifacts or {}).keys())}, correlation_id=ctx.correlation_id))
      - L253 return ok(ctx)
    - L255 assign steps = [PipelineStep('write_upload', _write), PipelineStep('inspect_zip', _inspect), PipelineStep('allocate_id', _allocate), PipelineStep('extract_and_persist', _extract), PipelineStep('emit_complete', _emit_complete)]
    - L263 async with self._semaphore:
      - L264 assign runner = PipelineRunner(steps, bus=self.event_bus, logger=self.logger, correlation_id=correlation_id)
      - L270 try:
        - L271 assign result = await runner.run(ctx)
        - L273 finally:
          - L273 with contextlib.suppress(Exception):
            - L274 for path in tmp_paths:
              - L275 expr path.unlink(missing_ok=True)
    - L277 if result.is_err:
      - L278 assign error = result.unwrap_err()
      - L279 if isinstance(error, TemplateImportError):
        - L280 raise error
      - L281 raise TemplateImportError(code='import_failed', message='Template import failed', detail=str(error))
    - L283 assign final_ctx = result.unwrap()
    - L284 return {'template_id': final_ctx.template_id, 'name': final_ctx.name, 'kind': final_ctx.kind, 'artifacts': final_ctx.artifacts, 'correlation_id': correlation_id}
  - L292 async def export_zipself, template_id: str, correlation_id: Optional[str]:
    - L297 docstring: "Export a template directory as a zip file."
    - L298 from fastapi import HTTPException
    - L301 assign template_record = state_store.get_template_record(template_id)
    - L302 if not template_record:
      - L303 raise HTTPException(status_code=404, detail=f"Template '{template_id}' not found")
    - L305 assign template_kind = template_record.get('kind') or template_record.get('template_type') or 'pdf'
    - L306 assign template_name = template_record.get('name') or template_id
    - L309 if template_kind == 'excel':
      - L310 assign template_dir = self.excel_uploads_root / template_id
      - L312 else:
        - L312 assign template_dir = self.uploads_root / template_id
    - L314 if not template_dir.exists():
      - L315 raise HTTPException(status_code=404, detail=f"Template directory not found for '{template_id}'")
    - L318 assign safe_name = ''.join((c if c.isalnum() or c in '-_' else '_' for c in template_name))
    - L319 assign zip_filename = f'{safe_name}-{template_id[:8]}.zip'
    - L320 assign tmp_zip_path = _create_temp_path(suffix='.zip')
    - L322 try:
      - L323 with zipfile.ZipFile(tmp_zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        - L324 for file_path in template_dir.rglob('*'):
          - L325 if file_path.is_file():
            - L327 if file_path.name.startswith('.') or file_path.suffix == '.lock':
              - L328 continue
            - L329 assign arcname = file_path.relative_to(template_dir)
            - L330 expr zf.write(file_path, arcname)
      - L331 except Exception as exc:
        - L332 with contextlib.suppress(Exception):
          - L333 expr tmp_zip_path.unlink(missing_ok=True)
        - L334 raise HTTPException(status_code=500, detail=f'Failed to create export zip: {exc}')
    - L336 expr self.logger.info('template_exported', extra={'event': 'template_exported', 'template_id': template_id, 'kind': template_kind, 'zip_path': str(tmp_zip_path), 'correlation_id': correlation_id})
    - L347 return {'zip_path': str(tmp_zip_path), 'filename': zip_filename, 'template_id': template_id, 'kind': template_kind}
  - L354 async def duplicateself, template_id: str, new_name: Optional[str], correlation_id: Optional[str]:
    - L360 docstring: "Duplicate a template by copying its directory to a new ID."
    - L361 import shutil
    - L362 from fastapi import HTTPException
    - L365 assign template_record = state_store.get_template_record(template_id)
    - L366 if not template_record:
      - L367 raise HTTPException(status_code=404, detail=f"Template '{template_id}' not found")
    - L369 assign template_kind = template_record.get('kind') or template_record.get('template_type') or 'pdf'
    - L370 assign original_name = template_record.get('name') or template_id
    - L373 if template_kind == 'excel':
      - L374 assign source_dir = self.excel_uploads_root / template_id
      - L376 else:
        - L376 assign source_dir = self.uploads_root / template_id
    - L378 if not source_dir.exists():
      - L379 raise HTTPException(status_code=404, detail=f"Template directory not found for '{template_id}'")
    - L382 assign strategy = self.kind_registry.resolve(template_kind)
    - L383 assign display_name = new_name or f'{original_name} (Copy)'
    - L384 assign new_template_id = strategy.generate_id(display_name)
    - L385 assign target_dir = strategy.ensure_target_dir(new_template_id)
    - L387 try:
      - L389 for file_path in source_dir.rglob('*'):
        - L390 if file_path.is_file():
          - L392 if file_path.name.startswith('.') or file_path.suffix == '.lock':
            - L393 continue
          - L394 assign rel_path = file_path.relative_to(source_dir)
          - L395 assign dest_path = target_dir / rel_path
          - L396 expr dest_path.parent.mkdir(parents=True, exist_ok=True)
          - L397 expr shutil.copy2(file_path, dest_path)
      - L400 assign manifest = load_manifest(target_dir) or {}
      - L401 assign artifacts = manifest.get('artifacts') or {}
      - L402 assign status = 'approved' if (target_dir / 'contract.json').exists() else 'draft'
      - L405 expr state_store.upsert_template(new_template_id, name=display_name, status=status, artifacts=artifacts, connection_id=None, mapping_keys=[], template_type=template_kind)
      - L415 expr self.logger.info('template_duplicated', extra={'event': 'template_duplicated', 'source_id': template_id, 'new_id': new_template_id, 'kind': template_kind, 'correlation_id': correlation_id})
      - L426 return {'template_id': new_template_id, 'name': display_name, 'kind': template_kind, 'status': status, 'artifacts': artifacts, 'source_id': template_id}
      - L434 except Exception as exc:
        - L436 with contextlib.suppress(Exception):
          - L437 if target_dir.exists():
            - L438 expr shutil.rmtree(target_dir)
        - L439 raise HTTPException(status_code=500, detail=f'Failed to duplicate template: {exc}')
  - L441 async def update_tagsself, template_id: str, tags: list[str]:
    - L446 docstring: "Update tags for a template."
    - L447 from fastapi import HTTPException
    - L449 assign template_record = state_store.get_template_record(template_id)
    - L450 if not template_record:
      - L451 raise HTTPException(status_code=404, detail=f"Template '{template_id}' not found")
    - L454 assign cleaned_tags = sorted(set((tag.strip().lower() for tag in tags if tag.strip())))
    - L457 expr state_store.upsert_template(template_id, name=template_record.get('name') or template_id, status=template_record.get('status') or 'draft', artifacts=template_record.get('artifacts'), tags=cleaned_tags, connection_id=template_record.get('last_connection_id'), mapping_keys=template_record.get('mapping_keys'), template_type=template_record.get('kind'), description=template_record.get('description'))
    - L469 return {'template_id': template_id, 'tags': cleaned_tags}
  - L474 async def get_all_tagsself:
    - L475 docstring: "Get all unique tags across all templates."
    - L476 assign templates = state_store.list_templates()
    - L477 assign all_tags = set()
    - L478 assign tag_counts = {}
    - L480 for template in templates:
      - L481 assign tags = template.get('tags') or []
      - L482 for tag in tags:
        - L483 expr all_tags.add(tag)
        - L484 assign tag_counts[tag] = tag_counts.get(tag, 0) + 1
    - L487 assign sorted_tags = sorted(all_tags, key=lambda t: (-tag_counts.get(t, 0), t))
    - L489 return {'tags': sorted_tags, 'tagCounts': tag_counts, 'total': len(sorted_tags)}

## backend\app\domain\templates\strategies.py
- L1 from __future__ import annotations
- L3 import re
- L4 import uuid
- L5 from dataclasses import dataclass
- L6 from pathlib import Path
- L8 from backend.app.core.strategies import StrategyRegistry
- L11 def _slugifyvalue: str:
  - L12 assign value = value.lower()
  - L13 assign value = re.sub('[^a-z0-9]+', '-', value).strip('-')
  - L14 return value or 'template'
- L18 class TemplateKindStrategy:
  - L19 annotated assign kind: str
  - L20 annotated assign base_dir: Path
  - L22 def generate_idself, hint: str | None:
    - L23 assign name = _slugify(hint or 'template')
    - L24 return f'{name}-{uuid.uuid4().hex[:6]}-{self.kind}'
  - L26 def target_dirself, template_id: str:
    - L27 return (self.base_dir / template_id).resolve()
  - L29 def ensure_target_dirself, template_id: str:
    - L30 assign tdir = self.target_dir(template_id)
    - L31 expr tdir.mkdir(parents=True, exist_ok=True)
    - L32 return tdir
- L35 def build_template_kind_registrypdf_root: Path, excel_root: Path:
  - L36 annotated assign registry: StrategyRegistry[TemplateKindStrategy] = StrategyRegistry(default_factory=lambda: TemplateKindStrategy(kind='pdf', base_dir=pdf_root))
  - L39 expr registry.register('pdf', TemplateKindStrategy(kind='pdf', base_dir=pdf_root))
  - L40 expr registry.register('excel', TemplateKindStrategy(kind='excel', base_dir=excel_root))
  - L41 return registry

## backend\app\env_loader.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import os
- L5 from pathlib import Path
- L6 from typing import Iterator
- L8 assign logger = logging.getLogger('neura.env')
- L11 def _iter_candidate_paths:
  - L12 docstring: "\n    Yield possible .env files from highest to lowest priority.\n\n    Preceden..."
  - L20 assign env_override = os.getenv('NEURA_ENV_FILE')
  - L21 if env_override:
    - L22 expr (yield Path(env_override).expanduser())
  - L24 assign backend_dir = Path(__file__).resolve().parents[1]
  - L25 assign repo_root = backend_dir.parent
  - L26 expr (yield (repo_root / '.env'))
  - L27 expr (yield (backend_dir / '.env'))
- L30 def _strip_quotesvalue: str:
  - L31 if not value:
    - L32 return value
  - L33 if value.startswith('"') and value.endswith('"') or (value.startswith("'") and value.endswith("'")):
    - L34 return value[1:-1]
  - L35 return value
- L38 def load_env_file:
  - L39 docstring: "\n    Load KEY=VALUE pairs from the first existing candidate .env file.\n    Exi..."
  - L43 for candidate in _iter_candidate_paths():
    - L44 try:
      - L45 assign resolved = candidate if candidate.is_absolute() else Path.cwd() / candidate
      - L46 if not resolved.exists():
        - L47 continue
      - L48 expr _apply_env_file(resolved)
      - L49 expr logger.info('loaded_env_file', extra={'event': 'loaded_env_file', 'path': str(resolved)})
      - L50 return resolved
      - L51 except Exception:
        - L52 expr logger.exception('env_file_load_failed', extra={'event': 'env_file_load_failed', 'path': str(candidate)})
  - L56 return None
- L59 def _apply_env_filepath: Path:
  - L60 for raw_line in path.read_text(encoding='utf-8').splitlines():
    - L61 assign line = raw_line.strip()
    - L62 if not line or line.startswith('#'):
      - L63 continue
    - L64 if line.startswith('export '):
      - L65 assign line = line[7:].lstrip()
    - L66 if '=' not in line:
      - L67 continue
    - L68 assign (key, value) = line.split('=', 1)
    - L69 assign key = key.strip()
    - L70 assign value = _strip_quotes(value.strip())
    - L71 if not key or key.startswith('#'):
      - L72 continue
    - L73 expr os.environ.setdefault(key, value)

## backend\app\features\analyze\__init__.py
- L2 docstring: "\nDocument Analysis Feature.\n\nAI-powered extraction and analysis of PDF/Excel ..."
- L7 from __future__ import annotations
- L9 from .services import DocumentAnalysisService, analyze_document_streaming, get_analysis, get_analysis_data, suggest_charts_for_analysis, ExtractedContent, extract_document_content
- L18 from .routes import router
- L20 assign __all__ = ['DocumentAnalysisService', 'analyze_document_streaming', 'get_analysis', 'get_analysis_data', 'suggest_charts_for_analysis', 'ExtractedContent', 'extract_document_content', 'router']

## backend\app\features\analyze\routes\__init__.py
- L2 from __future__ import annotations
- L4 from .analysis_routes import router
- L5 from . import enhanced_analysis_routes
- L7 assign __all__ = ['router', 'enhanced_analysis_routes']

## backend\app\features\analyze\routes\analysis_routes.py
- L2 docstring: "API routes for document analysis."
- L3 from __future__ import annotations
- L5 import contextlib
- L6 import json
- L7 import logging
- L8 import tempfile
- L9 from pathlib import Path
- L10 from typing import Optional
- L12 from fastapi import APIRouter, File, Form, HTTPException, Query, Request, UploadFile
- L13 from fastapi.responses import StreamingResponse
- L15 from backend.app.core.config import get_settings
- L16 from backend.app.core.validation import validate_file_extension
- L17 from backend.app.features.analyze.schemas.analysis import AnalysisSuggestChartsPayload
- L18 from backend.app.features.analyze.services.document_analysis_service import analyze_document_streaming, get_analysis, get_analysis_data, suggest_charts_for_analysis
- L24 from backend.app.features.analyze.services.extraction_pipeline import extract_document_content
- L25 from backend.app.services.background_tasks import enqueue_background_job, run_event_stream_async
- L27 assign logger = logging.getLogger('neura.analyze.routes')
- L29 assign router = APIRouter()
- L31 assign ALLOWED_EXTENSIONS = ['.pdf', '.xlsx', '.xls', '.xlsm']
- L32 assign ALLOWED_CONTENT_TYPES = {'application/pdf', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'application/vnd.ms-excel', 'application/vnd.ms-excel.sheet.macroEnabled.12', 'application/octet-stream'}
- L39 assign MAX_FILENAME_LENGTH = 255
- L40 assign READ_CHUNK_BYTES = 1024 * 1024
- L41 assign MAX_ANALYSIS_DATA_LIMIT = 2000
- L44 def _validate_uploadfile: UploadFile:
  - L45 assign filename = Path(file.filename or '').name
  - L46 if not filename:
    - L47 raise HTTPException(status_code=400, detail='Filename is required')
  - L48 if len(filename) > MAX_FILENAME_LENGTH:
    - L49 raise HTTPException(status_code=400, detail=f'Filename too long (max {MAX_FILENAME_LENGTH} characters)')
  - L50 assign (is_valid, error) = validate_file_extension(filename, ALLOWED_EXTENSIONS)
  - L51 if not is_valid:
    - L52 raise HTTPException(status_code=400, detail=error)
  - L53 if file.content_type and file.content_type not in ALLOWED_CONTENT_TYPES:
    - L54 raise HTTPException(status_code=415, detail=f"Unsupported content type '{file.content_type}'")
  - L58 return filename
- L61 async def _read_upload_with_limitupload: UploadFile, max_bytes: int:
  - L62 assign size = 0
  - L63 annotated assign chunks: list[bytes] = []
  - L64 while True:
    - L65 assign chunk = await upload.read(READ_CHUNK_BYTES)
    - L66 if not chunk:
      - L67 break
    - L68 aug assign size Add len(chunk)
    - L69 if size > max_bytes:
      - L70 raise HTTPException(status_code=413, detail=f'File too large. Maximum size is {max_bytes} bytes.')
    - L74 expr chunks.append(chunk)
  - L75 return b''.join(chunks)
- L78 async def _persist_upload_with_limitupload: UploadFile, max_bytes: int, suffix: str:
  - L79 assign size = 0
  - L80 assign tmp = tempfile.NamedTemporaryFile(prefix='nr-analysis-', suffix=suffix, delete=False)
  - L81 try:
    - L82 with tmp:
      - L83 while True:
        - L84 assign chunk = await upload.read(READ_CHUNK_BYTES)
        - L85 if not chunk:
          - L86 break
        - L87 aug assign size Add len(chunk)
        - L88 if size > max_bytes:
          - L89 raise HTTPException(status_code=413, detail=f'File too large. Maximum size is {max_bytes} bytes.')
        - L93 expr tmp.write(chunk)
    - L95 finally:
      - L95 with contextlib.suppress(Exception):
        - L96 expr await upload.close()
  - L97 return (Path(tmp.name), size)
- L100 async def _streaming_generatorfile_bytes: bytes, file_name: str, template_id: Optional[str], connection_id: Optional[str], correlation_id: Optional[str]:
  - L107 docstring: "Generate NDJSON streaming response."
  - L108 try:
    - L109 async for event in analyze_document_streaming(file_bytes=file_bytes, file_name=file_name, template_id=template_id, connection_id=connection_id, correlation_id=correlation_id):
      - L116 expr (yield (json.dumps(event) + '\n'))
    - L117 except Exception as exc:
      - L118 expr logger.exception('analysis_stream_failed', extra={'event': 'analysis_stream_failed', 'error': str(exc), 'correlation_id': correlation_id})
      - L122 assign error_event = {'event': 'error', 'detail': 'Analysis failed. Please try again.'}
      - L126 if correlation_id:
        - L127 assign error_event['correlation_id'] = correlation_id
      - L128 expr (yield (json.dumps(error_event) + '\n'))
- L132 async def upload_and_analyzerequest: Request, file: UploadFile=File(...), template_id: Optional[str]=Form(None), connection_id: Optional[str]=Form(None), background: bool=Query(False):
  - L139 docstring: "\n    Upload a document (PDF or Excel) and analyze it with AI.\n\n    Returns a ..."
  - L150 assign settings = get_settings()
  - L151 assign file_name = _validate_upload(file)
  - L152 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L154 if background:
    - L155 assign suffix = Path(file_name).suffix or '.bin'
    - L156 assign (upload_path, size_bytes) = await _persist_upload_with_limit(file, settings.max_upload_bytes, suffix=suffix)
    - L158 async def runnerjob_id: str:
      - L159 try:
        - L160 assign file_bytes = upload_path.read_bytes()
        - L162 async def _events:
          - L163 async for event in analyze_document_streaming(file_bytes=file_bytes, file_name=file_name, template_id=template_id, connection_id=connection_id, correlation_id=correlation_id):
            - L170 expr (yield event)
        - L172 def _result_builderevent: dict:
          - L173 if event.get('event') != 'result':
            - L174 return {}
          - L175 assign tables = event.get('tables') or []
          - L176 assign charts = event.get('chart_suggestions') or []
          - L177 return {'analysis_id': event.get('analysis_id'), 'document_name': event.get('document_name'), 'summary': event.get('summary'), 'table_count': len(tables), 'chart_count': len(charts), 'warnings': event.get('warnings') or []}
        - L186 expr await run_event_stream_async(job_id, _events(), result_builder=_result_builder)
        - L188 finally:
          - L188 with contextlib.suppress(FileNotFoundError):
            - L189 expr upload_path.unlink(missing_ok=True)
    - L191 assign job = await enqueue_background_job(job_type='analyze_document', template_id=template_id, connection_id=connection_id, template_name=file_name, meta={'filename': file_name, 'size_bytes': size_bytes, 'background': True}, runner=runner)
    - L204 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
  - L206 try:
    - L207 assign file_bytes = await _read_upload_with_limit(file, settings.max_upload_bytes)
    - L209 finally:
      - L209 expr await file.close()
  - L211 return StreamingResponse(_streaming_generator(file_bytes, file_name, template_id, connection_id, correlation_id), media_type='application/x-ndjson', headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})
- L228 async def extract_documentrequest: Request, file: UploadFile=File(...):
  - L232 docstring: "Quickly extract raw tables and text without full AI analysis."
  - L233 assign settings = get_settings()
  - L234 assign file_name = _validate_upload(file)
  - L235 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L237 try:
    - L238 assign file_bytes = await _read_upload_with_limit(file, settings.max_upload_bytes)
    - L240 finally:
      - L240 expr await file.close()
  - L242 assign extracted = extract_document_content(file_bytes=file_bytes, file_name=file_name)
  - L244 return {'status': 'ok', 'file_name': extracted.file_name, 'document_type': extracted.document_type, 'page_count': extracted.page_count, 'tables': extracted.tables_raw, 'sheets': extracted.sheets, 'text': extracted.text_content, 'errors': extracted.errors, 'correlation_id': correlation_id}
- L258 async def get_analysis_resultanalysis_id: str, request: Request:
  - L259 docstring: "Get a previously computed analysis result."
  - L260 assign result = get_analysis(analysis_id)
  - L261 if not result:
    - L262 raise HTTPException(status_code=404, detail='Analysis not found')
  - L264 return {'status': 'ok', **result.dict(), 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L272 async def get_analysis_raw_dataanalysis_id: str, request: Request, limit: int=Query(500, ge=1, le=MAX_ANALYSIS_DATA_LIMIT), offset: int=Query(0, ge=0):
  - L278 docstring: "Get raw data from an analysis for charting."
  - L279 assign data = get_analysis_data(analysis_id)
  - L280 if data is None:
    - L281 raise HTTPException(status_code=404, detail='Analysis not found')
  - L283 assign paginated = data[offset:offset + limit]
  - L285 return {'status': 'ok', 'analysis_id': analysis_id, 'data': paginated, 'total': len(data), 'offset': offset, 'limit': limit, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L297 async def suggest_chartsanalysis_id: str, payload: AnalysisSuggestChartsPayload, request: Request:
  - L302 docstring: "Generate additional chart suggestions for an existing analysis."
  - L303 assign result = get_analysis(analysis_id)
  - L304 if not result:
    - L305 raise HTTPException(status_code=404, detail='Analysis not found')
  - L307 assign charts = suggest_charts_for_analysis(analysis_id, payload)
  - L309 assign sample_data = result.raw_data[:100] if payload.include_sample_data else None
  - L311 return {'status': 'ok', 'analysis_id': analysis_id, 'charts': [c.dict() for c in charts], 'sample_data': sample_data, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L320 assign __all__ = ['router']

## backend\app\features\analyze\routes\enhanced_analysis_routes.py
- L2 docstring: "\nEnhanced Analysis API Routes - Comprehensive endpoints for AI-powered document..."
- L13 from __future__ import annotations
- L15 import json
- L16 import logging
- L17 from typing import Any, Dict, List, Optional
- L19 from fastapi import APIRouter, File, Form, HTTPException, Query, UploadFile, BackgroundTasks
- L20 from fastapi.responses import StreamingResponse, Response
- L21 from pydantic import BaseModel
- L23 from backend.app.features.analyze.schemas.enhanced_analysis import AnalysisDepth, AnalysisPreferences, ChartType, ExportFormat, SummaryMode
- L30 from backend.app.features.analyze.services.enhanced_analysis_orchestrator import get_orchestrator
- L34 assign logger = logging.getLogger('neura.analyze.routes')
- L36 assign router = APIRouter(prefix='/analyze/v2', tags=['Enhanced Analysis'])
- L43 class AnalyzePreferencesRequest(BaseModel):
  - L44 docstring: "Request model for analysis preferences."
  - L45 annotated assign analysis_depth: str = 'standard'
  - L46 annotated assign focus_areas: List[str] = []
  - L47 annotated assign output_format: str = 'executive'
  - L48 annotated assign language: str = 'en'
  - L49 annotated assign industry: Optional[str] = None
  - L50 annotated assign enable_predictions: bool = True
  - L51 annotated assign enable_recommendations: bool = True
  - L52 annotated assign auto_chart_generation: bool = True
  - L53 annotated assign max_charts: int = 10
  - L54 annotated assign summary_mode: str = 'executive'
- L57 class QuestionRequest(BaseModel):
  - L58 docstring: "Request model for asking questions."
  - L59 annotated assign question: str
  - L60 annotated assign include_sources: bool = True
  - L61 annotated assign max_context_chunks: int = 5
- L64 class ChartRequest(BaseModel):
  - L65 docstring: "Request model for generating charts."
  - L66 annotated assign query: str
  - L67 annotated assign chart_type: Optional[str] = None
  - L68 annotated assign include_trends: bool = True
  - L69 annotated assign include_forecasts: bool = False
- L72 class ExportRequest(BaseModel):
  - L73 docstring: "Request model for exporting analysis."
  - L74 annotated assign format: str = 'json'
  - L75 annotated assign include_raw_data: bool = True
  - L76 annotated assign include_charts: bool = True
- L79 class CompareRequest(BaseModel):
  - L80 docstring: "Request model for comparing documents."
  - L81 annotated assign analysis_id_1: str
  - L82 annotated assign analysis_id_2: str
- L85 class CommentRequest(BaseModel):
  - L86 docstring: "Request model for adding comments."
  - L87 annotated assign content: str
  - L88 annotated assign element_type: Optional[str] = None
  - L89 annotated assign element_id: Optional[str] = None
  - L90 annotated assign user_id: str = 'anonymous'
  - L91 annotated assign user_name: str = 'Anonymous'
- L94 class ShareRequest(BaseModel):
  - L95 docstring: "Request model for creating share links."
  - L96 annotated assign access_level: str = 'view'
  - L97 annotated assign expires_hours: Optional[int] = None
  - L98 annotated assign password_protected: bool = False
  - L99 annotated assign allowed_emails: List[str] = []
- L107 async def analyze_documentfile: UploadFile=File(...), preferences: Optional[str]=Form(None), background: bool=Query(False, description='Run in background'), background_tasks: BackgroundTasks=None:
  - L113 docstring: "\n    Upload and analyze a document with AI-powered analysis.\n\n    Returns str..."
  - L129 if not file.filename:
    - L130 raise HTTPException(status_code=400, detail='No file provided')
  - L133 assign prefs = None
  - L134 if preferences:
    - L135 try:
      - L136 assign prefs_dict = json.loads(preferences)
      - L137 assign prefs = AnalysisPreferences(analysis_depth=AnalysisDepth[prefs_dict.get('analysis_depth', 'standard').upper()], focus_areas=prefs_dict.get('focus_areas', []), output_format=prefs_dict.get('output_format', 'executive'), language=prefs_dict.get('language', 'en'), industry=prefs_dict.get('industry'), enable_predictions=prefs_dict.get('enable_predictions', True), enable_recommendations=prefs_dict.get('enable_recommendations', True), auto_chart_generation=prefs_dict.get('auto_chart_generation', True), max_charts=prefs_dict.get('max_charts', 10), summary_mode=SummaryMode[prefs_dict.get('summary_mode', 'executive').upper()])
      - L149 except Exception as e:
        - L150 expr logger.warning(f'Failed to parse preferences: {e}')
  - L153 assign file_bytes = await file.read()
  - L154 assign file_name = file.filename
  - L156 assign orchestrator = get_orchestrator()
  - L158 async def generate_events:
    - L159 async for event in orchestrator.analyze_document_streaming(file_bytes=file_bytes, file_name=file_name, preferences=prefs):
      - L164 expr (yield (json.dumps(event) + '\n'))
  - L166 return StreamingResponse(generate_events(), media_type='application/x-ndjson', headers={'X-Content-Type-Options': 'nosniff'})
- L174 async def get_analysisanalysis_id: str:
  - L175 docstring: "Get a previously computed analysis result."
  - L176 assign orchestrator = get_orchestrator()
  - L177 assign result = orchestrator.get_analysis(analysis_id)
  - L179 if not result:
    - L180 raise HTTPException(status_code=404, detail='Analysis not found')
  - L182 return result.dict()
- L186 async def get_summaryanalysis_id: str, mode: str:
  - L190 docstring: "Get a specific summary mode for an analysis."
  - L191 assign orchestrator = get_orchestrator()
  - L192 assign result = orchestrator.get_analysis(analysis_id)
  - L194 if not result:
    - L195 raise HTTPException(status_code=404, detail='Analysis not found')
  - L197 assign summary = result.summaries.get(mode)
  - L198 if not summary:
    - L199 raise HTTPException(status_code=404, detail=f"Summary mode '{mode}' not found")
  - L201 return summary.dict()
- L209 async def ask_questionanalysis_id: str, request: QuestionRequest:
  - L213 docstring: "\n    Ask a natural language question about the analyzed document.\n\n    Uses R..."
  - L219 assign orchestrator = get_orchestrator()
  - L221 assign response = await orchestrator.ask_question(analysis_id=analysis_id, question=request.question, include_sources=request.include_sources, max_context_chunks=request.max_context_chunks)
  - L228 return response.dict()
- L232 async def get_suggested_questionsanalysis_id: str:
  - L233 docstring: "Get AI-generated suggested questions for an analysis."
  - L234 assign orchestrator = get_orchestrator()
  - L235 assign result = orchestrator.get_analysis(analysis_id)
  - L237 if not result:
    - L238 raise HTTPException(status_code=404, detail='Analysis not found')
  - L240 assign questions = orchestrator.ux_service.generate_suggested_questions(tables=result.tables, metrics=result.metrics, entities=result.entities)
  - L246 return {'questions': questions}
- L254 async def generate_chartsanalysis_id: str, request: ChartRequest:
  - L258 docstring: "\n    Generate charts from natural language query.\n\n    Examples:\n    - "Show..."
  - L266 assign orchestrator = get_orchestrator()
  - L268 assign charts = await orchestrator.generate_charts_from_query(analysis_id=analysis_id, query=request.query, include_trends=request.include_trends, include_forecasts=request.include_forecasts)
  - L275 return {'charts': charts}
- L279 async def get_chartsanalysis_id: str:
  - L280 docstring: "Get all charts for an analysis."
  - L281 assign orchestrator = get_orchestrator()
  - L282 assign result = orchestrator.get_analysis(analysis_id)
  - L284 if not result:
    - L285 raise HTTPException(status_code=404, detail='Analysis not found')
  - L287 return {'charts': [c.dict() for c in result.chart_suggestions], 'suggestions': [s.dict() for s in result.visualization_suggestions]}
- L298 async def get_tablesanalysis_id: str, limit: int=Query(10, ge=1, le=50):
  - L302 docstring: "Get extracted tables from an analysis."
  - L303 assign orchestrator = get_orchestrator()
  - L304 assign result = orchestrator.get_analysis(analysis_id)
  - L306 if not result:
    - L307 raise HTTPException(status_code=404, detail='Analysis not found')
  - L309 return {'tables': [t.dict() for t in result.tables[:limit]], 'total': len(result.tables)}
- L316 async def get_metricsanalysis_id: str:
  - L317 docstring: "Get extracted metrics from an analysis."
  - L318 assign orchestrator = get_orchestrator()
  - L319 assign result = orchestrator.get_analysis(analysis_id)
  - L321 if not result:
    - L322 raise HTTPException(status_code=404, detail='Analysis not found')
  - L324 return {'metrics': [m.dict() for m in result.metrics], 'total': len(result.metrics)}
- L331 async def get_entitiesanalysis_id: str:
  - L332 docstring: "Get extracted entities from an analysis."
  - L333 assign orchestrator = get_orchestrator()
  - L334 assign result = orchestrator.get_analysis(analysis_id)
  - L336 if not result:
    - L337 raise HTTPException(status_code=404, detail='Analysis not found')
  - L339 return {'entities': [e.dict() for e in result.entities], 'total': len(result.entities)}
- L346 async def get_insightsanalysis_id: str:
  - L347 docstring: "Get AI-generated insights, risks, and opportunities."
  - L348 assign orchestrator = get_orchestrator()
  - L349 assign result = orchestrator.get_analysis(analysis_id)
  - L351 if not result:
    - L352 raise HTTPException(status_code=404, detail='Analysis not found')
  - L354 return {'insights': [i.dict() for i in result.insights], 'risks': [r.dict() for r in result.risks], 'opportunities': [o.dict() for o in result.opportunities], 'action_items': [a.dict() for a in result.action_items]}
- L363 async def get_data_qualityanalysis_id: str:
  - L364 docstring: "Get data quality assessment for an analysis."
  - L365 assign orchestrator = get_orchestrator()
  - L366 assign result = orchestrator.get_analysis(analysis_id)
  - L368 if not result:
    - L369 raise HTTPException(status_code=404, detail='Analysis not found')
  - L371 if not result.data_quality:
    - L372 raise HTTPException(status_code=404, detail='Data quality assessment not available')
  - L374 return result.data_quality.dict()
- L382 async def export_analysisanalysis_id: str, request: ExportRequest:
  - L386 docstring: "\n    Export analysis in various formats.\n\n    Supported formats:\n    - json:..."
  - L397 assign orchestrator = get_orchestrator()
  - L399 try:
    - L400 assign format_enum = ExportFormat[request.format.upper()]
    - L401 except KeyError:
      - L402 raise HTTPException(status_code=400, detail=f'Invalid format: {request.format}')
  - L404 try:
    - L405 assign (content, filename) = await orchestrator.export_analysis(analysis_id=analysis_id, format=format_enum, include_raw_data=request.include_raw_data, include_charts=request.include_charts)
    - L411 except ValueError as e:
      - L412 raise HTTPException(status_code=404, detail=str(e))
  - L415 assign content_types = {ExportFormat.JSON: 'application/json', ExportFormat.CSV: 'text/csv', ExportFormat.EXCEL: 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', ExportFormat.PDF: 'application/pdf', ExportFormat.MARKDOWN: 'text/markdown', ExportFormat.HTML: 'text/html'}
  - L424 return Response(content=content, media_type=content_types.get(format_enum, 'application/octet-stream'), headers={'Content-Disposition': f'attachment; filename={filename}'})
- L436 async def compare_documentsrequest: CompareRequest:
  - L437 docstring: "Compare two analyzed documents."
  - L438 assign orchestrator = get_orchestrator()
  - L440 assign result = await orchestrator.compare_documents(analysis_id_1=request.analysis_id_1, analysis_id_2=request.analysis_id_2)
  - L445 if 'error' in result:
    - L446 raise HTTPException(status_code=404, detail=result['error'])
  - L448 return result
- L456 async def add_commentanalysis_id: str, request: CommentRequest:
  - L460 docstring: "Add a comment to an analysis."
  - L461 assign orchestrator = get_orchestrator()
  - L464 assign result = orchestrator.get_analysis(analysis_id)
  - L465 if not result:
    - L466 raise HTTPException(status_code=404, detail='Analysis not found')
  - L468 assign comment = orchestrator.ux_service.add_comment(analysis_id=analysis_id, user_id=request.user_id, user_name=request.user_name, content=request.content, element_type=request.element_type, element_id=request.element_id)
  - L477 return {'id': comment.id, 'content': comment.content, 'created_at': comment.created_at.isoformat()}
- L485 async def get_commentsanalysis_id: str:
  - L486 docstring: "Get all comments for an analysis."
  - L487 assign orchestrator = get_orchestrator()
  - L489 assign comments = orchestrator.ux_service.get_comments(analysis_id)
  - L491 return {'comments': [{'id': c.id, 'content': c.content, 'user_name': c.user_name, 'element_type': c.element_type, 'element_id': c.element_id, 'created_at': c.created_at.isoformat(), 'replies': [{'id': r.id, 'content': r.content, 'user_name': r.user_name} for r in c.replies]} for c in comments]}
- L511 async def create_share_linkanalysis_id: str, request: ShareRequest:
  - L515 docstring: "Create a shareable link for an analysis."
  - L516 assign orchestrator = get_orchestrator()
  - L519 assign result = orchestrator.get_analysis(analysis_id)
  - L520 if not result:
    - L521 raise HTTPException(status_code=404, detail='Analysis not found')
  - L523 assign share = orchestrator.ux_service.create_share_link(analysis_id=analysis_id, created_by='api', access_level=request.access_level, expires_hours=request.expires_hours, password_protected=request.password_protected, allowed_emails=request.allowed_emails)
  - L532 return {'share_id': share.id, 'share_url': f'/analyze/v2/shared/{share.id}', 'access_level': share.access_level, 'expires_at': share.expires_at.isoformat() if share.expires_at else None}
- L545 async def get_industry_options:
  - L546 docstring: "Get available industry options for analysis configuration."
  - L547 assign orchestrator = get_orchestrator()
  - L548 return {'industries': orchestrator.get_industry_options()}
- L552 async def get_export_formats:
  - L553 docstring: "Get available export formats."
  - L554 assign orchestrator = get_orchestrator()
  - L555 return {'formats': orchestrator.get_export_formats()}
- L559 async def get_chart_types:
  - L560 docstring: "Get available chart types."
  - L561 return {'chart_types': [{'value': ct.value, 'label': ct.value.replace('_', ' ').title()} for ct in ChartType]}
- L570 async def get_summary_modes:
  - L571 docstring: "Get available summary modes."
  - L572 return {'modes': [{'value': sm.value, 'label': sm.value.replace('_', ' ').title(), 'description': {'executive': 'C-suite 3-bullet overview', 'data': 'Key figures and trends', 'quick': 'One sentence essence', 'comprehensive': 'Full structured analysis', 'action_items': 'To-dos and next steps', 'risks': 'Potential issues and concerns', 'opportunities': 'Growth areas identified'}.get(sm.value, '')} for sm in SummaryMode]}

## backend\app\features\analyze\schemas\__init__.py
- L2 from __future__ import annotations
- L4 from .analysis import AnalysisPayload, AnalysisResult, ExtractedDataPoint, ExtractedTable, FieldInfo, TimeSeriesCandidate
- L12 from .enhanced_analysis import DocumentType, EntityType, MetricType, SummaryMode, ChartType, ExportFormat, AnalysisDepth, ExtractedEntity, ExtractedMetric, FormField, ExtractedForm, InvoiceLineItem, ExtractedInvoice, ContractClause, ExtractedContract, EnhancedExtractedTable, TableRelationship, DocumentSummary, SentimentAnalysis, TextAnalytics, FinancialAnalysis, StatisticalAnalysis, EnhancedChartSpec, VisualizationSuggestion, Insight, RiskItem, OpportunityItem, ActionItem, DataQualityIssue, DataQualityReport, AnalysisPreferences, EnhancedAnalysisResult, QAResponse
- L48 assign __all__ = ['AnalysisPayload', 'AnalysisResult', 'ExtractedDataPoint', 'ExtractedTable', 'FieldInfo', 'TimeSeriesCandidate', 'DocumentType', 'EntityType', 'MetricType', 'SummaryMode', 'ChartType', 'ExportFormat', 'AnalysisDepth', 'ExtractedEntity', 'ExtractedMetric', 'FormField', 'ExtractedForm', 'InvoiceLineItem', 'ExtractedInvoice', 'ContractClause', 'ExtractedContract', 'EnhancedExtractedTable', 'TableRelationship', 'DocumentSummary', 'SentimentAnalysis', 'TextAnalytics', 'FinancialAnalysis', 'StatisticalAnalysis', 'EnhancedChartSpec', 'VisualizationSuggestion', 'Insight', 'RiskItem', 'OpportunityItem', 'ActionItem', 'DataQualityIssue', 'DataQualityReport', 'AnalysisPreferences', 'EnhancedAnalysisResult', 'QAResponse']

## backend\app\features\analyze\schemas\analysis.py
- L2 from __future__ import annotations
- L4 from typing import Any, Optional
- L6 from pydantic import BaseModel, Field
- L8 from backend.app.features.generate.schemas.charts import ChartSpec
- L11 class ExtractedTable(BaseModel):
  - L12 docstring: "A table extracted from the document."
  - L14 annotated assign id: str
  - L15 annotated assign title: Optional[str] = None
  - L16 annotated assign headers: list[str]
  - L17 annotated assign rows: list[list[Any]]
  - L18 annotated assign data_types: Optional[list[str]] = None
  - L19 annotated assign source_page: Optional[int] = None
  - L20 annotated assign source_sheet: Optional[str] = None
- L23 class ExtractedDataPoint(BaseModel):
  - L24 docstring: "A key metric or data point extracted from the document."
  - L26 annotated assign key: str
  - L27 annotated assign value: Any
  - L28 annotated assign data_type: str = 'text'
  - L29 annotated assign unit: Optional[str] = None
  - L30 annotated assign confidence: float = 1.0
  - L31 annotated assign context: Optional[str] = None
- L34 class TimeSeriesCandidate(BaseModel):
  - L35 docstring: "Information about potential time series data in the document."
  - L37 annotated assign date_column: str
  - L38 annotated assign value_columns: list[str]
  - L39 annotated assign frequency: Optional[str] = None
  - L40 annotated assign table_id: Optional[str] = None
- L43 class FieldInfo(BaseModel):
  - L44 docstring: "Metadata about a field in the extracted data."
  - L46 annotated assign name: str
  - L47 annotated assign type: str
  - L48 annotated assign description: Optional[str] = None
  - L49 annotated assign sample_values: Optional[list[Any]] = None
- L52 class AnalysisPayload(BaseModel):
  - L53 docstring: "Request payload for document analysis."
  - L55 annotated assign template_id: Optional[str] = None
  - L56 annotated assign connection_id: Optional[str] = None
  - L57 annotated assign analysis_mode: str = Field(default='standalone', description="'standalone' for ad-hoc analysis, 'template_linked' for template association")
- L63 class AnalysisResult(BaseModel):
  - L64 docstring: "Complete result of document analysis."
  - L66 annotated assign analysis_id: str
  - L67 annotated assign document_name: str
  - L68 annotated assign document_type: str
  - L69 annotated assign processing_time_ms: int
  - L70 annotated assign summary: Optional[str] = None
  - L72 annotated assign tables: list[ExtractedTable] = Field(default_factory=list)
  - L73 annotated assign data_points: list[ExtractedDataPoint] = Field(default_factory=list)
  - L74 annotated assign time_series_candidates: list[TimeSeriesCandidate] = Field(default_factory=list)
  - L75 annotated assign chart_suggestions: list[ChartSpec] = Field(default_factory=list)
  - L77 annotated assign raw_data: list[dict[str, Any]] = Field(default_factory=list)
  - L78 annotated assign field_catalog: list[FieldInfo] = Field(default_factory=list)
  - L80 annotated assign template_id: Optional[str] = None
  - L81 annotated assign warnings: list[str] = Field(default_factory=list)
- L84 class AnalysisSuggestChartsPayload(BaseModel):
  - L85 docstring: "Request payload for chart suggestions on an existing analysis."
  - L87 annotated assign question: Optional[str] = None
  - L88 annotated assign include_sample_data: bool = True
  - L89 annotated assign table_ids: Optional[list[str]] = None
  - L90 annotated assign date_range: Optional[dict[str, str]] = None

## backend\app\features\analyze\schemas\enhanced_analysis.py
- L2 docstring: "\nEnhanced Analysis Schemas - Comprehensive data models for AI-powered document ..."
- L12 from __future__ import annotations
- L14 from dataclasses import dataclass, field
- L15 from datetime import datetime
- L16 from enum import Enum
- L17 from typing import Any, Dict, List, Optional, Union
- L18 from pydantic import BaseModel, Field
- L25 class DocumentType(str, Enum):
  - L26 assign PDF = 'pdf'
  - L27 assign EXCEL = 'excel'
  - L28 assign CSV = 'csv'
  - L29 assign IMAGE = 'image'
  - L30 assign WORD = 'word'
  - L31 assign TEXT = 'text'
  - L32 assign UNKNOWN = 'unknown'
- L35 class EntityType(str, Enum):
  - L36 assign PERSON = 'person'
  - L37 assign ORGANIZATION = 'organization'
  - L38 assign LOCATION = 'location'
  - L39 assign DATE = 'date'
  - L40 assign MONEY = 'money'
  - L41 assign PERCENTAGE = 'percentage'
  - L42 assign PRODUCT = 'product'
  - L43 assign EMAIL = 'email'
  - L44 assign PHONE = 'phone'
  - L45 assign URL = 'url'
  - L46 assign CUSTOM = 'custom'
- L49 class MetricType(str, Enum):
  - L50 assign CURRENCY = 'currency'
  - L51 assign PERCENTAGE = 'percentage'
  - L52 assign COUNT = 'count'
  - L53 assign RATIO = 'ratio'
  - L54 assign DURATION = 'duration'
  - L55 assign QUANTITY = 'quantity'
  - L56 assign SCORE = 'score'
  - L57 assign RATE = 'rate'
- L60 class SummaryMode(str, Enum):
  - L61 assign EXECUTIVE = 'executive'
  - L62 assign DATA = 'data'
  - L63 assign QUICK = 'quick'
  - L64 assign COMPREHENSIVE = 'comprehensive'
  - L65 assign ACTION_ITEMS = 'action_items'
  - L66 assign RISKS = 'risks'
  - L67 assign OPPORTUNITIES = 'opportunities'
- L70 class SentimentLevel(str, Enum):
  - L71 assign VERY_POSITIVE = 'very_positive'
  - L72 assign POSITIVE = 'positive'
  - L73 assign NEUTRAL = 'neutral'
  - L74 assign NEGATIVE = 'negative'
  - L75 assign VERY_NEGATIVE = 'very_negative'
- L78 class ChartType(str, Enum):
  - L79 assign LINE = 'line'
  - L80 assign BAR = 'bar'
  - L81 assign PIE = 'pie'
  - L82 assign SCATTER = 'scatter'
  - L83 assign AREA = 'area'
  - L84 assign HISTOGRAM = 'histogram'
  - L85 assign BOX = 'box'
  - L86 assign HEATMAP = 'heatmap'
  - L87 assign TREEMAP = 'treemap'
  - L88 assign SANKEY = 'sankey'
  - L89 assign FUNNEL = 'funnel'
  - L90 assign RADAR = 'radar'
  - L91 assign CANDLESTICK = 'candlestick'
  - L92 assign BUBBLE = 'bubble'
  - L93 assign SUNBURST = 'sunburst'
  - L94 assign WATERFALL = 'waterfall'
  - L95 assign GAUGE = 'gauge'
- L98 class ExportFormat(str, Enum):
  - L99 assign EXCEL = 'excel'
  - L100 assign PDF = 'pdf'
  - L101 assign CSV = 'csv'
  - L102 assign JSON = 'json'
  - L103 assign HTML = 'html'
  - L104 assign MARKDOWN = 'markdown'
  - L105 assign POWERPOINT = 'powerpoint'
  - L106 assign WORD = 'word'
- L109 class AnalysisDepth(str, Enum):
  - L110 assign QUICK = 'quick'
  - L111 assign STANDARD = 'standard'
  - L112 assign COMPREHENSIVE = 'comprehensive'
  - L113 assign DEEP = 'deep'
- L116 class RiskLevel(str, Enum):
  - L117 assign CRITICAL = 'critical'
  - L118 assign HIGH = 'high'
  - L119 assign MEDIUM = 'medium'
  - L120 assign LOW = 'low'
  - L121 assign MINIMAL = 'minimal'
- L124 class Priority(str, Enum):
  - L125 assign CRITICAL = 'critical'
  - L126 assign HIGH = 'high'
  - L127 assign MEDIUM = 'medium'
  - L128 assign LOW = 'low'
- L135 class ExtractedEntity(BaseModel):
  - L136 docstring: "An extracted named entity from the document."
  - L137 annotated assign id: str
  - L138 annotated assign type: EntityType
  - L139 annotated assign value: str
  - L140 annotated assign normalized_value: Optional[str] = None
  - L141 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
  - L142 annotated assign context: Optional[str] = None
  - L143 annotated assign page: Optional[int] = None
  - L144 annotated assign position: Optional[Dict[str, int]] = None
  - L145 annotated assign metadata: Dict[str, Any] = Field(default_factory=dict)
- L148 class ExtractedMetric(BaseModel):
  - L149 docstring: "A key metric or KPI extracted from the document."
  - L150 annotated assign id: str
  - L151 annotated assign name: str
  - L152 annotated assign value: Union[float, int, str]
  - L153 annotated assign raw_value: str
  - L154 annotated assign metric_type: MetricType
  - L155 annotated assign unit: Optional[str] = None
  - L156 annotated assign currency: Optional[str] = None
  - L157 annotated assign period: Optional[str] = None
  - L158 annotated assign normalized_period: Optional[str] = None
  - L159 annotated assign change: Optional[float] = None
  - L160 annotated assign change_direction: Optional[str] = None
  - L161 annotated assign comparison_base: Optional[str] = None
  - L162 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
  - L163 annotated assign context: Optional[str] = None
  - L164 annotated assign page: Optional[int] = None
  - L165 annotated assign importance_score: float = Field(ge=0.0, le=1.0, default=0.5)
- L168 class FormField(BaseModel):
  - L169 docstring: "An extracted form field."
  - L170 annotated assign id: str
  - L171 annotated assign label: str
  - L172 annotated assign value: Optional[str] = None
  - L173 annotated assign field_type: str = 'text'
  - L174 annotated assign required: bool = False
  - L175 annotated assign section: Optional[str] = None
  - L176 annotated assign validation_pattern: Optional[str] = None
  - L177 annotated assign options: Optional[List[str]] = None
  - L178 annotated assign is_filled: bool = False
  - L179 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L182 class ExtractedForm(BaseModel):
  - L183 docstring: "Structured form data."
  - L184 annotated assign id: str
  - L185 annotated assign title: Optional[str] = None
  - L186 annotated assign form_type: Optional[str] = None
  - L187 annotated assign fields: List[FormField] = Field(default_factory=list)
  - L188 annotated assign sections: List[Dict[str, Any]] = Field(default_factory=list)
  - L189 annotated assign submission_status: str = 'incomplete'
  - L190 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L193 class InvoiceLineItem(BaseModel):
  - L194 docstring: "A line item from an invoice."
  - L195 annotated assign id: str
  - L196 annotated assign description: str
  - L197 annotated assign quantity: Optional[float] = None
  - L198 annotated assign unit_price: Optional[float] = None
  - L199 annotated assign total: Optional[float] = None
  - L200 annotated assign tax: Optional[float] = None
  - L201 annotated assign discount: Optional[float] = None
  - L202 annotated assign sku: Optional[str] = None
  - L203 annotated assign category: Optional[str] = None
- L206 class ExtractedInvoice(BaseModel):
  - L207 docstring: "Structured invoice data."
  - L208 annotated assign id: str
  - L209 annotated assign vendor_name: Optional[str] = None
  - L210 annotated assign vendor_address: Optional[str] = None
  - L211 annotated assign vendor_tax_id: Optional[str] = None
  - L212 annotated assign customer_name: Optional[str] = None
  - L213 annotated assign customer_address: Optional[str] = None
  - L214 annotated assign invoice_number: Optional[str] = None
  - L215 annotated assign invoice_date: Optional[str] = None
  - L216 annotated assign due_date: Optional[str] = None
  - L217 annotated assign purchase_order: Optional[str] = None
  - L218 annotated assign line_items: List[InvoiceLineItem] = Field(default_factory=list)
  - L219 annotated assign subtotal: Optional[float] = None
  - L220 annotated assign tax_total: Optional[float] = None
  - L221 annotated assign discount_total: Optional[float] = None
  - L222 annotated assign grand_total: Optional[float] = None
  - L223 annotated assign currency: str = 'USD'
  - L224 annotated assign payment_terms: Optional[str] = None
  - L225 annotated assign notes: Optional[str] = None
  - L226 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L229 class ContractClause(BaseModel):
  - L230 docstring: "A clause from a contract."
  - L231 annotated assign id: str
  - L232 annotated assign clause_type: str
  - L233 annotated assign title: Optional[str] = None
  - L234 annotated assign content: str
  - L235 annotated assign section: Optional[str] = None
  - L236 annotated assign page: Optional[int] = None
  - L237 annotated assign obligations: List[str] = Field(default_factory=list)
  - L238 annotated assign risks: List[str] = Field(default_factory=list)
  - L239 annotated assign importance: str = 'medium'
  - L240 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L243 class ExtractedContract(BaseModel):
  - L244 docstring: "Structured contract data."
  - L245 annotated assign id: str
  - L246 annotated assign contract_type: Optional[str] = None
  - L247 annotated assign parties: List[Dict[str, str]] = Field(default_factory=list)
  - L248 annotated assign effective_date: Optional[str] = None
  - L249 annotated assign expiration_date: Optional[str] = None
  - L250 annotated assign auto_renewal: bool = False
  - L251 annotated assign renewal_terms: Optional[str] = None
  - L252 annotated assign key_terms: List[str] = Field(default_factory=list)
  - L253 annotated assign clauses: List[ContractClause] = Field(default_factory=list)
  - L254 annotated assign obligations: List[Dict[str, Any]] = Field(default_factory=list)
  - L255 annotated assign termination_clauses: List[str] = Field(default_factory=list)
  - L256 annotated assign governing_law: Optional[str] = None
  - L257 annotated assign signatures: List[Dict[str, Any]] = Field(default_factory=list)
  - L258 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L261 class TableRelationship(BaseModel):
  - L262 docstring: "Relationship between tables (for cross-page stitching)."
  - L263 annotated assign table1_id: str
  - L264 annotated assign table2_id: str
  - L265 annotated assign relationship_type: str
  - L266 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
- L269 class EnhancedExtractedTable(BaseModel):
  - L270 docstring: "Enhanced table with additional metadata."
  - L271 annotated assign id: str
  - L272 annotated assign title: Optional[str] = None
  - L273 annotated assign headers: List[str] = Field(default_factory=list)
  - L274 annotated assign rows: List[List[Any]] = Field(default_factory=list)
  - L275 annotated assign data_types: List[str] = Field(default_factory=list)
  - L276 annotated assign column_descriptions: List[str] = Field(default_factory=list)
  - L277 annotated assign source_page: Optional[int] = None
  - L278 annotated assign source_sheet: Optional[str] = None
  - L279 annotated assign is_nested: bool = False
  - L280 annotated assign parent_table_id: Optional[str] = None
  - L281 annotated assign related_tables: List[str] = Field(default_factory=list)
  - L282 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.9)
  - L283 annotated assign row_count: int = 0
  - L284 annotated assign column_count: int = 0
  - L285 annotated assign has_totals_row: bool = False
  - L286 annotated assign has_header_row: bool = True
  - L287 annotated assign statistics: Dict[str, Any] = Field(default_factory=dict)
- L294 class DocumentSummary(BaseModel):
  - L295 docstring: "Multi-mode document summary."
  - L296 annotated assign mode: SummaryMode
  - L297 annotated assign title: str
  - L298 annotated assign content: str
  - L299 annotated assign bullet_points: List[str] = Field(default_factory=list)
  - L300 annotated assign key_figures: List[Dict[str, Any]] = Field(default_factory=list)
  - L301 annotated assign word_count: int = 0
  - L302 annotated assign reading_time_minutes: float = 0
  - L303 annotated assign generated_at: datetime = Field(default_factory=datetime.utcnow)
- L306 class SentimentAnalysis(BaseModel):
  - L307 docstring: "Document sentiment analysis results."
  - L308 annotated assign overall_sentiment: SentimentLevel
  - L309 annotated assign overall_score: float = Field(ge=-1.0, le=1.0)
  - L310 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L311 annotated assign section_sentiments: List[Dict[str, Any]] = Field(default_factory=list)
  - L312 annotated assign emotional_tone: str = 'neutral'
  - L313 annotated assign urgency_level: str = 'normal'
  - L314 annotated assign bias_indicators: List[str] = Field(default_factory=list)
  - L315 annotated assign key_phrases: Dict[str, List[str]] = Field(default_factory=dict)
- L318 class TextAnalytics(BaseModel):
  - L319 docstring: "Text analytics results."
  - L320 annotated assign word_count: int = 0
  - L321 annotated assign sentence_count: int = 0
  - L322 annotated assign paragraph_count: int = 0
  - L323 annotated assign avg_sentence_length: float = 0
  - L324 annotated assign readability_score: float = 0
  - L325 annotated assign readability_grade: str = ''
  - L326 annotated assign keywords: List[Dict[str, Any]] = Field(default_factory=list)
  - L327 annotated assign topics: List[Dict[str, Any]] = Field(default_factory=list)
  - L328 annotated assign named_entities_summary: Dict[str, int] = Field(default_factory=dict)
  - L329 annotated assign language: str = 'en'
  - L330 annotated assign language_confidence: float = 0.95
- L333 class FinancialAnalysis(BaseModel):
  - L334 docstring: "Financial analysis results."
  - L335 annotated assign metrics_found: int = 0
  - L336 annotated assign currency: str = 'USD'
  - L339 annotated assign gross_margin: Optional[float] = None
  - L340 annotated assign operating_margin: Optional[float] = None
  - L341 annotated assign net_margin: Optional[float] = None
  - L342 annotated assign roe: Optional[float] = None
  - L343 annotated assign roa: Optional[float] = None
  - L346 annotated assign current_ratio: Optional[float] = None
  - L347 annotated assign quick_ratio: Optional[float] = None
  - L348 annotated assign cash_ratio: Optional[float] = None
  - L351 annotated assign inventory_turnover: Optional[float] = None
  - L352 annotated assign receivables_turnover: Optional[float] = None
  - L353 annotated assign asset_turnover: Optional[float] = None
  - L356 annotated assign revenue_growth: Optional[float] = None
  - L357 annotated assign profit_growth: Optional[float] = None
  - L358 annotated assign yoy_comparison: Dict[str, Any] = Field(default_factory=dict)
  - L361 annotated assign variance_analysis: List[Dict[str, Any]] = Field(default_factory=list)
  - L364 annotated assign insights: List[str] = Field(default_factory=list)
  - L365 annotated assign warnings: List[str] = Field(default_factory=list)
- L368 class StatisticalAnalysis(BaseModel):
  - L369 docstring: "Statistical analysis of numeric data."
  - L370 annotated assign column_stats: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
  - L373 annotated assign correlations: List[Dict[str, Any]] = Field(default_factory=list)
  - L376 annotated assign outliers: List[Dict[str, Any]] = Field(default_factory=list)
  - L379 annotated assign distributions: Dict[str, str] = Field(default_factory=dict)
  - L382 annotated assign trends: List[Dict[str, Any]] = Field(default_factory=list)
- L386 class ComparativeAnalysis(BaseModel):
  - L387 docstring: "Comparison between documents or versions."
  - L388 annotated assign comparison_type: str
  - L389 annotated assign documents_compared: List[str] = Field(default_factory=list)
  - L392 annotated assign additions: List[Dict[str, Any]] = Field(default_factory=list)
  - L393 annotated assign deletions: List[Dict[str, Any]] = Field(default_factory=list)
  - L394 annotated assign modifications: List[Dict[str, Any]] = Field(default_factory=list)
  - L397 annotated assign metric_changes: List[Dict[str, Any]] = Field(default_factory=list)
  - L400 annotated assign similarity_score: float = Field(ge=0.0, le=1.0, default=0.0)
  - L401 annotated assign change_summary: str = ''
  - L402 annotated assign significant_changes: List[str] = Field(default_factory=list)
- L409 class ChartDataSeries(BaseModel):
  - L410 docstring: "A data series for charting."
  - L411 annotated assign name: str
  - L412 annotated assign data: List[Any] = Field(default_factory=list)
  - L413 annotated assign color: Optional[str] = None
  - L414 annotated assign type: Optional[str] = None
  - L415 annotated assign y_axis: Optional[int] = None
- L418 class ChartAnnotation(BaseModel):
  - L419 docstring: "Annotation on a chart."
  - L420 annotated assign type: str
  - L421 annotated assign label: str
  - L422 annotated assign value: Optional[Any] = None
  - L423 annotated assign position: Optional[Dict[str, Any]] = None
  - L424 annotated assign style: Dict[str, Any] = Field(default_factory=dict)
- L427 class EnhancedChartSpec(BaseModel):
  - L428 docstring: "Enhanced chart specification with AI insights."
  - L429 annotated assign id: str
  - L430 annotated assign type: ChartType
  - L431 annotated assign title: str
  - L432 annotated assign description: Optional[str] = None
  - L435 annotated assign x_field: str
  - L436 annotated assign y_fields: List[str] = Field(default_factory=list)
  - L437 annotated assign group_field: Optional[str] = None
  - L438 annotated assign size_field: Optional[str] = None
  - L439 annotated assign color_field: Optional[str] = None
  - L442 annotated assign data: List[Dict[str, Any]] = Field(default_factory=list)
  - L443 annotated assign series: List[ChartDataSeries] = Field(default_factory=list)
  - L446 annotated assign x_axis_label: Optional[str] = None
  - L447 annotated assign y_axis_label: Optional[str] = None
  - L448 annotated assign x_axis_type: str = 'category'
  - L449 annotated assign y_axis_type: str = 'linear'
  - L452 annotated assign colors: List[str] = Field(default_factory=list)
  - L453 annotated assign show_legend: bool = True
  - L454 annotated assign show_grid: bool = True
  - L455 annotated assign show_labels: bool = False
  - L458 annotated assign trend_line: Optional[Dict[str, Any]] = None
  - L459 annotated assign forecast: Optional[Dict[str, Any]] = None
  - L460 annotated assign anomalies: List[Dict[str, Any]] = Field(default_factory=list)
  - L461 annotated assign annotations: List[ChartAnnotation] = Field(default_factory=list)
  - L462 annotated assign ai_insights: List[str] = Field(default_factory=list)
  - L465 annotated assign is_interactive: bool = True
  - L466 annotated assign drill_down_enabled: bool = False
  - L469 annotated assign source_table_id: Optional[str] = None
  - L470 annotated assign confidence: float = Field(ge=0.0, le=1.0, default=0.8)
  - L471 annotated assign suggested_by_ai: bool = True
- L474 class VisualizationSuggestion(BaseModel):
  - L475 docstring: "AI suggestion for a visualization."
  - L476 annotated assign chart_spec: EnhancedChartSpec
  - L477 annotated assign rationale: str
  - L478 annotated assign relevance_score: float = Field(ge=0.0, le=1.0)
  - L479 annotated assign complexity: str = 'simple'
  - L480 annotated assign insights_potential: List[str] = Field(default_factory=list)
- L487 class Insight(BaseModel):
  - L488 docstring: "An AI-generated insight."
  - L489 annotated assign id: str
  - L490 annotated assign type: str
  - L491 annotated assign title: str
  - L492 annotated assign description: str
  - L493 annotated assign priority: Priority
  - L494 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L495 annotated assign supporting_data: List[Dict[str, Any]] = Field(default_factory=list)
  - L496 annotated assign source_references: List[str] = Field(default_factory=list)
  - L497 annotated assign actionable: bool = False
  - L498 annotated assign suggested_actions: List[str] = Field(default_factory=list)
- L501 class RiskItem(BaseModel):
  - L502 docstring: "An identified risk."
  - L503 annotated assign id: str
  - L504 annotated assign title: str
  - L505 annotated assign description: str
  - L506 annotated assign risk_level: RiskLevel
  - L507 annotated assign category: str
  - L508 annotated assign probability: float = Field(ge=0.0, le=1.0, default=0.5)
  - L509 annotated assign impact: float = Field(ge=0.0, le=1.0, default=0.5)
  - L510 annotated assign risk_score: float = Field(ge=0.0, le=1.0, default=0.0)
  - L511 annotated assign mitigation_suggestions: List[str] = Field(default_factory=list)
  - L512 annotated assign source_references: List[str] = Field(default_factory=list)
- L515 class OpportunityItem(BaseModel):
  - L516 docstring: "An identified opportunity."
  - L517 annotated assign id: str
  - L518 annotated assign title: str
  - L519 annotated assign description: str
  - L520 annotated assign opportunity_type: str
  - L521 annotated assign potential_value: Optional[str] = None
  - L522 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L523 annotated assign requirements: List[str] = Field(default_factory=list)
  - L524 annotated assign suggested_actions: List[str] = Field(default_factory=list)
  - L525 annotated assign source_references: List[str] = Field(default_factory=list)
- L528 class ActionItem(BaseModel):
  - L529 docstring: "A recommended action."
  - L530 annotated assign id: str
  - L531 annotated assign title: str
  - L532 annotated assign description: str
  - L533 annotated assign priority: Priority
  - L534 annotated assign category: str
  - L535 annotated assign assignee_suggestion: Optional[str] = None
  - L536 annotated assign due_date_suggestion: Optional[str] = None
  - L537 annotated assign dependencies: List[str] = Field(default_factory=list)
  - L538 annotated assign expected_outcome: Optional[str] = None
  - L539 annotated assign effort_estimate: Optional[str] = None
- L546 class DataTransformation(BaseModel):
  - L547 docstring: "Data transformation operation."
  - L548 annotated assign operation: str
  - L549 annotated assign source_columns: List[str] = Field(default_factory=list)
  - L550 annotated assign target_column: Optional[str] = None
  - L551 annotated assign parameters: Dict[str, Any] = Field(default_factory=dict)
  - L552 annotated assign description: str = ''
- L555 class DataQualityIssue(BaseModel):
  - L556 docstring: "A specific data quality issue."
  - L557 annotated assign id: str
  - L558 annotated assign issue_type: str
  - L559 annotated assign severity: str = 'medium'
  - L560 annotated assign column: Optional[str] = None
  - L561 annotated assign row_indices: List[int] = Field(default_factory=list)
  - L562 annotated assign description: str
  - L563 annotated assign suggested_fix: Optional[str] = None
  - L564 annotated assign affected_count: int = 0
- L567 class DataQualityReport(BaseModel):
  - L568 docstring: "Data quality assessment."
  - L569 annotated assign total_rows: int = 0
  - L570 annotated assign total_columns: int = 0
  - L573 annotated assign issues: List[DataQualityIssue] = Field(default_factory=list)
  - L576 annotated assign missing_values: Dict[str, int] = Field(default_factory=dict)
  - L577 annotated assign missing_percentage: Dict[str, float] = Field(default_factory=dict)
  - L580 annotated assign duplicate_rows: int = 0
  - L581 annotated assign unique_values_per_column: Dict[str, int] = Field(default_factory=dict)
  - L584 annotated assign invalid_values: Dict[str, List[Any]] = Field(default_factory=dict)
  - L585 annotated assign type_mismatches: Dict[str, List[int]] = Field(default_factory=dict)
  - L588 annotated assign format_inconsistencies: Dict[str, List[str]] = Field(default_factory=dict)
  - L591 annotated assign outliers_detected: Dict[str, List[int]] = Field(default_factory=dict)
  - L594 annotated assign quality_score: float = Field(ge=0.0, le=1.0, default=0.0)
  - L595 annotated assign recommendations: List[str] = Field(default_factory=list)
- L598 class ExportConfiguration(BaseModel):
  - L599 docstring: "Export configuration."
  - L600 annotated assign format: ExportFormat
  - L601 annotated assign include_raw_data: bool = True
  - L602 annotated assign include_charts: bool = True
  - L603 annotated assign include_analysis: bool = True
  - L604 annotated assign include_insights: bool = True
  - L605 annotated assign sections: List[str] = Field(default_factory=list)
  - L606 annotated assign styling: Dict[str, Any] = Field(default_factory=dict)
  - L607 annotated assign filename: Optional[str] = None
- L614 class AnalysisPreferences(BaseModel):
  - L615 docstring: "User preferences for analysis."
  - L616 annotated assign analysis_depth: AnalysisDepth = AnalysisDepth.STANDARD
  - L617 annotated assign focus_areas: List[str] = Field(default_factory=list)
  - L618 annotated assign output_format: str = 'executive'
  - L619 annotated assign language: str = 'en'
  - L620 annotated assign industry: Optional[str] = None
  - L621 annotated assign company_size: Optional[str] = None
  - L622 annotated assign currency_preference: str = 'USD'
  - L623 annotated assign date_format: str = 'YYYY-MM-DD'
  - L624 annotated assign number_format: str = '1,234.56'
  - L625 annotated assign timezone: str = 'UTC'
  - L626 annotated assign enable_predictions: bool = True
  - L627 annotated assign enable_recommendations: bool = True
  - L628 annotated assign auto_chart_generation: bool = True
  - L629 annotated assign max_charts: int = 10
  - L630 annotated assign summary_mode: SummaryMode = SummaryMode.EXECUTIVE
- L637 class WebhookConfig(BaseModel):
  - L638 docstring: "Webhook configuration for notifications."
  - L639 annotated assign url: str
  - L640 annotated assign events: List[str] = Field(default_factory=list)
  - L641 annotated assign secret: Optional[str] = None
  - L642 annotated assign enabled: bool = True
- L645 class IntegrationConfig(BaseModel):
  - L646 docstring: "External integration configuration."
  - L647 annotated assign type: str
  - L648 annotated assign enabled: bool = True
  - L649 annotated assign credentials: Dict[str, str] = Field(default_factory=dict)
  - L650 annotated assign settings: Dict[str, Any] = Field(default_factory=dict)
- L653 class ScheduledAnalysis(BaseModel):
  - L654 docstring: "Scheduled analysis configuration."
  - L655 annotated assign id: str
  - L656 annotated assign name: str
  - L657 annotated assign source_type: str
  - L658 annotated assign source_config: Dict[str, Any] = Field(default_factory=dict)
  - L659 annotated assign schedule: str
  - L660 annotated assign analysis_config: AnalysisPreferences = Field(default_factory=AnalysisPreferences)
  - L661 annotated assign notifications: List[str] = Field(default_factory=list)
  - L662 annotated assign last_run: Optional[datetime] = None
  - L663 annotated assign next_run: Optional[datetime] = None
  - L664 annotated assign enabled: bool = True
- L671 class EnhancedAnalysisResult(BaseModel):
  - L672 docstring: "Complete enhanced analysis result."
  - L674 annotated assign analysis_id: str
  - L675 annotated assign document_name: str
  - L676 annotated assign document_type: DocumentType
  - L677 annotated assign created_at: datetime = Field(default_factory=datetime.utcnow)
  - L678 annotated assign processing_time_ms: int = 0
  - L681 annotated assign tables: List[EnhancedExtractedTable] = Field(default_factory=list)
  - L682 annotated assign entities: List[ExtractedEntity] = Field(default_factory=list)
  - L683 annotated assign metrics: List[ExtractedMetric] = Field(default_factory=list)
  - L684 annotated assign forms: List[FormField] = Field(default_factory=list)
  - L685 annotated assign invoices: List[ExtractedInvoice] = Field(default_factory=list)
  - L686 annotated assign contracts: List[ExtractedContract] = Field(default_factory=list)
  - L687 annotated assign table_relationships: List[TableRelationship] = Field(default_factory=list)
  - L690 annotated assign summaries: Dict[str, DocumentSummary] = Field(default_factory=dict)
  - L691 annotated assign sentiment: Optional[SentimentAnalysis] = None
  - L692 annotated assign text_analytics: Optional[TextAnalytics] = None
  - L693 annotated assign financial_analysis: Optional[FinancialAnalysis] = None
  - L694 annotated assign statistical_analysis: Optional[StatisticalAnalysis] = None
  - L695 annotated assign comparative_analysis: Optional[ComparativeAnalysis] = None
  - L698 annotated assign chart_suggestions: List[EnhancedChartSpec] = Field(default_factory=list)
  - L699 annotated assign visualization_suggestions: List[VisualizationSuggestion] = Field(default_factory=list)
  - L702 annotated assign insights: List[Insight] = Field(default_factory=list)
  - L703 annotated assign risks: List[RiskItem] = Field(default_factory=list)
  - L704 annotated assign opportunities: List[OpportunityItem] = Field(default_factory=list)
  - L705 annotated assign action_items: List[ActionItem] = Field(default_factory=list)
  - L708 annotated assign data_quality: Optional[DataQualityReport] = None
  - L711 annotated assign page_count: int = 0
  - L712 annotated assign total_tables: int = 0
  - L713 annotated assign total_entities: int = 0
  - L714 annotated assign total_metrics: int = 0
  - L715 annotated assign confidence_score: float = Field(ge=0.0, le=1.0, default=0.8)
  - L718 annotated assign preferences: Optional[AnalysisPreferences] = None
  - L721 annotated assign warnings: List[str] = Field(default_factory=list)
  - L722 annotated assign errors: List[str] = Field(default_factory=list)
- L729 class AnalyzeRequest(BaseModel):
  - L730 docstring: "Request to analyze a document."
  - L731 annotated assign preferences: Optional[AnalysisPreferences] = None
  - L732 annotated assign focus_areas: List[str] = Field(default_factory=list)
  - L733 annotated assign comparison_document_ids: List[str] = Field(default_factory=list)
  - L734 annotated assign custom_prompts: Dict[str, str] = Field(default_factory=dict)
- L737 class ChartGenerationRequest(BaseModel):
  - L738 docstring: "Request to generate charts."
  - L739 annotated assign analysis_id: str
  - L740 annotated assign natural_language_query: Optional[str] = None
  - L741 annotated assign chart_type: Optional[ChartType] = None
  - L742 annotated assign data_columns: List[str] = Field(default_factory=list)
  - L743 annotated assign include_trends: bool = True
  - L744 annotated assign include_forecasts: bool = False
- L747 class ExportRequest(BaseModel):
  - L748 docstring: "Request to export analysis."
  - L749 annotated assign analysis_id: str
  - L750 annotated assign config: ExportConfiguration
- L753 class QuestionRequest(BaseModel):
  - L754 docstring: "Request to ask a question about the document."
  - L755 annotated assign analysis_id: str
  - L756 annotated assign question: str
  - L757 annotated assign include_sources: bool = True
  - L758 annotated assign max_context_chunks: int = 5
- L761 class QuestionResponse(BaseModel):
  - L762 docstring: "Response to a document question."
  - L763 annotated assign answer: str
  - L764 annotated assign confidence: float
  - L765 annotated assign sources: List[Dict[str, Any]] = Field(default_factory=list)
  - L766 annotated assign suggested_followups: List[str] = Field(default_factory=list)
- L769 class QAResponse(BaseModel):
  - L770 docstring: "Enhanced Q&A response with detailed source information."
  - L771 annotated assign answer: str
  - L772 annotated assign confidence: float = Field(ge=0.0, le=1.0)
  - L773 annotated assign sources: List[Dict[str, Any]] = Field(default_factory=list)
  - L774 annotated assign context_used: List[str] = Field(default_factory=list)
  - L775 annotated assign suggested_followups: List[str] = Field(default_factory=list)
  - L776 annotated assign reasoning: Optional[str] = None
  - L777 annotated assign citations: List[Dict[str, Any]] = Field(default_factory=list)
- L780 class TransformRequest(BaseModel):
  - L781 docstring: "Request to transform data."
  - L782 annotated assign analysis_id: str
  - L783 annotated assign transformations: List[DataTransformation]
  - L784 annotated assign output_format: str = 'json'

## backend\app\features\analyze\services\__init__.py
- L2 from __future__ import annotations
- L4 from .document_analysis_service import analyze_document_streaming, get_analysis, get_analysis_data, suggest_charts_for_analysis
- L10 from .extraction_pipeline import ExtractedContent, extract_document_content, extract_pdf_content, extract_excel_content, format_content_for_llm
- L17 from .enhanced_extraction_service import EnhancedExtractionService
- L18 from .analysis_engines import AnalysisEngineService
- L19 from .visualization_engine import VisualizationEngine
- L20 from .data_transform_export import DataExportService
- L21 from .advanced_ai_features import AdvancedAIService
- L22 from .user_experience import UserExperienceService
- L23 from .integrations import IntegrationService
- L24 from .enhanced_analysis_orchestrator import EnhancedAnalysisOrchestrator, get_orchestrator
- L30 assign DocumentAnalysisService = type('DocumentAnalysisService', (), {'analyze_streaming': staticmethod(analyze_document_streaming), 'get_analysis': staticmethod(get_analysis), 'get_data': staticmethod(get_analysis_data), 'suggest_charts': staticmethod(suggest_charts_for_analysis)})
- L37 assign __all__ = ['DocumentAnalysisService', 'analyze_document_streaming', 'get_analysis', 'get_analysis_data', 'suggest_charts_for_analysis', 'ExtractedContent', 'extract_document_content', 'extract_pdf_content', 'extract_excel_content', 'format_content_for_llm', 'EnhancedExtractionService', 'AnalysisEngineService', 'VisualizationEngine', 'DataExportService', 'AdvancedAIService', 'UserExperienceService', 'IntegrationService', 'EnhancedAnalysisOrchestrator', 'get_orchestrator']

## backend\app\features\analyze\services\advanced_ai_features.py
- L2 docstring: "\nAdvanced AI Features - Multi-modal understanding, cross-document intelligence,..."
- L10 from __future__ import annotations
- L12 import json
- L13 import logging
- L14 import math
- L15 import re
- L16 import uuid
- L17 from collections import defaultdict
- L18 from dataclasses import dataclass, field
- L19 from datetime import datetime, timedelta
- L20 from typing import Any, Dict, List, Optional, Tuple, Union
- L22 from backend.app.features.analyze.schemas.enhanced_analysis import EnhancedChartSpec, EnhancedExtractedTable, ExtractedEntity, ExtractedMetric, Insight, Priority
- L30 from backend.app.services.utils.llm import call_chat_completion
- L31 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L33 assign logger = logging.getLogger('neura.analyze.advanced_ai')
- L41 class ImageAnalysisResult:
  - L42 docstring: "Result of image analysis within a document."
  - L43 annotated assign image_type: str
  - L44 annotated assign description: str
  - L45 annotated assign extracted_data: Dict[str, Any] = field(default_factory=dict)
  - L46 annotated assign confidence: float = 0.8
  - L47 annotated assign location: Optional[str] = None
- L51 class ChartDataExtraction:
  - L52 docstring: "Extracted data from a chart image."
  - L53 annotated assign chart_type: str
  - L54 annotated assign title: Optional[str]
  - L55 annotated assign x_axis: Dict[str, Any]
  - L56 annotated assign y_axis: Dict[str, Any]
  - L57 annotated assign data_series: List[Dict[str, Any]]
  - L58 annotated assign insights: List[str]
- L61 def analyze_document_imagesimages: List[Dict[str, Any]], document_context: str='':
  - L65 docstring: "Analyze images found in a document using VLM."
  - L66 assign results = []
  - L69 for img in images:
    - L70 assign img_data = img.get('data')
    - L71 assign page = img.get('page')
    - L73 if not img_data:
      - L74 continue
    - L76 assign prompt = f'Analyze this image from a document.\n\nDocument context: {document_context[:500]}\n\nDetermine:\n1. Image type (chart, diagram, photo, logo, signature, handwriting, table, other)\n2. Detailed description of content\n3. Any extractable data (for charts: data points, for forms: field values, etc.)\n\nReturn JSON:\n```json\n{{\n  "image_type": "chart|diagram|photo|logo|signature|handwriting|table|other",\n  "description": "Detailed description",\n  "extracted_data": {{\n    "chart_type": "bar",\n    "data_points": [{{"label": "Q1", "value": 100}}],\n    "title": "Revenue by Quarter"\n  }},\n  "confidence": 0.85,\n  "key_information": ["Item 1", "Item 2"]\n}}\n```'
    - L100 try:
      - L103 expr results.append(ImageAnalysisResult(image_type='chart', description='Analyzed image', extracted_data={}, confidence=0.8, location=f'Page {page}' if page else None))
      - L110 except Exception as e:
        - L111 expr logger.warning(f'Image analysis failed: {e}')
  - L113 return results
- L116 def extract_chart_data_from_imageimage_data: bytes:
  - L117 docstring: "Extract structured data from a chart image."
  - L118 assign prompt = 'Analyze this chart image and extract all data.\n\nReturn JSON:\n```json\n{\n  "chart_type": "bar|line|pie|scatter|area|other",\n  "title": "Chart title if visible",\n  "x_axis": {\n    "label": "X axis label",\n    "values": ["Jan", "Feb", "Mar"],\n    "type": "category|numeric|date"\n  },\n  "y_axis": {\n    "label": "Y axis label",\n    "min": 0,\n    "max": 100,\n    "type": "numeric"\n  },\n  "data_series": [\n    {\n      "name": "Series 1",\n      "values": [10, 20, 30],\n      "color": "blue"\n    }\n  ],\n  "insights": [\n    "Key observation 1",\n    "Key observation 2"\n  ]\n}\n```\n\nExtract as much data as you can accurately determine from the image.'
  - L154 return None
- L157 def detect_handwritingimage_data: bytes:
  - L158 docstring: "Detect and transcribe handwritten text."
  - L159 assign prompt = 'Analyze this image for handwritten text.\n\nReturn JSON:\n```json\n{\n  "has_handwriting": true,\n  "transcribed_text": "Full transcription of handwritten content",\n  "confidence": 0.75,\n  "words": [\n    {"text": "word", "confidence": 0.8, "position": {"x": 100, "y": 50}}\n  ],\n  "is_signature": false\n}\n```\n\nTranscribe all visible handwritten text accurately.'
  - L176 return {'has_handwriting': False, 'transcribed_text': '', 'confidence': 0}
- L183 def detect_logosimage_data: bytes:
  - L184 docstring: "Detect and identify logos in an image."
  - L185 return []
- L193 class KnowledgeGraphNode:
  - L194 docstring: "A node in the knowledge graph."
  - L195 annotated assign id: str
  - L196 annotated assign label: str
  - L197 annotated assign type: str
  - L198 annotated assign properties: Dict[str, Any] = field(default_factory=dict)
- L202 class KnowledgeGraphEdge:
  - L203 docstring: "An edge in the knowledge graph."
  - L204 annotated assign source_id: str
  - L205 annotated assign target_id: str
  - L206 annotated assign relationship: str
  - L207 annotated assign weight: float = 1.0
  - L208 annotated assign properties: Dict[str, Any] = field(default_factory=dict)
- L212 class KnowledgeGraph:
  - L213 docstring: "A knowledge graph built from document analysis."
  - L214 annotated assign nodes: List[KnowledgeGraphNode] = field(default_factory=list)
  - L215 annotated assign edges: List[KnowledgeGraphEdge] = field(default_factory=list)
  - L217 def add_nodeself, node: KnowledgeGraphNode:
    - L218 if not any((n.id == node.id for n in self.nodes)):
      - L219 expr self.nodes.append(node)
  - L221 def add_edgeself, edge: KnowledgeGraphEdge:
    - L222 expr self.edges.append(edge)
  - L224 def to_dictself:
    - L225 return {'nodes': [{'id': n.id, 'label': n.label, 'type': n.type, 'properties': n.properties} for n in self.nodes], 'edges': [{'source': e.source_id, 'target': e.target_id, 'relationship': e.relationship, 'weight': e.weight} for e in self.edges]}
- L234 def build_knowledge_graphentities: List[ExtractedEntity], metrics: List[ExtractedMetric], document_id: str:
  - L239 docstring: "Build a knowledge graph from extracted entities and metrics."
  - L240 assign graph = KnowledgeGraph()
  - L243 assign doc_node = KnowledgeGraphNode(id=document_id, label=document_id, type='document')
  - L248 expr graph.add_node(doc_node)
  - L251 for entity in entities:
    - L252 assign node = KnowledgeGraphNode(id=entity.id, label=entity.value, type=entity.type.value, properties={'normalized': entity.normalized_value, 'confidence': entity.confidence})
    - L261 expr graph.add_node(node)
    - L264 expr graph.add_edge(KnowledgeGraphEdge(source_id=document_id, target_id=entity.id, relationship='contains', weight=entity.confidence))
  - L272 for metric in metrics:
    - L273 assign node = KnowledgeGraphNode(id=metric.id, label=metric.name, type='metric', properties={'value': metric.value, 'raw_value': metric.raw_value, 'metric_type': metric.metric_type.value, 'period': metric.period})
    - L284 expr graph.add_node(node)
    - L287 expr graph.add_edge(KnowledgeGraphEdge(source_id=document_id, target_id=metric.id, relationship='reports', weight=metric.importance_score))
  - L295 for (i, e1) in enumerate(entities):
    - L296 for e2 in entities[i + 1:]:
      - L298 if e1.context and e2.context:
        - L300 if e1.value.lower() in (e2.context or '').lower() or e2.value.lower() in (e1.context or '').lower():
          - L301 expr graph.add_edge(KnowledgeGraphEdge(source_id=e1.id, target_id=e2.id, relationship='co_occurs', weight=0.7))
  - L308 return graph
- L311 def merge_knowledge_graphsgraphs: List[KnowledgeGraph]:
  - L312 docstring: "Merge multiple knowledge graphs into one."
  - L313 assign merged = KnowledgeGraph()
  - L316 annotated assign node_map: Dict[str, KnowledgeGraphNode] = {}
  - L317 for graph in graphs:
    - L318 for node in graph.nodes:
      - L320 assign key = f'{node.type}:{node.label.lower()}'
      - L321 if key not in node_map:
        - L322 assign node_map[key] = node
        - L325 else:
          - L325 expr node_map[key].properties.update(node.properties)
  - L327 assign merged.nodes = list(node_map.values())
  - L330 assign edge_set = set()
  - L331 for graph in graphs:
    - L332 for edge in graph.edges:
      - L333 assign edge_key = (edge.source_id, edge.target_id, edge.relationship)
      - L334 if edge_key not in edge_set:
        - L335 expr edge_set.add(edge_key)
        - L336 expr merged.add_edge(edge)
  - L338 return merged
- L342 class CitationLink:
  - L343 docstring: "A citation or reference link between documents."
  - L344 annotated assign source_doc_id: str
  - L345 annotated assign target_doc_id: str
  - L346 annotated assign citation_text: str
  - L347 annotated assign citation_type: str
  - L348 annotated assign confidence: float = 0.8
- L351 def detect_citationstext: str, document_id: str:
  - L352 docstring: "Detect citations and references in text."
  - L353 assign citations = []
  - L356 assign patterns = ['\\[(\\d+)\\]', '\\(([A-Za-z]+(?:\\s+et\\s+al\\.?)?,?\\s*\\d{4})\\)', '(?:Source|Reference|See|cf\\.?):\\s*(.+?)(?:\\.|$)', '(?:According to|As stated in|Per)\\s+(.+?)(?:,|\\.)']
  - L363 for pattern in patterns:
    - L364 for match in re.finditer(pattern, text, re.IGNORECASE):
      - L365 expr citations.append(CitationLink(source_doc_id=document_id, target_doc_id=f'ref_{uuid.uuid4().hex[:8]}', citation_text=match.group(0), citation_type='reference', confidence=0.7))
  - L373 return citations
- L377 class Contradiction:
  - L378 docstring: "A detected contradiction between statements or documents."
  - L379 annotated assign statement1: str
  - L380 annotated assign statement2: str
  - L381 annotated assign source1: str
  - L382 annotated assign source2: str
  - L383 annotated assign contradiction_type: str
  - L384 annotated assign severity: str
  - L385 annotated assign confidence: float = 0.7
- L388 def detect_contradictionstext1: str, text2: str, doc1_id: str='doc1', doc2_id: str='doc2':
  - L394 docstring: "Detect contradictions between two texts using LLM."
  - L395 assign prompt = f'Compare these two texts and identify any contradictions, inconsistencies, or conflicting information.\n\nText 1:\n{text1[:3000]}\n\nText 2:\n{text2[:3000]}\n\nReturn JSON:\n```json\n{{\n  "contradictions": [\n    {{\n      "statement1": "Quote or paraphrase from Text 1",\n      "statement2": "Conflicting statement from Text 2",\n      "type": "factual|numerical|temporal|logical",\n      "severity": "minor|moderate|major",\n      "explanation": "Why these are contradictory",\n      "confidence": 0.8\n    }}\n  ],\n  "overall_consistency": 0.85\n}}\n```\n\nOnly report genuine contradictions, not differences in scope or perspective.'
  - L422 try:
    - L423 assign client = get_openai_client()
    - L424 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='contradiction_detection', temperature=0.2)
    - L432 assign raw_text = response.choices[0].message.content or ''
    - L433 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L435 if json_match:
      - L436 assign data = json.loads(json_match.group())
      - L437 assign contradictions = []
      - L439 for item in data.get('contradictions', []):
        - L440 expr contradictions.append(Contradiction(statement1=item.get('statement1', ''), statement2=item.get('statement2', ''), source1=doc1_id, source2=doc2_id, contradiction_type=item.get('type', 'factual'), severity=item.get('severity', 'moderate'), confidence=item.get('confidence', 0.7)))
      - L450 return contradictions
    - L452 except Exception as e:
      - L453 expr logger.warning(f'Contradiction detection failed: {e}')
  - L455 return []
- L463 class Forecast:
  - L464 docstring: "A forecast prediction."
  - L465 annotated assign metric_name: str
  - L466 annotated assign current_value: float
  - L467 annotated assign predictions: List[Dict[str, Any]]
  - L468 annotated assign trend: str
  - L469 annotated assign confidence: float
  - L470 annotated assign method: str
  - L471 annotated assign factors: List[str] = field(default_factory=list)
- L475 class AnomalyPrediction:
  - L476 docstring: "Predicted anomaly or unusual pattern."
  - L477 annotated assign metric_name: str
  - L478 annotated assign predicted_date: Optional[str]
  - L479 annotated assign anomaly_type: str
  - L480 annotated assign probability: float
  - L481 annotated assign expected_value: float
  - L482 annotated assign threshold: float
  - L483 annotated assign reasoning: str
- L487 class GrowthModel:
  - L488 docstring: "Growth model for a metric."
  - L489 annotated assign metric_name: str
  - L490 annotated assign model_type: str
  - L491 annotated assign parameters: Dict[str, float]
  - L492 annotated assign r_squared: float
  - L493 annotated assign projected_values: List[Dict[str, Any]]
  - L494 annotated assign saturation_point: Optional[float] = None
- L497 def forecast_time_seriesdata: List[Tuple[str, float]], metric_name: str, periods: int=6:
  - L502 docstring: "Generate a forecast for time series data."
  - L503 if len(data) < 3:
    - L504 return Forecast(metric_name=metric_name, current_value=data[-1][1] if data else 0, predictions=[], trend='unknown', confidence=0.3, method='insufficient_data')
  - L513 assign values = [v for _, v in data]
  - L514 assign n = len(values)
  - L517 assign x = list(range(n))
  - L518 assign mean_x = sum(x) / n
  - L519 assign mean_y = sum(values) / n
  - L521 assign numerator = sum(((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, values)))
  - L522 assign denominator = sum(((xi - mean_x) ** 2 for xi in x))
  - L524 if denominator == 0:
    - L525 assign slope = 0
    - L526 assign intercept = mean_y
    - L528 else:
      - L528 assign slope = numerator / denominator
      - L529 assign intercept = mean_y - slope * mean_x
  - L532 assign ss_tot = sum(((yi - mean_y) ** 2 for yi in values))
  - L533 assign ss_res = sum(((yi - (slope * xi + intercept)) ** 2 for xi, yi in zip(x, values)))
  - L534 assign r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
  - L537 if slope > 0.05 * mean_y:
    - L538 assign trend = 'increasing'
    - L539 else:
      - L539 if slope < -0.05 * mean_y:
        - L540 assign trend = 'decreasing'
        - L542 else:
          - L542 assign trend = 'stable'
  - L545 assign predictions = []
  - L546 assign std_error = math.sqrt(ss_res / max(n - 2, 1)) if n > 2 else mean_y * 0.1
  - L548 for i in range(1, periods + 1):
    - L549 assign x_pred = n - 1 + i
    - L550 assign y_pred = slope * x_pred + intercept
    - L553 assign margin = 1.96 * std_error * math.sqrt(1 + 1 / n + (x_pred - mean_x) ** 2 / denominator) if denominator > 0 else y_pred * 0.2
    - L555 expr predictions.append({'period': i, 'value': round(y_pred, 2), 'lower_bound': round(y_pred - margin, 2), 'upper_bound': round(y_pred + margin, 2)})
  - L562 assign confidence = min(0.95, max(0.3, r_squared))
  - L564 return Forecast(metric_name=metric_name, current_value=values[-1], predictions=predictions, trend=trend, confidence=round(confidence, 2), method='linear', factors=[f'Based on {n} historical data points', f'R\xb2 = {r_squared:.3f}'])
- L575 def predict_anomaliesdata: List[Tuple[str, float]], metric_name: str, sensitivity: float=2.0:
  - L580 docstring: "Predict potential anomalies based on historical patterns."
  - L581 if len(data) < 10:
    - L582 return []
  - L584 assign values = [v for _, v in data]
  - L585 assign mean = sum(values) / len(values)
  - L586 assign std = math.sqrt(sum(((v - mean) ** 2 for v in values)) / len(values))
  - L588 if std == 0:
    - L589 return []
  - L592 assign window = min(5, len(values) // 2)
  - L593 assign predictions = []
  - L596 assign recent_values = values[-window:]
  - L597 assign recent_mean = sum(recent_values) / len(recent_values)
  - L600 if abs(recent_mean - mean) > std:
    - L601 if recent_mean > mean + std:
      - L602 expr predictions.append(AnomalyPrediction(metric_name=metric_name, predicted_date=None, anomaly_type='spike', probability=0.6, expected_value=recent_mean, threshold=mean + sensitivity * std, reasoning='Recent values significantly above historical average'))
      - L612 else:
        - L612 expr predictions.append(AnomalyPrediction(metric_name=metric_name, predicted_date=None, anomaly_type='dip', probability=0.6, expected_value=recent_mean, threshold=mean - sensitivity * std, reasoning='Recent values significantly below historical average'))
  - L623 assign recent_std = math.sqrt(sum(((v - recent_mean) ** 2 for v in recent_values)) / len(recent_values))
  - L624 if recent_std > std * 1.5:
    - L625 expr predictions.append(AnomalyPrediction(metric_name=metric_name, predicted_date=None, anomaly_type='deviation', probability=0.5, expected_value=recent_mean, threshold=recent_std, reasoning='Increased volatility in recent data'))
  - L635 return predictions
- L638 def build_growth_modeldata: List[Tuple[str, float]], metric_name: str, model_type: str='auto':
  - L643 docstring: "Build a growth model for a metric."
  - L644 if len(data) < 5:
    - L645 return GrowthModel(metric_name=metric_name, model_type='insufficient_data', parameters={}, r_squared=0, projected_values=[])
  - L653 assign values = [v for _, v in data]
  - L654 assign n = len(values)
  - L655 assign x = list(range(n))
  - L658 assign mean_x = sum(x) / n
  - L659 assign mean_y = sum(values) / n
  - L661 assign num = sum(((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, values)))
  - L662 assign den = sum(((xi - mean_x) ** 2 for xi in x))
  - L664 if den == 0:
    - L665 assign slope = 0
    - L666 assign intercept = mean_y
    - L668 else:
      - L668 assign slope = num / den
      - L669 assign intercept = mean_y - slope * mean_x
  - L672 assign ss_tot = sum(((yi - mean_y) ** 2 for yi in values))
  - L673 assign ss_res_linear = sum(((yi - (slope * xi + intercept)) ** 2 for xi, yi in zip(x, values)))
  - L674 assign r2_linear = 1 - ss_res_linear / ss_tot if ss_tot > 0 else 0
  - L677 assign positive_values = [max(0.01, v) for v in values]
  - L678 assign log_values = [math.log(v) for v in positive_values]
  - L679 assign mean_log_y = sum(log_values) / n
  - L681 assign num_exp = sum(((xi - mean_x) * (yi - mean_log_y) for xi, yi in zip(x, log_values)))
  - L682 if den > 0:
    - L683 assign exp_slope = num_exp / den
    - L684 assign exp_intercept = mean_log_y - exp_slope * mean_x
    - L686 assign predicted_exp = [math.exp(exp_slope * xi + exp_intercept) for xi in x]
    - L687 assign ss_res_exp = sum(((yi - pi) ** 2 for yi, pi in zip(values, predicted_exp)))
    - L688 assign r2_exp = 1 - ss_res_exp / ss_tot if ss_tot > 0 else 0
    - L690 else:
      - L690 assign r2_exp = 0
      - L691 assign exp_slope = 0
      - L692 assign exp_intercept = math.log(mean_y) if mean_y > 0 else 0
  - L695 if model_type == 'auto':
    - L696 if r2_exp > r2_linear + 0.1 and exp_slope > 0:
      - L697 assign model_type = 'exponential'
      - L699 else:
        - L699 assign model_type = 'linear'
  - L701 if model_type == 'exponential':
    - L702 assign parameters = {'growth_rate': exp_slope, 'initial_value': math.exp(exp_intercept)}
    - L703 assign r_squared = r2_exp
    - L705 assign projected = []
    - L706 for i in range(1, 7):
      - L707 assign x_pred = n - 1 + i
      - L708 assign y_pred = math.exp(exp_slope * x_pred + exp_intercept)
      - L709 expr projected.append({'period': i, 'value': round(y_pred, 2)})
    - L712 assign growth_rates = [values[i] / values[i - 1] if values[i - 1] > 0 else 1 for i in range(1, n)]
    - L713 if len(growth_rates) >= 3:
      - L714 assign recent_growth = sum(growth_rates[-3:]) / 3
      - L715 assign early_growth = sum(growth_rates[:3]) / 3
      - L716 if recent_growth < early_growth * 0.8:
        - L717 assign saturation = values[-1] * (1 / (1 - recent_growth)) if recent_growth < 1 else None
        - L719 else:
          - L719 assign saturation = None
      - L721 else:
        - L721 assign saturation = None
    - L724 else:
      - L724 assign parameters = {'slope': slope, 'intercept': intercept}
      - L725 assign r_squared = r2_linear
      - L726 assign saturation = None
      - L728 assign projected = []
      - L729 for i in range(1, 7):
        - L730 assign x_pred = n - 1 + i
        - L731 assign y_pred = slope * x_pred + intercept
        - L732 expr projected.append({'period': i, 'value': round(y_pred, 2)})
  - L734 return GrowthModel(metric_name=metric_name, model_type=model_type, parameters=parameters, r_squared=round(r_squared, 4), projected_values=projected, saturation_point=saturation)
- L744 def generate_ai_predictionsmetrics: List[ExtractedMetric], tables: List[EnhancedExtractedTable]:
  - L748 docstring: "Generate AI-powered predictions using LLM."
  - L750 assign metrics_context = '\n'.join([f'- {m.name}: {m.raw_value}' + (f' ({m.change}% change)' if m.change else '') for m in metrics[:15]])
  - L755 assign prompt = f'Based on these metrics, provide strategic predictions and insights.\n\nMetrics:\n{metrics_context}\n\nReturn JSON:\n```json\n{{\n  "predictions": [\n    {{\n      "metric": "Revenue",\n      "prediction": "Expected to grow 15-20% based on current trajectory",\n      "confidence": "medium",\n      "timeframe": "next 6 months",\n      "factors": ["Market expansion", "New product launch"]\n    }}\n  ],\n  "strategic_insights": [\n    "Key strategic observation 1",\n    "Key strategic observation 2"\n  ],\n  "risk_indicators": [\n    {{\n      "indicator": "Declining margins",\n      "severity": "moderate",\n      "recommendation": "Review cost structure"\n    }}\n  ],\n  "growth_opportunities": [\n    {{\n      "opportunity": "Market segment X",\n      "potential": "20% revenue increase",\n      "requirements": ["Investment needed", "Timeline"]\n    }}\n  ]\n}}\n```\n\nBe specific and data-driven in your predictions.'
  - L795 try:
    - L796 assign client = get_openai_client()
    - L797 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='ai_predictions', temperature=0.4)
    - L805 assign raw_text = response.choices[0].message.content or ''
    - L806 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L808 if json_match:
      - L809 return json.loads(json_match.group())
    - L811 except Exception as e:
      - L812 expr logger.warning(f'AI predictions failed: {e}')
  - L814 return {'predictions': [], 'strategic_insights': [], 'risk_indicators': [], 'growth_opportunities': []}
- L821 class AdvancedAIService:
  - L822 docstring: "Orchestrates all advanced AI features."
  - L824 def analyze_imagesself, images: List[Dict[str, Any]], document_context: str='':
    - L829 docstring: "Analyze images in the document."
    - L830 return analyze_document_images(images, document_context)
  - L832 def build_knowledge_graphself, entities: List[ExtractedEntity], metrics: List[ExtractedMetric], document_id: str:
    - L838 docstring: "Build a knowledge graph from extracted data."
    - L839 return build_knowledge_graph(entities, metrics, document_id)
  - L841 def detect_citationsself, text: str, document_id: str:
    - L842 docstring: "Detect citations in text."
    - L843 return detect_citations(text, document_id)
  - L845 def detect_contradictionsself, text1: str, text2: str, doc1_id: str='doc1', doc2_id: str='doc2':
    - L852 docstring: "Detect contradictions between texts."
    - L853 return detect_contradictions(text1, text2, doc1_id, doc2_id)
  - L855 def generate_forecastsself, tables: List[EnhancedExtractedTable]:
    - L859 docstring: "Generate forecasts for time series data in tables."
    - L860 assign forecasts = []
    - L862 for table in tables:
      - L864 assign datetime_cols = [i for i, d in enumerate(table.data_types) if d == 'datetime']
      - L865 assign numeric_cols = [i for i, d in enumerate(table.data_types) if d == 'numeric']
      - L867 if not datetime_cols or not numeric_cols:
        - L868 continue
      - L870 assign date_idx = datetime_cols[0]
      - L872 for num_idx in numeric_cols[:3]:
        - L874 assign data = []
        - L875 for row in table.rows:
          - L876 if date_idx < len(row) and num_idx < len(row):
            - L877 assign date_val = str(row[date_idx])
            - L878 try:
              - L879 assign num_val = float(str(row[num_idx]).replace(',', '').replace('$', '').replace('%', ''))
              - L880 expr data.append((date_val, num_val))
              - L881 except (ValueError, TypeError):
                - L882 pass
        - L884 if len(data) >= 3:
          - L885 assign forecast = forecast_time_series(data, table.headers[num_idx], periods=6)
          - L890 expr forecasts.append(forecast)
    - L892 return forecasts
  - L894 def predict_anomaliesself, tables: List[EnhancedExtractedTable]:
    - L898 docstring: "Predict anomalies in the data."
    - L899 assign all_predictions = []
    - L901 for table in tables:
      - L902 for (col_idx, (header, dtype)) in enumerate(zip(table.headers, table.data_types)):
        - L903 if dtype != 'numeric':
          - L904 continue
        - L907 assign data = []
        - L908 for (i, row) in enumerate(table.rows):
          - L909 if col_idx < len(row):
            - L910 try:
              - L911 assign val = float(str(row[col_idx]).replace(',', '').replace('$', '').replace('%', ''))
              - L912 expr data.append((str(i), val))
              - L913 except (ValueError, TypeError):
                - L914 pass
        - L916 if len(data) >= 10:
          - L917 assign predictions = predict_anomalies(data, header)
          - L918 expr all_predictions.extend(predictions)
    - L920 return all_predictions
  - L922 def build_growth_modelsself, tables: List[EnhancedExtractedTable]:
    - L926 docstring: "Build growth models for metrics."
    - L927 assign models = []
    - L929 for table in tables:
      - L930 assign numeric_cols = [(i, h) for i, (h, d) in enumerate(zip(table.headers, table.data_types)) if d == 'numeric']
      - L932 for (col_idx, header) in numeric_cols[:3]:
        - L933 assign data = []
        - L934 for (i, row) in enumerate(table.rows):
          - L935 if col_idx < len(row):
            - L936 try:
              - L937 assign val = float(str(row[col_idx]).replace(',', '').replace('$', '').replace('%', ''))
              - L938 expr data.append((str(i), val))
              - L939 except (ValueError, TypeError):
                - L940 pass
        - L942 if len(data) >= 5:
          - L943 assign model = build_growth_model(data, header)
          - L944 if model.r_squared > 0.5:
            - L945 expr models.append(model)
    - L947 return models
  - L949 def generate_ai_predictionsself, metrics: List[ExtractedMetric], tables: List[EnhancedExtractedTable]:
    - L954 docstring: "Generate AI-powered strategic predictions."
    - L955 return generate_ai_predictions(metrics, tables)
  - L957 def run_all_advanced_featuresself, text: str, entities: List[ExtractedEntity], metrics: List[ExtractedMetric], tables: List[EnhancedExtractedTable], document_id: str:
    - L965 docstring: "Run all advanced AI features."
    - L966 return {'knowledge_graph': self.build_knowledge_graph(entities, metrics, document_id).to_dict(), 'citations': [c.__dict__ for c in self.detect_citations(text, document_id)], 'forecasts': [f.__dict__ for f in self.generate_forecasts(tables)], 'anomaly_predictions': [a.__dict__ for a in self.predict_anomalies(tables)], 'growth_models': [m.__dict__ for m in self.build_growth_models(tables)], 'ai_predictions': self.generate_ai_predictions(metrics, tables)}

## backend\app\features\analyze\services\analysis_engines.py
- L2 docstring: "\nAI-Powered Analysis Engines - Document summarization, sentiment, and statistic..."
- L10 from __future__ import annotations
- L12 import json
- L13 import logging
- L14 import math
- L15 import re
- L16 import uuid
- L17 from collections import Counter
- L18 from datetime import datetime
- L19 from typing import Any, Dict, List, Optional, Tuple
- L21 from backend.app.features.analyze.schemas.enhanced_analysis import ActionItem, ComparativeAnalysis, DocumentSummary, EnhancedExtractedTable, ExtractedMetric, FinancialAnalysis, Insight, OpportunityItem, Priority, RiskItem, RiskLevel, SentimentAnalysis, SentimentLevel, StatisticalAnalysis, SummaryMode, TextAnalytics
- L39 from backend.app.services.utils.llm import call_chat_completion
- L40 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L42 assign logger = logging.getLogger('neura.analyze.engines')
- L49 assign SUMMARY_PROMPTS = {SummaryMode.EXECUTIVE: 'Create a C-suite executive summary of this document.\n- Maximum 3 bullet points\n- Focus on key business decisions and bottom-line impact\n- Use clear, decisive language\n- Highlight the most critical number or outcome\n\nFormat:\nTitle: [Brief title]\nBullets:\n- [Key point 1]\n- [Key point 2]\n- [Key point 3]\nKey Figure: [The single most important number/metric]', SummaryMode.DATA: 'Create a data-focused summary of this document.\n- List all key figures, metrics, and KPIs found\n- Include trends and comparisons\n- Note data quality or completeness issues\n\nFormat:\nTitle: Data Summary\nKey Metrics:\n- [Metric 1]: [Value] ([context])\n- [Metric 2]: [Value] ([context])\nTrends: [Notable trends]\nData Quality: [Any issues noted]', SummaryMode.QUICK: 'Provide a one-sentence summary capturing the essence of this document.\nKeep it under 30 words. Focus on the main purpose and key outcome.', SummaryMode.COMPREHENSIVE: 'Create a comprehensive structured summary of this document.\n\nFormat:\nTitle: [Document title]\nOverview: [2-3 sentence overview]\nKey Sections:\n1. [Section 1 name]: [Summary]\n2. [Section 2 name]: [Summary]\nKey Findings:\n- [Finding 1]\n- [Finding 2]\n- [Finding 3]\nData Highlights:\n- [Key metric/number 1]\n- [Key metric/number 2]\nConclusions: [Main conclusions]\nLimitations: [Any caveats or limitations noted]', SummaryMode.ACTION_ITEMS: 'Extract all action items, to-dos, and next steps from this document.\n\nFormat:\nTitle: Action Items Summary\nImmediate Actions:\n- [Action 1] (Priority: High/Medium/Low)\n- [Action 2] (Priority: High/Medium/Low)\nFollow-up Required:\n- [Follow-up 1]\nDeadlines Mentioned:\n- [Deadline 1]: [Date]\nResponsibilities:\n- [Person/Team]: [Their action items]', SummaryMode.RISKS: 'Identify and summarize all risks, concerns, and potential issues mentioned in this document.\n\nFormat:\nTitle: Risk Summary\nCritical Risks:\n- [Risk 1]: [Description] - Impact: [High/Medium/Low]\nWarnings/Concerns:\n- [Concern 1]\nCompliance Issues:\n- [Any compliance or regulatory concerns]\nMitigation Mentioned:\n- [Any risk mitigation strategies noted]\nOverall Risk Level: [Low/Medium/High/Critical]', SummaryMode.OPPORTUNITIES: 'Identify opportunities, growth areas, and positive developments in this document.\n\nFormat:\nTitle: Opportunities Summary\nGrowth Opportunities:\n- [Opportunity 1]: [Description] - Potential: [High/Medium/Low]\nPositive Trends:\n- [Trend 1]\nRecommendations for Action:\n- [Recommendation 1]\nQuick Wins:\n- [Any easily achievable improvements noted]'}
- L141 def generate_summarytext: str, mode: SummaryMode, tables: List[EnhancedExtractedTable]=None, metrics: List[ExtractedMetric]=None:
  - L147 docstring: "Generate a document summary in the specified mode."
  - L149 assign context_parts = [f'Document text:\n{text[:8000]}']
  - L151 if tables:
    - L152 assign table_info = '\n\nTables found:\n'
    - L153 for t in tables[:5]:
      - L154 aug assign table_info Add f"- {t.title or t.id}: {t.row_count} rows, columns: {', '.join(t.headers[:5])}\n"
    - L155 expr context_parts.append(table_info)
  - L157 if metrics:
    - L158 assign metrics_info = '\n\nKey metrics extracted:\n'
    - L159 for m in metrics[:10]:
      - L160 aug assign metrics_info Add f'- {m.name}: {m.raw_value}\n'
    - L161 expr context_parts.append(metrics_info)
  - L163 assign context = '\n'.join(context_parts)
  - L164 assign prompt = f'{SUMMARY_PROMPTS[mode]}\n\nDocument:\n{context}'
  - L166 try:
    - L167 assign client = get_openai_client()
    - L168 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description=f'summary_{mode.value}', temperature=0.3)
    - L176 assign content = response.choices[0].message.content or ''
    - L179 assign title = ''
    - L180 assign bullet_points = []
    - L181 assign key_figures = []
    - L184 assign title_match = re.search('Title:\\s*(.+?)(?:\\n|$)', content)
    - L185 if title_match:
      - L186 assign title = title_match.group(1).strip()
    - L189 assign bullet_matches = re.findall('^[\\s]*[-\u2022*]\\s*(.+?)$', content, re.MULTILINE)
    - L190 assign bullet_points = [b.strip() for b in bullet_matches if b.strip()]
    - L193 assign figure_matches = re.findall('[\\$\u20ac\xa3\xa5]?\\d[\\d,]*(?:\\.\\d+)?[%]?', content)
    - L194 assign key_figures = [{'value': f, 'context': ''} for f in figure_matches[:5]]
    - L197 assign words = len(content.split())
    - L198 assign reading_time = max(1, words / 200)
    - L200 return DocumentSummary(mode=mode, title=title or f'{mode.value.title()} Summary', content=content, bullet_points=bullet_points[:10], key_figures=key_figures, word_count=words, reading_time_minutes=reading_time)
    - L209 except Exception as e:
      - L210 expr logger.error(f'Summary generation failed: {e}')
      - L211 return DocumentSummary(mode=mode, title='Summary Generation Failed', content=f'Could not generate summary: {str(e)}')
- L218 def generate_all_summariestext: str, tables: List[EnhancedExtractedTable]=None, metrics: List[ExtractedMetric]=None:
  - L223 docstring: "Generate all summary types."
  - L224 assign summaries = {}
  - L225 for mode in SummaryMode:
    - L226 assign summaries[mode.value] = generate_summary(text, mode, tables, metrics)
  - L227 return summaries
- L234 def analyze_sentimenttext: str:
  - L235 docstring: "Analyze document sentiment and tone."
  - L236 assign prompt = f'Analyze the sentiment and tone of this document.\n\nDocument:\n{text[:8000]}\n\nProvide analysis in JSON format:\n```json\n{{\n  "overall_sentiment": "positive|negative|neutral|very_positive|very_negative",\n  "overall_score": 0.5,  // -1.0 (very negative) to 1.0 (very positive)\n  "confidence": 0.85,\n  "emotional_tone": "formal|casual|urgent|optimistic|pessimistic|neutral|analytical|persuasive",\n  "urgency_level": "low|normal|high|critical",\n  "section_sentiments": [\n    {{"section": "Introduction", "sentiment": "positive", "score": 0.6}},\n    {{"section": "Financial Results", "sentiment": "negative", "score": -0.3}}\n  ],\n  "positive_phrases": ["exceeded expectations", "strong growth"],\n  "negative_phrases": ["challenges ahead", "declining margins"],\n  "bias_indicators": ["overly optimistic language", "missing context for claims"]\n}}\n```\n\nBe objective and thorough in your analysis.'
  - L261 try:
    - L262 assign client = get_openai_client()
    - L263 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='sentiment_analysis', temperature=0.2)
    - L271 assign raw_text = response.choices[0].message.content or ''
    - L272 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L274 if json_match:
      - L275 assign data = json.loads(json_match.group())
      - L277 assign sentiment_map = {'very_positive': SentimentLevel.VERY_POSITIVE, 'positive': SentimentLevel.POSITIVE, 'neutral': SentimentLevel.NEUTRAL, 'negative': SentimentLevel.NEGATIVE, 'very_negative': SentimentLevel.VERY_NEGATIVE}
      - L285 return SentimentAnalysis(overall_sentiment=sentiment_map.get(data.get('overall_sentiment', 'neutral').lower(), SentimentLevel.NEUTRAL), overall_score=float(data.get('overall_score', 0)), confidence=float(data.get('confidence', 0.8)), section_sentiments=data.get('section_sentiments', []), emotional_tone=data.get('emotional_tone', 'neutral'), urgency_level=data.get('urgency_level', 'normal'), bias_indicators=data.get('bias_indicators', []), key_phrases={'positive': data.get('positive_phrases', []), 'negative': data.get('negative_phrases', [])})
    - L301 except Exception as e:
      - L302 expr logger.warning(f'Sentiment analysis failed: {e}')
  - L304 return SentimentAnalysis(overall_sentiment=SentimentLevel.NEUTRAL, overall_score=0.0, confidence=0.5)
- L315 def analyze_texttext: str:
  - L316 docstring: "Perform text analytics including readability and keyword extraction."
  - L318 assign words = text.split()
  - L319 assign word_count = len(words)
  - L320 assign sentences = re.split('[.!?]+', text)
  - L321 assign sentence_count = len([s for s in sentences if s.strip()])
  - L322 assign paragraphs = text.split('\n\n')
  - L323 assign paragraph_count = len([p for p in paragraphs if p.strip()])
  - L325 assign avg_sentence_length = word_count / max(sentence_count, 1)
  - L328 assign syllables = sum((_count_syllables(word) for word in words))
  - L329 if word_count > 0 and sentence_count > 0:
    - L330 assign flesch_score = 206.835 - 1.015 * (word_count / sentence_count) - 84.6 * (syllables / word_count)
    - L331 assign flesch_score = max(0, min(100, flesch_score))
    - L333 else:
      - L333 assign flesch_score = 50
  - L336 if flesch_score >= 90:
    - L337 assign grade = '5th grade'
    - L338 else:
      - L338 if flesch_score >= 80:
        - L339 assign grade = '6th grade'
        - L340 else:
          - L340 if flesch_score >= 70:
            - L341 assign grade = '7th grade'
            - L342 else:
              - L342 if flesch_score >= 60:
                - L343 assign grade = '8th-9th grade'
                - L344 else:
                  - L344 if flesch_score >= 50:
                    - L345 assign grade = '10th-12th grade'
                    - L346 else:
                      - L346 if flesch_score >= 30:
                        - L347 assign grade = 'College'
                        - L349 else:
                          - L349 assign grade = 'College graduate'
  - L352 assign stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'and', 'or', 'but', 'if', 'because', 'as', 'of', 'at', 'by', 'for', 'with', 'to', 'from', 'in', 'on', 'not', 'no', 'so', 'than', 'too', 'very'}
  - L359 assign word_freq = Counter((w.lower() for w in re.findall('\\b[a-zA-Z]{3,}\\b', text) if w.lower() not in stopwords))
  - L361 assign keywords = [{'word': word, 'frequency': count, 'importance': min(1.0, count / 50)} for word, count in word_freq.most_common(20)]
  - L367 assign language = 'en'
  - L369 return TextAnalytics(word_count=word_count, sentence_count=sentence_count, paragraph_count=paragraph_count, avg_sentence_length=round(avg_sentence_length, 1), readability_score=round(flesch_score, 1), readability_grade=grade, keywords=keywords, language=language, language_confidence=0.95)
- L382 def _count_syllablesword: str:
  - L383 docstring: "Count syllables in a word."
  - L384 assign word = word.lower()
  - L385 assign vowels = 'aeiouy'
  - L386 assign count = 0
  - L387 assign prev_vowel = False
  - L389 for char in word:
    - L390 assign is_vowel = char in vowels
    - L391 if is_vowel and (not prev_vowel):
      - L392 aug assign count Add 1
    - L393 assign prev_vowel = is_vowel
  - L396 if word.endswith('e'):
    - L397 aug assign count Sub 1
  - L399 return max(1, count)
- L406 def analyze_statisticstables: List[EnhancedExtractedTable]:
  - L407 docstring: "Perform statistical analysis on numeric data in tables."
  - L408 assign column_stats = {}
  - L409 assign correlations = []
  - L410 assign outliers = []
  - L411 assign distributions = {}
  - L412 assign trends = []
  - L414 for table in tables:
    - L415 assign numeric_columns = {}
    - L418 for (col_idx, (header, dtype)) in enumerate(zip(table.headers, table.data_types)):
      - L419 if dtype == 'numeric':
        - L420 assign values = []
        - L421 for (row_idx, row) in enumerate(table.rows):
          - L422 if col_idx < len(row):
            - L423 try:
              - L424 assign val = float(str(row[col_idx]).replace(',', '').replace('$', '').replace('%', ''))
              - L425 expr values.append((row_idx, val))
              - L426 except (ValueError, TypeError):
                - L427 pass
        - L429 if len(values) >= 3:
          - L430 assign numeric_columns[header] = values
    - L433 for (header, indexed_values) in numeric_columns.items():
      - L434 assign values = [v for _, v in indexed_values]
      - L435 assign n = len(values)
      - L437 assign mean = sum(values) / n
      - L438 assign sorted_vals = sorted(values)
      - L439 assign median = sorted_vals[n // 2] if n % 2 == 1 else (sorted_vals[n // 2 - 1] + sorted_vals[n // 2]) / 2
      - L441 assign variance = sum(((x - mean) ** 2 for x in values)) / n
      - L442 assign std = math.sqrt(variance)
      - L445 assign p25 = sorted_vals[int(n * 0.25)]
      - L446 assign p75 = sorted_vals[int(n * 0.75)]
      - L448 assign column_stats[f'{table.id}.{header}'] = {'count': n, 'mean': round(mean, 4), 'median': round(median, 4), 'std': round(std, 4), 'min': min(values), 'max': max(values), 'p25': p25, 'p75': p75}
      - L460 if std > 0:
        - L461 for (row_idx, val) in indexed_values:
          - L462 assign zscore = abs((val - mean) / std)
          - L463 if zscore > 2:
            - L464 expr outliers.append({'table': table.id, 'column': header, 'row_index': row_idx, 'value': val, 'zscore': round(zscore, 2)})
      - L473 if n >= 5:
        - L474 assign first_half = sum(values[:n // 2]) / (n // 2)
        - L475 assign second_half = sum(values[n // 2:]) / (n - n // 2)
        - L477 if second_half > first_half * 1.1:
          - L478 assign trend_dir = 'increasing'
          - L479 else:
            - L479 if second_half < first_half * 0.9:
              - L480 assign trend_dir = 'decreasing'
              - L482 else:
                - L482 assign trend_dir = 'stable'
        - L484 expr trends.append({'table': table.id, 'column': header, 'trend_direction': trend_dir, 'change_ratio': round(second_half / first_half, 4) if first_half != 0 else 0})
    - L492 assign col_names = list(numeric_columns.keys())
    - L493 for i in range(len(col_names)):
      - L494 for j in range(i + 1, len(col_names)):
        - L495 assign (col1, col2) = (col_names[i], col_names[j])
        - L496 assign vals1 = [v for _, v in numeric_columns[col1]]
        - L497 assign vals2 = [v for _, v in numeric_columns[col2]]
        - L500 assign min_len = min(len(vals1), len(vals2))
        - L501 if min_len >= 5:
          - L502 assign corr = _pearson_correlation(vals1[:min_len], vals2[:min_len])
          - L503 if abs(corr) > 0.3:
            - L504 expr correlations.append({'table': table.id, 'column1': col1, 'column2': col2, 'correlation': round(corr, 4)})
  - L511 return StatisticalAnalysis(column_stats=column_stats, correlations=correlations, outliers=outliers[:20], distributions=distributions, trends=trends)
- L520 def _pearson_correlationx: List[float], y: List[float]:
  - L521 docstring: "Calculate Pearson correlation coefficient."
  - L522 assign n = len(x)
  - L523 if n < 2:
    - L524 return 0
  - L526 assign mean_x = sum(x) / n
  - L527 assign mean_y = sum(y) / n
  - L529 assign numerator = sum(((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y)))
  - L530 assign denom_x = math.sqrt(sum(((xi - mean_x) ** 2 for xi in x)))
  - L531 assign denom_y = math.sqrt(sum(((yi - mean_y) ** 2 for yi in y)))
  - L533 if denom_x == 0 or denom_y == 0:
    - L534 return 0
  - L536 return numerator / (denom_x * denom_y)
- L543 def analyze_financialstext: str, metrics: List[ExtractedMetric], tables: List[EnhancedExtractedTable]:
  - L548 docstring: "Perform financial analysis using LLM."
  - L550 assign metrics_context = '\n'.join([f'- {m.name}: {m.raw_value}' + (f' ({m.change}% {m.change_direction})' if m.change else '') for m in metrics[:20]])
  - L555 assign prompt = f'Analyze this document for financial insights. Calculate ratios where data is available.\n\nMetrics found:\n{metrics_context}\n\nDocument excerpt:\n{text[:5000]}\n\nReturn JSON:\n```json\n{{\n  "currency": "USD",\n  "gross_margin": 0.35,\n  "operating_margin": 0.20,\n  "net_margin": 0.15,\n  "revenue_growth": 0.12,\n  "profit_growth": 0.08,\n  "yoy_comparison": {{"revenue": {{"current": 1000000, "previous": 900000, "change": 0.11}}}},\n  "variance_analysis": [\n    {{"metric": "Revenue", "actual": 1000000, "budget": 950000, "variance": 50000, "variance_pct": 5.26}}\n  ],\n  "insights": [\n    "Revenue grew 11% year-over-year, exceeding industry average of 8%",\n    "Operating margin improved despite increased costs"\n  ],\n  "warnings": [\n    "Debt-to-equity ratio increased significantly",\n    "Cash reserves declining quarter-over-quarter"\n  ]\n}}\n```\n\nOnly include metrics you can calculate or find. Use null for unavailable data.'
  - L589 try:
    - L590 assign client = get_openai_client()
    - L591 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='financial_analysis', temperature=0.2)
    - L599 assign raw_text = response.choices[0].message.content or ''
    - L600 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L602 if json_match:
      - L603 assign data = json.loads(json_match.group())
      - L604 return FinancialAnalysis(metrics_found=len(metrics), currency=data.get('currency', 'USD'), gross_margin=data.get('gross_margin'), operating_margin=data.get('operating_margin'), net_margin=data.get('net_margin'), revenue_growth=data.get('revenue_growth'), profit_growth=data.get('profit_growth'), yoy_comparison=data.get('yoy_comparison', {}), variance_analysis=data.get('variance_analysis', []), insights=data.get('insights', []), warnings=data.get('warnings', []))
    - L617 except Exception as e:
      - L618 expr logger.warning(f'Financial analysis failed: {e}')
  - L620 return FinancialAnalysis(metrics_found=len(metrics))
- L627 def generate_insightstext: str, metrics: List[ExtractedMetric], tables: List[EnhancedExtractedTable], sentiment: SentimentAnalysis, stats: StatisticalAnalysis:
  - L634 docstring: "Generate insights, risks, opportunities, and action items."
  - L635 assign context = f"Document analysis context:\n- Sentiment: {sentiment.overall_sentiment.value} (score: {sentiment.overall_score})\n- Urgency: {sentiment.urgency_level}\n- Key metrics: {len(metrics)}\n- Tables: {len(tables)}\n- Outliers detected: {len(stats.outliers)}\n- Trends: {[t['trend_direction'] for t in stats.trends]}\n\nMetrics:\n{chr(10).join([f'- {m.name}: {m.raw_value}' for m in metrics[:15]])}\n\nDocument excerpt:\n{text[:4000]}"
  - L649 assign prompt = f'{context}\n\nAnalyze this document and generate:\n1. Key insights (findings, trends, anomalies)\n2. Risks and concerns\n3. Opportunities\n4. Recommended action items\n\nReturn JSON:\n```json\n{{\n  "insights": [\n    {{\n      "type": "finding|trend|anomaly|recommendation|warning",\n      "title": "Short title",\n      "description": "Detailed description",\n      "priority": "critical|high|medium|low",\n      "confidence": 0.85,\n      "supporting_data": ["Revenue: $1.5M", "Growth: 15%"],\n      "actionable": true,\n      "suggested_actions": ["Review pricing strategy"]\n    }}\n  ],\n  "risks": [\n    {{\n      "title": "Declining Cash Reserves",\n      "description": "Cash reserves have decreased 20% this quarter",\n      "risk_level": "high",\n      "category": "financial",\n      "probability": 0.7,\n      "impact": 0.8,\n      "mitigation_suggestions": ["Reduce non-essential spending"]\n    }}\n  ],\n  "opportunities": [\n    {{\n      "title": "Market Expansion",\n      "description": "Emerging market shows 30% growth potential",\n      "opportunity_type": "growth",\n      "potential_value": "$500K annual revenue",\n      "confidence": 0.75,\n      "requirements": ["Localization", "Partner network"],\n      "suggested_actions": ["Conduct market research"]\n    }}\n  ],\n  "action_items": [\n    {{\n      "title": "Review Q3 budget",\n      "description": "Budget variance exceeds 10% threshold",\n      "priority": "high",\n      "category": "financial",\n      "expected_outcome": "Realigned budget for Q4"\n    }}\n  ]\n}}\n```\n\nBe specific and actionable. Base insights on actual data found.'
  - L708 assign insights = []
  - L709 assign risks = []
  - L710 assign opportunities = []
  - L711 assign action_items = []
  - L713 try:
    - L714 assign client = get_openai_client()
    - L715 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='insights_generation', temperature=0.3)
    - L723 assign raw_text = response.choices[0].message.content or ''
    - L724 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L726 if json_match:
      - L727 assign data = json.loads(json_match.group())
      - L730 for item in data.get('insights', []):
        - L731 assign priority_map = {'critical': Priority.CRITICAL, 'high': Priority.HIGH, 'medium': Priority.MEDIUM, 'low': Priority.LOW}
        - L733 expr insights.append(Insight(id=f'ins_{uuid.uuid4().hex[:8]}', type=item.get('type', 'finding'), title=item.get('title', ''), description=item.get('description', ''), priority=priority_map.get(item.get('priority', 'medium'), Priority.MEDIUM), confidence=item.get('confidence', 0.8), supporting_data=item.get('supporting_data', []), actionable=item.get('actionable', False), suggested_actions=item.get('suggested_actions', [])))
      - L746 for item in data.get('risks', []):
        - L747 assign risk_map = {'critical': RiskLevel.CRITICAL, 'high': RiskLevel.HIGH, 'medium': RiskLevel.MEDIUM, 'low': RiskLevel.LOW, 'minimal': RiskLevel.MINIMAL}
        - L749 assign risk_level = risk_map.get(item.get('risk_level', 'medium'), RiskLevel.MEDIUM)
        - L750 assign prob = item.get('probability', 0.5)
        - L751 assign impact = item.get('impact', 0.5)
        - L753 expr risks.append(RiskItem(id=f'risk_{uuid.uuid4().hex[:8]}', title=item.get('title', ''), description=item.get('description', ''), risk_level=risk_level, category=item.get('category', 'general'), probability=prob, impact=impact, risk_score=prob * impact, mitigation_suggestions=item.get('mitigation_suggestions', [])))
      - L766 for item in data.get('opportunities', []):
        - L767 expr opportunities.append(OpportunityItem(id=f'opp_{uuid.uuid4().hex[:8]}', title=item.get('title', ''), description=item.get('description', ''), opportunity_type=item.get('opportunity_type', 'growth'), potential_value=item.get('potential_value'), confidence=item.get('confidence', 0.7), requirements=item.get('requirements', []), suggested_actions=item.get('suggested_actions', [])))
      - L779 for item in data.get('action_items', []):
        - L780 assign priority_map = {'critical': Priority.CRITICAL, 'high': Priority.HIGH, 'medium': Priority.MEDIUM, 'low': Priority.LOW}
        - L782 expr action_items.append(ActionItem(id=f'act_{uuid.uuid4().hex[:8]}', title=item.get('title', ''), description=item.get('description', ''), priority=priority_map.get(item.get('priority', 'medium'), Priority.MEDIUM), category=item.get('category', 'general'), expected_outcome=item.get('expected_outcome')))
    - L791 except Exception as e:
      - L792 expr logger.warning(f'Insights generation failed: {e}')
  - L794 return (insights, risks, opportunities, action_items)
- L801 def compare_documentstext1: str, text2: str, metrics1: List[ExtractedMetric]=None, metrics2: List[ExtractedMetric]=None:
  - L807 docstring: "Compare two documents and identify differences."
  - L808 assign metrics1 = metrics1 or []
  - L809 assign metrics2 = metrics2 or []
  - L812 assign metrics1_dict = {m.name: m for m in metrics1}
  - L813 assign metrics2_dict = {m.name: m for m in metrics2}
  - L815 assign metric_changes = []
  - L816 assign all_metric_names = set(metrics1_dict.keys()) | set(metrics2_dict.keys())
  - L818 for name in all_metric_names:
    - L819 assign m1 = metrics1_dict.get(name)
    - L820 assign m2 = metrics2_dict.get(name)
    - L822 if m1 and m2:
      - L823 try:
        - L824 assign v1 = float(m1.value) if isinstance(m1.value, (int, float)) else 0
        - L825 assign v2 = float(m2.value) if isinstance(m2.value, (int, float)) else 0
        - L826 assign change = (v2 - v1) / v1 * 100 if v1 != 0 else 0
        - L827 expr metric_changes.append({'metric': name, 'value_doc1': m1.raw_value, 'value_doc2': m2.raw_value, 'change_pct': round(change, 2)})
        - L833 except (ValueError, TypeError):
          - L834 pass
      - L835 else:
        - L835 if m1:
          - L836 expr metric_changes.append({'metric': name, 'value_doc1': m1.raw_value, 'value_doc2': None, 'status': 'removed'})
          - L842 else:
            - L842 if m2:
              - L843 expr metric_changes.append({'metric': name, 'value_doc1': None, 'value_doc2': m2.raw_value, 'status': 'added'})
  - L850 assign prompt = f'Compare these two document excerpts and identify key differences.\n\nDocument 1:\n{text1[:3000]}\n\nDocument 2:\n{text2[:3000]}\n\nReturn JSON:\n```json\n{{\n  "similarity_score": 0.75,\n  "additions": [\n    {{"location": "Section 3", "content": "New paragraph about...", "significance": "high"}}\n  ],\n  "deletions": [\n    {{"location": "Section 2", "content": "Removed reference to...", "significance": "medium"}}\n  ],\n  "modifications": [\n    {{"location": "Section 1", "original": "Revenue was $1M", "modified": "Revenue was $1.2M", "significance": "high"}}\n  ],\n  "change_summary": "Brief summary of overall changes",\n  "significant_changes": ["Key change 1", "Key change 2"]\n}}\n```'
  - L876 try:
    - L877 assign client = get_openai_client()
    - L878 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='document_comparison', temperature=0.2)
    - L886 assign raw_text = response.choices[0].message.content or ''
    - L887 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L889 if json_match:
      - L890 assign data = json.loads(json_match.group())
      - L891 return ComparativeAnalysis(comparison_type='version_diff', documents_compared=['document_1', 'document_2'], additions=data.get('additions', []), deletions=data.get('deletions', []), modifications=data.get('modifications', []), metric_changes=metric_changes, similarity_score=data.get('similarity_score', 0.5), change_summary=data.get('change_summary', ''), significant_changes=data.get('significant_changes', []))
    - L902 except Exception as e:
      - L903 expr logger.warning(f'Document comparison failed: {e}')
  - L905 return ComparativeAnalysis(comparison_type='version_diff', documents_compared=['document_1', 'document_2'], metric_changes=metric_changes)
- L916 class AnalysisEngineService:
  - L917 docstring: "Orchestrates all analysis engines."
  - L919 def run_all_analysesself, text: str, tables: List[EnhancedExtractedTable], metrics: List[ExtractedMetric]:
    - L925 docstring: "Run all analysis engines."
    - L927 assign summaries = generate_all_summaries(text, tables, metrics)
    - L930 assign sentiment = analyze_sentiment(text)
    - L933 assign text_analytics = analyze_text(text)
    - L936 assign statistical = analyze_statistics(tables)
    - L939 assign financial = analyze_financials(text, metrics, tables)
    - L942 assign (insights, risks, opportunities, action_items) = generate_insights(text, metrics, tables, sentiment, statistical)
    - L946 return {'summaries': summaries, 'sentiment': sentiment, 'text_analytics': text_analytics, 'statistical_analysis': statistical, 'financial_analysis': financial, 'insights': insights, 'risks': risks, 'opportunities': opportunities, 'action_items': action_items}

## backend\app\features\analyze\services\data_transform_export.py
- L2 docstring: "\nData Transformation & Export Service - Smart data cleaning and multi-format ex..."
- L10 from __future__ import annotations
- L12 import csv
- L13 import io
- L14 import json
- L15 import logging
- L16 import math
- L17 import re
- L18 import uuid
- L19 from datetime import datetime
- L20 from pathlib import Path
- L21 from typing import Any, Dict, List, Optional, Tuple, Union
- L23 from backend.app.features.analyze.schemas.enhanced_analysis import DataQualityReport, DataTransformation, EnhancedAnalysisResult, EnhancedChartSpec, EnhancedExtractedTable, ExportConfiguration, ExportFormat
- L33 assign logger = logging.getLogger('neura.analyze.export')
- L40 def assess_data_qualitytables: List[EnhancedExtractedTable]:
  - L41 docstring: "Assess data quality across all tables."
  - L42 assign total_rows = sum((t.row_count for t in tables))
  - L43 assign total_columns = sum((len(t.headers) for t in tables))
  - L45 annotated assign missing_values: Dict[str, int] = {}
  - L46 annotated assign missing_percentage: Dict[str, float] = {}
  - L47 annotated assign unique_values_per_column: Dict[str, int] = {}
  - L48 annotated assign invalid_values: Dict[str, List[Any]] = {}
  - L49 annotated assign type_mismatches: Dict[str, List[int]] = {}
  - L50 annotated assign format_inconsistencies: Dict[str, List[str]] = {}
  - L51 annotated assign outliers_detected: Dict[str, List[int]] = {}
  - L53 assign duplicate_rows = 0
  - L55 for table in tables:
    - L57 assign seen_rows = set()
    - L58 for row in table.rows:
      - L59 assign row_key = tuple((str(v) for v in row))
      - L60 if row_key in seen_rows:
        - L61 aug assign duplicate_rows Add 1
      - L62 expr seen_rows.add(row_key)
    - L65 for (col_idx, (header, dtype)) in enumerate(zip(table.headers, table.data_types)):
      - L66 assign col_key = f'{table.id}.{header}'
      - L69 assign values = []
      - L70 assign missing_count = 0
      - L71 assign unique_set = set()
      - L72 assign numeric_values = []
      - L74 for (row_idx, row) in enumerate(table.rows):
        - L75 if col_idx < len(row):
          - L76 assign val = row[col_idx]
          - L77 expr values.append((row_idx, val))
          - L80 if val is None or str(val).strip() == '' or str(val).lower() in ('null', 'n/a', 'na', '-'):
            - L81 aug assign missing_count Add 1
            - L83 else:
              - L83 expr unique_set.add(str(val))
              - L86 if dtype == 'numeric':
                - L87 try:
                  - L88 assign cleaned = re.sub('[$,% ]', '', str(val))
                  - L89 assign num_val = float(cleaned)
                  - L90 expr numeric_values.append((row_idx, num_val))
                  - L91 except (ValueError, TypeError):
                    - L92 if col_key not in type_mismatches:
                      - L93 assign type_mismatches[col_key] = []
                    - L94 expr type_mismatches[col_key].append(row_idx)
      - L97 if missing_count > 0:
        - L98 assign missing_values[col_key] = missing_count
        - L99 assign missing_percentage[col_key] = round(missing_count / len(table.rows) * 100, 2)
      - L101 assign unique_values_per_column[col_key] = len(unique_set)
      - L104 if numeric_values and len(numeric_values) >= 5:
        - L105 assign vals = [v for _, v in numeric_values]
        - L106 assign mean = sum(vals) / len(vals)
        - L107 assign std = math.sqrt(sum(((v - mean) ** 2 for v in vals)) / len(vals))
        - L109 if std > 0:
          - L110 assign outlier_indices = []
          - L111 for (row_idx, val) in numeric_values:
            - L112 if abs((val - mean) / std) > 3:
              - L113 expr outlier_indices.append(row_idx)
          - L115 if outlier_indices:
            - L116 assign outliers_detected[col_key] = outlier_indices
      - L119 if dtype == 'text' and unique_set:
        - L121 assign patterns_found = set()
        - L122 for val in list(unique_set)[:50]:
          - L123 if val.isupper():
            - L124 expr patterns_found.add('UPPERCASE')
            - L125 else:
              - L125 if val.islower():
                - L126 expr patterns_found.add('lowercase')
                - L127 else:
                  - L127 if val.istitle():
                    - L128 expr patterns_found.add('Title Case')
                    - L130 else:
                      - L130 expr patterns_found.add('Mixed case')
        - L132 if len(patterns_found) > 1:
          - L133 assign format_inconsistencies[col_key] = list(patterns_found)
  - L136 assign total_issues = sum(missing_values.values()) + duplicate_rows * 2 + sum((len(v) for v in type_mismatches.values())) + sum((len(v) for v in outliers_detected.values()))
  - L143 assign max_possible_issues = total_rows * total_columns if total_rows and total_columns else 1
  - L144 assign quality_score = max(0, 1 - total_issues / max_possible_issues)
  - L147 assign recommendations = []
  - L148 if missing_values:
    - L149 assign high_missing = [k for k, v in missing_percentage.items() if v > 20]
    - L150 if high_missing:
      - L151 expr recommendations.append(f"High missing data in columns: {', '.join(high_missing[:5])}")
  - L152 if duplicate_rows > 0:
    - L153 expr recommendations.append(f'Found {duplicate_rows} duplicate rows - consider deduplication')
  - L154 if type_mismatches:
    - L155 expr recommendations.append(f'Type mismatches detected in {len(type_mismatches)} columns')
  - L156 if outliers_detected:
    - L157 expr recommendations.append(f'Outliers detected in {len(outliers_detected)} numeric columns')
  - L159 return DataQualityReport(total_rows=total_rows, total_columns=total_columns, missing_values=missing_values, missing_percentage=missing_percentage, duplicate_rows=duplicate_rows, unique_values_per_column=unique_values_per_column, invalid_values=invalid_values, type_mismatches=type_mismatches, format_inconsistencies=format_inconsistencies, outliers_detected=outliers_detected, quality_score=round(quality_score, 3), recommendations=recommendations)
- L179 def clean_tabletable: EnhancedExtractedTable, operations: List[str]=None:
  - L183 docstring: "Apply cleaning operations to a table."
  - L184 if operations is None:
    - L185 assign operations = ['trim', 'normalize_case', 'fill_missing']
  - L187 assign cleaned_rows = [list(row) for row in table.rows]
  - L189 for (col_idx, (header, dtype)) in enumerate(zip(table.headers, table.data_types)):
    - L190 for (row_idx, row) in enumerate(cleaned_rows):
      - L191 if col_idx >= len(row):
        - L192 continue
      - L194 assign val = row[col_idx]
      - L196 if 'trim' in operations:
        - L197 if isinstance(val, str):
          - L198 assign val = val.strip()
      - L200 if 'normalize_case' in operations:
        - L201 if isinstance(val, str) and dtype == 'text':
          - L202 assign val = val.title()
      - L204 if 'fill_missing' in operations:
        - L205 if val is None or str(val).strip() == '' or str(val).lower() in ('null', 'n/a', 'na'):
          - L206 if dtype == 'numeric':
            - L208 assign col_values = []
            - L209 for r in table.rows:
              - L210 if col_idx < len(r):
                - L211 try:
                  - L212 expr col_values.append(float(str(r[col_idx]).replace(',', '').replace('$', '')))
                  - L213 except (ValueError, TypeError):
                    - L214 pass
            - L215 if col_values:
              - L216 assign sorted_vals = sorted(col_values)
              - L217 assign val = sorted_vals[len(sorted_vals) // 2]
              - L219 else:
                - L219 assign val = 0
            - L221 else:
              - L221 assign val = ''
      - L223 if 'normalize_numbers' in operations:
        - L224 if dtype == 'numeric':
          - L225 try:
            - L226 assign cleaned = re.sub('[$,% ]', '', str(val))
            - L227 assign val = float(cleaned)
            - L228 except (ValueError, TypeError):
              - L229 pass
      - L231 assign cleaned_rows[row_idx][col_idx] = val
  - L233 return EnhancedExtractedTable(id=table.id, title=table.title, headers=table.headers, rows=cleaned_rows, data_types=table.data_types, source_page=table.source_page, source_sheet=table.source_sheet, confidence=table.confidence, row_count=len(cleaned_rows), column_count=len(table.headers), has_totals_row=table.has_totals_row, has_header_row=table.has_header_row, statistics=table.statistics)
- L250 def apply_transformationtable: EnhancedExtractedTable, transformation: DataTransformation:
  - L254 docstring: "Apply a single transformation to a table."
  - L255 if transformation.operation == 'clean':
    - L256 return clean_table(table, transformation.parameters.get('operations'))
    - L258 else:
      - L258 if transformation.operation == 'normalize':
        - L260 for col_name in transformation.source_columns:
          - L261 if col_name not in table.headers:
            - L262 continue
          - L263 assign col_idx = table.headers.index(col_name)
          - L265 assign values = []
          - L266 for row in table.rows:
            - L267 if col_idx < len(row):
              - L268 try:
                - L269 expr values.append(float(str(row[col_idx]).replace(',', '')))
                - L270 except (ValueError, TypeError):
                  - L271 expr values.append(None)
          - L273 assign valid_values = [v for v in values if v is not None]
          - L274 if not valid_values:
            - L275 continue
          - L277 assign min_val = min(valid_values)
          - L278 assign max_val = max(valid_values)
          - L279 assign range_val = max_val - min_val if max_val != min_val else 1
          - L281 for (row_idx, row) in enumerate(table.rows):
            - L282 if col_idx < len(row) and values[row_idx] is not None:
              - L283 assign row[col_idx] = round((values[row_idx] - min_val) / range_val, 4)
        - L285 else:
          - L285 if transformation.operation == 'aggregate':
            - L287 assign group_col = transformation.parameters.get('group_by')
            - L288 assign agg_func = transformation.parameters.get('function', 'sum')
            - L290 if group_col not in table.headers:
              - L291 return table
            - L293 assign group_idx = table.headers.index(group_col)
            - L294 assign value_cols = transformation.source_columns
            - L296 annotated assign groups: Dict[str, Dict[str, List[float]]] = {}
            - L297 for row in table.rows:
              - L298 if group_idx >= len(row):
                - L299 continue
              - L300 assign group_key = str(row[group_idx])
              - L302 if group_key not in groups:
                - L303 assign groups[group_key] = {col: [] for col in value_cols}
              - L305 for col in value_cols:
                - L306 if col in table.headers:
                  - L307 assign col_idx = table.headers.index(col)
                  - L308 if col_idx < len(row):
                    - L309 try:
                      - L310 expr groups[group_key][col].append(float(str(row[col_idx]).replace(',', '')))
                      - L311 except (ValueError, TypeError):
                        - L312 pass
            - L315 assign new_headers = [group_col] + value_cols
            - L316 assign new_rows = []
            - L318 for (group_key, values_dict) in groups.items():
              - L319 assign new_row = [group_key]
              - L320 for col in value_cols:
                - L321 assign vals = values_dict.get(col, [])
                - L322 if not vals:
                  - L323 expr new_row.append(0)
                  - L324 else:
                    - L324 if agg_func == 'sum':
                      - L325 expr new_row.append(sum(vals))
                      - L326 else:
                        - L326 if agg_func == 'mean':
                          - L327 expr new_row.append(sum(vals) / len(vals))
                          - L328 else:
                            - L328 if agg_func == 'count':
                              - L329 expr new_row.append(len(vals))
                              - L330 else:
                                - L330 if agg_func == 'min':
                                  - L331 expr new_row.append(min(vals))
                                  - L332 else:
                                    - L332 if agg_func == 'max':
                                      - L333 expr new_row.append(max(vals))
                                      - L335 else:
                                        - L335 expr new_row.append(sum(vals))
              - L337 expr new_rows.append(new_row)
            - L339 return EnhancedExtractedTable(id=f'{table.id}_agg', title=f'{table.title or table.id} (Aggregated)', headers=new_headers, rows=new_rows, data_types=['text'] + ['numeric'] * len(value_cols), row_count=len(new_rows), column_count=len(new_headers))
  - L349 return table
- L356 def export_to_csvtables: List[EnhancedExtractedTable], include_headers: bool=True:
  - L360 docstring: "Export tables to CSV format."
  - L361 assign output = io.StringIO()
  - L362 assign writer = csv.writer(output)
  - L364 for table in tables:
    - L365 if include_headers:
      - L366 expr writer.writerow([f'# Table: {table.title or table.id}'])
      - L367 expr writer.writerow(table.headers)
    - L369 for row in table.rows:
      - L370 expr writer.writerow(row)
    - L372 expr writer.writerow([])
  - L374 return output.getvalue()
- L377 def export_to_jsonresult: EnhancedAnalysisResult, include_raw_data: bool=True:
  - L381 docstring: "Export analysis result to JSON."
  - L382 assign data = result.dict()
  - L384 if not include_raw_data:
    - L386 for table in data.get('tables', []):
      - L387 assign table['rows'] = table['rows'][:10]
  - L389 return json.dumps(data, indent=2, default=str)
- L392 def export_to_markdownresult: EnhancedAnalysisResult:
  - L393 docstring: "Export analysis result to Markdown format."
  - L394 assign lines = []
  - L397 expr lines.append(f'# Analysis Report: {result.document_name}')
  - L398 expr lines.append(f"\n*Generated: {result.created_at.strftime('%Y-%m-%d %H:%M')}*\n")
  - L401 if 'executive' in result.summaries:
    - L402 expr lines.append('## Executive Summary\n')
    - L403 expr lines.append(result.summaries['executive'].content)
    - L404 expr lines.append('')
  - L407 if result.metrics:
    - L408 expr lines.append('## Key Metrics\n')
    - L409 for metric in result.metrics[:10]:
      - L410 assign change_str = f' ({metric.change:+.1f}%)' if metric.change else ''
      - L411 expr lines.append(f'- **{metric.name}**: {metric.raw_value}{change_str}')
    - L412 expr lines.append('')
  - L415 if result.tables:
    - L416 expr lines.append('## Data Tables\n')
    - L417 for table in result.tables[:5]:
      - L418 expr lines.append(f'### {table.title or table.id}\n')
      - L420 expr lines.append('| ' + ' | '.join(table.headers) + ' |')
      - L421 expr lines.append('| ' + ' | '.join(['---'] * len(table.headers)) + ' |')
      - L423 for row in table.rows[:10]:
        - L424 expr lines.append('| ' + ' | '.join((str(v) for v in row)) + ' |')
      - L425 if len(table.rows) > 10:
        - L426 expr lines.append(f'\n*...and {len(table.rows) - 10} more rows*\n')
      - L427 expr lines.append('')
  - L430 if result.insights:
    - L431 expr lines.append('## Key Insights\n')
    - L432 for insight in result.insights:
      - L433 expr lines.append(f'### {insight.title}')
      - L434 expr lines.append(f'\n{insight.description}\n')
      - L435 if insight.suggested_actions:
        - L436 expr lines.append('**Suggested Actions:**')
        - L437 for action in insight.suggested_actions:
          - L438 expr lines.append(f'- {action}')
      - L439 expr lines.append('')
  - L442 if result.risks:
    - L443 expr lines.append('## Risks Identified\n')
    - L444 for risk in result.risks:
      - L445 expr lines.append(f'- **{risk.title}** ({risk.risk_level.value}): {risk.description}')
    - L446 expr lines.append('')
  - L449 if result.opportunities:
    - L450 expr lines.append('## Opportunities\n')
    - L451 for opp in result.opportunities:
      - L452 expr lines.append(f'- **{opp.title}**: {opp.description}')
    - L453 expr lines.append('')
  - L455 return '\n'.join(lines)
- L458 def export_to_htmlresult: EnhancedAnalysisResult:
  - L459 docstring: "Export analysis result to HTML format."
  - L460 assign html_parts = ['<!DOCTYPE html>\n<html>\n<head>\n    <meta charset="UTF-8">\n    <title>Analysis Report</title>\n    <style>\n        body { font-family: -apple-system, BlinkMacSystemFont, \'Segoe UI\', Roboto, sans-serif; margin: 40px; background: #f5f5f5; }\n        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n        h1 { color: #1a1a2e; border-bottom: 3px solid #4f46e5; padding-bottom: 10px; }\n        h2 { color: #4f46e5; margin-top: 30px; }\n        h3 { color: #374151; }\n        .metric { display: inline-block; background: #f0f9ff; padding: 15px 25px; margin: 5px; border-radius: 8px; border-left: 4px solid #4f46e5; }\n        .metric-value { font-size: 24px; font-weight: bold; color: #1a1a2e; }\n        .metric-name { color: #6b7280; font-size: 14px; }\n        table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #e5e7eb; }\n        th { background: #f9fafb; font-weight: 600; }\n        tr:hover { background: #f9fafb; }\n        .insight { background: #fef3c7; padding: 15px; border-radius: 8px; margin: 10px 0; border-left: 4px solid #f59e0b; }\n        .risk { background: #fee2e2; padding: 15px; border-radius: 8px; margin: 10px 0; border-left: 4px solid #ef4444; }\n        .opportunity { background: #d1fae5; padding: 15px; border-radius: 8px; margin: 10px 0; border-left: 4px solid #10b981; }\n        .badge { display: inline-block; padding: 4px 8px; border-radius: 4px; font-size: 12px; font-weight: 600; }\n        .badge-high { background: #fee2e2; color: #dc2626; }\n        .badge-medium { background: #fef3c7; color: #d97706; }\n        .badge-low { background: #d1fae5; color: #059669; }\n    </style>\n</head>\n<body>\n<div class="container">\n']
  - L492 expr html_parts.append(f'<h1>Analysis Report: {result.document_name}</h1>')
  - L493 expr html_parts.append(f"<p style='color:#6b7280;'>Generated: {result.created_at.strftime('%Y-%m-%d %H:%M')}</p>")
  - L496 if result.metrics:
    - L497 expr html_parts.append('<h2>Key Metrics</h2><div>')
    - L498 for metric in result.metrics[:8]:
      - L499 expr html_parts.append(f'<div class="metric">\n                <div class="metric-value">{metric.raw_value}</div>\n                <div class="metric-name">{metric.name}</div>\n            </div>')
    - L503 expr html_parts.append('</div>')
  - L506 if 'executive' in result.summaries:
    - L507 expr html_parts.append('<h2>Executive Summary</h2>')
    - L508 expr html_parts.append(f"<p>{result.summaries['executive'].content}</p>")
  - L511 if result.tables:
    - L512 expr html_parts.append('<h2>Data Tables</h2>')
    - L513 for table in result.tables[:3]:
      - L514 expr html_parts.append(f'<h3>{table.title or table.id}</h3>')
      - L515 expr html_parts.append('<table><thead><tr>')
      - L516 for header in table.headers:
        - L517 expr html_parts.append(f'<th>{header}</th>')
      - L518 expr html_parts.append('</tr></thead><tbody>')
      - L519 for row in table.rows[:15]:
        - L520 expr html_parts.append('<tr>')
        - L521 for val in row:
          - L522 expr html_parts.append(f'<td>{val}</td>')
        - L523 expr html_parts.append('</tr>')
      - L524 expr html_parts.append('</tbody></table>')
      - L525 if len(table.rows) > 15:
        - L526 expr html_parts.append(f"<p style='color:#6b7280;'>...and {len(table.rows) - 15} more rows</p>")
  - L529 if result.insights:
    - L530 expr html_parts.append('<h2>Key Insights</h2>')
    - L531 for insight in result.insights:
      - L532 expr html_parts.append(f'<div class="insight">\n                <strong>{insight.title}</strong>\n                <span class="badge badge-{insight.priority.value}">{insight.priority.value}</span>\n                <p>{insight.description}</p>\n            </div>')
  - L539 if result.risks:
    - L540 expr html_parts.append('<h2>Risks Identified</h2>')
    - L541 for risk in result.risks:
      - L542 expr html_parts.append(f'<div class="risk">\n                <strong>{risk.title}</strong>\n                <span class="badge badge-{risk.risk_level.value}">{risk.risk_level.value}</span>\n                <p>{risk.description}</p>\n            </div>')
  - L549 if result.opportunities:
    - L550 expr html_parts.append('<h2>Opportunities</h2>')
    - L551 for opp in result.opportunities:
      - L552 expr html_parts.append(f'<div class="opportunity">\n                <strong>{opp.title}</strong>\n                <p>{opp.description}</p>\n            </div>')
  - L557 expr html_parts.append('</div></body></html>')
  - L559 return '\n'.join(html_parts)
- L562 async def export_to_excelresult: EnhancedAnalysisResult, include_charts: bool=True:
  - L566 docstring: "Export analysis result to Excel format."
  - L567 try:
    - L568 import openpyxl
    - L569 from openpyxl.utils import get_column_letter
    - L570 from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
    - L571 except ImportError:
      - L572 raise RuntimeError('openpyxl is required for Excel export')
  - L574 assign wb = openpyxl.Workbook()
  - L577 assign header_font = Font(bold=True, color='FFFFFF')
  - L578 assign header_fill = PatternFill(start_color='4F46E5', end_color='4F46E5', fill_type='solid')
  - L579 assign thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))
  - L587 assign ws_summary = wb.active
  - L588 assign ws_summary.title = 'Summary'
  - L589 assign ws_summary['A1'] = 'Analysis Report'
  - L590 assign ws_summary['A1'].font = Font(bold=True, size=16)
  - L591 assign ws_summary['A3'] = 'Document:'
  - L592 assign ws_summary['B3'] = result.document_name
  - L593 assign ws_summary['A4'] = 'Generated:'
  - L594 assign ws_summary['B4'] = result.created_at.strftime('%Y-%m-%d %H:%M')
  - L595 assign ws_summary['A5'] = 'Tables Found:'
  - L596 assign ws_summary['B5'] = result.total_tables
  - L597 assign ws_summary['A6'] = 'Metrics Extracted:'
  - L598 assign ws_summary['B6'] = result.total_metrics
  - L601 if result.metrics:
    - L602 assign ws_metrics = wb.create_sheet('Metrics')
    - L603 assign headers = ['Name', 'Value', 'Type', 'Period', 'Change (%)', 'Context']
    - L604 for (col, header) in enumerate(headers, 1):
      - L605 assign cell = ws_metrics.cell(row=1, column=col, value=header)
      - L606 assign cell.font = header_font
      - L607 assign cell.fill = header_fill
      - L608 assign cell.alignment = Alignment(horizontal='center')
    - L610 for (row, metric) in enumerate(result.metrics, 2):
      - L611 expr ws_metrics.cell(row=row, column=1, value=metric.name)
      - L612 expr ws_metrics.cell(row=row, column=2, value=metric.raw_value)
      - L613 expr ws_metrics.cell(row=row, column=3, value=metric.metric_type.value)
      - L614 expr ws_metrics.cell(row=row, column=4, value=metric.period or '')
      - L615 expr ws_metrics.cell(row=row, column=5, value=metric.change or '')
      - L616 expr ws_metrics.cell(row=row, column=6, value=metric.context or '')
  - L619 for table in result.tables:
    - L621 assign sheet_name = (table.title or table.id)[:30].replace('/', '-').replace('\\', '-')
    - L622 try:
      - L623 assign ws_data = wb.create_sheet(sheet_name)
      - L624 except Exception:
        - L625 assign ws_data = wb.create_sheet(f'Table_{table.id[:20]}')
    - L628 for (col, header) in enumerate(table.headers, 1):
      - L629 assign cell = ws_data.cell(row=1, column=col, value=header)
      - L630 assign cell.font = header_font
      - L631 assign cell.fill = header_fill
      - L632 assign cell.alignment = Alignment(horizontal='center')
      - L633 assign ws_data.column_dimensions[get_column_letter(col)].width = max(12, len(header) + 2)
    - L636 for (row_idx, row) in enumerate(table.rows, 2):
      - L637 for (col_idx, val) in enumerate(row, 1):
        - L638 expr ws_data.cell(row=row_idx, column=col_idx, value=val)
  - L641 if result.insights:
    - L642 assign ws_insights = wb.create_sheet('Insights')
    - L643 assign headers = ['Type', 'Priority', 'Title', 'Description', 'Actions']
    - L644 for (col, header) in enumerate(headers, 1):
      - L645 assign cell = ws_insights.cell(row=1, column=col, value=header)
      - L646 assign cell.font = header_font
      - L647 assign cell.fill = header_fill
    - L649 for (row, insight) in enumerate(result.insights, 2):
      - L650 expr ws_insights.cell(row=row, column=1, value=insight.type)
      - L651 expr ws_insights.cell(row=row, column=2, value=insight.priority.value)
      - L652 expr ws_insights.cell(row=row, column=3, value=insight.title)
      - L653 expr ws_insights.cell(row=row, column=4, value=insight.description)
      - L654 expr ws_insights.cell(row=row, column=5, value='; '.join(insight.suggested_actions))
  - L657 assign output = io.BytesIO()
  - L658 expr wb.save(output)
  - L659 expr output.seek(0)
  - L660 return output.getvalue()
- L663 async def export_to_pdfresult: EnhancedAnalysisResult:
  - L664 docstring: "Export analysis result to PDF format."
  - L667 assign html_content = export_to_html(result)
  - L671 return html_content.encode('utf-8')
- L678 class DataExportService:
  - L679 docstring: "Orchestrates data transformation and export operations."
  - L681 def assess_qualityself, tables: List[EnhancedExtractedTable]:
    - L682 docstring: "Assess data quality."
    - L683 return assess_data_quality(tables)
  - L685 def clean_dataself, tables: List[EnhancedExtractedTable], operations: List[str]=None:
    - L690 docstring: "Clean all tables."
    - L691 return [clean_table(t, operations) for t in tables]
  - L693 def apply_transformationsself, table: EnhancedExtractedTable, transformations: List[DataTransformation]:
    - L698 docstring: "Apply multiple transformations to a table."
    - L699 assign result = table
    - L700 for transform in transformations:
      - L701 assign result = apply_transformation(result, transform)
    - L702 return result
  - L704 async def exportself, result: EnhancedAnalysisResult, config: ExportConfiguration:
    - L709 docstring: "Export analysis result in specified format."
    - L710 assign filename = config.filename or f'analysis_{result.analysis_id}'
    - L712 if config.format == ExportFormat.CSV:
      - L713 assign content = export_to_csv(result.tables)
      - L714 return (content.encode('utf-8'), f'{filename}.csv')
      - L716 else:
        - L716 if config.format == ExportFormat.JSON:
          - L717 assign content = export_to_json(result, config.include_raw_data)
          - L718 return (content.encode('utf-8'), f'{filename}.json')
          - L720 else:
            - L720 if config.format == ExportFormat.MARKDOWN:
              - L721 assign content = export_to_markdown(result)
              - L722 return (content.encode('utf-8'), f'{filename}.md')
              - L724 else:
                - L724 if config.format == ExportFormat.HTML:
                  - L725 assign content = export_to_html(result)
                  - L726 return (content.encode('utf-8'), f'{filename}.html')
                  - L728 else:
                    - L728 if config.format == ExportFormat.EXCEL:
                      - L729 assign content = await export_to_excel(result, config.include_charts)
                      - L730 return (content, f'{filename}.xlsx')
                      - L732 else:
                        - L732 if config.format == ExportFormat.PDF:
                          - L733 assign content = await export_to_pdf(result)
                          - L734 return (content, f'{filename}.pdf')
                          - L738 else:
                            - L738 assign content = export_to_json(result)
                            - L739 return (content.encode('utf-8'), f'{filename}.json')

## backend\app\features\analyze\services\document_analysis_service.py
- L2 docstring: "Document analysis service that orchestrates extraction and LLM processing."
- L3 from __future__ import annotations
- L5 import asyncio
- L6 import json
- L7 import logging
- L8 import os
- L9 import threading
- L10 import time
- L11 import uuid
- L12 from dataclasses import dataclass, field
- L13 from pathlib import Path
- L14 from typing import Any, AsyncGenerator, Optional
- L16 from backend.app.core.config import get_settings
- L17 from backend.app.features.analyze.schemas.analysis import AnalysisResult, AnalysisSuggestChartsPayload, ExtractedDataPoint, ExtractedTable, FieldInfo, TimeSeriesCandidate
- L25 from backend.app.features.generate.schemas.charts import ChartSpec
- L26 from backend.app.services.prompts.llm_prompts_analysis import build_analysis_prompt, build_chart_suggestion_prompt, infer_data_type, parse_analysis_response, strip_code_fences
- L33 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L34 from backend.app.services.utils.llm import call_chat_completion
- L36 from .extraction_pipeline import ExtractedContent, extract_document_content, format_content_for_llm
- L42 assign logger = logging.getLogger('neura.analyze.service')
- L46 class CacheEntry:
  - L47 docstring: "Cache entry with TTL support."
  - L48 annotated assign value: AnalysisResult
  - L49 annotated assign created_at: float = field(default_factory=time.time)
  - L50 annotated assign last_accessed: float = field(default_factory=time.time)
- L53 class TTLCache:
  - L54 docstring: "Thread-safe TTL cache with LRU eviction and size limits."
  - L56 def __init__self, max_items: int=100, ttl_seconds: int=3600:
    - L57 assign self.max_items = max_items
    - L58 assign self.ttl_seconds = ttl_seconds
    - L59 annotated assign self._cache: dict[str, CacheEntry] = {}
    - L60 assign self._lock = threading.Lock()
    - L61 assign self._eviction_threshold = max(1, max_items * 8 // 10)
  - L63 def _is_expiredself, entry: CacheEntry:
    - L64 docstring: "Check if cache entry has expired based on TTL from creation time."
    - L65 return time.time() - entry.created_at > self.ttl_seconds
  - L67 def _evict_staleself:
    - L68 docstring: "Remove expired entries. Returns number of entries evicted."
    - L69 assign now = time.time()
    - L70 assign expired_keys = [key for key, entry in self._cache.items() if now - entry.created_at > self.ttl_seconds]
    - L74 for key in expired_keys:
      - L75 delete self._cache[key]
    - L76 if expired_keys:
      - L77 expr logger.debug(f'Evicted {len(expired_keys)} stale cache entries')
    - L78 return len(expired_keys)
  - L80 def _evict_lruself:
    - L81 docstring: "\n        Evict least recently used entries if at or over capacity.\n        Evi..."
    - L86 if len(self._cache) < self.max_items:
      - L87 return 0
    - L90 assign sorted_keys = sorted(self._cache.keys(), key=lambda k: self._cache[k].last_accessed)
    - L96 assign num_to_evict = len(self._cache) - self._eviction_threshold
    - L97 assign num_to_evict = max(1, num_to_evict)
    - L99 for key in sorted_keys[:num_to_evict]:
      - L100 delete self._cache[key]
    - L102 expr logger.debug(f'LRU evicted {num_to_evict} cache entries, size now {len(self._cache)}')
    - L103 return num_to_evict
  - L105 def getself, key: str:
    - L106 docstring: "Get value from cache, returns None if not found or expired."
    - L107 with self._lock:
      - L108 assign entry = self._cache.get(key)
      - L109 if entry is None:
        - L110 return None
      - L111 if self._is_expired(entry):
        - L112 delete self._cache[key]
        - L113 expr logger.debug(f'Cache entry {key} expired on access')
        - L114 return None
      - L116 assign entry.last_accessed = time.time()
      - L117 return entry.value
  - L119 def setself, key: str, value: AnalysisResult:
    - L120 docstring: "Set value in cache with automatic eviction."
    - L121 with self._lock:
      - L123 expr self._evict_stale()
      - L125 expr self._evict_lru()
      - L127 assign self._cache[key] = CacheEntry(value=value)
      - L128 expr logger.debug(f'Cache set {key}, size now {len(self._cache)}')
  - L130 def __contains__self, key: str:
    - L131 docstring: "Check if key exists and is not expired."
    - L132 return self.get(key) is not None
  - L134 def sizeself:
    - L135 docstring: "Return current cache size."
    - L136 with self._lock:
      - L137 return len(self._cache)
  - L139 def clearself:
    - L140 docstring: "Clear all cache entries."
    - L141 with self._lock:
      - L142 expr self._cache.clear()
      - L143 expr logger.debug('Cache cleared')
- L147 assign _settings = get_settings()
- L148 assign _ANALYSIS_CACHE = TTLCache(max_items=_settings.analysis_cache_max_items, ttl_seconds=_settings.analysis_cache_ttl_seconds)
- L153 annotated assign _ANALYSIS_SEMAPHORE: asyncio.Semaphore | None = None
- L156 def _analysis_size_limits:
  - L157 assign override = os.getenv('ANALYZE_MAX_FILE_SIZE_MB')
  - L158 if override:
    - L159 try:
      - L160 assign mb = int(override)
      - L161 except ValueError:
        - L162 assign mb = None
    - L163 if mb and mb > 0:
      - L164 return (mb * 1024 * 1024, mb)
  - L165 assign max_bytes = _settings.max_upload_bytes
  - L166 assign max_mb = max(1, int(max_bytes / (1024 * 1024)))
  - L167 return (max_bytes, max_mb)
- L170 def _get_analysis_semaphore:
  - L171 Global
  - L172 if _ANALYSIS_SEMAPHORE is None:
    - L173 assign _ANALYSIS_SEMAPHORE = asyncio.Semaphore(max(1, int(_settings.analysis_max_concurrency or 1)))
  - L174 return _ANALYSIS_SEMAPHORE
- L177 def _generate_analysis_id:
  - L178 docstring: "Generate a unique analysis ID."
  - L179 return f'ana_{uuid.uuid4().hex[:12]}'
- L182 def _attach_event_metadatapayload: dict[str, Any], analysis_id: str, correlation_id: Optional[str]:
  - L187 assign payload['analysis_id'] = analysis_id
  - L188 if correlation_id:
    - L189 assign payload['correlation_id'] = correlation_id
  - L190 return payload
- L193 def _convert_llm_tables_to_schemallm_tables: list[dict[str, Any]]:
  - L194 docstring: "Convert LLM-extracted tables to schema objects."
  - L195 annotated assign result: list[ExtractedTable] = []
  - L196 for (idx, table) in enumerate(llm_tables):
    - L197 try:
      - L198 expr result.append(ExtractedTable(id=table.get('id', f'table_{idx + 1}'), title=table.get('title'), headers=table.get('headers', []), rows=table.get('rows', []), data_types=table.get('data_types'), source_page=table.get('source_page'), source_sheet=table.get('source_sheet')))
      - L207 except Exception as exc:
        - L208 expr logger.warning(f'Failed to convert table {idx}: {exc}')
  - L209 return result
- L212 def _convert_llm_metrics_to_schemallm_metrics: list[dict[str, Any]]:
  - L213 docstring: "Convert LLM-extracted metrics to schema objects."
  - L214 annotated assign result: list[ExtractedDataPoint] = []
  - L215 for metric in llm_metrics:
    - L216 try:
      - L217 expr result.append(ExtractedDataPoint(key=metric.get('name', 'Unknown'), value=metric.get('value'), data_type='numeric' if isinstance(metric.get('value'), (int, float)) else 'text', unit=metric.get('unit'), context=metric.get('context')))
      - L224 except Exception as exc:
        - L225 expr logger.warning(f'Failed to convert metric: {exc}')
  - L226 return result
- L229 def _convert_llm_time_series_to_schemallm_ts: list[dict[str, Any]]:
  - L230 docstring: "Convert LLM time series candidates to schema objects."
  - L231 annotated assign result: list[TimeSeriesCandidate] = []
  - L232 for ts in llm_ts:
    - L233 try:
      - L234 expr result.append(TimeSeriesCandidate(date_column=ts.get('date_column', ''), value_columns=ts.get('value_columns', []), frequency=ts.get('frequency'), table_id=ts.get('table_id')))
      - L240 except Exception as exc:
        - L241 expr logger.warning(f'Failed to convert time series: {exc}')
  - L242 return result
- L245 def _convert_llm_charts_to_schemallm_charts: list[dict[str, Any]]:
  - L246 docstring: "Convert LLM chart recommendations to ChartSpec objects."
  - L247 annotated assign result: list[ChartSpec] = []
  - L248 for (idx, chart) in enumerate(llm_charts):
    - L249 try:
      - L250 assign chart_type = chart.get('type', 'bar').lower()
      - L251 if chart_type not in ('line', 'bar', 'pie', 'scatter'):
        - L252 assign chart_type = 'bar'
      - L254 assign y_fields = chart.get('y_fields') or chart.get('yFields') or []
      - L255 if isinstance(y_fields, str):
        - L256 assign y_fields = [y_fields]
      - L258 expr result.append(ChartSpec(id=chart.get('id', f'chart_{idx + 1}'), type=chart_type, xField=chart.get('x_field') or chart.get('xField') or '', yFields=y_fields, groupField=chart.get('group_field') or chart.get('groupField'), aggregation=chart.get('aggregation'), title=chart.get('title'), description=chart.get('description') or chart.get('rationale')))
      - L268 except Exception as exc:
        - L269 expr logger.warning(f'Failed to convert chart {idx}: {exc}')
  - L270 return result
- L273 def _build_field_catalogtables: list[ExtractedTable]:
  - L274 docstring: "Build field catalog from extracted tables."
  - L275 annotated assign fields: list[FieldInfo] = []
  - L276 annotated assign seen_names: set[str] = set()
  - L278 for table in tables:
    - L279 for (idx, header) in enumerate(table.headers):
      - L280 if header in seen_names:
        - L281 continue
      - L282 expr seen_names.add(header)
      - L284 assign data_type = 'text'
      - L285 if table.data_types and idx < len(table.data_types):
        - L286 assign data_type = table.data_types[idx]
      - L288 annotated assign sample_values: list[Any] = []
      - L289 for row in table.rows[:5]:
        - L290 if idx < len(row):
          - L291 expr sample_values.append(row[idx])
      - L293 expr fields.append(FieldInfo(name=header, type=data_type, sample_values=sample_values[:3] if sample_values else None))
  - L299 return fields
- L302 def _build_raw_datatables: list[ExtractedTable], max_rows: int=500:
  - L303 docstring: "Flatten extracted tables into raw data records."
  - L304 annotated assign raw_data: list[dict[str, Any]] = []
  - L306 for table in tables:
    - L307 for row in table.rows[:max_rows]:
      - L308 annotated assign record: dict[str, Any] = {}
      - L309 for (idx, header) in enumerate(table.headers):
        - L310 if idx < len(row):
          - L311 assign record[header] = row[idx]
      - L312 if record:
        - L313 expr raw_data.append(record)
    - L315 if len(raw_data) >= max_rows:
      - L316 break
  - L318 return raw_data[:max_rows]
- L321 def _merge_extracted_tablescontent_tables: list[dict[str, Any]], llm_tables: list[dict[str, Any]]:
  - L325 docstring: "Merge tables from extraction pipeline with LLM-enhanced tables."
  - L326 if not llm_tables:
    - L327 return content_tables
  - L329 assign merged_ids = {t.get('id') for t in content_tables}
  - L330 assign merged = list(content_tables)
  - L332 for llm_table in llm_tables:
    - L333 assign llm_id = llm_table.get('id')
    - L334 if llm_id and llm_id not in merged_ids:
      - L335 expr merged.append(llm_table)
      - L336 expr merged_ids.add(llm_id)
  - L338 return merged
- L341 async def analyze_document_streamingfile_bytes: bytes, file_name: str, template_id: Optional[str]=None, connection_id: Optional[str]=None, correlation_id: Optional[str]=None:
  - L348 docstring: "Analyze a document with streaming progress updates."
  - L349 assign analysis_id = _generate_analysis_id()
  - L350 assign started = time.time()
  - L351 assign (max_bytes, max_mb) = _analysis_size_limits()
  - L353 assign semaphore = _get_analysis_semaphore()
  - L354 async with semaphore:
    - L355 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'uploading', 'progress': 10}, analysis_id, correlation_id))
    - L361 if not file_bytes:
      - L362 expr (yield _attach_event_metadata({'event': 'error', 'detail': 'Empty file provided.'}, analysis_id, correlation_id))
      - L367 return None
    - L369 if len(file_bytes) > max_bytes:
      - L370 expr (yield _attach_event_metadata({'event': 'error', 'detail': f'File too large. Maximum size is {max_mb}MB.'}, analysis_id, correlation_id))
      - L375 return None
    - L377 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'parsing', 'progress': 20, 'detail': 'Extracting content...'}, analysis_id, correlation_id))
    - L383 assign content = extract_document_content(file_bytes=file_bytes, file_name=file_name)
    - L388 if content.errors and (not content.tables_raw) and (not content.text_content):
      - L389 expr (yield _attach_event_metadata({'event': 'error', 'detail': f"Failed to extract content: {'; '.join(content.errors)}"}, analysis_id, correlation_id))
      - L394 return None
    - L396 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'table_extraction', 'progress': 40, 'detail': f'Found {len(content.tables_raw)} tables'}, analysis_id, correlation_id))
    - L407 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'llm_analysis', 'progress': 60, 'detail': 'Analyzing with AI...'}, analysis_id, correlation_id))
    - L413 assign llm_result = {'tables': [], 'key_metrics': [], 'time_series_candidates': [], 'chart_recommendations': []}
    - L415 try:
      - L416 assign client = get_openai_client()
      - L417 assign formatted_content = format_content_for_llm(content)
      - L419 assign prompt = build_analysis_prompt(document_type=content.document_type, file_name=content.file_name, page_count=content.page_count, extracted_content=formatted_content)
      - L426 assign messages = [{'role': 'user', 'content': prompt}]
      - L428 assign response = call_chat_completion(client, model=MODEL, messages=messages, description='document_analysis', temperature=0.2)
      - L436 assign raw_text = ''
      - L437 if hasattr(response, 'choices') and response.choices:
        - L438 assign choice = response.choices[0]
        - L439 if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
          - L440 assign raw_text = choice.message.content or ''
      - L442 assign llm_result = parse_analysis_response(raw_text)
      - L444 except Exception as exc:
        - L445 expr logger.warning(f'LLM analysis failed: {exc}')
        - L446 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'llm_analysis', 'progress': 70, 'detail': 'AI analysis skipped (using extracted data)'}, analysis_id, correlation_id))
    - L457 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'chart_generation', 'progress': 80, 'detail': 'Generating visualizations...'}, analysis_id, correlation_id))
    - L463 assign merged_tables = _merge_extracted_tables(content.tables_raw, llm_result.get('tables', []))
    - L464 assign tables = _convert_llm_tables_to_schema(merged_tables)
    - L465 assign data_points = _convert_llm_metrics_to_schema(llm_result.get('key_metrics', []))
    - L466 assign time_series = _convert_llm_time_series_to_schema(llm_result.get('time_series_candidates', []))
    - L467 assign charts = _convert_llm_charts_to_schema(llm_result.get('chart_recommendations', []))
    - L469 if not charts and tables:
      - L470 assign charts = _generate_fallback_charts(tables)
    - L472 assign field_catalog = _build_field_catalog(tables)
    - L473 assign raw_data = _build_raw_data(tables)
    - L475 assign processing_time_ms = int((time.time() - started) * 1000)
    - L477 annotated assign warnings: list[str] = []
    - L478 if content.errors:
      - L479 expr warnings.extend(content.errors)
    - L481 assign result = AnalysisResult(analysis_id=analysis_id, document_name=file_name, document_type=content.document_type, processing_time_ms=processing_time_ms, summary=llm_result.get('summary'), tables=tables, data_points=data_points, time_series_candidates=time_series, chart_suggestions=charts, raw_data=raw_data, field_catalog=field_catalog, template_id=template_id, warnings=warnings)
    - L497 expr _ANALYSIS_CACHE.set(analysis_id, result)
    - L499 expr (yield _attach_event_metadata({'event': 'stage', 'stage': 'complete', 'progress': 100}, analysis_id, correlation_id))
    - L505 assign result_payload = {'event': 'result', **result.dict()}
    - L506 if correlation_id:
      - L507 assign result_payload['correlation_id'] = correlation_id
    - L508 expr (yield result_payload)
- L511 def _generate_fallback_chartstables: list[ExtractedTable]:
  - L512 docstring: "Generate basic chart suggestions when LLM doesn't provide any."
  - L513 annotated assign charts: list[ChartSpec] = []
  - L515 for table in tables[:3]:
    - L516 annotated assign datetime_cols: list[str] = []
    - L517 annotated assign numeric_cols: list[str] = []
    - L518 annotated assign text_cols: list[str] = []
    - L520 for (idx, header) in enumerate(table.headers):
      - L521 assign data_type = table.data_types[idx] if table.data_types and idx < len(table.data_types) else 'text'
      - L522 if data_type in ('datetime', 'date'):
        - L523 expr datetime_cols.append(header)
        - L524 else:
          - L524 if data_type == 'numeric':
            - L525 expr numeric_cols.append(header)
            - L527 else:
              - L527 expr text_cols.append(header)
    - L529 if datetime_cols and numeric_cols:
      - L530 expr charts.append(ChartSpec(id=f'fallback_line_{table.id}', type='line', xField=datetime_cols[0], yFields=numeric_cols[:3], title=f'Time Series: {table.title or table.id}', description='Numeric values over time'))
    - L539 if text_cols and numeric_cols:
      - L540 expr charts.append(ChartSpec(id=f'fallback_bar_{table.id}', type='bar', xField=text_cols[0], yFields=numeric_cols[:2], title=f'Comparison: {table.title or table.id}', description='Numeric values by category'))
  - L549 return charts[:5]
- L552 def get_analysisanalysis_id: str:
  - L553 docstring: "Retrieve a cached analysis result."
  - L554 return _ANALYSIS_CACHE.get(analysis_id)
- L557 def get_analysis_dataanalysis_id: str:
  - L558 docstring: "Get raw data for an analysis."
  - L559 assign result = get_analysis(analysis_id)
  - L560 if result:
    - L561 return result.raw_data
  - L562 return None
- L565 def suggest_charts_for_analysisanalysis_id: str, payload: AnalysisSuggestChartsPayload:
  - L569 docstring: "Generate additional chart suggestions for an existing analysis."
  - L570 assign result = get_analysis(analysis_id)
  - L571 if not result:
    - L572 return []
  - L574 try:
    - L575 assign client = get_openai_client()
    - L577 assign data_summary = f'Document: {result.document_name}\n'
    - L578 aug assign data_summary Add f'Tables: {len(result.tables)}\n'
    - L579 for table in result.tables[:5]:
      - L580 aug assign data_summary Add f"  - {table.title or table.id}: {len(table.rows)} rows, columns: {', '.join(table.headers[:5])}\n"
    - L582 assign field_catalog_str = '\n'.join([f'  - {f.name}: {f.type}' + (f' (samples: {f.sample_values})' if f.sample_values else '') for f in result.field_catalog[:20]])
    - L587 assign prompt = build_chart_suggestion_prompt(data_summary=data_summary, field_catalog=field_catalog_str, user_question=payload.question)
    - L593 assign messages = [{'role': 'user', 'content': prompt}]
    - L595 assign response = call_chat_completion(client, model=MODEL, messages=messages, description='chart_suggestion_analysis', temperature=0.3)
    - L603 assign raw_text = ''
    - L604 if hasattr(response, 'choices') and response.choices:
      - L605 assign choice = response.choices[0]
      - L606 if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
        - L607 assign raw_text = choice.message.content or ''
    - L609 assign cleaned = strip_code_fences(raw_text)
    - L610 assign data = json.loads(cleaned)
    - L611 assign charts = data.get('charts', [])
    - L612 return _convert_llm_charts_to_schema(charts)
    - L614 except Exception as exc:
      - L615 expr logger.warning(f'Chart suggestion failed: {exc}')
      - L616 return _generate_fallback_charts(result.tables)
- L619 assign __all__ = ['analyze_document_streaming', 'get_analysis', 'get_analysis_data', 'suggest_charts_for_analysis']

## backend\app\features\analyze\services\enhanced_analysis_orchestrator.py
- L2 docstring: "\nEnhanced Analysis Orchestrator - Main service that orchestrates all AI-powered..."
- L14 from __future__ import annotations
- L16 import asyncio
- L17 import json
- L18 import logging
- L19 import time
- L20 import uuid
- L21 from datetime import datetime
- L22 from typing import Any, AsyncGenerator, Dict, List, Optional
- L24 from backend.app.features.analyze.schemas.enhanced_analysis import AnalysisDepth, AnalysisPreferences, ChartGenerationRequest, DocumentType, EnhancedAnalysisResult, ExportConfiguration, ExportFormat, QuestionRequest, QuestionResponse, SummaryMode
- L36 from backend.app.features.analyze.services.enhanced_extraction_service import EnhancedExtractionService
- L39 from backend.app.features.analyze.services.analysis_engines import AnalysisEngineService
- L42 from backend.app.features.analyze.services.visualization_engine import VisualizationEngine
- L45 from backend.app.features.analyze.services.data_transform_export import DataExportService
- L48 from backend.app.features.analyze.services.advanced_ai_features import AdvancedAIService
- L51 from backend.app.features.analyze.services.user_experience import UserExperienceService, StreamingAnalysisSession
- L55 from backend.app.features.analyze.services.integrations import IntegrationService
- L58 from backend.app.features.analyze.services.extraction_pipeline import ExtractedContent, extract_document_content, format_content_for_llm
- L63 from backend.app.services.llm.rag import RAGRetriever
- L64 from backend.app.services.utils.llm import call_chat_completion
- L65 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L67 assign logger = logging.getLogger('neura.analyze.orchestrator')
- L71 annotated assign _ANALYSIS_CACHE: Dict[str, EnhancedAnalysisResult] = {}
- L74 def _generate_analysis_id:
  - L75 docstring: "Generate a unique analysis ID."
  - L76 return f'eana_{uuid.uuid4().hex[:12]}'
- L79 def _detect_document_typefile_name: str:
  - L80 docstring: "Detect document type from file name."
  - L81 assign name = file_name.lower()
  - L82 if name.endswith('.pdf'):
    - L83 return DocumentType.PDF
    - L84 else:
      - L84 if name.endswith(('.xlsx', '.xls', '.xlsm')):
        - L85 return DocumentType.EXCEL
        - L86 else:
          - L86 if name.endswith('.csv'):
            - L87 return DocumentType.CSV
            - L88 else:
              - L88 if name.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):
                - L89 return DocumentType.IMAGE
                - L90 else:
                  - L90 if name.endswith(('.doc', '.docx')):
                    - L91 return DocumentType.WORD
                    - L92 else:
                      - L92 if name.endswith('.txt'):
                        - L93 return DocumentType.TEXT
  - L94 return DocumentType.UNKNOWN
- L97 class EnhancedAnalysisOrchestrator:
  - L98 docstring: "\n    Main orchestrator for enhanced document analysis.\n\n    Coordinates all A..."
  - L105 def __init__self:
    - L106 assign self.extraction_service = EnhancedExtractionService()
    - L107 assign self.analysis_engine = AnalysisEngineService()
    - L108 assign self.visualization_engine = VisualizationEngine()
    - L109 assign self.export_service = DataExportService()
    - L110 assign self.advanced_ai = AdvancedAIService()
    - L111 assign self.ux_service = UserExperienceService()
    - L112 assign self.integration_service = IntegrationService()
    - L113 annotated assign self._rag_retrievers: Dict[str, RAGRetriever] = {}
  - L115 async def analyze_document_streamingself, file_bytes: bytes, file_name: str, preferences: Optional[AnalysisPreferences]=None, correlation_id: Optional[str]=None:
    - L122 docstring: "\n        Perform comprehensive document analysis with streaming progress update..."
    - L127 assign analysis_id = _generate_analysis_id()
    - L128 assign started = time.time()
    - L131 if preferences is None:
      - L132 assign preferences = AnalysisPreferences()
    - L135 assign config = self.ux_service.build_configuration(preferences)
    - L138 assign session = self.ux_service.create_streaming_session()
    - L140 try:
      - L142 expr (yield self._event('stage', 'Validating document...', 5, analysis_id, correlation_id))
      - L144 if not file_bytes:
        - L145 expr (yield self._event('error', 'Empty file provided', 0, analysis_id, correlation_id))
        - L146 return None
      - L148 assign document_type = _detect_document_type(file_name)
      - L151 expr (yield self._event('stage', 'Extracting content...', 15, analysis_id, correlation_id))
      - L153 assign content = extract_document_content(file_bytes=file_bytes, file_name=file_name)
      - L158 if content.errors and (not content.tables_raw) and (not content.text_content):
        - L159 expr (yield self._event('error', f"Extraction failed: {'; '.join(content.errors)}", 0, analysis_id, correlation_id))
        - L160 return None
      - L163 expr (yield self._event('stage', 'Running intelligent extraction...', 30, analysis_id, correlation_id))
      - L165 assign extraction_result = self.extraction_service.extract_all(text=content.text_content, raw_tables=content.tables_raw)
      - L170 assign tables = extraction_result['tables']
      - L171 assign entities = extraction_result['entities']
      - L172 assign metrics = extraction_result['metrics']
      - L173 assign forms = extraction_result['forms']
      - L174 assign invoices = extraction_result['invoices']
      - L175 assign contracts = extraction_result['contracts']
      - L176 assign table_relationships = extraction_result['table_relationships']
      - L178 expr (yield self._event('stage', f'Found {len(tables)} tables, {len(metrics)} metrics', 40, analysis_id, correlation_id))
      - L181 expr (yield self._event('stage', 'Running AI analysis...', 50, analysis_id, correlation_id))
      - L183 assign analysis_results = self.analysis_engine.run_all_analyses(text=content.text_content, tables=tables, metrics=metrics)
      - L189 assign summaries = analysis_results['summaries']
      - L190 assign sentiment = analysis_results['sentiment']
      - L191 assign text_analytics = analysis_results['text_analytics']
      - L192 assign statistical_analysis = analysis_results['statistical_analysis']
      - L193 assign financial_analysis = analysis_results['financial_analysis']
      - L194 assign insights = analysis_results['insights']
      - L195 assign risks = analysis_results['risks']
      - L196 assign opportunities = analysis_results['opportunities']
      - L197 assign action_items = analysis_results['action_items']
      - L199 expr (yield self._event('stage', f'Generated {len(insights)} insights', 60, analysis_id, correlation_id))
      - L202 expr (yield self._event('stage', 'Generating visualizations...', 70, analysis_id, correlation_id))
      - L204 assign viz_results = self.visualization_engine.generate_all_visualizations(tables=tables, metrics=metrics, max_charts=preferences.max_charts)
      - L210 assign charts = viz_results['charts']
      - L211 assign viz_suggestions = viz_results['suggestions']
      - L213 expr (yield self._event('stage', f'Created {len(charts)} charts', 75, analysis_id, correlation_id))
      - L216 expr (yield self._event('stage', 'Assessing data quality...', 80, analysis_id, correlation_id))
      - L218 assign data_quality = self.export_service.assess_quality(tables)
      - L221 assign advanced_results = {}
      - L222 if preferences.enable_predictions:
        - L223 expr (yield self._event('stage', 'Running predictive analytics...', 85, analysis_id, correlation_id))
        - L225 assign advanced_results = self.advanced_ai.run_all_advanced_features(text=content.text_content, entities=entities, metrics=metrics, tables=tables, document_id=analysis_id)
      - L234 expr (yield self._event('stage', 'Building knowledge index...', 90, analysis_id, correlation_id))
      - L236 assign rag = RAGRetriever(use_embeddings=False)
      - L237 expr rag.add_document(content=content.text_content, doc_id=analysis_id, metadata={'file_name': file_name, 'analysis_id': analysis_id})
      - L242 assign self._rag_retrievers[analysis_id] = rag
      - L245 assign suggested_questions = self.ux_service.generate_suggested_questions(tables=tables, metrics=metrics, entities=entities)
      - L252 assign processing_time_ms = int((time.time() - started) * 1000)
      - L254 expr (yield self._event('stage', 'Finalizing...', 95, analysis_id, correlation_id))
      - L257 assign result = EnhancedAnalysisResult(analysis_id=analysis_id, document_name=file_name, document_type=document_type, created_at=datetime.utcnow(), processing_time_ms=processing_time_ms, tables=tables, entities=entities, metrics=metrics, forms=forms, invoices=invoices, contracts=contracts, table_relationships=table_relationships, summaries=summaries, sentiment=sentiment, text_analytics=text_analytics, financial_analysis=financial_analysis, statistical_analysis=statistical_analysis, chart_suggestions=charts, visualization_suggestions=viz_suggestions, insights=insights, risks=risks, opportunities=opportunities, action_items=action_items, data_quality=data_quality, page_count=content.page_count, total_tables=len(tables), total_entities=len(entities), total_metrics=len(metrics), confidence_score=0.85, preferences=preferences, warnings=content.errors)
      - L308 assign _ANALYSIS_CACHE[analysis_id] = result
      - L310 expr (yield self._event('stage', 'Complete', 100, analysis_id, correlation_id))
      - L313 assign result_dict = result.dict()
      - L314 assign result_dict['event'] = 'result'
      - L315 assign result_dict['suggested_questions'] = suggested_questions
      - L316 assign result_dict['advanced_analytics'] = advanced_results
      - L318 if correlation_id:
        - L319 assign result_dict['correlation_id'] = correlation_id
      - L321 expr (yield result_dict)
      - L323 except asyncio.CancelledError:
        - L324 expr (yield self._event('cancelled', 'Analysis cancelled', 0, analysis_id, correlation_id))
        - L325 raise
      - L326 except Exception as e:
        - L327 expr logger.error(f'Analysis failed: {e}', exc_info=True)
        - L328 expr (yield self._event('error', str(e), 0, analysis_id, correlation_id))
  - L330 def _eventself, event_type: str, detail: str, progress: int, analysis_id: str, correlation_id: Optional[str]:
    - L338 docstring: "Build an event dict."
    - L339 assign event = {'event': event_type if event_type != 'stage' else 'stage', 'stage': event_type, 'detail': detail, 'progress': progress, 'analysis_id': analysis_id}
    - L346 if correlation_id:
      - L347 assign event['correlation_id'] = correlation_id
    - L348 return event
  - L350 def get_analysisself, analysis_id: str:
    - L351 docstring: "Get a cached analysis result."
    - L352 return _ANALYSIS_CACHE.get(analysis_id)
  - L354 async def ask_questionself, analysis_id: str, question: str, include_sources: bool=True, max_context_chunks: int=5:
    - L361 docstring: "Ask a question about the analyzed document."
    - L363 assign result = self.get_analysis(analysis_id)
    - L364 if not result:
      - L365 return QuestionResponse(answer='Analysis not found. Please upload and analyze the document first.', confidence=0.0, sources=[], suggested_followups=[])
    - L373 assign rag = self._rag_retrievers.get(analysis_id)
    - L374 if not rag:
      - L375 return QuestionResponse(answer='Knowledge index not available. Please re-analyze the document.', confidence=0.0, sources=[], suggested_followups=[])
    - L383 assign rag_result = rag.query_with_context(question=question, top_k=max_context_chunks, include_sources=include_sources)
    - L390 assign suggested_followups = self._generate_followup_questions(question, rag_result['answer'])
    - L392 return QuestionResponse(answer=rag_result['answer'], confidence=0.8 if rag_result.get('context_used') else 0.5, sources=rag_result.get('sources', []) if include_sources else [], suggested_followups=suggested_followups)
  - L399 def _generate_followup_questionsself, question: str, answer: str:
    - L400 docstring: "Generate follow-up questions based on Q&A."
    - L401 try:
      - L402 assign client = get_openai_client()
      - L403 assign prompt = f'Based on this Q&A, suggest 3 relevant follow-up questions.\n\nQuestion: {question}\nAnswer: {answer[:500]}\n\nReturn JSON array of questions:\n["Question 1", "Question 2", "Question 3"]'
      - L411 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='followup_questions', temperature=0.5)
      - L419 assign raw = response.choices[0].message.content or '[]'
      - L420 import re
      - L421 assign match = re.search('\\[[\\s\\S]*\\]', raw)
      - L422 if match:
        - L423 return json.loads(match.group())
      - L424 except Exception as e:
        - L425 expr logger.warning(f'Follow-up generation failed: {e}')
    - L427 return []
  - L429 async def generate_charts_from_queryself, analysis_id: str, query: str, include_trends: bool=True, include_forecasts: bool=False:
    - L436 docstring: "Generate charts from natural language query."
    - L437 assign result = self.get_analysis(analysis_id)
    - L438 if not result:
      - L439 return []
    - L441 assign charts = self.visualization_engine.generate_from_query(query=query, tables=result.tables, metrics=result.metrics)
    - L448 if include_trends or include_forecasts:
      - L449 assign charts = [self.visualization_engine.add_intelligence_to_chart(chart, include_forecast=include_forecasts) for chart in charts]
    - L457 return [c.dict() for c in charts]
  - L459 async def export_analysisself, analysis_id: str, format: ExportFormat, include_raw_data: bool=True, include_charts: bool=True:
    - L466 docstring: "Export analysis in specified format."
    - L467 assign result = self.get_analysis(analysis_id)
    - L468 if not result:
      - L469 raise ValueError(f'Analysis not found: {analysis_id}')
    - L471 assign config = ExportConfiguration(format=format, include_raw_data=include_raw_data, include_charts=include_charts, include_analysis=True, include_insights=True)
    - L479 return await self.export_service.export(result, config)
  - L481 async def compare_documentsself, analysis_id_1: str, analysis_id_2: str:
    - L486 docstring: "Compare two analyzed documents."
    - L487 assign result1 = self.get_analysis(analysis_id_1)
    - L488 assign result2 = self.get_analysis(analysis_id_2)
    - L490 if not result1 or not result2:
      - L491 return {'error': 'One or both analyses not found'}
    - L493 from backend.app.features.analyze.services.analysis_engines import compare_documents
    - L496 assign text1 = result1.summaries.get('comprehensive', result1.summaries.get('executive'))
    - L497 assign text2 = result2.summaries.get('comprehensive', result2.summaries.get('executive'))
    - L499 assign text1_content = text1.content if text1 else ''
    - L500 assign text2_content = text2.content if text2 else ''
    - L502 assign comparison = compare_documents(text1=text1_content, text2=text2_content, metrics1=result1.metrics, metrics2=result2.metrics)
    - L509 return comparison.dict()
  - L511 def get_industry_optionsself:
    - L512 docstring: "Get available industry options."
    - L513 return self.ux_service.get_industry_options()
  - L515 def get_export_formatsself:
    - L516 docstring: "Get available export formats."
    - L517 return [{'value': f.value, 'label': f.value.upper()} for f in ExportFormat]
- L524 annotated assign _orchestrator: Optional[EnhancedAnalysisOrchestrator] = None
- L527 def get_orchestrator:
  - L528 docstring: "Get the singleton orchestrator instance."
  - L529 Global
  - L530 if _orchestrator is None:
    - L531 assign _orchestrator = EnhancedAnalysisOrchestrator()
  - L532 return _orchestrator

## backend\app\features\analyze\services\enhanced_extraction_service.py
- L2 docstring: "\nEnhanced Extraction Service - Intelligent data extraction from documents.\n\nF..."
- L10 from __future__ import annotations
- L12 import hashlib
- L13 import json
- L14 import logging
- L15 import re
- L16 import uuid
- L17 from datetime import datetime
- L18 from typing import Any, Dict, List, Optional, Tuple
- L20 from backend.app.features.analyze.schemas.enhanced_analysis import DocumentType, EnhancedExtractedTable, EntityType, ExtractedContract, ExtractedEntity, ExtractedInvoice, ExtractedMetric, FormField, InvoiceLineItem, MetricType, TableRelationship
- L33 from backend.app.services.utils.llm import call_chat_completion
- L34 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L36 assign logger = logging.getLogger('neura.analyze.extraction')
- L43 assign ENTITY_PATTERNS = {EntityType.EMAIL: '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', EntityType.PHONE: '\\b(?:\\+?1[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b', EntityType.URL: 'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[/\\w\\-.?=%&]*', EntityType.PERCENTAGE: '\\b\\d+(?:\\.\\d+)?%\\b', EntityType.MONEY: '(?:\\$|\u20ac|\xa3|\xa5|USD|EUR|GBP)\\s*\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\\b|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\\s*(?:dollars?|euros?|pounds?|USD|EUR|GBP)\\b', EntityType.DATE: '\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4}|Q[1-4]\\s*\\d{4}|FY\\d{2,4})\\b'}
- L53 def extract_entities_regextext: str:
  - L54 docstring: "Extract entities using regex patterns."
  - L55 assign entities = []
  - L56 assign seen = set()
  - L58 for (entity_type, pattern) in ENTITY_PATTERNS.items():
    - L59 for match in re.finditer(pattern, text, re.IGNORECASE):
      - L60 assign value = match.group().strip()
      - L61 assign key = f'{entity_type}:{value.lower()}'
      - L63 if key not in seen:
        - L64 expr seen.add(key)
        - L65 expr entities.append(ExtractedEntity(id=f'ent_{uuid.uuid4().hex[:8]}', type=entity_type, value=value, confidence=0.9, position={'start': match.start(), 'end': match.end()}, context=text[max(0, match.start() - 50):match.end() + 50]))
  - L74 return entities
- L77 def extract_entities_llmtext: str, client: Any:
  - L78 docstring: "Extract entities using LLM for named entity recognition."
  - L79 assign prompt = f'Extract all named entities from the following text. Identify:\n- PERSON: Names of people\n- ORGANIZATION: Company names, institutions\n- LOCATION: Cities, countries, addresses\n- PRODUCT: Product or service names\n- DATE: Dates, time periods (normalize to ISO format when possible)\n- MONEY: Currency amounts (normalize to number + currency code)\n- PERCENTAGE: Percentage values\n\nText:\n{text[:8000]}\n\nReturn JSON array:\n```json\n[\n  {{"type": "PERSON", "value": "John Smith", "normalized": "John Smith", "context": "CEO John Smith announced..."}},\n  {{"type": "MONEY", "value": "$1.5M", "normalized": 1500000, "currency": "USD", "context": "revenue of $1.5M"}},\n  {{"type": "DATE", "value": "Q3 2025", "normalized": "2025-07-01/2025-09-30", "context": "in Q3 2025"}}\n]\n```\n\nExtract ALL entities found. Be thorough.'
  - L102 try:
    - L103 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='entity_extraction', temperature=0.1)
    - L111 assign raw_text = response.choices[0].message.content or ''
    - L112 assign json_match = re.search('\\[[\\s\\S]*\\]', raw_text)
    - L113 if json_match:
      - L114 assign data = json.loads(json_match.group())
      - L115 assign entities = []
      - L116 for item in data:
        - L117 assign entity_type = item.get('type', '').upper()
        - L118 try:
          - L119 assign etype = EntityType[entity_type]
          - L120 except KeyError:
            - L121 assign etype = EntityType.CUSTOM
        - L123 expr entities.append(ExtractedEntity(id=f'ent_{uuid.uuid4().hex[:8]}', type=etype, value=item.get('value', ''), normalized_value=str(item.get('normalized', '')), confidence=0.85, context=item.get('context'), metadata={'currency': item.get('currency')} if item.get('currency') else {}))
      - L132 return entities
    - L133 except Exception as e:
      - L134 expr logger.warning(f'LLM entity extraction failed: {e}')
  - L136 return []
- L139 def extract_all_entitiestext: str, use_llm: bool=True:
  - L140 docstring: "Extract entities using both regex and LLM."
  - L142 assign entities = extract_entities_regex(text)
  - L143 assign entity_values = {e.value.lower() for e in entities}
  - L146 if use_llm:
    - L147 try:
      - L148 assign client = get_openai_client()
      - L149 assign llm_entities = extract_entities_llm(text, client)
      - L152 for ent in llm_entities:
        - L153 if ent.value.lower() not in entity_values:
          - L154 expr entities.append(ent)
          - L155 expr entity_values.add(ent.value.lower())
      - L156 except Exception as e:
        - L157 expr logger.warning(f'LLM extraction skipped: {e}')
  - L159 return entities
- L166 def extract_metrics_llmtext: str, tables: List[EnhancedExtractedTable]:
  - L167 docstring: "Extract key metrics and KPIs using LLM."
  - L169 assign table_context = ''
  - L170 for table in tables[:5]:
    - L171 aug assign table_context Add f'\nTable: {table.title or table.id}\n'
    - L172 aug assign table_context Add f"Headers: {', '.join(table.headers[:10])}\n"
    - L173 if table.rows:
      - L174 aug assign table_context Add f'Sample row: {table.rows[0][:10]}\n'
  - L176 assign prompt = f'Analyze this document and extract ALL key metrics, KPIs, and important numerical data.\n\nText excerpt:\n{text[:6000]}\n\nTables found:\n{table_context}\n\nExtract metrics with context. Return JSON array:\n```json\n[\n  {{\n    "name": "Revenue",\n    "value": 1500000,\n    "raw_value": "$1.5M",\n    "metric_type": "currency",\n    "unit": null,\n    "currency": "USD",\n    "period": "Q3 2025",\n    "change": 15.5,\n    "change_direction": "increase",\n    "comparison_base": "vs Q3 2024",\n    "context": "Revenue reached $1.5M in Q3 2025, up 15.5% YoY",\n    "importance": 0.9\n  }},\n  {{\n    "name": "Customer Count",\n    "value": 50000,\n    "raw_value": "50,000",\n    "metric_type": "count",\n    "period": "2025",\n    "importance": 0.7\n  }}\n]\n```\n\nMetric types: currency, percentage, count, ratio, duration, quantity, score, rate\nExtract ALL significant numbers with their context. Focus on KPIs.'
  - L215 try:
    - L216 assign client = get_openai_client()
    - L217 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='metric_extraction', temperature=0.1)
    - L225 assign raw_text = response.choices[0].message.content or ''
    - L226 assign json_match = re.search('\\[[\\s\\S]*\\]', raw_text)
    - L227 if json_match:
      - L228 assign data = json.loads(json_match.group())
      - L229 assign metrics = []
      - L230 for item in data:
        - L231 try:
          - L232 assign mtype = MetricType[item.get('metric_type', 'count').upper()]
          - L233 except KeyError:
            - L234 assign mtype = MetricType.COUNT
        - L236 expr metrics.append(ExtractedMetric(id=f'met_{uuid.uuid4().hex[:8]}', name=item.get('name', 'Unknown'), value=item.get('value', 0), raw_value=str(item.get('raw_value', '')), metric_type=mtype, unit=item.get('unit'), currency=item.get('currency'), period=item.get('period'), change=item.get('change'), change_direction=item.get('change_direction'), comparison_base=item.get('comparison_base'), confidence=0.85, context=item.get('context'), importance_score=item.get('importance', 0.5)))
      - L252 return metrics
    - L253 except Exception as e:
      - L254 expr logger.warning(f'Metric extraction failed: {e}')
  - L256 return []
- L263 def extract_form_fieldstext: str, tables: List[EnhancedExtractedTable]:
  - L264 docstring: "Extract form fields from document."
  - L265 assign prompt = f'Analyze this document and identify if it contains a form. If so, extract all form fields.\n\nText:\n{text[:6000]}\n\nIf this is a form, return JSON:\n```json\n{{\n  "is_form": true,\n  "form_title": "Application Form",\n  "fields": [\n    {{\n      "label": "Full Name",\n      "value": "John Doe",\n      "type": "text",\n      "required": true,\n      "section": "Personal Information"\n    }},\n    {{\n      "label": "Date of Birth",\n      "value": "1990-05-15",\n      "type": "date",\n      "required": true\n    }},\n    {{\n      "label": "Agree to Terms",\n      "value": "checked",\n      "type": "checkbox",\n      "required": true\n    }}\n  ]\n}}\n```\n\nField types: text, checkbox, radio, date, signature, dropdown, number, email, phone\nIf not a form, return {{"is_form": false, "fields": []}}'
  - L302 try:
    - L303 assign client = get_openai_client()
    - L304 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='form_extraction', temperature=0.1)
    - L312 assign raw_text = response.choices[0].message.content or ''
    - L313 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L314 if json_match:
      - L315 assign data = json.loads(json_match.group())
      - L316 if data.get('is_form'):
        - L317 assign fields = []
        - L318 for item in data.get('fields', []):
          - L319 expr fields.append(FormField(id=f'field_{uuid.uuid4().hex[:8]}', label=item.get('label', ''), value=item.get('value'), field_type=item.get('type', 'text'), required=item.get('required', False), section=item.get('section'), is_filled=bool(item.get('value')), confidence=0.85))
        - L329 return fields
    - L330 except Exception as e:
      - L331 expr logger.warning(f'Form extraction failed: {e}')
  - L333 return []
- L340 def extract_invoicetext: str, tables: List[EnhancedExtractedTable]:
  - L341 docstring: "Extract invoice data from document."
  - L343 assign table_context = ''
  - L344 for table in tables[:3]:
    - L345 aug assign table_context Add f"\nTable: {table.title or 'Untitled'}\n"
    - L346 for row in table.rows[:10]:
      - L347 aug assign table_context Add f'  {row}\n'
  - L349 assign prompt = f"""Analyze this document and determine if it's an invoice. If so, extract all invoice data.\n\nText:\n{text[:5000]}\n\nTables:\n{table_context}\n\nIf this is an invoice, return JSON:\n```json\n{{\n  "is_invoice": true,\n  "vendor_name": "Acme Corp",\n  "vendor_address": "123 Main St, City, ST 12345",\n  "vendor_tax_id": "12-3456789",\n  "customer_name": "Client Inc",\n  "customer_address": "456 Oak Ave",\n  "invoice_number": "INV-2025-001",\n  "invoice_date": "2025-01-15",\n  "due_date": "2025-02-15",\n  "purchase_order": "PO-12345",\n  "line_items": [\n    {{\n      "description": "Consulting Services",\n      "quantity": 10,\n      "unit_price": 150.00,\n      "total": 1500.00,\n      "tax": 120.00\n    }}\n  ],\n  "subtotal": 1500.00,\n  "tax_total": 120.00,\n  "discount_total": 0,\n  "grand_total": 1620.00,\n  "currency": "USD",\n  "payment_terms": "Net 30",\n  "notes": "Thank you for your business"\n}}\n```\n\nIf not an invoice, return {{"is_invoice": false}}"""
  - L391 try:
    - L392 assign client = get_openai_client()
    - L393 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='invoice_extraction', temperature=0.1)
    - L401 assign raw_text = response.choices[0].message.content or ''
    - L402 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L403 if json_match:
      - L404 assign data = json.loads(json_match.group())
      - L405 if data.get('is_invoice'):
        - L406 assign line_items = []
        - L407 for item in data.get('line_items', []):
          - L408 expr line_items.append(InvoiceLineItem(id=f'li_{uuid.uuid4().hex[:8]}', description=item.get('description', ''), quantity=item.get('quantity'), unit_price=item.get('unit_price'), total=item.get('total'), tax=item.get('tax'), discount=item.get('discount'), sku=item.get('sku'), category=item.get('category')))
        - L420 return ExtractedInvoice(id=f'inv_{uuid.uuid4().hex[:8]}', vendor_name=data.get('vendor_name'), vendor_address=data.get('vendor_address'), vendor_tax_id=data.get('vendor_tax_id'), customer_name=data.get('customer_name'), customer_address=data.get('customer_address'), invoice_number=data.get('invoice_number'), invoice_date=data.get('invoice_date'), due_date=data.get('due_date'), purchase_order=data.get('purchase_order'), line_items=line_items, subtotal=data.get('subtotal'), tax_total=data.get('tax_total'), discount_total=data.get('discount_total'), grand_total=data.get('grand_total'), currency=data.get('currency', 'USD'), payment_terms=data.get('payment_terms'), notes=data.get('notes'), confidence=0.85)
    - L441 except Exception as e:
      - L442 expr logger.warning(f'Invoice extraction failed: {e}')
  - L444 return None
- L451 def extract_contracttext: str:
  - L452 docstring: "Extract contract data from document."
  - L453 assign prompt = f"""Analyze this document and determine if it's a contract or legal agreement. If so, extract key information.\n\nText:\n{text[:8000]}\n\nIf this is a contract, return JSON:\n```json\n{{\n  "is_contract": true,\n  "contract_type": "Service Agreement",\n  "parties": [\n    {{"name": "Company A", "role": "Provider"}},\n    {{"name": "Company B", "role": "Client"}}\n  ],\n  "effective_date": "2025-01-01",\n  "expiration_date": "2026-01-01",\n  "auto_renewal": true,\n  "renewal_terms": "Automatically renews for 1-year periods",\n  "key_terms": [\n    "Monthly payment of $5,000",\n    "30-day termination notice required"\n  ],\n  "obligations": [\n    {{"party": "Provider", "obligation": "Deliver services monthly"}},\n    {{"party": "Client", "obligation": "Pay within 30 days"}}\n  ],\n  "termination_clauses": [\n    "Either party may terminate with 30 days written notice",\n    "Immediate termination for material breach"\n  ],\n  "governing_law": "State of California",\n  "signatures": [\n    {{"name": "John Doe", "title": "CEO", "date": "2025-01-01", "signed": true}}\n  ]\n}}\n```\n\nIf not a contract, return {{"is_contract": false}}"""
  - L492 try:
    - L493 assign client = get_openai_client()
    - L494 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='contract_extraction', temperature=0.1)
    - L502 assign raw_text = response.choices[0].message.content or ''
    - L503 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L504 if json_match:
      - L505 assign data = json.loads(json_match.group())
      - L506 if data.get('is_contract'):
        - L507 return ExtractedContract(id=f'con_{uuid.uuid4().hex[:8]}', contract_type=data.get('contract_type'), parties=data.get('parties', []), effective_date=data.get('effective_date'), expiration_date=data.get('expiration_date'), auto_renewal=data.get('auto_renewal', False), renewal_terms=data.get('renewal_terms'), key_terms=data.get('key_terms', []), obligations=data.get('obligations', []), termination_clauses=data.get('termination_clauses', []), governing_law=data.get('governing_law'), signatures=data.get('signatures', []), confidence=0.8)
    - L522 except Exception as e:
      - L523 expr logger.warning(f'Contract extraction failed: {e}')
  - L525 return None
- L532 def enhance_tabletable: Dict[str, Any], all_tables: List[Dict[str, Any]]:
  - L533 docstring: "Enhance a table with additional metadata and analysis."
  - L534 assign headers = table.get('headers', [])
  - L535 assign rows = table.get('rows', [])
  - L538 assign data_types = []
  - L539 for col_idx in range(len(headers)):
    - L540 assign col_values = [row[col_idx] for row in rows if col_idx < len(row)]
    - L541 expr data_types.append(_infer_column_type(col_values))
  - L544 assign statistics = {}
  - L545 for (col_idx, dtype) in enumerate(data_types):
    - L546 if dtype == 'numeric':
      - L547 assign values = []
      - L548 for row in rows:
        - L549 if col_idx < len(row):
          - L550 try:
            - L551 assign val = float(str(row[col_idx]).replace(',', '').replace('$', '').replace('%', ''))
            - L552 expr values.append(val)
            - L553 except (ValueError, TypeError):
              - L554 pass
      - L556 if values:
        - L557 assign statistics[headers[col_idx]] = {'min': min(values), 'max': max(values), 'mean': sum(values) / len(values), 'count': len(values)}
  - L565 assign has_totals = False
  - L566 if rows:
    - L567 assign last_row = rows[-1]
    - L568 if any(('total' in str(cell).lower() for cell in last_row)):
      - L569 assign has_totals = True
  - L571 return EnhancedExtractedTable(id=table.get('id', f'table_{uuid.uuid4().hex[:8]}'), title=table.get('title'), headers=headers, rows=rows, data_types=data_types, source_page=table.get('source_page'), source_sheet=table.get('source_sheet'), confidence=table.get('confidence', 0.9), row_count=len(rows), column_count=len(headers), has_totals_row=has_totals, has_header_row=True, statistics=statistics)
- L588 def detect_table_relationshipstables: List[EnhancedExtractedTable]:
  - L589 docstring: "Detect relationships between tables (e.g., continuation across pages)."
  - L590 assign relationships = []
  - L592 for (i, table1) in enumerate(tables):
    - L593 for (j, table2) in enumerate(tables):
      - L594 if i >= j:
        - L595 continue
      - L598 if table1.headers == table2.headers:
        - L600 if table1.source_page and table2.source_page and (abs(table1.source_page - table2.source_page) == 1):
          - L602 expr relationships.append(TableRelationship(table1_id=table1.id, table2_id=table2.id, relationship_type='continuation', confidence=0.9))
        - L609 else:
          - L609 if set(table1.headers) & set(table2.headers):
            - L610 assign shared = len(set(table1.headers) & set(table2.headers))
            - L611 assign total = len(set(table1.headers) | set(table2.headers))
            - L612 if shared / total > 0.3:
              - L613 expr relationships.append(TableRelationship(table1_id=table1.id, table2_id=table2.id, relationship_type='related', confidence=shared / total))
  - L620 return relationships
- L623 def stitch_continuation_tablestables: List[EnhancedExtractedTable], relationships: List[TableRelationship]:
  - L627 docstring: "Merge tables that are continuations of each other."
  - L628 assign continuations = {r.table1_id: r.table2_id for r in relationships if r.relationship_type == 'continuation'}
  - L630 if not continuations:
    - L631 return tables
  - L633 assign merged_ids = set()
  - L634 assign result = []
  - L636 for table in tables:
    - L637 if table.id in merged_ids:
      - L638 continue
    - L641 assign current_id = table.id
    - L642 assign all_rows = list(table.rows)
    - L644 while current_id in continuations:
      - L645 assign next_id = continuations[current_id]
      - L646 expr merged_ids.add(next_id)
      - L649 for t in tables:
        - L650 if t.id == next_id:
          - L651 expr all_rows.extend(t.rows)
          - L652 break
      - L654 assign current_id = next_id
    - L657 assign merged = EnhancedExtractedTable(id=table.id, title=table.title, headers=table.headers, rows=all_rows, data_types=table.data_types, source_page=table.source_page, confidence=table.confidence, row_count=len(all_rows), column_count=len(table.headers), has_totals_row=table.has_totals_row, has_header_row=True, statistics=table.statistics, related_tables=list(merged_ids) if merged_ids else [])
    - L672 expr result.append(merged)
  - L674 return result
- L677 def _infer_column_typevalues: List[Any]:
  - L678 docstring: "Infer the data type of a column."
  - L679 if not values:
    - L680 return 'text'
  - L682 assign numeric_count = 0
  - L683 assign date_count = 0
  - L684 assign total_valid = 0
  - L686 assign date_patterns = ['^\\d{4}-\\d{2}-\\d{2}$', '^\\d{1,2}/\\d{1,2}/\\d{2,4}$', '^\\d{1,2}-\\d{1,2}-\\d{2,4}$']
  - L692 for value in values:
    - L693 assign value_str = str(value).strip()
    - L694 if not value_str:
      - L695 continue
    - L697 aug assign total_valid Add 1
    - L700 try:
      - L701 assign cleaned = re.sub('[$,% ]', '', value_str)
      - L702 expr float(cleaned)
      - L703 aug assign numeric_count Add 1
      - L704 continue
      - L705 except (ValueError, TypeError):
        - L706 pass
    - L709 for pattern in date_patterns:
      - L710 if re.match(pattern, value_str):
        - L711 aug assign date_count Add 1
        - L712 break
  - L714 if total_valid == 0:
    - L715 return 'text'
  - L717 if numeric_count / total_valid >= 0.7:
    - L718 return 'numeric'
  - L719 if date_count / total_valid >= 0.7:
    - L720 return 'datetime'
  - L722 return 'text'
- L729 class EnhancedExtractionService:
  - L730 docstring: "Orchestrates all intelligent extraction operations."
  - L732 def __init__self, use_llm: bool=True:
    - L733 assign self.use_llm = use_llm
    - L734 assign self._client = None
  - L737 def clientself:
    - L738 if self._client is None:
      - L739 assign self._client = get_openai_client()
    - L740 return self._client
  - L742 def extract_allself, text: str, raw_tables: List[Dict[str, Any]]:
    - L747 docstring: "Perform all extraction operations."
    - L749 assign enhanced_tables = [enhance_table(t, raw_tables) for t in raw_tables]
    - L752 assign relationships = detect_table_relationships(enhanced_tables)
    - L753 assign stitched_tables = stitch_continuation_tables(enhanced_tables, relationships)
    - L756 assign entities = extract_all_entities(text, use_llm=self.use_llm)
    - L759 assign metrics = extract_metrics_llm(text, stitched_tables) if self.use_llm else []
    - L762 assign forms = extract_form_fields(text, stitched_tables) if self.use_llm else []
    - L765 assign invoice = extract_invoice(text, stitched_tables) if self.use_llm else None
    - L766 assign invoices = [invoice] if invoice else []
    - L769 assign contract = extract_contract(text) if self.use_llm else None
    - L770 assign contracts = [contract] if contract else []
    - L772 return {'tables': stitched_tables, 'table_relationships': relationships, 'entities': entities, 'metrics': metrics, 'forms': forms, 'invoices': invoices, 'contracts': contracts}

## backend\app\features\analyze\services\extraction_pipeline.py
- L2 docstring: "Extraction pipeline for PDF and Excel documents."
- L3 from __future__ import annotations
- L5 import logging
- L6 import re
- L7 from dataclasses import dataclass, field
- L8 from io import BytesIO
- L9 from pathlib import Path
- L10 from typing import Any, Optional
- L12 assign logger = logging.getLogger('neura.analyze.extraction')
- L14 try:
  - L15 import fitz
  - L16 except ImportError:
    - L17 assign fitz = None
- L19 try:
  - L20 import openpyxl
  - L21 except ImportError:
    - L22 assign openpyxl = None
- L26 class ExtractedContent:
  - L27 docstring: "Content extracted from a document before LLM processing."
  - L29 annotated assign document_type: str
  - L30 annotated assign file_name: str
  - L31 annotated assign page_count: int = 1
  - L32 annotated assign text_content: str = ''
  - L33 annotated assign tables_raw: list[dict[str, Any]] = field(default_factory=list)
  - L34 annotated assign images: list[bytes] = field(default_factory=list)
  - L35 annotated assign sheets: list[dict[str, Any]] = field(default_factory=list)
  - L36 annotated assign errors: list[str] = field(default_factory=list)
- L39 def _stringify_cellvalue: object:
  - L40 docstring: "Convert cell value to string."
  - L41 if value is None:
    - L42 return ''
  - L43 try:
    - L44 assign text = str(value)
    - L45 except Exception as exc:
      - L46 expr logger.debug(f'Failed to stringify cell value: {exc}')
      - L47 return ''
  - L48 return text.strip()
- L51 def _row_has_valuesvalues:
  - L52 docstring: "Check if a row has any non-empty values."
  - L53 for value in values:
    - L54 if value is None:
      - L55 continue
    - L56 if isinstance(value, str):
      - L57 if value.strip():
        - L58 return True
      - L59 continue
    - L60 return True
  - L61 return False
- L64 def _ensure_labelvalue: object, idx: int:
  - L65 docstring: "Ensure a column has a label."
  - L66 if value not in (None, ''):
    - L67 assign text = str(value).strip()
    - L68 if text:
      - L69 return text
  - L70 return f'Column {idx + 1}'
- L73 def _infer_data_type_from_valuesvalues: list[str]:
  - L74 docstring: "Infer data type from a list of string values."
  - L75 if not values:
    - L76 return 'text'
  - L78 assign date_patterns = ['^\\d{4}-\\d{2}-\\d{2}', '^\\d{2}/\\d{2}/\\d{4}', '^\\d{2}-\\d{2}-\\d{4}', '^\\d{1,2}/\\d{1,2}/\\d{2,4}']
  - L85 assign numeric_count = 0
  - L86 assign date_count = 0
  - L87 assign total_valid = 0
  - L89 for val in values[:30]:
    - L90 if not val or not val.strip():
      - L91 continue
    - L92 aug assign total_valid Add 1
    - L93 assign str_val = val.strip()
    - L95 for pattern in date_patterns:
      - L96 if re.match(pattern, str_val):
        - L97 aug assign date_count Add 1
        - L98 break
      - L100 else:
        - L100 try:
          - L101 assign cleaned = str_val.replace(',', '').replace('$', '').replace('%', '').replace(' ', '')
          - L102 expr float(cleaned)
          - L103 aug assign numeric_count Add 1
          - L104 except (ValueError, TypeError):
            - L105 pass
  - L107 if total_valid == 0:
    - L108 return 'text'
  - L109 if date_count >= total_valid * 0.7:
    - L110 return 'datetime'
  - L111 if numeric_count >= total_valid * 0.7:
    - L112 return 'numeric'
  - L113 return 'text'
- L116 def extract_pdf_contentfile_path: Path | str, file_bytes: bytes | None=None:
  - L117 docstring: "Extract text, tables, and images from a PDF file."
  - L118 if fitz is None:
    - L119 return ExtractedContent(document_type='pdf', file_name=str(file_path), errors=['PyMuPDF (fitz) is not installed. Cannot extract PDF content.'])
  - L125 assign file_name = Path(file_path).name if file_path else 'document.pdf'
  - L127 try:
    - L128 if file_bytes:
      - L129 assign doc = fitz.open(stream=file_bytes, filetype='pdf')
      - L131 else:
        - L131 assign doc = fitz.open(str(file_path))
    - L132 except Exception as exc:
      - L133 return ExtractedContent(document_type='pdf', file_name=file_name, errors=[f'Failed to open PDF: {exc}'])
  - L139 assign page_count = len(doc)
  - L140 annotated assign text_parts: list[str] = []
  - L141 annotated assign tables_raw: list[dict[str, Any]] = []
  - L142 annotated assign images: list[bytes] = []
  - L143 annotated assign errors: list[str] = []
  - L145 for (page_num, page) in enumerate(doc):
    - L146 try:
      - L147 assign text = page.get_text('text')
      - L148 if text.strip():
        - L149 expr text_parts.append(f'--- Page {page_num + 1} ---\n{text}')
      - L150 except Exception as exc:
        - L151 expr errors.append(f'Failed to extract text from page {page_num + 1}: {exc}')
    - L153 try:
      - L154 assign page_tables = page.find_tables()
      - L155 if page_tables and page_tables.tables:
        - L156 for (table_idx, table) in enumerate(page_tables.tables):
          - L157 try:
            - L158 assign extracted = table.extract()
            - L159 if extracted and len(extracted) > 0:
              - L160 assign headers = [_stringify_cell(c) or f'Col{i + 1}' for i, c in enumerate(extracted[0])]
              - L161 assign num_cols = len(headers)
              - L162 assign rows = []
              - L163 for row in extracted[1:]:
                - L165 assign normalized_row = [_stringify_cell(row[i] if i < len(row) else '') for i in range(num_cols)]
                - L169 expr rows.append(normalized_row)
              - L171 annotated assign col_values: dict[int, list[str]] = {i: [] for i in range(len(headers))}
              - L172 for row in rows[:30]:
                - L173 for (i, cell) in enumerate(row):
                  - L174 if i < len(headers):
                    - L175 expr col_values[i].append(cell)
              - L177 assign data_types = [_infer_data_type_from_values(col_values.get(i, [])) for i in range(len(headers))]
              - L179 expr tables_raw.append({'id': f'table_p{page_num + 1}_{table_idx + 1}', 'headers': headers, 'rows': rows, 'data_types': data_types, 'source_page': page_num + 1})
            - L186 except Exception as table_exc:
              - L187 expr errors.append(f'Failed to extract table {table_idx + 1} from page {page_num + 1}: {table_exc}')
      - L188 except Exception as exc:
        - L189 expr logger.debug(f'Table extraction not available for page {page_num + 1}: {exc}')
    - L191 try:
      - L192 assign image_list = page.get_images(full=True)
      - L193 for (img_idx, img_info) in enumerate(image_list[:3]):
        - L194 try:
          - L195 assign xref = img_info[0]
          - L196 assign pix = fitz.Pixmap(doc, xref)
          - L197 if pix.n > 4:
            - L198 assign pix = fitz.Pixmap(fitz.csRGB, pix)
          - L199 assign img_bytes = pix.tobytes('png')
          - L200 expr images.append(img_bytes)
          - L201 except Exception as img_exc:
            - L202 expr logger.debug(f'Failed to extract image {img_idx + 1} from page {page_num + 1}: {img_exc}')
      - L203 except Exception as img_list_exc:
        - L204 expr logger.debug(f'Failed to get image list from page {page_num + 1}: {img_list_exc}')
  - L206 expr doc.close()
  - L208 return ExtractedContent(document_type='pdf', file_name=file_name, page_count=page_count, text_content='\n\n'.join(text_parts), tables_raw=tables_raw, images=images[:10], errors=errors)
- L219 def extract_excel_contentfile_path: Path | str, file_bytes: bytes | None=None:
  - L220 docstring: "Extract tables and data from an Excel file."
  - L221 if openpyxl is None:
    - L222 return ExtractedContent(document_type='excel', file_name=str(file_path), errors=['openpyxl is not installed. Cannot extract Excel content.'])
  - L228 assign file_name = Path(file_path).name if file_path else 'document.xlsx'
  - L230 try:
    - L231 if file_bytes:
      - L232 assign wb = openpyxl.load_workbook(filename=BytesIO(file_bytes), data_only=True, read_only=True)
      - L234 else:
        - L234 assign wb = openpyxl.load_workbook(filename=str(file_path), data_only=True, read_only=True)
    - L235 except Exception as exc:
      - L236 return ExtractedContent(document_type='excel', file_name=file_name, errors=[f'Failed to open Excel file: {exc}'])
  - L242 assign sheet_count = len(wb.sheetnames)
  - L243 annotated assign tables_raw: list[dict[str, Any]] = []
  - L244 annotated assign sheets_info: list[dict[str, Any]] = []
  - L245 annotated assign text_parts: list[str] = []
  - L246 annotated assign errors: list[str] = []
  - L249 assign max_sheets = 10
  - L250 if sheet_count > max_sheets:
    - L251 expr logger.warning(f'Excel file has {sheet_count} sheets, processing only first {max_sheets}')
    - L252 expr errors.append(f'File has {sheet_count} sheets - only the first {max_sheets} were processed')
  - L254 for (sheet_idx, sheet_name) in enumerate(wb.sheetnames[:max_sheets]):
    - L255 try:
      - L256 assign sheet = wb[sheet_name]
      - L257 assign rows = list(sheet.iter_rows(values_only=True))
      - L259 assign header_row = None
      - L260 assign header_index = -1
      - L261 for (idx, row) in enumerate(rows):
        - L262 if _row_has_values(row):
          - L263 assign header_row = row
          - L264 assign header_index = idx
          - L265 break
      - L267 if header_row is None:
        - L268 continue
      - L270 assign headers = [_ensure_label(v, i) for i, v in enumerate(header_row)]
      - L271 annotated assign data_rows: list[list[str]] = []
      - L273 for row in rows[header_index + 1:]:
        - L274 if _row_has_values(row):
          - L275 expr data_rows.append([_stringify_cell(row[i] if i < len(row) else '') for i in range(len(headers))])
      - L277 if not data_rows:
        - L278 continue
      - L280 annotated assign col_values: dict[int, list[str]] = {i: [] for i in range(len(headers))}
      - L281 for row in data_rows[:30]:
        - L282 for (i, cell) in enumerate(row):
          - L283 if i < len(headers):
            - L284 expr col_values[i].append(cell)
      - L286 assign data_types = [_infer_data_type_from_values(col_values.get(i, [])) for i in range(len(headers))]
      - L289 assign max_rows = 500
      - L290 assign total_rows = len(data_rows)
      - L291 assign truncated = total_rows > max_rows
      - L292 if truncated:
        - L293 expr logger.info(f"Sheet '{sheet_name}' truncated from {total_rows} to {max_rows} rows")
      - L295 assign table_id = f'table_sheet_{sheet_idx + 1}'
      - L296 expr tables_raw.append({'id': table_id, 'title': sheet_name, 'headers': headers, 'rows': data_rows[:max_rows], 'data_types': data_types, 'source_sheet': sheet_name, 'truncated': truncated, 'total_row_count': total_rows})
      - L307 expr sheets_info.append({'name': sheet_name, 'row_count': total_rows, 'column_count': len(headers), 'headers': headers, 'truncated': truncated})
      - L315 assign preview_rows = min(5, len(data_rows))
      - L316 assign text_preview = f'--- Sheet: {sheet_name} ---\n'
      - L317 aug assign text_preview Add f"Headers: {', '.join(headers)}\n"
      - L318 aug assign text_preview Add f'Rows: {len(data_rows)}\n'
      - L319 for i in range(preview_rows):
        - L320 aug assign text_preview Add f"Row {i + 1}: {', '.join(data_rows[i][:10])}\n"
      - L321 expr text_parts.append(text_preview)
      - L323 except Exception as exc:
        - L324 expr errors.append(f"Failed to process sheet '{sheet_name}': {exc}")
  - L326 expr wb.close()
  - L328 return ExtractedContent(document_type='excel', file_name=file_name, page_count=sheet_count, text_content='\n\n'.join(text_parts), tables_raw=tables_raw, sheets=sheets_info, errors=errors)
- L339 def extract_document_contentfile_path: Path | str | None=None, file_bytes: bytes | None=None, file_name: str | None=None:
  - L344 docstring: "Extract content from a document, auto-detecting type from extension."
  - L345 if file_name is None and file_path:
    - L346 assign file_name = Path(file_path).name
  - L348 if not file_name:
    - L349 assign file_name = 'document'
  - L351 assign ext = Path(file_name).suffix.lower()
  - L353 if ext == '.pdf':
    - L354 return extract_pdf_content(file_path or file_name, file_bytes)
    - L355 else:
      - L355 if ext in ('.xlsx', '.xls', '.xlsm'):
        - L356 return extract_excel_content(file_path or file_name, file_bytes)
        - L358 else:
          - L358 return ExtractedContent(document_type='unknown', file_name=file_name, errors=[f'Unsupported file type: {ext}. Only PDF and Excel files are supported.'])
- L365 def format_content_for_llmcontent: ExtractedContent, max_chars: int=50000:
  - L366 docstring: "Format extracted content into a string for LLM processing."
  - L367 annotated assign parts: list[str] = []
  - L369 if content.text_content:
    - L370 assign text_preview = content.text_content[:max_chars // 2]
    - L371 expr parts.append(f'TEXT CONTENT:\n{text_preview}')
  - L373 if content.tables_raw:
    - L374 expr parts.append(f'\nEXTRACTED TABLES ({len(content.tables_raw)} found):')
    - L375 for table in content.tables_raw[:10]:
      - L376 assign table_str = f"\n[Table: {table.get('id', 'unknown')}]"
      - L377 if table.get('title'):
        - L378 aug assign table_str Add f" Title: {table['title']}"
      - L379 if table.get('source_page'):
        - L380 aug assign table_str Add f" (Page {table['source_page']})"
      - L381 if table.get('source_sheet'):
        - L382 aug assign table_str Add f" (Sheet: {table['source_sheet']})"
      - L384 assign headers = table.get('headers', [])
      - L385 aug assign table_str Add f"\nHeaders: {', '.join(headers)}"
      - L387 assign rows = table.get('rows', [])
      - L388 aug assign table_str Add f'\nRow count: {len(rows)}'
      - L390 for (i, row) in enumerate(rows[:5]):
        - L391 assign row_preview = [str(c)[:50] for c in row[:8]]
        - L392 aug assign table_str Add f"\n  Row {i + 1}: {' | '.join(row_preview)}"
      - L394 if len(rows) > 5:
        - L395 aug assign table_str Add f'\n  ... and {len(rows) - 5} more rows'
      - L397 expr parts.append(table_str)
  - L399 if content.sheets:
    - L400 expr parts.append(f'\nSHEET SUMMARY ({len(content.sheets)} sheets):')
    - L401 for sheet in content.sheets:
      - L402 expr parts.append(f"  - {sheet['name']}: {sheet['row_count']} rows, {sheet['column_count']} columns")
  - L404 assign result = '\n'.join(parts)
  - L405 if len(result) > max_chars:
    - L406 assign result = result[:max_chars] + '\n... (truncated)'
  - L408 return result
- L411 assign __all__ = ['ExtractedContent', 'extract_pdf_content', 'extract_excel_content', 'extract_document_content', 'format_content_for_llm']

## backend\app\features\analyze\services\integrations.py
- L2 docstring: "\nIntegration Capabilities - Data source connections, workflow automation, and e..."
- L10 from __future__ import annotations
- L12 import asyncio
- L13 import hashlib
- L14 import json
- L15 import logging
- L16 import os
- L17 import re
- L18 import uuid
- L19 from abc import ABC, abstractmethod
- L20 from dataclasses import dataclass, field
- L21 from datetime import datetime, timedelta
- L22 from enum import Enum
- L23 from typing import Any, Callable, Dict, List, Optional, Tuple
- L25 from backend.app.features.analyze.schemas.enhanced_analysis import EnhancedAnalysisResult, IntegrationConfig, ScheduledAnalysis, WebhookConfig
- L32 assign logger = logging.getLogger('neura.analyze.integrations')
- L39 class DataSourceType(str, Enum):
  - L40 assign DATABASE = 'database'
  - L41 assign REST_API = 'rest_api'
  - L42 assign CLOUD_STORAGE = 'cloud_storage'
  - L43 assign EMAIL = 'email'
  - L44 assign WEBHOOK = 'webhook'
- L48 class DataSourceConnection:
  - L49 docstring: "A data source connection configuration."
  - L50 annotated assign id: str
  - L51 annotated assign name: str
  - L52 annotated assign type: DataSourceType
  - L53 annotated assign config: Dict[str, Any]
  - L54 annotated assign created_at: datetime = field(default_factory=datetime.utcnow)
  - L55 annotated assign last_used: Optional[datetime] = None
  - L56 annotated assign is_active: bool = True
- L60 class FetchResult:
  - L61 docstring: "Result of fetching data from a source."
  - L62 annotated assign success: bool
  - L63 annotated assign data: Optional[Any] = None
  - L64 annotated assign error: Optional[str] = None
  - L65 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
- L68 class DataSourceConnector(ABC):
  - L69 docstring: "Abstract base class for data source connectors."
  - L72 async def connectself:
    - L73 docstring: "Establish connection to the data source."
    - L74 pass
  - L77 async def fetchself, query: Optional[str]=None:
    - L78 docstring: "Fetch data from the source."
    - L79 pass
  - L82 async def disconnectself:
    - L83 docstring: "Disconnect from the data source."
    - L84 pass
- L87 class DatabaseConnector(DataSourceConnector):
  - L88 docstring: "Connector for database sources."
  - L90 def __init__self, config: Dict[str, Any]:
    - L91 assign self.config = config
    - L92 assign self.connection = None
  - L94 async def connectself:
    - L95 docstring: "Connect to database."
    - L96 assign db_type = self.config.get('type', 'postgresql')
    - L97 assign host = self.config.get('host', 'localhost')
    - L98 assign port = self.config.get('port', 5432)
    - L99 assign database = self.config.get('database', '')
    - L101 expr logger.info(f'Connecting to {db_type} database at {host}:{port}/{database}')
    - L105 assign self.connection = {'connected': True, 'config': self.config}
    - L106 return True
  - L108 async def fetchself, query: Optional[str]=None:
    - L109 docstring: "Execute query and fetch results."
    - L110 if not self.connection:
      - L111 return FetchResult(success=False, error='Not connected')
    - L113 try:
      - L115 return FetchResult(success=True, data=[], metadata={'query': query, 'rows_fetched': 0})
      - L120 except Exception as e:
        - L121 return FetchResult(success=False, error=str(e))
  - L123 async def disconnectself:
    - L124 docstring: "Disconnect from database."
    - L125 assign self.connection = None
- L128 class RestAPIConnector(DataSourceConnector):
  - L129 docstring: "Connector for REST API sources."
  - L131 def __init__self, config: Dict[str, Any]:
    - L132 assign self.config = config
    - L133 assign self.base_url = config.get('base_url', '')
    - L134 assign self.headers = config.get('headers', {})
    - L135 assign self.auth_type = config.get('auth_type', 'none')
  - L137 async def connectself:
    - L138 docstring: "Validate API connection."
    - L140 return bool(self.base_url)
  - L142 async def fetchself, query: Optional[str]=None:
    - L143 docstring: "Fetch data from API endpoint."
    - L144 import aiohttp
    - L146 assign endpoint = query or self.config.get('endpoint', '/')
    - L147 assign url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
    - L149 try:
      - L150 async with aiohttp.ClientSession() as session:
        - L151 async with session.get(url, headers=self.headers) as response:
          - L152 if response.status == 200:
            - L153 assign data = await response.json()
            - L154 return FetchResult(success=True, data=data, metadata={'url': url, 'status': response.status})
            - L160 else:
              - L160 return FetchResult(success=False, error=f'HTTP {response.status}', metadata={'url': url})
      - L165 except Exception as e:
        - L166 return FetchResult(success=False, error=str(e))
  - L168 async def disconnectself:
    - L169 docstring: "No persistent connection to close."
    - L170 pass
- L173 class CloudStorageConnector(DataSourceConnector):
  - L174 docstring: "Connector for cloud storage (S3, GCS, Azure Blob)."
  - L176 def __init__self, config: Dict[str, Any]:
    - L177 assign self.config = config
    - L178 assign self.provider = config.get('provider', 's3')
  - L180 async def connectself:
    - L181 docstring: "Validate cloud storage credentials."
    - L183 return True
  - L185 async def fetchself, query: Optional[str]=None:
    - L186 docstring: "Fetch file from cloud storage."
    - L187 assign bucket = self.config.get('bucket', '')
    - L188 assign key = query or self.config.get('key', '')
    - L190 try:
      - L191 if self.provider == 's3':
        - L193 pass
        - L194 else:
          - L194 if self.provider == 'gcs':
            - L196 pass
            - L197 else:
              - L197 if self.provider == 'azure':
                - L199 pass
      - L201 return FetchResult(success=True, data=b'', metadata={'bucket': bucket, 'key': key, 'provider': self.provider})
      - L206 except Exception as e:
        - L207 return FetchResult(success=False, error=str(e))
  - L209 async def disconnectself:
    - L210 docstring: "No persistent connection to close."
    - L211 pass
- L214 class DataSourceManager:
  - L215 docstring: "Manages data source connections."
  - L217 def __init__self:
    - L218 annotated assign self._connections: Dict[str, DataSourceConnection] = {}
    - L219 annotated assign self._connectors: Dict[str, DataSourceConnector] = {}
  - L221 def register_connectionself, name: str, source_type: DataSourceType, config: Dict[str, Any]:
    - L227 docstring: "Register a new data source connection."
    - L228 assign conn_id = f'ds_{uuid.uuid4().hex[:12]}'
    - L230 assign connection = DataSourceConnection(id=conn_id, name=name, type=source_type, config=config)
    - L237 assign self._connections[conn_id] = connection
    - L240 if source_type == DataSourceType.DATABASE:
      - L241 assign self._connectors[conn_id] = DatabaseConnector(config)
      - L242 else:
        - L242 if source_type == DataSourceType.REST_API:
          - L243 assign self._connectors[conn_id] = RestAPIConnector(config)
          - L244 else:
            - L244 if source_type == DataSourceType.CLOUD_STORAGE:
              - L245 assign self._connectors[conn_id] = CloudStorageConnector(config)
    - L247 return connection
  - L249 async def fetch_dataself, connection_id: str, query: Optional[str]=None:
    - L254 docstring: "Fetch data from a registered connection."
    - L255 assign connector = self._connectors.get(connection_id)
    - L256 if not connector:
      - L257 return FetchResult(success=False, error='Connection not found')
    - L259 assign connection = self._connections.get(connection_id)
    - L260 if connection:
      - L261 assign connection.last_used = datetime.utcnow()
    - L263 expr await connector.connect()
    - L264 assign result = await connector.fetch(query)
    - L265 expr await connector.disconnect()
    - L267 return result
  - L269 def list_connectionsself:
    - L270 docstring: "List all registered connections."
    - L271 return list(self._connections.values())
- L279 class AnalysisTrigger:
  - L280 docstring: "A trigger for automated analysis."
  - L281 annotated assign id: str
  - L282 annotated assign name: str
  - L283 annotated assign trigger_type: str
  - L284 annotated assign config: Dict[str, Any]
  - L285 annotated assign action: str
  - L286 annotated assign enabled: bool = True
  - L287 annotated assign last_triggered: Optional[datetime] = None
- L291 class AnalysisPipeline:
  - L292 docstring: "A multi-step analysis pipeline."
  - L293 annotated assign id: str
  - L294 annotated assign name: str
  - L295 annotated assign steps: List[Dict[str, Any]]
  - L296 annotated assign created_at: datetime = field(default_factory=datetime.utcnow)
  - L297 annotated assign enabled: bool = True
- L301 class PipelineExecution:
  - L302 docstring: "An execution of an analysis pipeline."
  - L303 annotated assign id: str
  - L304 annotated assign pipeline_id: str
  - L305 annotated assign started_at: datetime
  - L306 annotated assign completed_at: Optional[datetime] = None
  - L307 annotated assign status: str = 'running'
  - L308 annotated assign step_results: Dict[str, Any] = field(default_factory=dict)
  - L309 annotated assign error: Optional[str] = None
- L312 class WorkflowAutomationService:
  - L313 docstring: "Manages workflow automation."
  - L315 def __init__self:
    - L316 annotated assign self._triggers: Dict[str, AnalysisTrigger] = {}
    - L317 annotated assign self._pipelines: Dict[str, AnalysisPipeline] = {}
    - L318 annotated assign self._schedules: Dict[str, ScheduledAnalysis] = {}
    - L319 annotated assign self._executions: Dict[str, PipelineExecution] = {}
    - L320 annotated assign self._webhooks: Dict[str, WebhookConfig] = {}
  - L322 def create_triggerself, name: str, trigger_type: str, config: Dict[str, Any], action: str:
    - L329 docstring: "Create a new analysis trigger."
    - L330 assign trigger = AnalysisTrigger(id=f'trig_{uuid.uuid4().hex[:12]}', name=name, trigger_type=trigger_type, config=config, action=action)
    - L337 assign self._triggers[trigger.id] = trigger
    - L338 return trigger
  - L340 def create_pipelineself, name: str, steps: List[Dict[str, Any]]:
    - L345 docstring: "Create an analysis pipeline."
    - L346 assign pipeline = AnalysisPipeline(id=f'pipe_{uuid.uuid4().hex[:12]}', name=name, steps=steps)
    - L351 assign self._pipelines[pipeline.id] = pipeline
    - L352 return pipeline
  - L354 async def execute_pipelineself, pipeline_id: str, input_data: Dict[str, Any]:
    - L359 docstring: "Execute an analysis pipeline."
    - L360 assign pipeline = self._pipelines.get(pipeline_id)
    - L361 if not pipeline:
      - L362 raise ValueError(f'Pipeline not found: {pipeline_id}')
    - L364 assign execution = PipelineExecution(id=f'exec_{uuid.uuid4().hex[:12]}', pipeline_id=pipeline_id, started_at=datetime.utcnow())
    - L369 assign self._executions[execution.id] = execution
    - L371 try:
      - L372 assign context = {'input': input_data}
      - L374 for (i, step) in enumerate(pipeline.steps):
        - L375 assign step_id = step.get('id', f'step_{i}')
        - L376 assign step_type = step.get('type')
        - L378 expr logger.info(f'Executing pipeline step: {step_id} ({step_type})')
        - L381 if step_type == 'extract':
          - L383 assign result = {'extracted': True}
          - L384 else:
            - L384 if step_type == 'analyze':
              - L386 assign result = {'analyzed': True}
              - L387 else:
                - L387 if step_type == 'transform':
                  - L389 assign result = {'transformed': True}
                  - L390 else:
                    - L390 if step_type == 'export':
                      - L392 assign result = {'exported': True}
                      - L393 else:
                        - L393 if step_type == 'notify':
                          - L395 assign result = {'notified': True}
                          - L397 else:
                            - L397 assign result = {'unknown_step': step_type}
        - L399 assign execution.step_results[step_id] = result
        - L400 assign context[step_id] = result
      - L402 assign execution.status = 'completed'
      - L403 assign execution.completed_at = datetime.utcnow()
      - L405 except Exception as e:
        - L406 assign execution.status = 'failed'
        - L407 assign execution.error = str(e)
        - L408 assign execution.completed_at = datetime.utcnow()
        - L409 expr logger.error(f'Pipeline execution failed: {e}')
    - L411 return execution
  - L413 def schedule_analysisself, name: str, source_config: Dict[str, Any], schedule: str, analysis_config: Dict[str, Any], notifications: List[str]=None:
    - L421 docstring: "Schedule a recurring analysis."
    - L422 assign scheduled = ScheduledAnalysis(id=f'sched_{uuid.uuid4().hex[:12]}', name=name, source_type=source_config.get('type', 'upload'), source_config=source_config, schedule=schedule, notifications=notifications or [], enabled=True)
    - L431 assign self._schedules[scheduled.id] = scheduled
    - L432 return scheduled
  - L434 def register_webhookself, url: str, events: List[str], secret: Optional[str]=None:
    - L440 docstring: "Register a webhook for notifications."
    - L441 assign webhook = WebhookConfig(url=url, events=events, secret=secret, enabled=True)
    - L447 assign webhook_id = f'hook_{uuid.uuid4().hex[:12]}'
    - L448 assign self._webhooks[webhook_id] = webhook
    - L449 return webhook
  - L451 async def send_webhookself, webhook_id: str, event: str, payload: Dict[str, Any]:
    - L457 docstring: "Send a webhook notification."
    - L458 assign webhook = self._webhooks.get(webhook_id)
    - L459 if not webhook or not webhook.enabled:
      - L460 return False
    - L462 if event not in webhook.events:
      - L463 return False
    - L465 import aiohttp
    - L467 assign headers = {'Content-Type': 'application/json'}
    - L470 if webhook.secret:
      - L471 assign signature = hashlib.sha256(f'{webhook.secret}{json.dumps(payload)}'.encode()).hexdigest()
      - L474 assign headers['X-Webhook-Signature'] = signature
    - L476 try:
      - L477 async with aiohttp.ClientSession() as session:
        - L478 async with session.post(webhook.url, json={'event': event, 'data': payload}, headers=headers) as response:
          - L483 return response.status < 400
      - L484 except Exception as e:
        - L485 expr logger.error(f'Webhook delivery failed: {e}')
        - L486 return False
- L493 class ExternalToolIntegration(ABC):
  - L494 docstring: "Abstract base class for external tool integrations."
  - L497 async def send_messageself, message: str, **kwargs:
    - L498 docstring: "Send a message to the external tool."
    - L499 pass
  - L502 async def create_itemself, data: Dict[str, Any]:
    - L503 docstring: "Create an item (task, ticket, etc.) in the external tool."
    - L504 pass
- L507 class SlackIntegration(ExternalToolIntegration):
  - L508 docstring: "Slack integration."
  - L510 def __init__self, config: Dict[str, Any]:
    - L511 assign self.webhook_url = config.get('webhook_url', '')
    - L512 assign self.channel = config.get('channel', '')
    - L513 assign self.bot_token = config.get('bot_token', '')
  - L515 async def send_messageself, message: str, **kwargs:
    - L516 docstring: "Send message to Slack channel."
    - L517 import aiohttp
    - L519 assign payload = {'text': message, 'channel': kwargs.get('channel', self.channel)}
    - L525 if 'blocks' in kwargs:
      - L526 assign payload['blocks'] = kwargs['blocks']
    - L528 try:
      - L529 async with aiohttp.ClientSession() as session:
        - L530 async with session.post(self.webhook_url, json=payload) as response:
          - L531 return response.status == 200
      - L532 except Exception as e:
        - L533 expr logger.error(f'Slack message failed: {e}')
        - L534 return False
  - L536 async def create_itemself, data: Dict[str, Any]:
    - L537 docstring: "Create a Slack reminder or scheduled message."
    - L539 return None
  - L541 def format_analysis_summaryself, result: EnhancedAnalysisResult:
    - L542 docstring: "Format analysis result as Slack blocks."
    - L543 assign blocks = [{'type': 'header', 'text': {'type': 'plain_text', 'text': f'Analysis Complete: {result.document_name}'}}, {'type': 'section', 'fields': [{'type': 'mrkdwn', 'text': f'*Tables:* {result.total_tables}'}, {'type': 'mrkdwn', 'text': f'*Metrics:* {result.total_metrics}'}, {'type': 'mrkdwn', 'text': f'*Insights:* {len(result.insights)}'}, {'type': 'mrkdwn', 'text': f'*Risks:* {len(result.risks)}'}]}]
    - L563 if result.insights:
      - L564 assign insight_text = '\n'.join([f'\u2022 {i.title}' for i in result.insights[:3]])
      - L565 expr blocks.append({'type': 'section', 'text': {'type': 'mrkdwn', 'text': f'*Key Insights:*\n{insight_text}'}})
    - L570 return {'blocks': blocks}
- L573 class TeamsIntegration(ExternalToolIntegration):
  - L574 docstring: "Microsoft Teams integration."
  - L576 def __init__self, config: Dict[str, Any]:
    - L577 assign self.webhook_url = config.get('webhook_url', '')
  - L579 async def send_messageself, message: str, **kwargs:
    - L580 docstring: "Send message to Teams channel."
    - L581 import aiohttp
    - L584 assign payload = {'@type': 'MessageCard', '@context': 'http://schema.org/extensions', 'summary': message[:50], 'themeColor': '0076D7', 'title': kwargs.get('title', 'NeuraReport Analysis'), 'text': message}
    - L593 try:
      - L594 async with aiohttp.ClientSession() as session:
        - L595 async with session.post(self.webhook_url, json=payload) as response:
          - L596 return response.status == 200
      - L597 except Exception as e:
        - L598 expr logger.error(f'Teams message failed: {e}')
        - L599 return False
  - L601 async def create_itemself, data: Dict[str, Any]:
    - L602 docstring: "Create Teams task or Planner item."
    - L603 return None
- L606 class JiraIntegration(ExternalToolIntegration):
  - L607 docstring: "Jira integration."
  - L609 def __init__self, config: Dict[str, Any]:
    - L610 assign self.base_url = config.get('base_url', '')
    - L611 assign self.email = config.get('email', '')
    - L612 assign self.api_token = config.get('api_token', '')
    - L613 assign self.project_key = config.get('project_key', '')
  - L615 async def send_messageself, message: str, **kwargs:
    - L616 docstring: "Add comment to a Jira issue."
    - L617 assign issue_key = kwargs.get('issue_key')
    - L618 if not issue_key:
      - L619 return False
    - L621 import aiohttp
    - L622 from aiohttp import BasicAuth
    - L624 assign url = f'{self.base_url}/rest/api/3/issue/{issue_key}/comment'
    - L625 assign payload = {'body': {'type': 'doc', 'version': 1, 'content': [{'type': 'paragraph', 'content': [{'type': 'text', 'text': message}]}]}}
    - L633 try:
      - L634 async with aiohttp.ClientSession() as session:
        - L635 async with session.post(url, json=payload, auth=BasicAuth(self.email, self.api_token)) as response:
          - L640 return response.status < 400
      - L641 except Exception as e:
        - L642 expr logger.error(f'Jira comment failed: {e}')
        - L643 return False
  - L645 async def create_itemself, data: Dict[str, Any]:
    - L646 docstring: "Create a Jira issue from analysis findings."
    - L647 import aiohttp
    - L648 from aiohttp import BasicAuth
    - L650 assign url = f'{self.base_url}/rest/api/3/issue'
    - L651 assign payload = {'fields': {'project': {'key': self.project_key}, 'summary': data.get('title', 'Analysis Finding'), 'description': {'type': 'doc', 'version': 1, 'content': [{'type': 'paragraph', 'content': [{'type': 'text', 'text': data.get('description', '')}]}]}, 'issuetype': {'name': data.get('issue_type', 'Task')}, 'priority': {'name': data.get('priority', 'Medium')}}}
    - L665 try:
      - L666 async with aiohttp.ClientSession() as session:
        - L667 async with session.post(url, json=payload, auth=BasicAuth(self.email, self.api_token)) as response:
          - L672 if response.status < 400:
            - L673 assign result = await response.json()
            - L674 return result.get('key')
      - L675 except Exception as e:
        - L676 expr logger.error(f'Jira issue creation failed: {e}')
    - L678 return None
- L681 class EmailIntegration(ExternalToolIntegration):
  - L682 docstring: "Email integration for sending analysis reports."
  - L684 def __init__self, config: Dict[str, Any]:
    - L685 assign self.smtp_host = config.get('smtp_host', '')
    - L686 assign self.smtp_port = config.get('smtp_port', 587)
    - L687 assign self.username = config.get('username', '')
    - L688 assign self.password = config.get('password', '')
    - L689 assign self.from_email = config.get('from_email', '')
  - L691 async def send_messageself, message: str, **kwargs:
    - L692 docstring: "Send email with analysis summary."
    - L693 assign to_emails = kwargs.get('to', [])
    - L694 assign subject = kwargs.get('subject', 'Analysis Report')
    - L695 assign html_content = kwargs.get('html', '')
    - L697 if not to_emails:
      - L698 return False
    - L700 import smtplib
    - L701 from email.mime.multipart import MIMEMultipart
    - L702 from email.mime.text import MIMEText
    - L704 try:
      - L705 assign msg = MIMEMultipart('alternative')
      - L706 assign msg['Subject'] = subject
      - L707 assign msg['From'] = self.from_email
      - L708 assign msg['To'] = ', '.join(to_emails)
      - L711 expr msg.attach(MIMEText(message, 'plain'))
      - L714 if html_content:
        - L715 expr msg.attach(MIMEText(html_content, 'html'))
      - L717 with smtplib.SMTP(self.smtp_host, self.smtp_port) as server:
        - L718 expr server.starttls()
        - L719 expr server.login(self.username, self.password)
        - L720 expr server.sendmail(self.from_email, to_emails, msg.as_string())
      - L722 return True
      - L723 except Exception as e:
        - L724 expr logger.error(f'Email send failed: {e}')
        - L725 return False
  - L727 async def create_itemself, data: Dict[str, Any]:
    - L728 docstring: "N/A for email integration."
    - L729 return None
- L736 class IntegrationService:
  - L737 docstring: "Orchestrates all integration capabilities."
  - L739 def __init__self:
    - L740 assign self.data_sources = DataSourceManager()
    - L741 assign self.workflows = WorkflowAutomationService()
    - L742 annotated assign self._integrations: Dict[str, ExternalToolIntegration] = {}
  - L744 def register_integrationself, name: str, integration_type: str, config: Dict[str, Any]:
    - L750 docstring: "Register an external tool integration."
    - L751 assign integration_id = f'int_{uuid.uuid4().hex[:12]}'
    - L753 if integration_type == 'slack':
      - L754 assign self._integrations[integration_id] = SlackIntegration(config)
      - L755 else:
        - L755 if integration_type == 'teams':
          - L756 assign self._integrations[integration_id] = TeamsIntegration(config)
          - L757 else:
            - L757 if integration_type == 'jira':
              - L758 assign self._integrations[integration_id] = JiraIntegration(config)
              - L759 else:
                - L759 if integration_type == 'email':
                  - L760 assign self._integrations[integration_id] = EmailIntegration(config)
                  - L762 else:
                    - L762 raise ValueError(f'Unknown integration type: {integration_type}')
    - L764 return integration_id
  - L766 async def send_notificationself, integration_id: str, message: str, **kwargs:
    - L772 docstring: "Send notification via integration."
    - L773 assign integration = self._integrations.get(integration_id)
    - L774 if not integration:
      - L775 return False
    - L776 return await integration.send_message(message, **kwargs)
  - L778 async def create_external_itemself, integration_id: str, data: Dict[str, Any]:
    - L783 docstring: "Create item in external tool."
    - L784 assign integration = self._integrations.get(integration_id)
    - L785 if not integration:
      - L786 return None
    - L787 return await integration.create_item(data)
  - L789 async def broadcast_analysis_completeself, result: EnhancedAnalysisResult:
    - L793 docstring: "Broadcast analysis completion to all integrations."
    - L794 assign results = {}
    - L796 assign message = f'Analysis complete: {result.document_name}\n'
    - L797 aug assign message Add f'Found {result.total_tables} tables, {result.total_metrics} metrics\n'
    - L798 if result.insights:
      - L799 aug assign message Add f'Top insight: {result.insights[0].title}'
    - L801 for (int_id, integration) in self._integrations.items():
      - L802 try:
        - L803 assign success = await integration.send_message(message)
        - L804 assign results[int_id] = success
        - L805 except Exception as e:
          - L806 expr logger.error(f'Broadcast to {int_id} failed: {e}')
          - L807 assign results[int_id] = False
    - L809 return results
  - L812 def register_data_sourceself, *args, **kwargs:
    - L813 return self.data_sources.register_connection(*args, **kwargs)
  - L815 async def fetch_from_sourceself, *args, **kwargs:
    - L816 return await self.data_sources.fetch_data(*args, **kwargs)
  - L819 def create_triggerself, *args, **kwargs:
    - L820 return self.workflows.create_trigger(*args, **kwargs)
  - L822 def create_pipelineself, *args, **kwargs:
    - L823 return self.workflows.create_pipeline(*args, **kwargs)
  - L825 async def execute_pipelineself, *args, **kwargs:
    - L826 return await self.workflows.execute_pipeline(*args, **kwargs)
  - L828 def schedule_analysisself, *args, **kwargs:
    - L829 return self.workflows.schedule_analysis(*args, **kwargs)
  - L831 def register_webhookself, *args, **kwargs:
    - L832 return self.workflows.register_webhook(*args, **kwargs)

## backend\app\features\analyze\services\user_experience.py
- L2 docstring: "\nUser Experience Features - Analysis customization, collaboration, and real-tim..."
- L10 from __future__ import annotations
- L12 import asyncio
- L13 import json
- L14 import logging
- L15 import uuid
- L16 from dataclasses import dataclass, field
- L17 from datetime import datetime
- L18 from typing import Any, AsyncGenerator, Callable, Dict, List, Optional
- L20 from backend.app.features.analyze.schemas.enhanced_analysis import AnalysisDepth, AnalysisPreferences, EnhancedAnalysisResult, SummaryMode
- L27 assign logger = logging.getLogger('neura.analyze.ux')
- L35 assign INDUSTRY_CONFIGS = {'finance': {'focus_areas': ['financial', 'risk', 'compliance'], 'key_metrics': ['revenue', 'margin', 'roi', 'debt', 'equity'], 'terminology': ['EBITDA', 'P/E ratio', 'liquidity', 'leverage'], 'analysis_prompts': {'summary': 'Focus on financial performance, risk indicators, and compliance matters.', 'insights': 'Identify financial risks, opportunities, and key performance drivers.'}}, 'healthcare': {'focus_areas': ['operational', 'compliance', 'patient'], 'key_metrics': ['patient_volume', 'readmission_rate', 'cost_per_case'], 'terminology': ['HIPAA', 'CMS', 'quality metrics', 'patient outcomes'], 'analysis_prompts': {'summary': 'Focus on patient care metrics, compliance, and operational efficiency.', 'insights': 'Identify quality improvement opportunities and compliance risks.'}}, 'technology': {'focus_areas': ['growth', 'innovation', 'technical'], 'key_metrics': ['mrr', 'arr', 'churn', 'cac', 'ltv'], 'terminology': ['SaaS', 'API', 'scalability', 'uptime'], 'analysis_prompts': {'summary': 'Focus on growth metrics, product performance, and technical capabilities.', 'insights': 'Identify growth opportunities, technical risks, and market trends.'}}, 'retail': {'focus_areas': ['sales', 'inventory', 'customer'], 'key_metrics': ['same_store_sales', 'inventory_turnover', 'customer_acquisition'], 'terminology': ['SKU', 'foot traffic', 'conversion rate', 'basket size'], 'analysis_prompts': {'summary': 'Focus on sales performance, inventory management, and customer behavior.', 'insights': 'Identify sales trends, inventory optimization, and customer insights.'}}, 'manufacturing': {'focus_areas': ['operational', 'quality', 'supply_chain'], 'key_metrics': ['oee', 'defect_rate', 'cycle_time', 'inventory_days'], 'terminology': ['lean', 'six sigma', 'yield', 'throughput'], 'analysis_prompts': {'summary': 'Focus on production efficiency, quality metrics, and supply chain.', 'insights': 'Identify operational improvements, quality issues, and supply risks.'}}}
- L84 assign OUTPUT_FORMATS = {'executive': {'max_summary_words': 150, 'max_insights': 5, 'include_technical_details': False, 'visualization_style': 'simple', 'language_style': 'concise'}, 'technical': {'max_summary_words': 500, 'max_insights': 15, 'include_technical_details': True, 'visualization_style': 'detailed', 'language_style': 'technical'}, 'visual': {'max_summary_words': 100, 'max_insights': 8, 'include_technical_details': False, 'visualization_style': 'rich', 'chart_priority': True, 'language_style': 'brief'}}
- L111 class AnalysisConfiguration:
  - L112 docstring: "Complete analysis configuration based on preferences."
  - L113 annotated assign preferences: AnalysisPreferences
  - L114 annotated assign industry_config: Dict[str, Any] = field(default_factory=dict)
  - L115 annotated assign output_config: Dict[str, Any] = field(default_factory=dict)
  - L116 annotated assign custom_prompts: Dict[str, str] = field(default_factory=dict)
- L119 def build_analysis_configurationpreferences: AnalysisPreferences:
  - L122 docstring: "Build complete analysis configuration from preferences."
  - L124 assign industry_config = INDUSTRY_CONFIGS.get(preferences.industry, {})
  - L127 assign output_config = OUTPUT_FORMATS.get(preferences.output_format, OUTPUT_FORMATS['executive'])
  - L130 assign custom_prompts = {}
  - L132 assign focus_str = ', '.join(preferences.focus_areas) if preferences.focus_areas else 'general'
  - L133 assign depth_modifier = {AnalysisDepth.QUICK: 'Be brief and highlight only the most critical points.', AnalysisDepth.STANDARD: 'Provide a balanced analysis with key details.', AnalysisDepth.COMPREHENSIVE: 'Provide thorough analysis with supporting details.', AnalysisDepth.DEEP: 'Provide exhaustive analysis with all available details and nuances.'}
  - L140 assign base_prompt = f"Focus areas: {focus_str}. {depth_modifier.get(preferences.analysis_depth, '')}"
  - L142 if industry_config:
    - L143 aug assign base_prompt Add f' Industry context: {preferences.industry}. '
    - L144 aug assign base_prompt Add industry_config.get('analysis_prompts', {}).get('summary', '')
  - L146 assign custom_prompts['base'] = base_prompt
  - L147 assign custom_prompts['summary'] = industry_config.get('analysis_prompts', {}).get('summary', '')
  - L148 assign custom_prompts['insights'] = industry_config.get('analysis_prompts', {}).get('insights', '')
  - L150 return AnalysisConfiguration(preferences=preferences, industry_config=industry_config, output_config=output_config, custom_prompts=custom_prompts)
- L158 def get_default_preferences:
  - L159 docstring: "Get default analysis preferences."
  - L160 return AnalysisPreferences(analysis_depth=AnalysisDepth.STANDARD, focus_areas=['financial', 'operational'], output_format='executive', language='en', currency_preference='USD', enable_predictions=True, enable_recommendations=True, auto_chart_generation=True, max_charts=10, summary_mode=SummaryMode.EXECUTIVE)
- L179 class AnalysisComment:
  - L180 docstring: "A comment on an analysis or specific element."
  - L181 annotated assign id: str
  - L182 annotated assign analysis_id: str
  - L183 annotated assign user_id: str
  - L184 annotated assign user_name: str
  - L185 annotated assign content: str
  - L186 annotated assign element_type: Optional[str] = None
  - L187 annotated assign element_id: Optional[str] = None
  - L188 annotated assign created_at: datetime = field(default_factory=datetime.utcnow)
  - L189 annotated assign updated_at: datetime = field(default_factory=datetime.utcnow)
  - L190 annotated assign replies: List['AnalysisComment'] = field(default_factory=list)
  - L191 annotated assign resolved: bool = False
- L195 class AnalysisShare:
  - L196 docstring: "Sharing configuration for an analysis."
  - L197 annotated assign id: str
  - L198 annotated assign analysis_id: str
  - L199 annotated assign share_type: str
  - L200 annotated assign access_level: str
  - L201 annotated assign created_by: str
  - L202 annotated assign created_at: datetime = field(default_factory=datetime.utcnow)
  - L203 annotated assign expires_at: Optional[datetime] = None
  - L204 annotated assign password_protected: bool = False
  - L205 annotated assign access_count: int = 0
  - L206 annotated assign allowed_emails: List[str] = field(default_factory=list)
- L210 class AnalysisVersion:
  - L211 docstring: "A version of an analysis."
  - L212 annotated assign version_id: str
  - L213 annotated assign analysis_id: str
  - L214 annotated assign version_number: int
  - L215 annotated assign created_at: datetime
  - L216 annotated assign created_by: str
  - L217 annotated assign description: str
  - L218 annotated assign changes: List[str]
  - L219 annotated assign snapshot: Dict[str, Any]
- L222 class CollaborationService:
  - L223 docstring: "Manages collaboration features."
  - L225 def __init__self:
    - L226 annotated assign self._comments: Dict[str, List[AnalysisComment]] = {}
    - L227 annotated assign self._shares: Dict[str, List[AnalysisShare]] = {}
    - L228 annotated assign self._versions: Dict[str, List[AnalysisVersion]] = {}
  - L230 def add_commentself, analysis_id: str, user_id: str, user_name: str, content: str, element_type: Optional[str]=None, element_id: Optional[str]=None, parent_comment_id: Optional[str]=None:
    - L240 docstring: "Add a comment to an analysis."
    - L241 assign comment = AnalysisComment(id=f'comment_{uuid.uuid4().hex[:12]}', analysis_id=analysis_id, user_id=user_id, user_name=user_name, content=content, element_type=element_type, element_id=element_id)
    - L251 if analysis_id not in self._comments:
      - L252 assign self._comments[analysis_id] = []
    - L254 if parent_comment_id:
      - L256 for existing in self._comments[analysis_id]:
        - L257 if existing.id == parent_comment_id:
          - L258 expr existing.replies.append(comment)
          - L259 break
      - L261 else:
        - L261 expr self._comments[analysis_id].append(comment)
    - L263 return comment
  - L265 def get_commentsself, analysis_id: str:
    - L266 docstring: "Get all comments for an analysis."
    - L267 return self._comments.get(analysis_id, [])
  - L269 def create_share_linkself, analysis_id: str, created_by: str, access_level: str='view', expires_hours: Optional[int]=None, password_protected: bool=False, allowed_emails: List[str]=None:
    - L278 docstring: "Create a shareable link for an analysis."
    - L279 assign share = AnalysisShare(id=f'share_{uuid.uuid4().hex[:12]}', analysis_id=analysis_id, share_type='link', access_level=access_level, created_by=created_by, expires_at=datetime.utcnow() + timedelta(hours=expires_hours) if expires_hours else None, password_protected=password_protected, allowed_emails=allowed_emails or [])
    - L290 if analysis_id not in self._shares:
      - L291 assign self._shares[analysis_id] = []
    - L292 expr self._shares[analysis_id].append(share)
    - L294 return share
  - L296 def save_versionself, analysis_id: str, created_by: str, description: str, analysis_snapshot: Dict[str, Any]:
    - L303 docstring: "Save a version of the analysis."
    - L304 if analysis_id not in self._versions:
      - L305 assign self._versions[analysis_id] = []
    - L307 assign version_number = len(self._versions[analysis_id]) + 1
    - L310 assign changes = []
    - L311 if self._versions[analysis_id]:
      - L312 assign prev = self._versions[analysis_id][-1].snapshot
      - L314 assign prev_metrics = len(prev.get('metrics', []))
      - L315 assign curr_metrics = len(analysis_snapshot.get('metrics', []))
      - L316 if curr_metrics != prev_metrics:
        - L317 expr changes.append(f'Metrics: {prev_metrics} -> {curr_metrics}')
    - L319 assign version = AnalysisVersion(version_id=f'v_{uuid.uuid4().hex[:12]}', analysis_id=analysis_id, version_number=version_number, created_at=datetime.utcnow(), created_by=created_by, description=description, changes=changes, snapshot=analysis_snapshot)
    - L330 expr self._versions[analysis_id].append(version)
    - L331 return version
  - L333 def get_version_historyself, analysis_id: str:
    - L334 docstring: "Get version history for an analysis."
    - L335 return self._versions.get(analysis_id, [])
- L339 from datetime import timedelta
- L347 class ProgressUpdate:
  - L348 docstring: "A progress update for streaming."
  - L349 annotated assign stage: str
  - L350 annotated assign progress: float
  - L351 annotated assign detail: str
  - L352 annotated assign timestamp: datetime = field(default_factory=datetime.utcnow)
  - L353 annotated assign data: Optional[Dict[str, Any]] = None
- L357 class IncrementalResult:
  - L358 docstring: "An incremental result during analysis."
  - L359 annotated assign result_type: str
  - L360 annotated assign data: Any
  - L361 annotated assign is_final: bool = False
- L364 class StreamingAnalysisSession:
  - L365 docstring: "Manages a streaming analysis session."
  - L367 def __init__self, session_id: str:
    - L368 assign self.session_id = session_id
    - L369 assign self.started_at = datetime.utcnow()
    - L370 assign self.progress = 0.0
    - L371 assign self.current_stage = 'initializing'
    - L372 assign self.is_cancelled = False
    - L373 assign self.is_complete = False
    - L374 annotated assign self._progress_callbacks: List[Callable[[ProgressUpdate], None]] = []
    - L375 annotated assign self._result_callbacks: List[Callable[[IncrementalResult], None]] = []
    - L376 annotated assign self._incremental_results: List[IncrementalResult] = []
  - L378 def add_progress_callbackself, callback: Callable[[ProgressUpdate], None]:
    - L379 docstring: "Add a callback for progress updates."
    - L380 expr self._progress_callbacks.append(callback)
  - L382 def add_result_callbackself, callback: Callable[[IncrementalResult], None]:
    - L383 docstring: "Add a callback for incremental results."
    - L384 expr self._result_callbacks.append(callback)
  - L386 def update_progressself, stage: str, progress: float, detail: str, data: Optional[Dict]=None:
    - L387 docstring: "Update progress and notify callbacks."
    - L388 if self.is_cancelled:
      - L389 raise asyncio.CancelledError('Analysis was cancelled')
    - L391 assign self.current_stage = stage
    - L392 assign self.progress = progress
    - L394 assign update = ProgressUpdate(stage=stage, progress=progress, detail=detail, data=data)
    - L401 for callback in self._progress_callbacks:
      - L402 try:
        - L403 expr callback(update)
        - L404 except Exception as e:
          - L405 expr logger.warning(f'Progress callback error: {e}')
  - L407 def emit_resultself, result_type: str, data: Any, is_final: bool=False:
    - L408 docstring: "Emit an incremental result."
    - L409 assign result = IncrementalResult(result_type=result_type, data=data, is_final=is_final)
    - L414 expr self._incremental_results.append(result)
    - L416 for callback in self._result_callbacks:
      - L417 try:
        - L418 expr callback(result)
        - L419 except Exception as e:
          - L420 expr logger.warning(f'Result callback error: {e}')
  - L422 def cancelself:
    - L423 docstring: "Cancel the analysis session."
    - L424 assign self.is_cancelled = True
  - L426 def completeself:
    - L427 docstring: "Mark the session as complete."
    - L428 assign self.is_complete = True
    - L429 assign self.progress = 100.0
- L432 async def stream_analysis_progresssession: StreamingAnalysisSession:
  - L435 docstring: "Stream analysis progress as server-sent events."
  - L436 assign last_progress = -1
  - L438 while not session.is_complete and (not session.is_cancelled):
    - L439 if session.progress != last_progress:
      - L440 assign last_progress = session.progress
      - L441 expr (yield {'event': 'progress', 'stage': session.current_stage, 'progress': session.progress, 'timestamp': datetime.utcnow().isoformat()})
    - L449 for result in session._incremental_results:
      - L450 expr (yield {'event': 'result', 'type': result.result_type, 'data': result.data if isinstance(result.data, dict) else str(result.data), 'is_final': result.is_final})
    - L456 expr session._incremental_results.clear()
    - L458 expr await asyncio.sleep(0.1)
  - L460 if session.is_cancelled:
    - L461 expr (yield {'event': 'cancelled', 'timestamp': datetime.utcnow().isoformat()})
    - L463 else:
      - L463 expr (yield {'event': 'complete', 'progress': 100, 'timestamp': datetime.utcnow().isoformat()})
- L470 def generate_suggested_questionstables: List[Any], metrics: List[Any], entities: List[Any]:
  - L475 docstring: "Generate suggested questions based on extracted data."
  - L476 assign questions = []
  - L479 if metrics:
    - L480 assign metric_names = [m.name for m in metrics[:5]]
    - L481 for name in metric_names:
      - L482 expr questions.append(f'What factors contributed to the {name}?')
      - L483 expr questions.append(f'How does the {name} compare to previous periods?')
    - L485 if len(metrics) >= 2:
      - L486 expr questions.append(f"What's the relationship between {metric_names[0]} and {metric_names[1]}?")
  - L489 if tables:
    - L490 for table in tables[:3]:
      - L491 if hasattr(table, 'title') and table.title:
        - L492 expr questions.append(f'What are the key insights from the {table.title} data?')
      - L493 if hasattr(table, 'headers'):
        - L494 assign numeric_cols = [h for h, d in zip(table.headers, getattr(table, 'data_types', [])) if d == 'numeric']
        - L495 if numeric_cols:
          - L496 expr questions.append(f"What's the trend for {numeric_cols[0]}?")
  - L499 if entities:
    - L500 assign org_entities = [e for e in entities if hasattr(e, 'type') and e.type.value == 'organization']
    - L501 if org_entities:
      - L502 expr questions.append(f'What is the role of {org_entities[0].value} in this document?')
  - L505 expr questions.extend(['What are the main risks identified in this document?', 'What opportunities are suggested by the data?', 'What actions should be taken based on these findings?', 'Are there any anomalies or unusual patterns?', 'How does this compare to industry benchmarks?'])
  - L513 return questions[:10]
- L520 class UserExperienceService:
  - L521 docstring: "Orchestrates user experience features."
  - L523 def __init__self:
    - L524 assign self.collaboration = CollaborationService()
    - L525 annotated assign self._active_sessions: Dict[str, StreamingAnalysisSession] = {}
  - L527 def build_configurationself, preferences: AnalysisPreferences:
    - L528 docstring: "Build analysis configuration from preferences."
    - L529 return build_analysis_configuration(preferences)
  - L531 def get_industry_optionsself:
    - L532 docstring: "Get available industry options."
    - L533 return [{'value': key, 'label': key.replace('_', ' ').title(), 'description': config.get('focus_areas', [])} for key, config in INDUSTRY_CONFIGS.items()]
  - L538 def get_output_format_optionsself:
    - L539 docstring: "Get available output format options."
    - L540 return [{'value': key, 'label': key.title(), 'config': config} for key, config in OUTPUT_FORMATS.items()]
  - L545 def create_streaming_sessionself:
    - L546 docstring: "Create a new streaming analysis session."
    - L547 assign session_id = f'session_{uuid.uuid4().hex[:12]}'
    - L548 assign session = StreamingAnalysisSession(session_id)
    - L549 assign self._active_sessions[session_id] = session
    - L550 return session
  - L552 def get_sessionself, session_id: str:
    - L553 docstring: "Get an active session."
    - L554 return self._active_sessions.get(session_id)
  - L556 def cancel_sessionself, session_id: str:
    - L557 docstring: "Cancel an active session."
    - L558 assign session = self._active_sessions.get(session_id)
    - L559 if session:
      - L560 expr session.cancel()
      - L561 return True
    - L562 return False
  - L564 def generate_suggested_questionsself, tables: List[Any], metrics: List[Any], entities: List[Any]:
    - L570 docstring: "Generate suggested questions."
    - L571 return generate_suggested_questions(tables, metrics, entities)
  - L574 def add_commentself, *args, **kwargs:
    - L575 return self.collaboration.add_comment(*args, **kwargs)
  - L577 def get_commentsself, analysis_id: str:
    - L578 return self.collaboration.get_comments(analysis_id)
  - L580 def create_share_linkself, *args, **kwargs:
    - L581 return self.collaboration.create_share_link(*args, **kwargs)
  - L583 def save_versionself, *args, **kwargs:
    - L584 return self.collaboration.save_version(*args, **kwargs)
  - L586 def get_version_historyself, analysis_id: str:
    - L587 return self.collaboration.get_version_history(analysis_id)

## backend\app\features\analyze\services\visualization_engine.py
- L2 docstring: "\nIntelligent Visualization Engine - AI-powered chart generation and suggestions..."
- L10 from __future__ import annotations
- L12 import json
- L13 import logging
- L14 import math
- L15 import re
- L16 import uuid
- L17 from datetime import datetime
- L18 from typing import Any, Dict, List, Optional, Tuple
- L20 from backend.app.features.analyze.schemas.enhanced_analysis import ChartAnnotation, ChartDataSeries, ChartType, EnhancedChartSpec, EnhancedExtractedTable, ExtractedMetric, VisualizationSuggestion
- L29 from backend.app.services.utils.llm import call_chat_completion
- L30 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L32 assign logger = logging.getLogger('neura.analyze.visualization')
- L39 class DataPattern:
  - L40 docstring: "Detected data pattern for chart recommendation."
  - L41 def __init__self, pattern_type: str, columns: List[str], recommended_charts: List[ChartType], confidence: float, description: str:
    - L49 assign self.pattern_type = pattern_type
    - L50 assign self.columns = columns
    - L51 assign self.recommended_charts = recommended_charts
    - L52 assign self.confidence = confidence
    - L53 assign self.description = description
- L56 def detect_data_patternstable: EnhancedExtractedTable:
  - L57 docstring: "Detect data patterns in a table for chart recommendations."
  - L58 assign patterns = []
  - L60 assign datetime_cols = []
  - L61 assign numeric_cols = []
  - L62 assign categorical_cols = []
  - L64 for (idx, (header, dtype)) in enumerate(zip(table.headers, table.data_types)):
    - L65 if dtype == 'datetime':
      - L66 expr datetime_cols.append(header)
      - L67 else:
        - L67 if dtype == 'numeric':
          - L68 expr numeric_cols.append(header)
          - L71 else:
            - L71 assign unique_vals = set()
            - L72 for row in table.rows[:100]:
              - L73 if idx < len(row):
                - L74 expr unique_vals.add(str(row[idx]))
            - L75 if len(unique_vals) <= 20:
              - L76 expr categorical_cols.append(header)
  - L79 if datetime_cols and numeric_cols:
    - L80 expr patterns.append(DataPattern(pattern_type='time_series', columns=datetime_cols + numeric_cols, recommended_charts=[ChartType.LINE, ChartType.AREA, ChartType.BAR], confidence=0.9, description=f'Time series data with {len(numeric_cols)} numeric variables over time'))
  - L89 if categorical_cols and numeric_cols:
    - L90 expr patterns.append(DataPattern(pattern_type='category_comparison', columns=categorical_cols + numeric_cols, recommended_charts=[ChartType.BAR, ChartType.PIE, ChartType.TREEMAP], confidence=0.85, description=f'Categorical data with {len(numeric_cols)} metrics per category'))
  - L99 if numeric_cols and len(table.rows) >= 10:
    - L100 expr patterns.append(DataPattern(pattern_type='distribution', columns=numeric_cols, recommended_charts=[ChartType.HISTOGRAM, ChartType.BOX], confidence=0.75, description=f'Numeric distribution analysis for {len(numeric_cols)} variables'))
  - L109 if len(numeric_cols) >= 2:
    - L110 expr patterns.append(DataPattern(pattern_type='correlation', columns=numeric_cols, recommended_charts=[ChartType.SCATTER, ChartType.BUBBLE, ChartType.HEATMAP], confidence=0.7, description=f'Potential correlations between {len(numeric_cols)} numeric variables'))
  - L119 if len(categorical_cols) >= 2 and numeric_cols:
    - L120 expr patterns.append(DataPattern(pattern_type='hierarchy', columns=categorical_cols + numeric_cols[:1], recommended_charts=[ChartType.SUNBURST, ChartType.TREEMAP], confidence=0.65, description='Hierarchical categorical data'))
  - L129 if len(numeric_cols) >= 3 and len(table.rows) <= 10:
    - L130 expr patterns.append(DataPattern(pattern_type='funnel', columns=numeric_cols, recommended_charts=[ChartType.FUNNEL, ChartType.WATERFALL], confidence=0.6, description='Sequential stage data suitable for funnel visualization'))
  - L138 return patterns
- L145 def auto_generate_chartstables: List[EnhancedExtractedTable], max_charts: int=10:
  - L149 docstring: "Automatically generate charts based on detected data patterns."
  - L150 assign charts = []
  - L152 for table in tables:
    - L153 if len(charts) >= max_charts:
      - L154 break
    - L156 assign patterns = detect_data_patterns(table)
    - L158 for pattern in patterns:
      - L159 if len(charts) >= max_charts:
        - L160 break
      - L162 assign chart = _create_chart_from_pattern(table, pattern)
      - L163 if chart:
        - L164 expr charts.append(chart)
  - L166 return charts
- L169 def _create_chart_from_patterntable: EnhancedExtractedTable, pattern: DataPattern:
  - L173 docstring: "Create a chart specification from a detected pattern."
  - L174 if not pattern.recommended_charts:
    - L175 return None
  - L177 assign chart_type = pattern.recommended_charts[0]
  - L180 assign datetime_cols = [h for h, d in zip(table.headers, table.data_types) if d == 'datetime']
  - L181 assign numeric_cols = [h for h, d in zip(table.headers, table.data_types) if d == 'numeric']
  - L182 assign categorical_cols = [h for h, d in zip(table.headers, table.data_types) if d == 'text']
  - L184 assign x_field = ''
  - L185 assign y_fields = []
  - L187 if pattern.pattern_type == 'time_series':
    - L188 assign x_field = datetime_cols[0] if datetime_cols else categorical_cols[0] if categorical_cols else table.headers[0]
    - L189 assign y_fields = numeric_cols[:3]
    - L190 else:
      - L190 if pattern.pattern_type == 'category_comparison':
        - L191 assign x_field = categorical_cols[0] if categorical_cols else table.headers[0]
        - L192 assign y_fields = numeric_cols[:2]
        - L193 else:
          - L193 if pattern.pattern_type == 'correlation':
            - L194 assign x_field = numeric_cols[0]
            - L195 assign y_fields = [numeric_cols[1]] if len(numeric_cols) > 1 else []
            - L196 else:
              - L196 if pattern.pattern_type == 'distribution':
                - L197 assign x_field = numeric_cols[0]
                - L198 assign y_fields = []
                - L200 else:
                  - L200 assign x_field = table.headers[0]
                  - L201 assign y_fields = numeric_cols[:2] if numeric_cols else []
  - L203 if not x_field:
    - L204 return None
  - L207 assign data = []
  - L208 for row in table.rows[:500]:
    - L209 assign record = {}
    - L210 for (idx, header) in enumerate(table.headers):
      - L211 if idx < len(row):
        - L212 assign record[header] = row[idx]
    - L213 expr data.append(record)
  - L216 assign title = f"{pattern.pattern_type.replace('_', ' ').title()}: {table.title or table.id}"
  - L218 return EnhancedChartSpec(id=f'auto_{chart_type.value}_{uuid.uuid4().hex[:8]}', type=chart_type, title=title, description=pattern.description, x_field=x_field, y_fields=y_fields, data=data, x_axis_label=x_field, y_axis_label=y_fields[0] if y_fields else None, show_legend=len(y_fields) > 1, ai_insights=[], source_table_id=table.id, confidence=pattern.confidence, suggested_by_ai=True)
- L240 def generate_chart_from_natural_languagequery: str, tables: List[EnhancedExtractedTable], metrics: List[ExtractedMetric]=None:
  - L245 docstring: "Generate charts from natural language query."
  - L247 assign context_parts = []
  - L248 for table in tables[:5]:
    - L249 expr context_parts.append(f"Table: {table.title or table.id}\nColumns: {', '.join([f'{h} ({d})' for h, d in zip(table.headers, table.data_types)])}\nRows: {table.row_count}\nSample: {(table.rows[0] if table.rows else 'N/A')}")
  - L254 assign context = '\n\n'.join(context_parts)
  - L256 assign prompt = f'Generate chart specifications based on this request.\n\nUser request: "{query}"\n\nAvailable data:\n{context}\n\nGenerate 1-3 appropriate charts. Return JSON array:\n```json\n[\n  {{\n    "chart_type": "line|bar|pie|scatter|area|histogram|box|heatmap|treemap|funnel|radar|bubble|sunburst|waterfall|gauge",\n    "title": "Chart title",\n    "description": "What this chart shows",\n    "x_field": "column_name",\n    "y_fields": ["column1", "column2"],\n    "group_field": null,\n    "x_axis_label": "X axis label",\n    "y_axis_label": "Y axis label",\n    "show_legend": true,\n    "source_table": "table_id",\n    "rationale": "Why this visualization is appropriate"\n  }}\n]\n```\n\nMatch column names exactly as provided. Choose the most appropriate chart type for the data and request.'
  - L284 try:
    - L285 assign client = get_openai_client()
    - L286 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='nl_chart_generation', temperature=0.3)
    - L294 assign raw_text = response.choices[0].message.content or ''
    - L295 assign json_match = re.search('\\[[\\s\\S]*\\]', raw_text)
    - L297 if json_match:
      - L298 assign specs = json.loads(json_match.group())
      - L299 assign charts = []
      - L301 for spec in specs:
        - L302 try:
          - L303 assign chart_type = ChartType[spec.get('chart_type', 'bar').upper()]
          - L304 except KeyError:
            - L305 assign chart_type = ChartType.BAR
        - L308 assign source_table_id = spec.get('source_table')
        - L309 assign source_table = next((t for t in tables if t.id == source_table_id or t.title == source_table_id), None)
        - L311 if not source_table and tables:
          - L312 assign source_table = tables[0]
        - L314 assign data = []
        - L315 if source_table:
          - L316 for row in source_table.rows[:500]:
            - L317 assign record = {}
            - L318 for (idx, header) in enumerate(source_table.headers):
              - L319 if idx < len(row):
                - L320 assign record[header] = row[idx]
            - L321 expr data.append(record)
        - L323 expr charts.append(EnhancedChartSpec(id=f'nl_{chart_type.value}_{uuid.uuid4().hex[:8]}', type=chart_type, title=spec.get('title', 'Generated Chart'), description=spec.get('description'), x_field=spec.get('x_field', ''), y_fields=spec.get('y_fields', []), group_field=spec.get('group_field'), data=data, x_axis_label=spec.get('x_axis_label'), y_axis_label=spec.get('y_axis_label'), show_legend=spec.get('show_legend', True), source_table_id=source_table.id if source_table else None, ai_insights=[spec.get('rationale', '')], confidence=0.85, suggested_by_ai=True))
      - L341 return charts
    - L343 except Exception as e:
      - L344 expr logger.warning(f'NL chart generation failed: {e}')
  - L346 return []
- L353 def add_trend_linechart: EnhancedChartSpec:
  - L354 docstring: "Add trend line to a chart."
  - L355 if not chart.data or not chart.y_fields:
    - L356 return chart
  - L358 assign y_field = chart.y_fields[0]
  - L359 assign x_field = chart.x_field
  - L362 assign values = []
  - L363 for (i, record) in enumerate(chart.data):
    - L364 try:
      - L365 assign y_val = float(str(record.get(y_field, 0)).replace(',', '').replace('$', '').replace('%', ''))
      - L366 expr values.append((i, y_val))
      - L367 except (ValueError, TypeError):
        - L368 pass
  - L370 if len(values) < 3:
    - L371 return chart
  - L374 assign n = len(values)
  - L375 assign sum_x = sum((x for x, _ in values))
  - L376 assign sum_y = sum((y for _, y in values))
  - L377 assign sum_xy = sum((x * y for x, y in values))
  - L378 assign sum_xx = sum((x * x for x, _ in values))
  - L380 assign denominator = n * sum_xx - sum_x * sum_x
  - L381 if denominator == 0:
    - L382 return chart
  - L384 assign slope = (n * sum_xy - sum_x * sum_y) / denominator
  - L385 assign intercept = (sum_y - slope * sum_x) / n
  - L388 assign mean_y = sum_y / n
  - L389 assign ss_tot = sum(((y - mean_y) ** 2 for _, y in values))
  - L390 assign ss_res = sum(((y - (slope * x + intercept)) ** 2 for x, y in values))
  - L391 assign r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
  - L393 assign chart.trend_line = {'type': 'linear', 'slope': round(slope, 4), 'intercept': round(intercept, 4), 'r_squared': round(r_squared, 4), 'direction': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable'}
  - L402 assign trend_desc = 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable'
  - L403 expr chart.ai_insights.append(f'Trend is {trend_desc} (slope: {slope:.2f}, R\xb2: {r_squared:.2f})')
  - L407 return chart
- L410 def add_forecastchart: EnhancedChartSpec, periods: int=3:
  - L411 docstring: "Add simple forecast to a chart."
  - L412 if not chart.trend_line or not chart.data:
    - L413 return chart
  - L415 assign slope = chart.trend_line.get('slope', 0)
  - L416 assign intercept = chart.trend_line.get('intercept', 0)
  - L418 assign last_x = len(chart.data) - 1
  - L419 assign forecast_values = []
  - L421 for i in range(1, periods + 1):
    - L422 assign x = last_x + i
    - L423 assign y = slope * x + intercept
    - L424 expr forecast_values.append({'period': f'+{i}', 'value': round(y, 2), 'lower_bound': round(y * 0.9, 2), 'upper_bound': round(y * 1.1, 2)})
  - L431 assign chart.forecast = {'periods': periods, 'method': 'linear_extrapolation', 'values': forecast_values, 'confidence': 0.7}
  - L438 expr chart.ai_insights.append(f'Forecast: Next {periods} periods projected based on linear trend')
  - L442 return chart
- L445 def detect_anomalieschart: EnhancedChartSpec:
  - L446 docstring: "Detect anomalies in chart data."
  - L447 if not chart.data or not chart.y_fields:
    - L448 return chart
  - L450 assign y_field = chart.y_fields[0]
  - L453 assign values = []
  - L454 for (i, record) in enumerate(chart.data):
    - L455 try:
      - L456 assign y_val = float(str(record.get(y_field, 0)).replace(',', '').replace('$', '').replace('%', ''))
      - L457 expr values.append((i, y_val, record))
      - L458 except (ValueError, TypeError):
        - L459 pass
  - L461 if len(values) < 5:
    - L462 return chart
  - L465 assign y_vals = [y for _, y, _ in values]
  - L466 assign mean = sum(y_vals) / len(y_vals)
  - L467 assign std = math.sqrt(sum(((y - mean) ** 2 for y in y_vals)) / len(y_vals))
  - L469 if std == 0:
    - L470 return chart
  - L473 assign anomalies = []
  - L474 for (idx, y_val, record) in values:
    - L475 assign z_score = abs((y_val - mean) / std)
    - L476 if z_score > 2:
      - L477 assign anomaly_type = 'spike' if y_val > mean else 'dip'
      - L478 expr anomalies.append({'index': idx, 'value': y_val, 'z_score': round(z_score, 2), 'type': anomaly_type, 'x_value': record.get(chart.x_field)})
      - L487 expr chart.annotations.append(ChartAnnotation(type='point', label=f'Anomaly: {anomaly_type}', value=y_val, position={'index': idx}, style={'color': 'red' if anomaly_type == 'spike' else 'orange'}))
  - L495 assign chart.anomalies = anomalies
  - L497 if anomalies:
    - L498 expr chart.ai_insights.append(f"Detected {len(anomalies)} anomal{('y' if len(anomalies) == 1 else 'ies')} in the data")
  - L502 return chart
- L505 def enhance_chart_with_intelligencechart: EnhancedChartSpec:
  - L506 docstring: "Apply all chart intelligence features."
  - L507 assign chart = add_trend_line(chart)
  - L508 assign chart = detect_anomalies(chart)
  - L509 return chart
- L516 def generate_chart_suggestionstables: List[EnhancedExtractedTable], metrics: List[ExtractedMetric]=None:
  - L520 docstring: "Generate visualization suggestions with rationale."
  - L521 assign suggestions = []
  - L523 for table in tables[:5]:
    - L524 assign patterns = detect_data_patterns(table)
    - L526 for pattern in patterns:
      - L527 for chart_type in pattern.recommended_charts[:2]:
        - L528 assign chart = _create_chart_from_pattern(table, pattern)
        - L529 if chart:
          - L530 assign chart.type = chart_type
          - L531 assign chart = enhance_chart_with_intelligence(chart)
          - L533 expr suggestions.append(VisualizationSuggestion(chart_spec=chart, rationale=f"{pattern.description}. {chart_type.value.title()} chart is ideal for visualizing {pattern.pattern_type.replace('_', ' ')} patterns.", relevance_score=pattern.confidence, complexity='simple' if chart_type in [ChartType.BAR, ChartType.LINE, ChartType.PIE] else 'moderate', insights_potential=chart.ai_insights))
  - L542 expr suggestions.sort(key=lambda s: s.relevance_score, reverse=True)
  - L544 return suggestions[:10]
- L551 def analyze_chart_with_llmchart: EnhancedChartSpec:
  - L552 docstring: "Use LLM to generate insights about a chart."
  - L554 assign data_summary = f'Chart: {chart.title}\nType: {chart.type.value}\n'
  - L555 aug assign data_summary Add f"X-axis: {chart.x_field}\nY-axis: {', '.join(chart.y_fields)}\n"
  - L557 if chart.data:
    - L558 aug assign data_summary Add f'Data points: {len(chart.data)}\n'
    - L559 aug assign data_summary Add f'Sample data: {chart.data[:5]}'
  - L561 if chart.trend_line:
    - L562 aug assign data_summary Add f'\nTrend: {chart.trend_line}'
  - L564 if chart.anomalies:
    - L565 aug assign data_summary Add f'\nAnomalies: {chart.anomalies[:3]}'
  - L567 assign prompt = f'Analyze this chart data and provide 3-5 key insights.\n\n{data_summary}\n\nReturn JSON:\n```json\n{{\n  "insights": [\n    "Clear, actionable insight 1",\n    "Clear, actionable insight 2",\n    "Clear, actionable insight 3"\n  ],\n  "key_finding": "The single most important observation",\n  "recommended_actions": ["Action 1", "Action 2"]\n}}\n```\n\nFocus on patterns, trends, outliers, and business implications.'
  - L586 try:
    - L587 assign client = get_openai_client()
    - L588 assign response = call_chat_completion(client, model=MODEL, messages=[{'role': 'user', 'content': prompt}], description='chart_analysis', temperature=0.3)
    - L596 assign raw_text = response.choices[0].message.content or ''
    - L597 assign json_match = re.search('\\{[\\s\\S]*\\}', raw_text)
    - L599 if json_match:
      - L600 assign data = json.loads(json_match.group())
      - L601 expr chart.ai_insights.extend(data.get('insights', []))
      - L603 if data.get('key_finding'):
        - L604 expr chart.annotations.append(ChartAnnotation(type='text', label=data['key_finding'], position={'location': 'top'}))
    - L610 except Exception as e:
      - L611 expr logger.warning(f'Chart analysis failed: {e}')
  - L613 return chart
- L620 class VisualizationEngine:
  - L621 docstring: "Orchestrates all visualization features."
  - L623 def generate_all_visualizationsself, tables: List[EnhancedExtractedTable], metrics: List[ExtractedMetric]=None, max_charts: int=10:
    - L629 docstring: "Generate all visualizations for the data."
    - L631 assign auto_charts = auto_generate_charts(tables, max_charts)
    - L634 assign enhanced_charts = [enhance_chart_with_intelligence(c) for c in auto_charts]
    - L637 assign suggestions = generate_chart_suggestions(tables, metrics)
    - L639 return {'charts': enhanced_charts, 'suggestions': suggestions}
  - L644 def generate_from_queryself, query: str, tables: List[EnhancedExtractedTable], metrics: List[ExtractedMetric]=None:
    - L650 docstring: "Generate charts from natural language query."
    - L651 assign charts = generate_chart_from_natural_language(query, tables, metrics)
    - L652 return [enhance_chart_with_intelligence(c) for c in charts]
  - L654 def add_intelligence_to_chartself, chart: EnhancedChartSpec, include_forecast: bool=False, forecast_periods: int=3:
    - L660 docstring: "Add intelligence features to a chart."
    - L661 assign chart = add_trend_line(chart)
    - L662 assign chart = detect_anomalies(chart)
    - L664 if include_forecast:
      - L665 assign chart = add_forecast(chart, forecast_periods)
    - L667 return analyze_chart_with_llm(chart)

## backend\app\features\generate\__init__.py

## backend\app\features\generate\routes\chart_suggest_routes.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Request
- L5 from ..schemas.charts import ChartSuggestPayload
- L6 from ..services.chart_suggestions_service import suggest_charts as suggest_charts_service
- L9 def build_chart_suggest_router*, template_dir_fn, db_path_fn, load_contract_fn, clean_key_values_fn, discover_pdf_fn, discover_excel_fn, build_field_catalog_fn, build_metrics_fn, build_prompt_fn, call_chat_completion_fn, model, strip_code_fences_fn, get_correlation_id_fn, logger:
  - L26 assign router = APIRouter()
  - L28 def _routetemplate_id: str, payload: ChartSuggestPayload, request: Request, kind: str:
    - L29 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id_fn()
    - L30 assign discover_fn = discover_pdf_fn if kind == 'pdf' else discover_excel_fn
    - L31 return suggest_charts_service(template_id, payload, kind=kind, correlation_id=correlation_id, template_dir_fn=lambda tpl: template_dir_fn(tpl, kind=kind), db_path_fn=db_path_fn, load_contract_fn=load_contract_fn, clean_key_values_fn=clean_key_values_fn, discover_fn=discover_fn, build_field_catalog_fn=build_field_catalog_fn, build_metrics_fn=build_metrics_fn, build_prompt_fn=build_prompt_fn, call_chat_completion_fn=call_chat_completion_fn, model=model, strip_code_fences_fn=strip_code_fences_fn, logger=logger)
  - L51 def suggest_chartstemplate_id: str, payload: ChartSuggestPayload, request: Request:
    - L52 return _route(template_id, payload, request, kind='pdf')
  - L55 def suggest_charts_exceltemplate_id: str, payload: ChartSuggestPayload, request: Request:
    - L56 return _route(template_id, payload, request, kind='excel')
  - L58 return router

## backend\app\features\generate\routes\discover_routes.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Request
- L5 from ..schemas.reports import DiscoverPayload
- L6 from ..services.discovery_service import discover_reports as discover_reports_service
- L9 def build_discover_router*, template_dir_fn, db_path_fn, load_contract_fn, clean_key_values_fn, discover_pdf_fn, discover_excel_fn, build_field_catalog_fn, build_batch_metrics_fn, load_manifest_fn, manifest_endpoint_fn_pdf, manifest_endpoint_fn_excel, logger:
  - L24 assign router = APIRouter()
  - L26 def _discoverkind: str, payload: DiscoverPayload:
    - L27 assign discover_fn = discover_pdf_fn if kind == 'pdf' else discover_excel_fn
    - L28 assign manifest_endpoint_fn = manifest_endpoint_fn_pdf if kind == 'pdf' else manifest_endpoint_fn_excel
    - L29 return discover_reports_service(payload, kind=kind, template_dir_fn=lambda tpl: template_dir_fn(tpl, kind=kind), db_path_fn=db_path_fn, load_contract_fn=load_contract_fn, clean_key_values_fn=clean_key_values_fn, discover_fn=discover_fn, build_field_catalog_fn=build_field_catalog_fn, build_batch_metrics_fn=build_batch_metrics_fn, load_manifest_fn=lambda tdir: load_manifest_fn(tdir), manifest_endpoint_fn=lambda tpl: manifest_endpoint_fn(tpl, kind=kind), logger=logger)
  - L45 def discover_reportspayload: DiscoverPayload, _request: Request:
    - L46 return _discover('pdf', payload)
  - L49 def discover_reports_excelpayload: DiscoverPayload, _request: Request:
    - L50 return _discover('excel', payload)
  - L52 return router

## backend\app\features\generate\routes\run_routes.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Request
- L5 from ..schemas.reports import RunPayload
- L8 def build_run_router*, reports_run_fn, enqueue_job_fn:
  - L13 docstring: "\n    Build router for run endpoints while delegating to existing handlers.\n   ..."
  - L16 assign router = APIRouter()
  - L19 def start_runpayload: RunPayload, request: Request:
    - L20 return reports_run_fn(payload, request, kind='pdf')
  - L23 def start_run_excelpayload: RunPayload, request: Request:
    - L24 return reports_run_fn(payload, request, kind='excel')
  - L27 async def enqueue_report_jobpayload: RunPayload | list[RunPayload], request: Request:
    - L28 return await enqueue_job_fn(payload, request, kind='pdf')
  - L31 async def enqueue_report_job_excelpayload: RunPayload | list[RunPayload], request: Request:
    - L32 return await enqueue_job_fn(payload, request, kind='excel')
  - L34 return router

## backend\app\features\generate\routes\saved_charts_routes.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter
- L5 from ..schemas.charts import SavedChartCreatePayload, SavedChartUpdatePayload
- L6 from ..services.saved_charts_service import create_saved_chart as create_saved_chart_service, delete_saved_chart as delete_saved_chart_service, list_saved_charts as list_saved_charts_service, update_saved_chart as update_saved_chart_service
- L14 def build_saved_charts_routerensure_template_exists, normalize_template_id:
  - L15 assign router = APIRouter()
  - L18 def list_saved_chartstemplate_id: str:
    - L19 return list_saved_charts_service(template_id, ensure_template_exists)
  - L22 def create_saved_charttemplate_id: str, payload: SavedChartCreatePayload:
    - L23 return create_saved_chart_service(template_id, payload, ensure_template_exists=ensure_template_exists, normalize_template_id=normalize_template_id)
  - L31 def update_saved_charttemplate_id: str, chart_id: str, payload: SavedChartUpdatePayload:
    - L32 return update_saved_chart_service(template_id, chart_id, payload, ensure_template_exists)
  - L35 def delete_saved_charttemplate_id: str, chart_id: str:
    - L36 return delete_saved_chart_service(template_id, chart_id, ensure_template_exists)
  - L39 def list_saved_charts_exceltemplate_id: str:
    - L40 return list_saved_charts_service(template_id, ensure_template_exists)
  - L43 def create_saved_chart_exceltemplate_id: str, payload: SavedChartCreatePayload:
    - L44 return create_saved_chart_service(template_id, payload, ensure_template_exists=ensure_template_exists, normalize_template_id=normalize_template_id)
  - L52 def update_saved_chart_exceltemplate_id: str, chart_id: str, payload: SavedChartUpdatePayload:
    - L53 return update_saved_chart_service(template_id, chart_id, payload, ensure_template_exists)
  - L56 def delete_saved_chart_exceltemplate_id: str, chart_id: str:
    - L57 return delete_saved_chart_service(template_id, chart_id, ensure_template_exists)
  - L59 return router

## backend\app\features\generate\schemas\__init__.py

## backend\app\features\generate\schemas\charts.py
- L1 from __future__ import annotations
- L3 from typing import Any, Optional
- L5 from pydantic import BaseModel
- L8 class ChartSpec(BaseModel):
  - L9 annotated assign id: Optional[str] = None
  - L10 annotated assign type: str
  - L11 annotated assign xField: str
  - L12 annotated assign yFields: list[str]
  - L13 annotated assign groupField: Optional[str] = None
  - L14 annotated assign aggregation: Optional[str] = None
  - L15 annotated assign chartTemplateId: Optional[str] = None
  - L16 annotated assign style: Optional[dict[str, Any]] = None
  - L17 annotated assign title: Optional[str] = None
  - L18 annotated assign description: Optional[str] = None
- L21 class ChartSuggestPayload(BaseModel):
  - L22 annotated assign connection_id: Optional[str] = None
  - L23 annotated assign start_date: str
  - L24 annotated assign end_date: str
  - L25 annotated assign key_values: Optional[dict[str, Any]] = None
  - L26 annotated assign question: str
  - L27 annotated assign include_sample_data: bool = False
- L30 class ChartSuggestResponse(BaseModel):
  - L31 annotated assign charts: list[ChartSpec]
  - L32 annotated assign sample_data: Optional[list[dict[str, Any]]] = None
- L35 class SavedChartSpec(BaseModel):
  - L36 annotated assign id: str
  - L37 annotated assign template_id: str
  - L38 annotated assign name: str
  - L39 annotated assign spec: ChartSpec
  - L40 annotated assign created_at: str
  - L41 annotated assign updated_at: str
- L44 class SavedChartCreatePayload(BaseModel):
  - L45 annotated assign template_id: str
  - L46 annotated assign name: str
  - L47 annotated assign spec: ChartSpec
- L50 class SavedChartUpdatePayload(BaseModel):
  - L51 annotated assign name: Optional[str] = None
  - L52 annotated assign spec: Optional[ChartSpec] = None

## backend\app\features\generate\schemas\reports.py
- L1 from __future__ import annotations
- L3 from typing import Any, Optional
- L5 from pydantic import BaseModel
- L8 class RunPayload(BaseModel):
  - L9 annotated assign template_id: str
  - L10 annotated assign connection_id: Optional[str] = None
  - L11 annotated assign start_date: str
  - L12 annotated assign end_date: str
  - L13 annotated assign batch_ids: Optional[list[str]] = None
  - L14 annotated assign key_values: Optional[dict[str, Any]] = None
  - L15 annotated assign docx: bool = False
  - L16 annotated assign xlsx: bool = False
  - L17 annotated assign email_recipients: Optional[list[str]] = None
  - L18 annotated assign email_subject: Optional[str] = None
  - L19 annotated assign email_message: Optional[str] = None
  - L20 annotated assign schedule_id: Optional[str] = None
  - L21 annotated assign schedule_name: Optional[str] = None
- L24 class DiscoverPayload(BaseModel):
  - L25 annotated assign template_id: str
  - L26 annotated assign connection_id: Optional[str] = None
  - L27 annotated assign start_date: str
  - L28 annotated assign end_date: str
  - L29 annotated assign key_values: Optional[dict[str, Any]] = None

## backend\app\features\generate\services\__init__.py

## backend\app\features\generate\services\chart_suggestions_service.py
- L1 from __future__ import annotations
- L3 import json
- L4 from typing import Any, Callable, Mapping, Optional, Sequence
- L6 from fastapi import HTTPException
- L8 from ....services.prompts.llm_prompts_charts import CHART_TEMPLATE_CATALOG
- L9 from ....services.state import state_store
- L10 from ..schemas.charts import ChartSpec, ChartSuggestPayload, ChartSuggestResponse
- L12 assign VALID_CHART_TYPES = {'bar', 'line', 'pie', 'scatter'}
- L13 assign VALID_AGGREGATIONS = {'sum', 'avg', 'count', 'none'}
- L14 assign NUMERIC_TYPES = {'number', 'numeric', 'float', 'int', 'integer', 'decimal'}
- L15 assign DATETIME_TYPES = {'datetime', 'date', 'timestamp', 'time'}
- L16 assign CATEGORICAL_TYPES = {'string', 'category', 'categorical', 'text'}
- L19 def _field_categoryraw_type: Any:
  - L20 assign normalized = str(raw_type or '').strip().lower()
  - L21 if normalized in NUMERIC_TYPES:
    - L22 return 'numeric'
  - L23 if normalized in DATETIME_TYPES:
    - L24 return 'datetime'
  - L25 return 'categorical'
- L28 def _build_field_lookupfield_catalog: Sequence[Mapping[str, Any]] | None:
  - L29 annotated assign lookup: dict[str, tuple[str, str]] = {}
  - L30 for field in field_catalog or []:
    - L31 if not isinstance(field, Mapping):
      - L32 continue
    - L33 assign name = str(field.get('name') or '').strip()
    - L34 if not name:
      - L35 continue
    - L36 assign field_type = str(field.get('type') or '').strip().lower() or 'string'
    - L37 assign lookup[name.lower()] = (name, field_type)
  - L38 return lookup
- L41 def _normalize_chart_typeraw: Any:
  - L42 assign text = str(raw or '').strip().lower()
  - L43 if not text:
    - L44 return None
  - L45 assign alias_map = {'barchart': 'bar', 'bar chart': 'bar', 'column': 'bar', 'columnchart': 'bar', 'linechart': 'line', 'line chart': 'line', 'piechart': 'pie', 'scatterplot': 'scatter', 'scatter plot': 'scatter'}
  - L56 assign candidates = {text, text.replace('_', ' '), text.replace('-', ' '), text.replace('chart', '').strip(), text.replace('chart', '').replace('_', ' ').replace('-', ' ').strip()}
  - L63 for candidate in candidates:
    - L64 assign candidate_clean = candidate.replace(' ', '')
    - L65 if candidate in VALID_CHART_TYPES:
      - L66 return candidate
    - L67 if candidate_clean in VALID_CHART_TYPES:
      - L68 return candidate_clean
    - L69 if candidate_clean in alias_map:
      - L70 return alias_map[candidate_clean]
  - L71 return None
- L74 def _normalize_aggregationraw: Any:
  - L75 if raw is None:
    - L76 return None
  - L77 assign text = str(raw).strip().lower()
  - L78 if not text:
    - L79 return None
  - L80 return text if text in VALID_AGGREGATIONS else None
- L83 def _normalize_field_nameraw: Any, field_lookup: Mapping[str, tuple[str, str]]:
  - L84 assign text = str(raw or '').strip()
  - L85 if not text:
    - L86 return (None, None)
  - L87 assign match = field_lookup.get(text.lower())
  - L88 if not match:
    - L89 return (None, None)
  - L90 return match
- L93 def _normalize_template_idraw: Any, *, chart_type: str, x_category: str, y_fields: Sequence[str]:
  - L94 assign template_id = str(raw or '').strip()
  - L95 if not template_id or template_id not in CHART_TEMPLATE_CATALOG:
    - L96 return None
  - L97 if template_id == 'time_series_basic':
    - L98 if x_category not in {'numeric', 'datetime'} or not y_fields or chart_type not in {'line', 'bar'}:
      - L99 return None
    - L100 else:
      - L100 if template_id == 'top_n_categories':
        - L101 if x_category != 'categorical' or len(y_fields) != 1:
          - L102 return None
        - L103 else:
          - L103 if template_id == 'distribution_histogram':
            - L104 if x_category != 'numeric' or chart_type not in {'bar', 'line'}:
              - L105 return None
  - L106 return template_id
- L109 def _normalize_chart_suggestionitem: Mapping[str, Any], *, idx: int, field_lookup: Mapping[str, tuple[str, str]]:
  - L115 if not isinstance(item, Mapping):
    - L116 return None
  - L118 assign chart_type = _normalize_chart_type(item.get('type'))
  - L119 if not chart_type:
    - L120 return None
  - L122 assign (x_field, x_type) = _normalize_field_name(item.get('xField'), field_lookup)
  - L123 if not x_field:
    - L124 return None
  - L126 assign y_fields_raw = item.get('yFields')
  - L127 annotated assign y_candidates: Sequence[Any] | None
  - L128 if isinstance(y_fields_raw, str):
    - L129 assign y_candidates = [y_fields_raw]
    - L130 else:
      - L130 if isinstance(y_fields_raw, Sequence):
        - L131 assign y_candidates = y_fields_raw
        - L133 else:
          - L133 assign single = item.get('yField') or item.get('y')
          - L134 if isinstance(single, str) and single.strip():
            - L135 assign y_candidates = [single]
            - L137 else:
              - L137 return None
  - L139 annotated assign y_field_info: list[tuple[str, str]] = []
  - L140 annotated assign seen_y: set[str] = set()
  - L141 for raw_y in y_candidates:
    - L142 assign (name, ftype) = _normalize_field_name(raw_y, field_lookup)
    - L143 if not name or name in seen_y:
      - L144 continue
    - L145 expr y_field_info.append((name, ftype or 'string'))
    - L146 expr seen_y.add(name)
  - L147 if not y_field_info:
    - L148 return None
  - L150 assign (group_field, group_type) = _normalize_field_name(item.get('groupField'), field_lookup)
  - L151 if group_field and _field_category(group_type) != 'categorical':
    - L152 assign group_field = None
  - L154 assign x_category = _field_category(x_type)
  - L155 assign numeric_y_fields = [name for name, ftype in y_field_info if _field_category(ftype) == 'numeric']
  - L157 if chart_type == 'pie':
    - L158 if x_category != 'categorical':
      - L159 return None
    - L160 if not numeric_y_fields:
      - L161 return None
    - L162 assign y_fields = [numeric_y_fields[0]]
    - L164 else:
      - L164 if chart_type in ('line', 'scatter') and x_category not in {'numeric', 'datetime'}:
        - L165 return None
      - L166 if chart_type == 'bar' and x_category not in {'numeric', 'datetime', 'categorical'}:
        - L167 return None
      - L168 if not numeric_y_fields:
        - L169 return None
      - L170 assign y_fields = numeric_y_fields
  - L172 assign aggregation = _normalize_aggregation(item.get('aggregation'))
  - L173 assign chart_template_id = _normalize_template_id(item.get('chartTemplateId'), chart_type=chart_type, x_category=x_category, y_fields=y_fields)
  - L180 assign style = item.get('style')
  - L181 assign style_payload = dict(style) if isinstance(style, Mapping) else None
  - L183 annotated assign normalized: dict[str, Any] = {'id': str(item.get('id') or f'chart_{idx + 1}'), 'type': chart_type, 'xField': x_field, 'yFields': y_fields, 'groupField': group_field, 'aggregation': aggregation, 'chartTemplateId': chart_template_id, 'style': style_payload}
  - L193 assign title = item.get('title')
  - L194 assign description = item.get('description')
  - L195 if isinstance(title, str) and title.strip():
    - L196 assign normalized['title'] = title.strip()
  - L197 if isinstance(description, str) and description.strip():
    - L198 assign normalized['description'] = description.strip()
  - L200 try:
    - L201 return ChartSpec(**normalized)
    - L202 except Exception:
      - L203 return None
- L206 def suggest_chartstemplate_id: str, payload: ChartSuggestPayload, *, kind: str, correlation_id: Optional[str], template_dir_fn: Callable[[str], Any], db_path_fn: Callable[[Optional[str]], Any], load_contract_fn: Callable[[Any], Any], clean_key_values_fn: Callable[[Optional[dict]], Optional[dict]], discover_fn: Callable[..., Mapping[str, Any]], build_field_catalog_fn: Callable[[list], tuple[list, Mapping[str, Any]]], build_metrics_fn: Callable[[list, Mapping[str, Any], int], list], build_prompt_fn: Callable[..., str], call_chat_completion_fn: Callable[..., Any], model: str, strip_code_fences_fn: Callable[[str], str], logger:
  - L225 assign template_dir = template_dir_fn(template_id)
  - L226 assign db_path = db_path_fn(payload.connection_id)
  - L227 if not db_path.exists():
    - L228 raise HTTPException(status_code=400, detail={'code': 'db_not_found', 'message': f'DB not found: {db_path}'})
  - L230 try:
    - L231 expr load_contract_fn(template_dir)
    - L232 except Exception as exc:
      - L233 expr logger.exception('chart_suggest_contract_load_failed', extra={'event': 'chart_suggest_contract_load_failed', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
      - L242 raise HTTPException(status_code=500, detail={'code': 'contract_load_failed', 'message': f'Failed to load contract artifacts: {exc}'})
  - L244 assign contract_path = template_dir / 'contract.json'
  - L245 if not contract_path.exists():
    - L246 raise HTTPException(status_code=400, detail={'code': 'contract_not_ready', 'message': 'Contract artifacts missing. Approve mapping first.'})
  - L250 try:
    - L251 assign contract_payload = json.loads(contract_path.read_text(encoding='utf-8'))
    - L252 except Exception as exc:
      - L253 raise HTTPException(status_code=500, detail={'code': 'contract_invalid', 'message': f'Invalid contract.json: {exc}'})
  - L255 assign key_values_payload = clean_key_values_fn(payload.key_values)
  - L257 try:
    - L258 assign summary = discover_fn(db_path=db_path, contract=contract_payload, start_date=payload.start_date, end_date=payload.end_date, key_values=key_values_payload)
    - L265 except Exception as exc:
      - L266 expr logger.exception('chart_suggest_discovery_failed', extra={'event': 'chart_suggest_discovery_failed', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
      - L275 raise HTTPException(status_code=500, detail={'code': 'discovery_failed', 'message': f'Discovery failed: {exc}'})
  - L277 assign batches = summary.get('batches') or []
  - L278 if not isinstance(batches, list):
    - L279 assign batches = []
  - L280 assign batch_metadata = summary.get('batch_metadata') or {}
  - L282 annotated assign sample_data: list[dict[str, Any]] | None = None
  - L283 if payload.include_sample_data:
    - L284 try:
      - L285 assign sample_data = build_metrics_fn(batches, batch_metadata, limit=100)
      - L286 except Exception:
        - L287 assign sample_data = None
        - L288 expr logger.exception('chart_suggest_sample_data_failed', extra={'event': 'chart_suggest_sample_data_failed', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
  - L298 if not batches:
    - L299 expr logger.info('chart_suggest_no_data', extra={'event': 'chart_suggest_no_data', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
    - L308 assign sample_payload = sample_data if payload.include_sample_data else None
    - L309 if sample_payload is None and payload.include_sample_data:
      - L310 assign sample_payload = []
    - L311 return ChartSuggestResponse(charts=[], sample_data=sample_payload)
  - L313 assign (field_catalog, stats) = build_field_catalog_fn(batches)
  - L315 assign prompt = build_prompt_fn(template_id=template_id, kind=kind, start_date=payload.start_date, end_date=payload.end_date, key_values=key_values_payload, field_catalog=field_catalog, data_stats=stats, question=payload.question)
  - L326 try:
    - L327 assign response = call_chat_completion_fn(model=model, messages=[{'role': 'user', 'content': prompt}])
    - L331 except Exception as exc:
      - L332 expr logger.exception('chart_suggest_llm_failed', extra={'event': 'chart_suggest_llm_failed', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
      - L341 raise HTTPException(status_code=500, detail={'code': 'chart_suggest_llm_failed', 'message': str(exc)})
  - L343 assign raw_text = (response.choices[0].message.content or '').strip()
  - L344 assign parsed_text = strip_code_fences_fn(raw_text)
  - L346 annotated assign charts: list[ChartSpec] = []
  - L347 assign field_lookup = _build_field_lookup(field_catalog)
  - L348 try:
    - L349 assign payload_json = json.loads(parsed_text)
    - L350 except Exception:
      - L351 expr logger.warning('chart_suggest_json_parse_failed', extra={'event': 'chart_suggest_json_parse_failed', 'template_id': template_id, 'template_kind': kind, 'correlation_id': correlation_id})
      - L360 assign payload_json = {}
  - L362 assign raw_charts = payload_json.get('charts') if isinstance(payload_json, dict) else None
  - L363 if isinstance(raw_charts, list):
    - L364 for (idx, item) in enumerate(raw_charts):
      - L365 assign chart = _normalize_chart_suggestion(item, idx=idx, field_lookup=field_lookup)
      - L366 if chart:
        - L367 expr charts.append(chart)
  - L370 if not charts:
    - L371 assign numeric_fields = [name for name, ftype in field_lookup.values() if _field_category(ftype) == 'numeric']
    - L372 assign time_like = [name for name, ftype in field_lookup.values() if _field_category(ftype) == 'datetime']
    - L373 assign categorical_fields = [name for name, ftype in field_lookup.values() if _field_category(ftype) == 'categorical']
    - L374 assign fallback_id = 0
    - L376 def _next_id:
      - L377 Nonlocal
      - L378 aug assign fallback_id Add 1
      - L379 return f'fallback_{fallback_id}'
    - L381 if time_like and numeric_fields:
      - L382 expr charts.append(ChartSpec(id=_next_id(), type='line', xField=time_like[0], yFields=[numeric_fields[0]], groupField=None, aggregation='sum', chartTemplateId='time_series_basic', title=f'{numeric_fields[0]} over time'))
    - L394 if categorical_fields and numeric_fields:
      - L395 expr charts.append(ChartSpec(id=_next_id(), type='bar', xField=categorical_fields[0], yFields=[numeric_fields[0]], groupField=None, aggregation='sum', chartTemplateId='top_n_categories', title=f'Top categories by {numeric_fields[0]}'))
    - L407 if numeric_fields:
      - L408 expr charts.append(ChartSpec(id=_next_id(), type='bar', xField=numeric_fields[0], yFields=[numeric_fields[0]], groupField=None, aggregation='count', chartTemplateId='distribution_histogram', title=f'{numeric_fields[0]} distribution'))
  - L421 expr state_store.set_last_used(payload.connection_id, template_id)
  - L423 expr logger.info('chart_suggest_complete', extra={'event': 'chart_suggest_complete', 'template_id': template_id, 'template_kind': kind, 'charts_returned': len(charts), 'correlation_id': correlation_id})
  - L433 return ChartSuggestResponse(charts=charts, sample_data=sample_data if payload.include_sample_data else None)

## backend\app\features\generate\services\discovery_service.py
- L1 from __future__ import annotations
- L3 import json
- L4 from typing import Any, Callable, Mapping, Optional
- L6 from fastapi import HTTPException
- L8 from ....services.reports.discovery_metrics import build_discovery_schema, build_resample_support
- L9 from ....services.state import state_store
- L12 def discover_reportspayload, *, kind: str, template_dir_fn: Callable[[str], Any], db_path_fn: Callable[[Optional[str]], Any], load_contract_fn: Callable[[Any], Any], clean_key_values_fn: Callable[[Optional[dict]], Optional[dict]], discover_fn: Callable[..., Mapping[str, Any]], build_field_catalog_fn: Callable[[list], tuple[list, Mapping[str, Any]]], build_batch_metrics_fn: Callable[[list, Mapping[str, Any]], list], load_manifest_fn: Callable[[Any], Mapping[str, Any]], manifest_endpoint_fn: Callable[[str], str], logger:
  - L27 def _normalize_field_catalograw_catalog:
    - L28 annotated assign normalized: list[dict[str, str]] = []
    - L29 if not isinstance(raw_catalog, list):
      - L30 return normalized
    - L31 for entry in raw_catalog:
      - L32 if not isinstance(entry, Mapping):
        - L33 continue
      - L34 assign name = str(entry.get('name') or '').strip()
      - L35 if not name:
        - L36 continue
      - L37 assign field_type = str(entry.get('type') or 'unknown').strip()
      - L38 assign description = str(entry.get('description') or '').strip()
      - L39 assign source = str(entry.get('source') or entry.get('table') or 'computed').strip() or 'computed'
      - L40 expr normalized.append({'name': name, 'type': field_type, 'description': description, 'source': source})
    - L48 return normalized
  - L50 assign template_dir = template_dir_fn(payload.template_id)
  - L51 assign db_path = db_path_fn(payload.connection_id)
  - L52 if not db_path.exists():
    - L53 raise HTTPException(status_code=400, detail={'code': 'db_not_found', 'message': f'DB not found: {db_path}'})
  - L55 try:
    - L56 expr load_contract_fn(template_dir)
    - L57 except Exception as exc:
      - L58 expr logger.exception('contract_artifacts_load_failed', extra={'event': 'contract_artifacts_load_failed', 'template_id': payload.template_id})
      - L65 raise HTTPException(status_code=500, detail={'code': 'contract_load_failed', 'message': f'Failed to load contract artifacts: {exc}'})
  - L67 assign contract_path = template_dir / 'contract.json'
  - L68 if not contract_path.exists():
    - L69 raise HTTPException(status_code=400, detail={'code': 'contract_not_ready', 'message': 'Contract artifacts missing. Approve mapping first.'})
  - L73 try:
    - L74 assign contract_payload = json.loads(contract_path.read_text(encoding='utf-8'))
    - L75 except Exception as exc:
      - L76 raise HTTPException(status_code=500, detail={'code': 'contract_invalid', 'message': f'Invalid contract.json: {exc}'})
  - L78 assign key_values_payload = clean_key_values_fn(payload.key_values)
  - L79 try:
    - L80 assign summary = discover_fn(db_path=db_path, contract=contract_payload, start_date=payload.start_date, end_date=payload.end_date, key_values=key_values_payload)
    - L87 except Exception as exc:
      - L88 raise HTTPException(status_code=500, detail={'code': 'discovery_failed', 'message': f'Discovery failed: {exc}'})
  - L90 assign manifest_data = load_manifest_fn(template_dir) or {}
  - L91 assign manifest_url = manifest_endpoint_fn(payload.template_id)
  - L92 assign tpl_record = state_store.get_template_record(payload.template_id) or {}
  - L93 assign tpl_name = tpl_record.get('name') or f'Template {payload.template_id[:8]}'
  - L94 expr state_store.set_last_used(payload.connection_id, payload.template_id)
  - L96 assign batches_raw = summary.get('batches') or []
  - L97 if not isinstance(batches_raw, list):
    - L98 assign batches_raw = []
  - L100 assign raw_batch_metadata = summary.get('batch_metadata')
  - L101 annotated assign batch_metadata: dict[str, dict[str, object]] = raw_batch_metadata if isinstance(raw_batch_metadata, Mapping) else {}
  - L103 assign raw_field_catalog = summary.get('field_catalog')
  - L104 assign raw_stats = summary.get('data_stats')
  - L105 if not isinstance(raw_field_catalog, list):
    - L106 assign (raw_field_catalog, raw_stats) = build_field_catalog_fn(batches_raw)
  - L107 assign field_catalog = _normalize_field_catalog(raw_field_catalog)
  - L108 if not field_catalog:
    - L109 assign (fallback_catalog, raw_stats) = build_field_catalog_fn(batches_raw)
    - L110 assign field_catalog = _normalize_field_catalog(fallback_catalog)
  - L111 assign data_stats = raw_stats if isinstance(raw_stats, Mapping) else {}
  - L113 assign discovery_schema = summary.get('discovery_schema')
  - L114 if not isinstance(discovery_schema, Mapping):
    - L115 assign discovery_schema = build_discovery_schema(field_catalog)
  - L117 assign batch_metrics = summary.get('batch_metrics')
  - L118 if not isinstance(batch_metrics, list):
    - L119 assign batch_metrics = build_batch_metrics_fn(batches_raw, batch_metadata)
  - L120 assign batch_metrics = batch_metrics if isinstance(batch_metrics, list) else []
  - L122 assign numeric_bins = summary.get('numeric_bins')
  - L123 assign category_groups = summary.get('category_groups')
  - L124 if not isinstance(numeric_bins, Mapping) or not isinstance(category_groups, Mapping):
    - L125 assign resample_support = build_resample_support(field_catalog, batch_metrics, schema=discovery_schema, default_metric=(discovery_schema or {}).get('defaults', {}).get('metric'))
    - L131 assign numeric_bins = resample_support.get('numeric_bins', {})
    - L132 assign category_groups = resample_support.get('category_groups', {})
  - L134 def _time_bounds:
    - L135 assign timestamps = []
    - L136 for meta in batch_metadata.values():
      - L137 if not isinstance(meta, Mapping):
        - L138 continue
      - L139 assign ts = meta.get('time')
      - L140 if ts:
        - L141 expr timestamps.append(ts)
    - L142 if not timestamps:
      - L143 return (None, None)
    - L144 try:
      - L145 assign ts_sorted = sorted(timestamps)
      - L146 return (ts_sorted[0], ts_sorted[-1])
      - L147 except Exception:
        - L148 return (None, None)
  - L150 assign (time_start, time_end) = _time_bounds()
  - L152 return {'template_id': payload.template_id, 'name': tpl_name, 'batches': [{'id': b['id'], 'rows': b['rows'], 'parent': b['parent'], 'selected': True, 'time': (batch_metadata.get(str(b['id'])) or {}).get('time'), 'category': (batch_metadata.get(str(b['id'])) or {}).get('category')} for b in batches_raw], 'batches_count': summary['batches_count'], 'rows_total': summary['rows_total'], 'manifest_url': manifest_url, 'manifest_produced_at': manifest_data.get('produced_at'), 'field_catalog': field_catalog, 'batch_metrics': batch_metrics, 'discovery_schema': discovery_schema, 'numeric_bins': numeric_bins, 'category_groups': category_groups, 'date_range': {'start': payload.start_date, 'end': payload.end_date, 'time_start': time_start, 'time_end': time_end}, 'data_stats': data_stats}

## backend\app\features\generate\services\saved_charts_service.py
- L1 from __future__ import annotations
- L3 from typing import Callable
- L5 from fastapi import HTTPException
- L7 from ....services.state import state_store
- L8 from ..schemas.charts import SavedChartCreatePayload, SavedChartSpec, SavedChartUpdatePayload
- L11 def _serialize_saved_chartrecord: dict:
  - L12 assign spec_payload = record.get('spec') or {}
  - L13 return SavedChartSpec(id=record['id'], template_id=record['template_id'], name=record['name'], spec=spec_payload, created_at=record.get('created_at', ''), updated_at=record.get('updated_at', ''))
- L23 assign EnsureTemplateExistsFn = Callable[[str], tuple[str, dict]]
- L24 assign NormalizeTemplateIdFn = Callable[[str], str]
- L27 def list_saved_chartstemplate_id: str, ensure_template_exists: EnsureTemplateExistsFn:
  - L28 assign (normalized, _) = ensure_template_exists(template_id)
  - L29 assign records = state_store.list_saved_charts(normalized)
  - L30 assign charts = [_serialize_saved_chart(rec) for rec in records]
  - L31 return {'charts': charts}
- L34 def create_saved_charttemplate_id: str, payload: SavedChartCreatePayload, ensure_template_exists: EnsureTemplateExistsFn, normalize_template_id: NormalizeTemplateIdFn:
  - L40 assign (path_template, _) = ensure_template_exists(template_id)
  - L41 assign body_template = normalize_template_id(payload.template_id)
  - L42 if body_template != path_template:
    - L43 raise HTTPException(status_code=400, detail={'code': 'template_mismatch', 'message': 'template_id in path and payload must match'})
  - L44 assign name = (payload.name or '').strip()
  - L45 if not name:
    - L46 raise HTTPException(status_code=400, detail={'code': 'name_required', 'message': 'Saved chart name is required.'})
  - L47 assign record = state_store.create_saved_chart(path_template, name, payload.spec.model_dump())
  - L48 return _serialize_saved_chart(record)
- L51 def update_saved_charttemplate_id: str, chart_id: str, payload: SavedChartUpdatePayload, ensure_template_exists: EnsureTemplateExistsFn:
  - L57 assign (path_template, _) = ensure_template_exists(template_id)
  - L58 assign existing = state_store.get_saved_chart(chart_id)
  - L59 if not existing or existing.get('template_id') != path_template:
    - L60 raise HTTPException(status_code=404, detail={'code': 'chart_not_found', 'message': 'Saved chart not found.'})
  - L61 annotated assign updates: dict[str, object] = {}
  - L62 if payload.name is not None:
    - L63 assign name = payload.name.strip()
    - L64 if not name:
      - L65 raise HTTPException(status_code=400, detail={'code': 'name_required', 'message': 'Saved chart name cannot be empty.'})
    - L66 assign updates['name'] = name
  - L67 if payload.spec is not None:
    - L68 assign updates['spec'] = payload.spec.model_dump()
  - L69 if not updates:
    - L70 return _serialize_saved_chart(existing)
  - L71 assign record = state_store.update_saved_chart(chart_id, name=updates.get('name'), spec=updates.get('spec'))
  - L72 if not record:
    - L73 raise HTTPException(status_code=404, detail={'code': 'chart_not_found', 'message': 'Saved chart not found.'})
  - L74 return _serialize_saved_chart(record)
- L77 def delete_saved_charttemplate_id: str, chart_id: str, ensure_template_exists: EnsureTemplateExistsFn:
  - L78 assign (path_template, _) = ensure_template_exists(template_id)
  - L79 assign existing = state_store.get_saved_chart(chart_id)
  - L80 if not existing or existing.get('template_id') != path_template:
    - L81 raise HTTPException(status_code=404, detail={'code': 'chart_not_found', 'message': 'Saved chart not found.'})
  - L82 assign removed = state_store.delete_saved_chart(chart_id)
  - L83 if not removed:
    - L84 raise HTTPException(status_code=404, detail={'code': 'chart_not_found', 'message': 'Saved chart not found.'})
  - L85 return {'status': 'ok'}

## backend\app\features\generate\utils\__init__.py

## backend\app\services\__init__.py
- L1 docstring: "Service package placeholder."

## backend\app\services\background_tasks.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import contextlib
- L5 import json
- L6 import logging
- L7 import os
- L8 import threading
- L9 from concurrent.futures import ThreadPoolExecutor
- L10 from typing import Any, AsyncIterable, Callable, Iterable, Optional
- L12 from backend.app.services.state import state_store
- L14 assign logger = logging.getLogger('neura.background_tasks')
- L16 assign _DEFAULT_TASK_WORKERS = os.cpu_count() or 4
- L17 assign _TASK_WORKERS = max(int(os.getenv('NR_TASK_WORKERS', str(_DEFAULT_TASK_WORKERS)) or _DEFAULT_TASK_WORKERS), 1)
- L18 assign _TASK_EXECUTOR = ThreadPoolExecutor(max_workers=_TASK_WORKERS)
- L20 annotated assign _BACKGROUND_TASKS: set[asyncio.Task] = set()
- L21 assign _BACKGROUND_LOCK = threading.Lock()
- L24 def _track_tasktask: asyncio.Task:
  - L25 with _BACKGROUND_LOCK:
    - L26 expr _BACKGROUND_TASKS.add(task)
  - L28 def _done_task: asyncio.Task:
    - L29 with _BACKGROUND_LOCK:
      - L30 expr _BACKGROUND_TASKS.discard(_task)
  - L32 expr task.add_done_callback(_done)
- L35 def _is_cancelledjob_id: str:
  - L36 assign job = state_store.get_job(job_id) or {}
  - L37 assign status = str(job.get('status') or '').strip().lower()
  - L38 return status == 'cancelled'
- L41 def _normalize_step_statusstatus: Optional[str]:
  - L42 if not status:
    - L43 return None
  - L44 assign value = str(status).strip().lower()
  - L45 if value in {'started', 'running', 'in_progress'}:
    - L46 return 'running'
  - L47 if value in {'complete', 'completed', 'done', 'success'}:
    - L48 return 'succeeded'
  - L49 if value in {'error', 'failed'}:
    - L50 return 'failed'
  - L51 if value in {'skipped'}:
    - L52 return 'succeeded'
  - L53 if value in {'cancelled', 'canceled'}:
    - L54 return 'cancelled'
  - L55 return value
- L58 def _apply_eventjob_id: str, event: dict, *, result_builder: Optional[Callable[[dict], dict]]=None:
  - L64 assign event_type = str(event.get('event') or '').strip().lower()
  - L65 if event_type == 'stage':
    - L66 assign stage = str(event.get('stage') or event.get('label') or 'stage').strip()
    - L67 assign label = str(event.get('label') or event.get('detail') or stage).strip()
    - L68 assign status = _normalize_step_status(event.get('status'))
    - L69 assign progress = event.get('progress')
    - L70 expr state_store.record_job_step(job_id, stage, label=label, status=status, progress=progress if isinstance(progress, (int, float)) else None)
    - L77 if isinstance(progress, (int, float)):
      - L78 expr state_store.record_job_progress(job_id, float(progress))
    - L79 return False
  - L81 if event_type == 'error':
    - L82 assign detail = event.get('detail') or event.get('message') or 'Task failed'
    - L83 expr state_store.record_job_completion(job_id, status='failed', error=str(detail))
    - L84 return True
  - L86 if event_type == 'result':
    - L87 assign result_payload = result_builder(event) if result_builder else dict(event)
    - L88 expr state_store.record_job_completion(job_id, status='succeeded', result=result_payload)
    - L89 return True
  - L91 return False
- L94 def iter_ndjson_eventschunks: Iterable[bytes]:
  - L95 assign buffer = ''
  - L96 for chunk in chunks:
    - L97 try:
      - L98 assign text = chunk.decode('utf-8')
      - L99 except Exception:
        - L100 continue
    - L101 aug assign buffer Add text
    - L102 while '\n' in buffer:
      - L103 assign (line, buffer) = buffer.split('\n', 1)
      - L104 assign line = line.strip()
      - L105 if not line:
        - L106 continue
      - L107 try:
        - L108 assign payload = json.loads(line)
        - L109 except Exception:
          - L110 continue
      - L111 if isinstance(payload, dict):
        - L112 expr (yield payload)
- L115 async def iter_ndjson_events_asyncchunks: AsyncIterable[bytes]:
  - L116 assign buffer = ''
  - L117 try:
    - L118 async for chunk in chunks:
      - L119 try:
        - L120 assign text = chunk.decode('utf-8')
        - L121 except Exception:
          - L122 continue
      - L123 aug assign buffer Add text
      - L124 while '\n' in buffer:
        - L125 assign (line, buffer) = buffer.split('\n', 1)
        - L126 assign line = line.strip()
        - L127 if not line:
          - L128 continue
        - L129 try:
          - L130 assign payload = json.loads(line)
          - L131 except Exception:
            - L132 continue
        - L133 if isinstance(payload, dict):
          - L134 expr (yield payload)
    - L136 finally:
      - L136 assign close_fn = getattr(chunks, 'aclose', None)
      - L137 if callable(close_fn):
        - L138 with contextlib.suppress(Exception):
          - L139 expr await close_fn()
- L142 def run_event_streamjob_id: str, events: Iterable[dict], *, result_builder: Optional[Callable[[dict], dict]]=None:
  - L148 if _is_cancelled(job_id):
    - L149 return None
  - L150 expr state_store.record_job_start(job_id)
  - L151 assign completed = False
  - L152 for event in events:
    - L153 if _is_cancelled(job_id):
      - L154 expr state_store.record_job_completion(job_id, status='cancelled', error='Cancelled by user')
      - L155 assign close_fn = getattr(events, 'close', None)
      - L156 if callable(close_fn):
        - L157 with contextlib.suppress(Exception):
          - L158 expr close_fn()
      - L159 return None
    - L160 assign completed = _apply_event(job_id, event, result_builder=result_builder)
    - L161 if completed:
      - L162 break
  - L163 if completed:
    - L164 assign close_fn = getattr(events, 'close', None)
    - L165 if callable(close_fn):
      - L166 with contextlib.suppress(Exception):
        - L167 expr close_fn()
  - L168 if not completed:
    - L169 expr state_store.record_job_completion(job_id, status='failed', error='Task finished without result')
- L172 async def run_event_stream_asyncjob_id: str, events: AsyncIterable[dict], *, result_builder: Optional[Callable[[dict], dict]]=None:
  - L178 if _is_cancelled(job_id):
    - L179 return None
  - L180 expr state_store.record_job_start(job_id)
  - L181 assign completed = False
  - L182 async for event in events:
    - L183 if _is_cancelled(job_id):
      - L184 expr state_store.record_job_completion(job_id, status='cancelled', error='Cancelled by user')
      - L185 assign close_fn = getattr(events, 'aclose', None)
      - L186 if callable(close_fn):
        - L187 with contextlib.suppress(Exception):
          - L188 expr await close_fn()
      - L189 return None
    - L190 assign completed = _apply_event(job_id, event, result_builder=result_builder)
    - L191 if completed:
      - L192 break
  - L193 if completed:
    - L194 assign close_fn = getattr(events, 'aclose', None)
    - L195 if callable(close_fn):
      - L196 with contextlib.suppress(Exception):
        - L197 expr await close_fn()
  - L198 if not completed:
    - L199 expr state_store.record_job_completion(job_id, status='failed', error='Task finished without result')
- L202 async def enqueue_background_job*, job_type: str, template_id: Optional[str]=None, connection_id: Optional[str]=None, template_name: Optional[str]=None, template_kind: Optional[str]=None, steps: Optional[Iterable[dict]]=None, meta: Optional[dict]=None, runner: Callable[[str], None]:
  - L213 assign job = state_store.create_job(job_type=job_type, template_id=template_id, connection_id=connection_id, template_name=template_name, template_kind=template_kind, steps=steps, meta=meta)
  - L223 async def _schedule:
    - L224 def _run:
      - L225 try:
        - L226 assign result = runner(job['id'])
        - L227 if asyncio.iscoroutine(result):
          - L228 expr asyncio.run(result)
        - L229 except Exception as exc:
          - L230 expr logger.exception('background_task_failed', extra={'event': 'background_task_failed', 'job_id': job.get('id'), 'error': str(exc)})
          - L234 expr state_store.record_job_completion(job['id'], status='failed', error=str(exc))
    - L236 assign loop = asyncio.get_running_loop()
    - L237 expr await loop.run_in_executor(_TASK_EXECUTOR, _run)
  - L239 assign task = asyncio.create_task(_schedule())
  - L240 expr _track_task(task)
  - L241 return job

## backend\app\services\charts\__init__.py
- L2 docstring: "\nChart Generation Module.\n\nProvides server-side chart generation using:\n- Qu..."
- L10 from .quickchart import QuickChartClient, ChartConfig, create_bar_chart, create_line_chart, create_pie_chart, create_scatter_chart, generate_chart_url, save_chart_image
- L21 assign __all__ = ['QuickChartClient', 'ChartConfig', 'create_bar_chart', 'create_line_chart', 'create_pie_chart', 'create_scatter_chart', 'generate_chart_url', 'save_chart_image']

## backend\app\services\charts\quickchart.py
- L2 docstring: "\nQuickChart Integration for Server-Side Chart Generation.\n\nQuickChart is an o..."
- L15 from __future__ import annotations
- L17 import json
- L18 import logging
- L19 import urllib.parse
- L20 import urllib.request
- L21 from dataclasses import dataclass, field
- L22 from typing import Any, Dict, List, Optional, Union
- L24 assign logger = logging.getLogger('neura.charts.quickchart')
- L28 class ChartConfig:
  - L29 docstring: "Configuration for a QuickChart chart."
  - L30 annotated assign type: str
  - L31 annotated assign data: Dict[str, Any]
  - L32 annotated assign options: Dict[str, Any] = field(default_factory=dict)
  - L33 annotated assign width: int = 500
  - L34 annotated assign height: int = 300
  - L35 annotated assign background_color: str = 'white'
  - L36 annotated assign device_pixel_ratio: float = 2.0
  - L37 annotated assign format: str = 'png'
- L40 class QuickChartClient:
  - L41 docstring: "\n    Client for QuickChart API.\n\n    Can use the public API or a self-hosted ..."
  - L47 assign DEFAULT_URL = 'https://quickchart.io'
  - L49 def __init__self, base_url: Optional[str]=None, api_key: Optional[str]=None:
    - L54 assign self.base_url = (base_url or self.DEFAULT_URL).rstrip('/')
    - L55 assign self.api_key = api_key
  - L57 def get_chart_urlself, config: ChartConfig:
    - L58 docstring: "\n        Get a URL that renders the chart.\n\n        The URL can be used direc..."
    - L63 assign chart_json = self._build_chart_json(config)
    - L64 assign encoded = urllib.parse.quote(json.dumps(chart_json, separators=(',', ':')))
    - L66 assign params = {'c': encoded, 'w': str(config.width), 'h': str(config.height), 'bkg': config.background_color, 'devicePixelRatio': str(config.device_pixel_ratio), 'f': config.format}
    - L75 if self.api_key:
      - L76 assign params['key'] = self.api_key
    - L78 assign query = urllib.parse.urlencode(params)
    - L79 return f'{self.base_url}/chart?{query}'
  - L81 def get_chart_bytesself, config: ChartConfig:
    - L82 docstring: "\n        Download the chart as bytes.\n\n        Returns PNG/SVG bytes that can..."
    - L87 assign url = self.get_chart_url(config)
    - L89 try:
      - L90 assign req = urllib.request.Request(url)
      - L91 with urllib.request.urlopen(req, timeout=30) as response:
        - L92 return response.read()
      - L93 except Exception as e:
        - L94 expr logger.error(f'Failed to download chart: {e}')
        - L95 raise
  - L97 def get_short_urlself, config: ChartConfig:
    - L98 docstring: "\n        Get a short URL for the chart.\n\n        Useful for sharing or embedd..."
    - L103 assign chart_json = self._build_chart_json(config)
    - L105 assign payload = json.dumps({'chart': chart_json, 'width': config.width, 'height': config.height, 'backgroundColor': config.background_color, 'devicePixelRatio': config.device_pixel_ratio, 'format': config.format}).encode('utf-8')
    - L114 assign headers = {'Content-Type': 'application/json'}
    - L115 if self.api_key:
      - L116 assign headers['X-QuickChart-Api-Key'] = self.api_key
    - L118 assign req = urllib.request.Request(f'{self.base_url}/chart/create', data=payload, headers=headers, method='POST')
    - L125 try:
      - L126 with urllib.request.urlopen(req, timeout=30) as response:
        - L127 assign result = json.loads(response.read().decode('utf-8'))
        - L128 return result.get('url', '')
      - L129 except Exception as e:
        - L130 expr logger.error(f'Failed to create short URL: {e}')
        - L131 raise
  - L133 def _build_chart_jsonself, config: ChartConfig:
    - L134 docstring: "Build the Chart.js configuration JSON."
    - L135 return {'type': config.type, 'data': config.data, 'options': config.options}
- L144 def create_bar_chartlabels: List[str], datasets: List[Dict[str, Any]], title: Optional[str]=None, stacked: bool=False, horizontal: bool=False, **kwargs:
  - L152 docstring: "\n    Create a bar chart configuration.\n\n    Args:\n        labels: X-axis lab..."
  - L166 assign processed_datasets = []
  - L167 assign colors = ['rgba(54, 162, 235, 0.8)', 'rgba(255, 99, 132, 0.8)', 'rgba(75, 192, 192, 0.8)', 'rgba(255, 206, 86, 0.8)', 'rgba(153, 102, 255, 0.8)']
  - L175 for (i, ds) in enumerate(datasets):
    - L176 assign processed = {'label': ds.get('label', f'Dataset {i + 1}'), 'data': ds.get('data', []), 'backgroundColor': ds.get('backgroundColor', colors[i % len(colors)])}
    - L181 expr processed_datasets.append(processed)
  - L183 annotated assign options: Dict[str, Any] = {'responsive': True, 'plugins': {}}
  - L188 if title:
    - L189 assign options['plugins']['title'] = {'display': True, 'text': title}
  - L194 if stacked:
    - L195 assign options['scales'] = {'x': {'stacked': True}, 'y': {'stacked': True}}
  - L200 assign chart_type = 'horizontalBar' if horizontal else 'bar'
  - L202 return ChartConfig(type=chart_type, data={'labels': labels, 'datasets': processed_datasets}, options=options, **kwargs)
- L213 def create_line_chartlabels: List[str], datasets: List[Dict[str, Any]], title: Optional[str]=None, fill: bool=False, smooth: bool=True, **kwargs:
  - L221 docstring: "\n    Create a line chart configuration.\n\n    Args:\n        labels: X-axis la..."
  - L234 assign colors = ['rgb(54, 162, 235)', 'rgb(255, 99, 132)', 'rgb(75, 192, 192)', 'rgb(255, 206, 86)', 'rgb(153, 102, 255)']
  - L242 assign processed_datasets = []
  - L243 for (i, ds) in enumerate(datasets):
    - L244 assign processed = {'label': ds.get('label', f'Dataset {i + 1}'), 'data': ds.get('data', []), 'borderColor': ds.get('borderColor', colors[i % len(colors)]), 'backgroundColor': ds.get('backgroundColor', colors[i % len(colors)].replace('rgb', 'rgba').replace(')', ', 0.2)')), 'fill': ds.get('fill', fill), 'tension': 0.4 if smooth else 0}
    - L252 expr processed_datasets.append(processed)
  - L254 annotated assign options: Dict[str, Any] = {'responsive': True, 'plugins': {}}
  - L259 if title:
    - L260 assign options['plugins']['title'] = {'display': True, 'text': title}
  - L265 return ChartConfig(type='line', data={'labels': labels, 'datasets': processed_datasets}, options=options, **kwargs)
- L276 def create_pie_chartlabels: List[str], data: List[Union[int, float]], title: Optional[str]=None, doughnut: bool=False, **kwargs:
  - L283 docstring: "\n    Create a pie/doughnut chart configuration.\n\n    Args:\n        labels: S..."
  - L295 assign colors = ['rgba(255, 99, 132, 0.8)', 'rgba(54, 162, 235, 0.8)', 'rgba(255, 206, 86, 0.8)', 'rgba(75, 192, 192, 0.8)', 'rgba(153, 102, 255, 0.8)', 'rgba(255, 159, 64, 0.8)', 'rgba(199, 199, 199, 0.8)', 'rgba(83, 102, 255, 0.8)']
  - L307 while len(colors) < len(data):
    - L308 expr colors.extend(colors)
  - L310 annotated assign options: Dict[str, Any] = {'responsive': True, 'plugins': {}}
  - L315 if title:
    - L316 assign options['plugins']['title'] = {'display': True, 'text': title}
  - L321 return ChartConfig(type='doughnut' if doughnut else 'pie', data={'labels': labels, 'datasets': [{'data': data, 'backgroundColor': colors[:len(data)]}]}, options=options, **kwargs)
- L335 def create_scatter_chartdatasets: List[Dict[str, Any]], title: Optional[str]=None, x_label: Optional[str]=None, y_label: Optional[str]=None, **kwargs:
  - L342 docstring: "\n    Create a scatter plot configuration.\n\n    Args:\n        datasets: List ..."
  - L354 assign colors = ['rgba(255, 99, 132, 0.8)', 'rgba(54, 162, 235, 0.8)', 'rgba(75, 192, 192, 0.8)']
  - L360 assign processed_datasets = []
  - L361 for (i, ds) in enumerate(datasets):
    - L362 assign processed = {'label': ds.get('label', f'Dataset {i + 1}'), 'data': ds.get('data', []), 'backgroundColor': ds.get('backgroundColor', colors[i % len(colors)]), 'pointRadius': ds.get('pointRadius', 5)}
    - L368 expr processed_datasets.append(processed)
  - L370 annotated assign options: Dict[str, Any] = {'responsive': True, 'plugins': {}, 'scales': {'x': {'type': 'linear', 'position': 'bottom'}, 'y': {'type': 'linear'}}}
  - L379 if title:
    - L380 assign options['plugins']['title'] = {'display': True, 'text': title}
  - L385 if x_label:
    - L386 assign options['scales']['x']['title'] = {'display': True, 'text': x_label}
  - L387 if y_label:
    - L388 assign options['scales']['y']['title'] = {'display': True, 'text': y_label}
  - L390 return ChartConfig(type='scatter', data={'datasets': processed_datasets}, options=options, **kwargs)
- L402 def generate_chart_urlchart_type: str, labels: List[str], data: Union[List, Dict], title: Optional[str]=None, **kwargs:
  - L409 docstring: "\n    Quick function to generate a chart URL.\n\n    Args:\n        chart_type: ..."
  - L422 assign client = QuickChartClient()
  - L424 if chart_type == 'bar':
    - L425 if isinstance(data, list):
      - L426 assign datasets = [{'label': title or 'Data', 'data': data}]
      - L428 else:
        - L428 assign datasets = data.get('datasets', [])
    - L429 assign config = create_bar_chart(labels, datasets, title=title, **kwargs)
    - L431 else:
      - L431 if chart_type == 'line':
        - L432 if isinstance(data, list):
          - L433 assign datasets = [{'label': title or 'Data', 'data': data}]
          - L435 else:
            - L435 assign datasets = data.get('datasets', [])
        - L436 assign config = create_line_chart(labels, datasets, title=title, **kwargs)
        - L438 else:
          - L438 if chart_type in ('pie', 'doughnut'):
            - L439 assign config = create_pie_chart(labels, data if isinstance(data, list) else [], title=title, doughnut=chart_type == 'doughnut', **kwargs)
            - L444 else:
              - L444 if chart_type == 'scatter':
                - L445 assign datasets = data.get('datasets', []) if isinstance(data, dict) else []
                - L446 assign config = create_scatter_chart(datasets, title=title, **kwargs)
                - L449 else:
                  - L449 raise ValueError(f'Unsupported chart type: {chart_type}')
  - L451 return client.get_chart_url(config)
- L454 def save_chart_imagechart_type: str, labels: List[str], data: Union[List, Dict], output_path: str, title: Optional[str]=None, **kwargs:
  - L462 docstring: "\n    Generate and save a chart image to a file.\n\n    Args:\n        chart_typ..."
  - L475 from pathlib import Path
  - L477 assign client = QuickChartClient()
  - L480 if chart_type == 'bar':
    - L481 assign datasets = [{'label': title or 'Data', 'data': data}] if isinstance(data, list) else data.get('datasets', [])
    - L482 assign config = create_bar_chart(labels, datasets, title=title, **kwargs)
    - L483 else:
      - L483 if chart_type == 'line':
        - L484 assign datasets = [{'label': title or 'Data', 'data': data}] if isinstance(data, list) else data.get('datasets', [])
        - L485 assign config = create_line_chart(labels, datasets, title=title, **kwargs)
        - L486 else:
          - L486 if chart_type in ('pie', 'doughnut'):
            - L487 assign config = create_pie_chart(labels, data if isinstance(data, list) else [], title=title, doughnut=chart_type == 'doughnut', **kwargs)
            - L489 else:
              - L489 raise ValueError(f'Unsupported chart type: {chart_type}')
  - L492 assign image_bytes = client.get_chart_bytes(config)
  - L494 assign output_path = Path(output_path)
  - L495 expr output_path.parent.mkdir(parents=True, exist_ok=True)
  - L496 expr output_path.write_bytes(image_bytes)
  - L498 return str(output_path)

## backend\app\services\connections\__init__.py
- L1 docstring: "Connections service placeholder."

## backend\app\services\connections\db_connection.py
- L2 from __future__ import annotations
- L4 import argparse
- L5 import json
- L6 import os
- L7 import tempfile
- L8 import uuid
- L9 from datetime import date, datetime
- L10 from pathlib import Path
- L11 from urllib.parse import urlparse
- L13 from ..dataframes import SQLiteDataFrameLoader, ensure_connection_loaded, dataframe_store
- L14 from ..dataframes import sqlite_shim
- L15 from ..state import state_store
- L17 assign STORAGE = os.path.join(tempfile.gettempdir(), 'neura_connections.jsonl')
- L20 def _strip_quotess: str | None:
  - L21 if s is None:
    - L22 return None
  - L23 return s.strip().strip('\'"')
- L26 def _sqlite_path_from_urldb_url: str:
  - L27 assign u = urlparse(db_url)
  - L28 assign raw = u.netloc + u.path if u.netloc else u.path or ''
  - L30 if raw.startswith('/') and len(raw) >= 3 and (raw[2] == ':'):
    - L31 assign raw = raw.lstrip('/')
  - L32 return raw.replace('/', os.sep)
- L35 def resolve_db_pathconnection_id: str | None, db_url: str | None, db_path: str | None:
  - L37 if connection_id:
    - L38 assign secrets = state_store.get_connection_secrets(connection_id)
    - L39 if secrets and secrets.get('database_path'):
      - L40 return Path(secrets['database_path'])
    - L41 assign record = state_store.get_connection_record(connection_id)
    - L42 if record and record.get('database_path'):
      - L43 return Path(record['database_path'])
    - L44 if os.path.exists(STORAGE):
      - L45 with open(STORAGE, 'r', encoding='utf-8') as f:
        - L46 for line in f:
          - L47 try:
            - L48 assign rec = json.loads(line)
            - L49 if rec.get('id') == connection_id:
              - L50 assign cfg = rec.get('cfg') or {}
              - L51 assign db = cfg.get('database')
              - L52 if db:
                - L54 expr state_store.upsert_connection(conn_id=connection_id, name=cfg.get('name') or f"{cfg.get('db_type') or 'sqlite'}@{Path(db).name}", db_type=cfg.get('db_type') or 'sqlite', database_path=str(db), secret_payload={'database': str(db), 'db_url': cfg.get('db_url')})
                - L61 return Path(db)
            - L62 except Exception:
              - L63 continue
    - L64 raise RuntimeError(f'connection_id {connection_id!r} not found in storage')
  - L67 assign db_url = _strip_quotes(db_url)
  - L68 if db_url:
    - L69 assign parsed = urlparse(db_url)
    - L70 if parsed.scheme:
      - L71 if parsed.scheme.lower() == 'sqlite':
        - L72 return Path(_sqlite_path_from_url(db_url))
      - L73 if len(parsed.scheme) == 1 and db_url[1:3] in (':\\', ':/'):
        - L74 return Path(db_url)
      - L75 raise RuntimeError('Only sqlite URLs are supported for now')
    - L76 return Path(db_url)
  - L79 assign db_path = _strip_quotes(db_path)
  - L80 if db_path:
    - L81 return Path(db_path)
  - L84 assign env_path = _strip_quotes(os.getenv('DB_PATH'))
  - L85 if env_path:
    - L86 return Path(env_path)
  - L88 raise RuntimeError('No DB specified. Provide --connection-id OR --db-url OR --db-path (or DB_PATH env).')
- L91 def verify_sqlitepath: Path:
  - L92 docstring: "Raise when the backing SQLite file cannot be materialized into DataFrames."
  - L93 assign db_file = Path(path)
  - L94 if not db_file.exists():
    - L95 raise FileNotFoundError(f'SQLite DB not found: {path}')
  - L96 try:
    - L97 assign loader = SQLiteDataFrameLoader(db_file)
    - L98 expr loader.table_names()
    - L99 except Exception as exc:
      - L100 raise RuntimeError(f'SQLite->DataFrame load error: {exc}')
- L103 def execute_queryconnection_id: str, sql: str, limit: int | None=None, offset: int=0:
  - L109 docstring: "Execute a SQL query on a connection and return results.\n\n    Uses DataFrames i..."
  - L124 assign db_path = resolve_db_path(connection_id=connection_id, db_url=None, db_path=None)
  - L127 expr ensure_connection_loaded(connection_id, db_path)
  - L130 assign sql_upper = sql.upper().strip()
  - L131 if not sql_upper.startswith('SELECT') and (not sql_upper.startswith('WITH')):
    - L132 raise ValueError('Only SELECT queries are allowed')
  - L135 assign final_sql = sql
  - L136 if limit is not None:
    - L137 assign final_sql = f'{sql} LIMIT {limit}'
    - L138 if offset > 0:
      - L139 aug assign final_sql Add f' OFFSET {offset}'
  - L141 def coerce_valueval:
    - L142 docstring: "Convert values to JSON-serializable types."
    - L143 if val is None:
      - L144 return None
    - L145 if isinstance(val, (date, datetime)):
      - L146 return val.isoformat()
    - L147 if isinstance(val, bytes):
      - L148 return val.decode('utf-8', errors='replace')
    - L149 return val
  - L152 with sqlite_shim.connect(str(db_path)) as con:
    - L153 assign con.row_factory = sqlite_shim.Row
    - L154 assign cur = con.execute(final_sql)
    - L155 assign rows_raw = cur.fetchall()
    - L157 assign columns = list(rows_raw[0].keys()) if rows_raw else []
    - L158 assign rows = [[coerce_value(row[col]) for col in columns] for row in rows_raw]
  - L160 return {'columns': columns, 'rows': rows, 'row_count': len(rows)}
- L167 def save_connectioncfg: dict:
  - L168 docstring: "Persist a minimal record and return a connection_id."
  - L169 assign cid = cfg.get('id') or str(uuid.uuid4())
  - L170 assign db_type = cfg.get('db_type') or 'sqlite'
  - L171 assign database = cfg.get('database')
  - L172 assign db_url = cfg.get('db_url')
  - L173 if db_url and (not database):
    - L174 assign database = _sqlite_path_from_url(db_url)
  - L175 assign database_path = str(database) if database else ''
  - L176 assign name = cfg.get('name') or f'{db_type}@{(Path(database_path).name if database_path else cid)}'
  - L177 expr state_store.upsert_connection(conn_id=cid, name=name, db_type=db_type, database_path=database_path, secret_payload={'database': database_path, 'db_url': db_url}, status=cfg.get('status'), latency_ms=cfg.get('latency_ms'), tags=cfg.get('tags'))
  - L187 return cid
- L191 if __name__ == '__main__':
  - L192 assign parser = argparse.ArgumentParser(description='NeuraReport DB resolver')
  - L193 expr parser.add_argument('--connection-id')
  - L194 expr parser.add_argument('--db-url')
  - L195 expr parser.add_argument('--db-path')
  - L196 assign args = parser.parse_args()
  - L198 assign DB_PATH = resolve_db_path(connection_id=_strip_quotes(args.connection_id) or _strip_quotes(os.getenv('CONNECTION_ID')), db_url=_strip_quotes(args.db_url) or _strip_quotes(os.getenv('DB_URL')), db_path=_strip_quotes(args.db_path) or _strip_quotes(os.getenv('DB_PATH')))
  - L203 expr verify_sqlite(DB_PATH)
  - L204 expr print(f'Resolved DB path: {DB_PATH}')

## backend\app\services\contract\__init__.py
- L1 docstring: "\nContract-related helpers (LLM Call 4 builders).\n"
- L5 from .ContractBuilderV2 import build_or_load_contract_v2, load_contract_v2
- L7 assign __all__ = ['build_or_load_contract_v2', 'load_contract_v2']

## backend\app\services\contract\ContractBuilderV2.py
- L2 from __future__ import annotations
- L4 import hashlib
- L5 import json
- L6 import logging
- L7 import os
- L8 import re
- L9 import time
- L10 from pathlib import Path
- L11 from typing import Any, Iterable, Mapping, Optional
- L13 from ..prompts.llm_prompts import PROMPT_VERSION_4, build_llm_call_4_prompt
- L14 from ..templates.TemplateVerify import get_openai_client
- L15 from ..utils import call_chat_completion, write_artifact_manifest, write_json_atomic, write_text_atomic
- L22 assign logger = logging.getLogger('neura.contract.builder_v2')
- L24 assign _META_FILENAME = 'contract_v2_meta.json'
- L25 assign _CONTRACT_FILENAME = 'contract.json'
- L26 assign _OVERVIEW_FILENAME = 'overview.md'
- L27 assign _STEP5_REQ_FILENAME = 'step5_requirements.json'
- L30 class ContractBuilderError(RuntimeError):
  - L31 docstring: "Raised when contract construction fails."
- L34 def _ensure_schemaschema: Mapping[str, Any] | None:
  - L35 assign payload = dict(schema or {})
  - L36 expr payload.setdefault('scalars', [])
  - L37 expr payload.setdefault('row_tokens', [])
  - L38 expr payload.setdefault('totals', [])
  - L39 return {'scalars': [str(tok) for tok in payload.get('scalars', [])], 'row_tokens': [str(tok) for tok in payload.get('row_tokens', [])], 'totals': [str(tok) for tok in payload.get('totals', [])]}
- L46 def _normalize_key_tokensvalues: Iterable[str] | None:
  - L47 if not values:
    - L48 return []
  - L49 annotated assign seen: set[str] = set()
  - L50 annotated assign normalized: list[str] = []
  - L51 for raw in values:
    - L52 assign text = str(raw or '').strip()
    - L53 if not text or text in seen:
      - L54 continue
    - L55 expr seen.add(text)
    - L56 expr normalized.append(text)
  - L57 return normalized
- L60 def _compute_input_signature*, final_template_html: str, page_summary: str, schema: Mapping[str, Any], auto_mapping_proposal: Mapping[str, Any], mapping_override: Mapping[str, Any] | None, user_instructions: str, catalog: Iterable[str], dialect_hint: str | None, key_tokens: Iterable[str]:
  - L72 assign normalized_payload = {'final_html_sha256': hashlib.sha256((final_template_html or '').encode('utf-8')).hexdigest(), 'page_summary_sha256': hashlib.sha256((page_summary or '').encode('utf-8')).hexdigest(), 'schema': schema, 'auto_mapping_proposal': auto_mapping_proposal, 'mapping_override': dict(mapping_override or {}), 'user_instructions': user_instructions or '', 'catalog': list(catalog), 'dialect_hint': dialect_hint or '', 'key_tokens': list(key_tokens)}
  - L83 assign payload_bytes = json.dumps(normalized_payload, ensure_ascii=False, sort_keys=True).encode('utf-8')
  - L84 return hashlib.sha256(payload_bytes).hexdigest()
- L87 def _load_jsonpath: Path:
  - L88 try:
    - L89 return json.loads(path.read_text(encoding='utf-8'))
    - L90 except FileNotFoundError:
      - L91 raise
    - L92 except Exception as exc:
      - L93 expr logger.warning('contract_v2_json_load_failed', extra={'event': 'contract_v2_json_load_failed', 'path': str(path)}, exc_info=exc)
      - L98 raise
- L101 def _load_cached_payloadtemplate_dir: Path:
  - L104 assign contract_path = template_dir / _CONTRACT_FILENAME
  - L105 assign overview_path = template_dir / _OVERVIEW_FILENAME
  - L106 assign step5_path = template_dir / _STEP5_REQ_FILENAME
  - L107 assign meta_path = template_dir / _META_FILENAME
  - L109 assign required_files = (overview_path, step5_path, meta_path)
  - L110 for required in required_files:
    - L111 if not required.exists():
      - L112 return None
  - L114 try:
    - L115 assign meta = _load_json(meta_path)
    - L116 assign step5 = _load_json(step5_path)
    - L117 assign overview = overview_path.read_text(encoding='utf-8')
    - L118 assign contract = None
    - L120 if contract_path.exists():
      - L121 try:
        - L122 assign contract = _load_json(contract_path)
        - L123 except Exception:
          - L124 assign contract = None
    - L126 if contract is None:
      - L127 assign contract = meta.get('contract_payload')
      - L128 else:
        - L128 if isinstance(meta, dict):
          - L129 assign meta['contract_payload'] = contract
    - L130 except (FileNotFoundError, json.JSONDecodeError):
      - L131 return None
  - L132 if contract is None:
    - L133 return None
  - L135 return {'meta': meta, 'contract': contract, 'overview_md': overview, 'step5_requirements': step5, 'artifacts': {'overview': overview_path, 'step5_requirements': step5_path, 'meta': meta_path, **({'contract': contract_path} if contract_path.exists() else {})}, 'key_tokens': list(meta.get('key_tokens') or [])}
- L150 def _load_mapping_override_from_disktemplate_dir: Path:
  - L151 assign path = template_dir / 'mapping_pdf_labels.json'
  - L152 if not path.exists():
    - L153 return {}
  - L154 try:
    - L155 assign payload = json.loads(path.read_text(encoding='utf-8'))
    - L156 except Exception:
      - L157 expr logger.warning('contract_v2_mapping_override_load_failed', extra={'event': 'contract_v2_mapping_override_load_failed', 'path': str(path)}, exc_info=True)
      - L165 return {}
  - L166 if not isinstance(payload, list):
    - L167 return {}
  - L168 annotated assign mapping: dict[str, str] = {}
  - L169 for entry in payload:
    - L170 if not isinstance(entry, dict):
      - L171 continue
    - L172 assign token = str(entry.get('header') or '').strip()
    - L173 assign value = str(entry.get('mapping') or '').strip()
    - L174 if token:
      - L175 assign mapping[token] = value
  - L176 return mapping
- L179 def _load_page_summarytemplate_dir: Path:
  - L180 assign summary_path = template_dir / 'page_summary.txt'
  - L181 if summary_path.exists():
    - L182 try:
      - L183 return summary_path.read_text(encoding='utf-8')
      - L184 except Exception:
        - L185 expr logger.warning('contract_v2_page_summary_read_failed', extra={'event': 'contract_v2_page_summary_read_failed', 'path': str(summary_path)}, exc_info=True)
  - L190 assign stage_path = template_dir / 'stage_3_5.json'
  - L191 if stage_path.exists():
    - L192 try:
      - L193 assign stage_payload = _load_json(stage_path)
      - L194 except Exception:
        - L195 assign stage_payload = None
    - L196 if isinstance(stage_payload, Mapping):
      - L197 assign processed = stage_payload.get('processed')
      - L198 if isinstance(processed, Mapping):
        - L199 assign summary = processed.get('page_summary')
        - L200 if isinstance(summary, str) and summary.strip():
          - L201 return summary
      - L202 assign raw_response = stage_payload.get('raw_response')
      - L203 if isinstance(raw_response, Mapping):
        - L204 assign summary = raw_response.get('page_summary')
        - L205 if isinstance(summary, str) and summary.strip():
          - L206 return summary
  - L207 return ''
- L210 def _augment_contract_for_compatcontract: dict[str, Any]:
  - L211 assign tokens = contract.get('tokens') or {}
  - L212 assign scalars = list(tokens.get('scalars') or [])
  - L213 assign row_tokens = list(tokens.get('row_tokens') or [])
  - L214 assign totals = list(tokens.get('totals') or [])
  - L216 expr contract.setdefault('header_tokens', scalars)
  - L217 expr contract.setdefault('row_tokens', row_tokens)
  - L219 if 'totals' not in contract:
    - L220 assign mapping = contract.get('mapping') or {}
    - L221 assign contract['totals'] = {tok: str(mapping.get(tok, '')) for tok in totals}
  - L223 if 'row_order' not in contract:
    - L224 annotated assign rows_order: list[str] = []
    - L225 assign order_by_block = contract.get('order_by')
    - L226 annotated assign rows_spec: Any = None
    - L227 if isinstance(order_by_block, Mapping):
      - L228 assign rows_spec = order_by_block.get('rows')
      - L229 else:
        - L229 if isinstance(order_by_block, list):
          - L230 assign rows_spec = list(order_by_block)
          - L231 assign contract['order_by'] = {'rows': list(rows_spec)}
          - L233 else:
            - L233 if order_by_block not in (None, {}):
              - L234 assign contract['order_by'] = {}
    - L235 if isinstance(rows_spec, list) and rows_spec:
      - L236 assign rows_order = [str(item) for item in rows_spec if str(item).strip()]
      - L237 else:
        - L237 if isinstance(rows_spec, str) and rows_spec.strip():
          - L238 assign rows_order = [rows_spec.strip()]
    - L239 assign contract['row_order'] = rows_order or ['ROWID']
  - L241 expr contract.setdefault('literals', contract.get('literals', {}))
  - L242 return contract
- L245 def _prepare_messagessystem_text: str, payload_messages: list[dict[str, Any]]:
  - L246 assign messages = [{'role': 'system', 'content': [{'type': 'text', 'text': system_text}]}]
  - L257 expr messages.extend(payload_messages)
  - L258 return messages
- L261 def _normalize_contract_payloadcontract: Mapping[str, Any] | None:
  - L262 docstring: "\n    Ensure the contract payload meets schema expectations before validation.\n..."
  - L266 annotated assign normalized: dict[str, Any] = json.loads(json.dumps(contract or {}, ensure_ascii=False))
  - L267 assign join = normalized.get('join')
  - L268 if isinstance(join, dict):
    - L269 assign required_keys = ('parent_table', 'parent_key', 'child_table', 'child_key')
    - L270 assign missing_keys = []
    - L272 for key in required_keys:
      - L273 assign value = join.get(key)
      - L274 if value is None:
        - L275 assign join[key] = ''
        - L276 expr missing_keys.append(key)
        - L277 else:
          - L277 if not isinstance(value, str):
            - L278 assign join[key] = str(value)
    - L281 if missing_keys:
      - L282 expr logger.warning('contract_join_incomplete', extra={'event': 'contract_join_incomplete', 'missing_keys': missing_keys})
    - L291 if join.get('parent_table') or join.get('child_table'):
      - L292 if not join.get('parent_table') or not join.get('parent_key'):
        - L293 expr logger.warning('contract_join_invalid', extra={'event': 'contract_join_invalid', 'reason': 'parent_table and parent_key are required for a valid join'})
  - L301 return normalized
- L304 def _extract_mapping_entriessource: Mapping[str, Any] | None:
  - L305 if not isinstance(source, Mapping):
    - L306 return {}
  - L307 assign mapping_section = source.get('mapping')
  - L308 if isinstance(mapping_section, Mapping):
    - L309 assign source = mapping_section
  - L310 return {str(token): str(expr) for token, expr in source.items()}
- L313 def _reshape_rule_from_step5step5_requirements: Mapping[str, Any] | None:
  - L314 if not isinstance(step5_requirements, Mapping):
    - L315 return None
  - L316 assign reshape = step5_requirements.get('reshape')
  - L317 if not isinstance(reshape, Mapping):
    - L318 return None
  - L319 assign selections = reshape.get('select') or reshape.get('selection') or []
  - L320 annotated assign columns: list[dict[str, Any]] = []
  - L321 for entry in selections:
    - L322 if not isinstance(entry, Mapping):
      - L323 continue
    - L324 assign expr = str(entry.get('expr') or entry.get('expression') or entry.get('column') or '').strip()
    - L325 assign alias = str(entry.get('as') or entry.get('alias') or '').strip() or expr
    - L326 if not alias or not expr:
      - L327 continue
    - L328 expr columns.append({'as': alias, 'from': [expr]})
  - L329 if not columns:
    - L330 return None
  - L331 annotated assign rule: dict[str, Any] = {'purpose': reshape.get('purpose') or 'Auto-generated reshape derived from Step-5 requirements.', 'strategy': reshape.get('strategy') or 'SELECT', 'columns': columns}
  - L336 assign explain = reshape.get('explain') or reshape.get('description')
  - L337 if explain:
    - L338 assign rule['explain'] = explain
  - L339 for key in ('order_by', 'ordering'):
    - L340 assign order_val = reshape.get(key)
    - L341 if order_val:
      - L342 assign rule['order_by'] = order_val
      - L343 break
  - L344 assign filters = reshape.get('where') or reshape.get('filters')
  - L345 if filters:
    - L346 assign rule['filters'] = filters
  - L347 assign group_by = reshape.get('group_by')
  - L348 if group_by:
    - L349 assign rule['group_by'] = group_by
  - L350 return rule
- L353 def _reshape_rule_from_mappingmapping: Mapping[str, str], row_tokens: list[str]:
  - L354 annotated assign columns: list[dict[str, Any]] = []
  - L355 for token in row_tokens:
    - L356 assign expr = str(mapping.get(token) or '').strip()
    - L357 if not expr:
      - L358 continue
    - L359 expr columns.append({'as': token, 'from': [expr]})
  - L360 if not columns:
    - L361 return None
  - L362 return {'purpose': 'Auto-generated reshape rule derived from mapping tokens.', 'strategy': 'SELECT', 'columns': columns}
- L369 def _validate_reshape_columncolumn: Any:
  - L370 docstring: "\n    Validate a single reshape column entry.\n    A valid column must have:\n  ..."
  - L376 if not isinstance(column, Mapping):
    - L377 return False
  - L380 assign alias = column.get('as') or column.get('alias')
  - L381 if not alias or not str(alias).strip():
    - L382 return False
  - L385 assign from_raw = column.get('from')
  - L386 if not from_raw:
    - L387 return False
  - L389 if isinstance(from_raw, str):
    - L390 return bool(from_raw.strip())
    - L391 else:
      - L391 if isinstance(from_raw, list):
        - L392 return any((str(item).strip() for item in from_raw if item))
  - L394 return False
- L397 def _has_valid_reshape_rulerules: Any:
  - L398 docstring: "\n    Check if reshape rules contain at least one valid rule with valid columns...."
  - L401 if not isinstance(rules, list):
    - L402 return False
  - L404 for rule in rules:
    - L405 if not isinstance(rule, Mapping):
      - L406 continue
    - L408 assign columns = rule.get('columns')
    - L409 if not isinstance(columns, list) or not columns:
      - L410 continue
    - L413 assign valid_columns = [col for col in columns if _validate_reshape_column(col)]
    - L414 if valid_columns:
      - L415 return True
  - L417 return False
- L420 def _ensure_contract_defaultscontract: dict[str, Any], *, schema: Mapping[str, Any], auto_mapping: Mapping[str, Any] | None, mapping_override: Mapping[str, Any] | None, step5_requirements: Mapping[str, Any] | None:
  - L428 assign tokens_block_raw = contract.get('tokens')
  - L429 assign tokens_block = tokens_block_raw if isinstance(tokens_block_raw, dict) else {}
  - L430 assign tokens_block = {key: value for key, value in tokens_block.items() if key in {'scalars', 'row_tokens', 'totals'}}
  - L432 assign schema_scalars = list(schema.get('scalars') or [])
  - L433 assign schema_rows = list(schema.get('row_tokens') or [])
  - L434 assign schema_totals = list(schema.get('totals') or [])
  - L436 assign scalars = list(tokens_block.get('scalars') or []) or schema_scalars
  - L437 assign row_tokens = list(tokens_block.get('row_tokens') or []) or schema_rows
  - L438 assign totals = list(tokens_block.get('totals') or []) or schema_totals
  - L440 if not row_tokens:
    - L441 assign auto_map = _extract_mapping_entries(auto_mapping)
    - L442 assign inferred_rows = [token for token in auto_map if token.startswith('row_')]
    - L443 if inferred_rows:
      - L444 assign row_tokens = inferred_rows
  - L446 assign contract['tokens'] = {'scalars': scalars, 'row_tokens': row_tokens, 'totals': totals}
  - L452 assign mapping_sources = {}
  - L453 expr mapping_sources.update(_extract_mapping_entries(auto_mapping))
  - L454 if isinstance(mapping_override, Mapping):
    - L455 expr mapping_sources.update({str(k): str(v) for k, v in mapping_override.items()})
  - L457 assign mapping_section_raw = contract.get('mapping')
  - L458 assign mapping_section = mapping_section_raw if isinstance(mapping_section_raw, dict) else {}
  - L459 assign contract['mapping'] = mapping_section
  - L460 for token in [*scalars, *row_tokens, *totals]:
    - L461 if token and token not in mapping_section and (token in mapping_sources):
      - L462 assign mapping_section[token] = mapping_sources[token]
  - L464 if not _has_valid_reshape_rule(contract.get('reshape_rules')):
    - L465 assign rule = _reshape_rule_from_step5(step5_requirements) or _reshape_rule_from_mapping(mapping_section, row_tokens)
    - L466 if rule:
      - L467 assign contract['reshape_rules'] = [rule]
  - L469 return mapping_sources
- L472 def _normalize_reshape_rulescontract: dict[str, Any]:
  - L473 assign rules = contract.get('reshape_rules')
  - L474 if not isinstance(rules, list):
    - L475 assign contract['reshape_rules'] = []
    - L476 return None
  - L478 annotated assign normalized_rules: list[dict[str, Any]] = []
  - L479 for raw_rule in rules:
    - L480 if not isinstance(raw_rule, Mapping):
      - L481 continue
    - L482 assign rule = dict(raw_rule)
    - L484 assign columns_raw = rule.get('columns')
    - L485 if not isinstance(columns_raw, list) or not columns_raw:
      - L486 continue
    - L488 annotated assign normalized_columns: list[dict[str, Any]] = []
    - L489 for entry in columns_raw:
      - L490 if not isinstance(entry, Mapping):
        - L491 continue
      - L492 assign alias = str(entry.get('as') or entry.get('alias') or '').strip()
      - L493 if not alias:
        - L494 continue
      - L495 assign from_raw = entry.get('from')
      - L496 annotated assign sources: list[str] = []
      - L497 if isinstance(from_raw, str):
        - L498 assign value = from_raw.strip()
        - L499 if value:
          - L500 assign sources = [value]
        - L501 else:
          - L501 if isinstance(from_raw, list):
            - L502 assign sources = [str(item).strip() for item in from_raw if str(item or '').strip()]
      - L503 if not sources:
        - L504 continue
      - L505 expr normalized_columns.append({'as': alias, 'from': sources})
    - L506 if not normalized_columns:
      - L507 continue
    - L508 assign rule['columns'] = normalized_columns
    - L510 assign order_by = rule.get('order_by')
    - L511 if isinstance(order_by, str):
      - L512 assign text = order_by.strip()
      - L513 assign rule['order_by'] = [text] if text else []
      - L514 else:
        - L514 if isinstance(order_by, list):
          - L515 assign rule['order_by'] = [str(item).strip() for item in order_by if str(item or '').strip()]
          - L517 else:
            - L517 assign rule['order_by'] = []
    - L519 expr normalized_rules.append(rule)
    - L520 expr rule.setdefault('purpose', 'Auto-generated reshape rule derived from Step-5 requirements.')
  - L522 assign contract['reshape_rules'] = normalized_rules
- L525 def _normalize_orderingcontract: dict[str, Any]:
  - L526 assign order_block = contract.get('order_by')
  - L527 if isinstance(order_block, Mapping):
    - L528 assign rows_val = order_block.get('rows')
    - L529 if isinstance(rows_val, str):
      - L530 assign rows_list = [rows_val.strip()] if rows_val.strip() else []
      - L531 assign order_block['rows'] = rows_list
      - L532 else:
        - L532 if isinstance(rows_val, list):
          - L533 assign order_block['rows'] = [str(item).strip() for item in rows_val if str(item or '').strip()]
          - L535 else:
            - L535 assign order_block['rows'] = []
    - L536 assign contract['order_by'] = order_block
    - L537 else:
      - L537 if isinstance(order_block, str):
        - L538 assign text = order_block.strip()
        - L539 assign contract['order_by'] = {'rows': [text] if text else []}
        - L541 else:
          - L541 assign contract['order_by'] = {'rows': []}
  - L543 assign row_order_val = contract.get('row_order')
  - L544 if isinstance(row_order_val, str):
    - L545 assign text = row_order_val.strip()
    - L546 assign contract['row_order'] = [text] if text else ['ROWID']
    - L547 else:
      - L547 if isinstance(row_order_val, list) and row_order_val:
        - L548 assign contract['row_order'] = [str(item).strip() for item in row_order_val if str(item or '').strip()]
        - L550 else:
          - L550 assign contract['row_order'] = ['ROWID']
- L553 def _clean_sql_fragmentvalue: Any:
  - L554 assign text = str(value or '').strip()
  - L555 if _LEGACY_WRAPPER_RE.search(text):
    - L556 raise ContractBuilderError('contract mapping contains legacy wrappers (DERIVED/TABLE_COLUMNS/COLUMN_EXP). Supply the executable SQL fragment directly.')
  - L560 return text
- L563 def _normalize_sql_mapping_sectionscontract: dict[str, Any], *, allow_list: Iterable[str], fallback_mapping: Mapping[str, str] | None=None:
  - L569 assign allow_catalog = {str(item).strip() for item in allow_list if str(item).strip()}
  - L570 assign allowed_tables = {entry.split('.')[0] for entry in allow_catalog if '.' in entry}
  - L572 def _validate_exprtoken: str, expr: str:
    - L573 assign cleaned = _clean_sql_fragment(expr)
    - L574 if not cleaned:
      - L575 raise ContractBuilderError(f"contract mapping for '{token}' is empty after normalization.")
    - L576 if _SUBQUERY_RE.search(cleaned):
      - L577 raise ContractBuilderError(f"contract mapping for '{token}' contains disallowed SQL (subqueries or statements).")
    - L580 assign referenced = {f"{match.group('table')}.{match.group('column')}" for match in _COLUMN_REF_RE.finditer(cleaned)}
    - L581 assign invalid = [ref for ref in referenced if ref not in allow_catalog and ref.split('.')[0] in allowed_tables]
    - L582 if invalid:
      - L583 raise ContractBuilderError(f"contract mapping for '{token}' references columns outside catalog: {sorted(invalid)}")
    - L586 return cleaned
  - L588 assign mapping_section = contract.get('mapping')
  - L589 if isinstance(mapping_section, dict):
    - L590 annotated assign normalized: dict[str, str] = {}
    - L591 for (token, expr) in mapping_section.items():
      - L592 assign token_name = str(token)
      - L593 try:
        - L594 assign normalized[token_name] = _validate_expr(token_name, expr)
        - L595 except ContractBuilderError as exc:
          - L596 assign fallback_expr = None
          - L597 if fallback_mapping:
            - L598 assign fallback_expr = fallback_mapping.get(token_name)
          - L599 if fallback_expr:
            - L600 expr logger.warning('contract_mapping_fallback', extra={'event': 'contract_mapping_fallback', 'token': token_name, 'error': str(exc)})
            - L608 assign normalized[token_name] = _validate_expr(token_name, fallback_expr)
            - L610 else:
              - L610 raise
    - L611 assign contract['mapping'] = normalized
  - L613 for section in ('totals', 'row_computed', 'totals_math'):
    - L614 assign block = contract.get(section)
    - L615 if isinstance(block, dict):
      - L616 assign contract[section] = {str(token): _validate_expr(str(token), expr) for token, expr in block.items()}
- L619 def _serialize_contractcontract: dict[str, Any]:
  - L620 docstring: "\n    Return a deep-ish copy safe for persistence (ensures JSON serialisable val..."
  - L623 return json.loads(json.dumps(contract, ensure_ascii=False))
- L626 def build_or_load_contract_v2template_dir: Path, catalog: Iterable[str], final_template_html: str, schema: Mapping[str, Any], auto_mapping_proposal: Mapping[str, Any], mapping_override: Mapping[str, Any] | None, user_instructions: str, dialect_hint: str | None, *, db_signature: str | None=None, key_tokens: Iterable[str] | None=None, prompt_builder=build_llm_call_4_prompt, prompt_version: str=PROMPT_VERSION_4:
  - L641 docstring: "\n    Build (or return cached) contract artifacts using LLM Call 4.\n    "
  - L644 assign template_dir = template_dir.resolve()
  - L645 expr template_dir.mkdir(parents=True, exist_ok=True)
  - L647 assign page_summary = _load_page_summary(template_dir)
  - L648 assign page_summary_sha = hashlib.sha256((page_summary or '').encode('utf-8')).hexdigest()
  - L649 assign schema_payload = _ensure_schema(schema)
  - L650 assign allow_list = [str(item) for item in catalog]
  - L651 assign mapping_override_payload = dict(mapping_override or {})
  - L652 if not mapping_override_payload:
    - L653 assign mapping_override_payload = _load_mapping_override_from_disk(template_dir)
  - L655 assign key_tokens_list = _normalize_key_tokens(key_tokens)
  - L657 assign input_signature = _compute_input_signature(final_template_html=final_template_html, page_summary=page_summary, schema=schema_payload, auto_mapping_proposal=auto_mapping_proposal, mapping_override=mapping_override_payload, user_instructions=user_instructions, catalog=allow_list, dialect_hint=dialect_hint, key_tokens=key_tokens_list)
  - L669 assign cached = _load_cached_payload(template_dir)
  - L670 if cached:
    - L671 assign meta = cached.get('meta') or {}
    - L672 if meta.get('input_signature') == input_signature and (db_signature is None or meta.get('db_signature') == db_signature):
      - L675 expr logger.info('contract_v2_cache_hit', extra={'event': 'contract_v2_cache_hit', 'template_dir': str(template_dir)})
      - L682 assign result = dict(cached)
      - L683 assign result['cached'] = True
      - L684 return result
  - L686 expr logger.info('contract_v2_build_start', extra={'event': 'contract_v2_build_start', 'template_dir': str(template_dir)})
  - L694 assign prompt_payload = prompt_builder(final_template_html=final_template_html, page_summary=page_summary, schema=schema_payload, auto_mapping_proposal=auto_mapping_proposal, mapping_override=mapping_override_payload, user_instructions=user_instructions, catalog=allow_list, dialect_hint=dialect_hint, key_tokens=key_tokens_list)
  - L706 assign system_text = prompt_payload.get('system', '')
  - L707 assign base_messages = prompt_payload.get('messages') or []
  - L708 if not base_messages:
    - L709 raise ContractBuilderError('Prompt builder did not return a user message for LLM Call 4.')
  - L711 assign messages = _prepare_messages(system_text, base_messages)
  - L712 assign client = get_openai_client()
  - L714 assign model_name = os.getenv('OPENAI_MODEL', 'gpt-5')
  - L716 try:
    - L717 assign raw_response = call_chat_completion(client, model=model_name, messages=messages, description=prompt_version)
    - L723 except Exception as exc:
      - L724 raise ContractBuilderError(f'LLM Call 4 request failed: {exc}')
  - L726 assign content = (raw_response.choices[0].message.content or '').strip()
  - L727 try:
    - L728 assign llm_payload = json.loads(content)
    - L729 except json.JSONDecodeError as exc:
      - L730 assign snippet = content[:200]
      - L731 raise ContractBuilderError(f'LLM Call 4 response was not valid JSON (snippet: {snippet!r})')
  - L733 assign overview_md = str(llm_payload.get('overview_md') or '').strip()
  - L734 if not overview_md:
    - L735 assign page_summary = str(llm_payload.get('page_summary') or '').strip()
    - L736 if page_summary:
      - L737 assign overview_md = page_summary
  - L738 if not overview_md:
    - L739 assign overview_md = '## Contract Overview\n\nThe user skipped Step 3.5 corrections or no summary was generated. Add narrative instructions in the Approve dialog to replace this placeholder.'
  - L744 assign step5_requirements = llm_payload.get('step5_requirements') or {}
  - L745 assign contract = _normalize_contract_payload(llm_payload.get('contract'))
  - L746 assign assumptions = list(llm_payload.get('assumptions') or [])
  - L747 assign warnings = list(llm_payload.get('warnings') or [])
  - L748 assign validation = llm_payload.get('validation') or {}
  - L750 expr validation.setdefault('unknown_columns', [])
  - L751 expr validation.setdefault('unknown_tokens', [])
  - L752 expr validation.setdefault('token_coverage', {'scalars_mapped_pct': 0, 'row_tokens_mapped_pct': 0, 'totals_mapped_pct': 0})
  - L761 assign fallback_mapping_sources = _ensure_contract_defaults(contract, schema=schema_payload, auto_mapping=auto_mapping_proposal, mapping_override=mapping_override_payload, step5_requirements=step5_requirements)
  - L768 expr _normalize_reshape_rules(contract)
  - L769 expr _normalize_ordering(contract)
  - L770 assign contract = _augment_contract_for_compat(_serialize_contract(contract))
  - L771 expr _normalize_sql_mapping_sections(contract, allow_list=allow_list, fallback_mapping=fallback_mapping_sources)
  - L777 assign now = int(time.time())
  - L778 assign overview_path = template_dir / _OVERVIEW_FILENAME
  - L779 assign step5_path = template_dir / _STEP5_REQ_FILENAME
  - L780 assign meta_path = template_dir / _META_FILENAME
  - L782 expr write_text_atomic(overview_path, overview_md, encoding='utf-8', step='contract_v2_overview_write')
  - L783 expr write_json_atomic(step5_path, step5_requirements, indent=2, ensure_ascii=False, step='contract_v2_step5_write')
  - L785 assign contract_path = template_dir / _CONTRACT_FILENAME
  - L787 assign meta_payload = {'prompt_version': prompt_version, 'model': model_name, 'input_signature': input_signature, 'db_signature': db_signature, 'page_summary_sha256': page_summary_sha, 'generated_at': now, 'assumptions': assumptions, 'warnings': warnings, 'validation': validation, 'overview_path': _OVERVIEW_FILENAME, 'step5_requirements_path': _STEP5_REQ_FILENAME, 'contract_payload': contract, 'key_tokens': key_tokens_list}
  - L802 expr write_json_atomic(meta_path, meta_payload, indent=2, ensure_ascii=False, step='contract_v2_meta_write')
  - L803 expr write_json_atomic(contract_path, contract, indent=2, ensure_ascii=False, step='contract_v2_contract_write')
  - L805 expr write_artifact_manifest(template_dir, step='contract_build_v2', files={_OVERVIEW_FILENAME: overview_path, _STEP5_REQ_FILENAME: step5_path, _META_FILENAME: meta_path, _CONTRACT_FILENAME: contract_path}, inputs=[f'contract_v2_input_signature={input_signature}', f"dialect_hint={dialect_hint or ''}"], correlation_id=None)
  - L821 expr logger.info('contract_v2_build_complete', extra={'event': 'contract_v2_build_complete', 'template_dir': str(template_dir)})
  - L829 return {'contract': contract, 'overview_md': overview_md, 'step5_requirements': step5_requirements, 'assumptions': assumptions, 'warnings': warnings, 'validation': validation, 'artifacts': {'overview': overview_path, 'step5_requirements': step5_path, 'meta': meta_path}, 'meta': meta_payload, 'cached': False, 'key_tokens': key_tokens_list}
- L847 def load_contract_v2template_dir: Path:
  - L848 docstring: "\n    Load persisted contract v2 artifacts without triggering a rebuild.\n    Re..."
  - L852 assign cached = _load_cached_payload(template_dir.resolve())
  - L853 if cached is None:
    - L854 return None
  - L855 assign cached['cached'] = True
  - L856 return cached
- L859 assign _COLUMN_REF_RE = re.compile('\n    ["`\\[]?\n    (?P<table>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    \\.\n    ["`\\[]?\n    (?P<column>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    ', re.VERBOSE)
- L871 assign _SUBQUERY_RE = re.compile('(?is)\\bSELECT\\b|;', re.IGNORECASE)
- L872 assign _LEGACY_WRAPPER_RE = re.compile('(?i)\\b(DERIVED\\s*:|TABLE_COLUMNS\\s*\\[|COLUMN_EXP\\s*\\[)')

## backend\app\services\dataframes\__init__.py
- L1 docstring: "Shared DataFrame helpers for SQL-lite pipelines."
- L3 from .sqlite_loader import DuckDBDataFrameQuery, SQLiteDataFrameLoader
- L4 from .sqlite_shim import connect, DataFrameConnection, DataFrameCursor, Row
- L5 from .store import DataFrameStore, dataframe_store, get_dataframe_store, ensure_connection_loaded
- L12 assign __all__ = ['SQLiteDataFrameLoader', 'DuckDBDataFrameQuery', 'connect', 'DataFrameConnection', 'DataFrameCursor', 'Row', 'DataFrameStore', 'dataframe_store', 'get_dataframe_store', 'ensure_connection_loaded']

## backend\app\services\dataframes\sqlite_loader.py
- L1 from __future__ import annotations
- L3 import os
- L4 import re
- L5 import sqlite3
- L6 import threading
- L7 from pathlib import Path
- L8 from typing import Any, Mapping, Sequence
- L10 import duckdb
- L11 import pandas as pd
- L14 class SQLiteDataFrameLoader:
  - L15 docstring: "Load SQLite tables into cached pandas DataFrames."
  - L17 def __init__self, db_path: Path:
    - L18 assign self.db_path = Path(db_path)
    - L19 annotated assign self._table_names: list[str] | None = None
    - L20 annotated assign self._frames: dict[str, pd.DataFrame] = {}
    - L21 assign self._lock = threading.Lock()
    - L22 assign self._mtime = os.path.getmtime(self.db_path) if self.db_path.exists() else 0.0
    - L23 annotated assign self._table_info_cache: dict[str, list[dict[str, Any]]] = {}
    - L24 annotated assign self._foreign_keys_cache: dict[str, list[dict[str, Any]]] = {}
  - L26 def table_namesself:
    - L27 docstring: "Return a cached list of user tables in the database."
    - L28 with self._lock:
      - L29 if self._table_names is not None:
        - L30 return list(self._table_names)
      - L32 with sqlite3.connect(str(self.db_path)) as con:
        - L33 assign cur = con.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%' ORDER BY name;")
        - L37 assign tables = [str(row[0]) for row in cur.fetchall() if row and row[0]]
      - L38 assign self._table_names = tables
      - L39 return list(self._table_names)
  - L41 def _assert_tableself, table_name: str:
    - L42 assign clean = str(table_name or '').strip()
    - L43 if not clean:
      - L44 raise ValueError('table_name must be a non-empty string')
    - L45 if clean not in self.table_names():
      - L46 raise RuntimeError(f'Table {clean!r} not found in {self.db_path}')
    - L47 return clean
  - L49 def frameself, table_name: str:
    - L50 docstring: "\n        Return the cached DataFrame for `table_name`, loading it from disk\n  ..."
    - L55 assign clean = self._assert_table(table_name)
    - L56 with self._lock:
      - L57 assign cached = self._frames.get(clean)
      - L58 if cached is not None:
        - L59 return cached
    - L60 assign df = self._read_table(clean)
    - L61 with self._lock:
      - L62 assign self._frames[clean] = df
    - L63 return df
  - L65 def framesself:
    - L66 docstring: "Eagerly load and return all user tables as DataFrames."
    - L67 for name in self.table_names():
      - L68 expr self.frame(name)
    - L69 with self._lock:
      - L70 return dict(self._frames)
  - L72 def _read_tableself, table_name: str:
    - L73 assign quoted = table_name.replace('"', '""')
    - L74 try:
      - L75 with sqlite3.connect(str(self.db_path)) as con:
        - L76 assign df = pd.read_sql_query(f'SELECT rowid AS "__rowid__", * FROM "{quoted}"', con)
      - L77 except Exception as exc:
        - L78 raise RuntimeError(f'Failed loading table {table_name!r} into DataFrame: {exc}')
    - L79 if '__rowid__' in df.columns:
      - L80 assign rowid_series = df['__rowid__'].copy()
      - L81 if 'rowid' not in df.columns:
        - L82 expr df.insert(0, 'rowid', rowid_series)
    - L83 return df
  - L85 def column_typeself, table_name: str, column_name: str:
    - L86 assign table = self.frame(table_name)
    - L87 if column_name not in table.columns:
      - L88 return ''
    - L89 assign series = table[column_name]
    - L90 if pd.api.types.is_datetime64_any_dtype(series):
      - L91 return 'DATETIME'
    - L92 if pd.api.types.is_integer_dtype(series):
      - L93 return 'INTEGER'
    - L94 if pd.api.types.is_float_dtype(series):
      - L95 return 'REAL'
    - L96 if pd.api.types.is_bool_dtype(series):
      - L97 return 'INTEGER'
    - L98 return 'TEXT'
  - L100 def table_infoself, table_name: str:
    - L101 assign table = self.frame(table_name)
    - L102 return [(col, str(table[col].dtype)) for col in table.columns]
  - L104 def _load_table_metadataself, table_name: str:
    - L107 assign clean = self._assert_table(table_name)
    - L108 with self._lock:
      - L109 assign cached_info = self._table_info_cache.get(clean)
      - L110 assign cached_fks = self._foreign_keys_cache.get(clean)
      - L111 if cached_info is not None and cached_fks is not None:
        - L112 return (cached_info, cached_fks)
    - L114 assign quoted = clean.replace("'", "''")
    - L115 annotated assign info_rows: list[dict[str, Any]] = []
    - L116 annotated assign fk_rows: list[dict[str, Any]] = []
    - L117 try:
      - L118 with sqlite3.connect(str(self.db_path)) as con:
        - L119 assign cur = con.execute(f"PRAGMA table_info('{quoted}')")
        - L120 assign info_rows = [{'cid': int(row[0]), 'name': str(row[1]), 'type': str(row[2] or ''), 'notnull': int(row[3] or 0), 'dflt_value': row[4], 'pk': int(row[5] or 0)} for row in cur.fetchall()]
        - L131 assign cur = con.execute(f"PRAGMA foreign_key_list('{quoted}')")
        - L132 assign fk_rows = [{'id': int(row[0]), 'seq': int(row[1]), 'table': str(row[2] or ''), 'from': str(row[3] or ''), 'to': str(row[4] or ''), 'on_update': str(row[5] or ''), 'on_delete': str(row[6] or ''), 'match': str(row[7] or '')} for row in cur.fetchall()]
      - L145 except Exception as exc:
        - L146 raise RuntimeError(f'Failed loading metadata for table {clean!r}: {exc}')
    - L148 with self._lock:
      - L149 assign self._table_info_cache[clean] = info_rows
      - L150 assign self._foreign_keys_cache[clean] = fk_rows
    - L151 return (info_rows, fk_rows)
  - L153 def pragma_table_infoself, table_name: str:
    - L154 assign (info, _) = self._load_table_metadata(table_name)
    - L155 return list(info)
  - L157 def foreign_keysself, table_name: str:
    - L158 assign (_, fks) = self._load_table_metadata(table_name)
    - L159 return list(fks)
- L162 annotated assign _LOADER_CACHE: dict[str, SQLiteDataFrameLoader] = {}
- L163 assign _LOADER_CACHE_LOCK = threading.Lock()
- L166 def get_loaderdb_path: Path:
  - L167 assign key = str(Path(db_path).resolve())
  - L168 with _LOADER_CACHE_LOCK:
    - L169 assign loader = _LOADER_CACHE.get(key)
    - L170 assign mtime = os.path.getmtime(key) if os.path.exists(key) else 0.0
    - L171 if loader is None or loader._mtime != mtime:
      - L172 assign loader = SQLiteDataFrameLoader(Path(key))
      - L173 assign loader._mtime = mtime
      - L174 assign _LOADER_CACHE[key] = loader
  - L175 return loader
- L178 assign _PARAM_PATTERN = re.compile(':([A-Za-z_][A-Za-z0-9_]*)')
- L179 assign _SQLITE_DATETIME_RE = re.compile('(?i)(?<!sqlite_)\\bdatetime\\s*\\(')
- L180 assign _SQLITE_STRFTIME_RE = re.compile('(?i)(?<!sqlite_)\\bstrftime\\s*\\(')
- L183 def _normalize_paramssql: str, params: Any | None:
  - L184 if params is None:
    - L185 return (sql, ())
  - L186 if isinstance(params, Mapping):
    - L187 annotated assign ordered: list[Any] = []
    - L189 def _replmatch: re.Match[str]:
      - L190 assign name = match.group(1)
      - L191 if name not in params:
        - L192 raise KeyError(f'Missing SQL parameter: {name}')
      - L193 expr ordered.append(params[name])
      - L194 return '?'
    - L196 assign prepared = _PARAM_PATTERN.sub(_repl, sql)
    - L197 return (prepared, tuple(ordered))
  - L198 if isinstance(params, (list, tuple)):
    - L199 return (sql, tuple(params))
  - L200 return (sql, (params,))
- L203 def _rewrite_sqlsql: str:
  - L204 docstring: "Apply lightweight rewrites so legacy SQLite SQL runs in DuckDB."
  - L205 assign updated = _SQLITE_DATETIME_RE.sub('sqlite_datetime(', sql)
  - L206 assign updated = _SQLITE_STRFTIME_RE.sub('sqlite_strftime(', updated)
  - L207 return updated
- L210 class DuckDBDataFrameQuery:
  - L211 docstring: "\n    Execute SQL statements against in-memory pandas DataFrames by delegating\n..."
  - L217 def __init__self, frames: Mapping[str, pd.DataFrame]:
    - L218 assign self._conn = duckdb.connect(database=':memory:')
    - L219 expr self._register_frames(frames)
    - L220 expr self._register_sqlite_macros()
  - L222 def _register_framesself, frames: Mapping[str, pd.DataFrame]:
    - L223 for (name, frame) in frames.items():
      - L224 if frame is None:
        - L225 continue
      - L226 expr self._conn.register(name, frame)
  - L228 def _register_sqlite_macrosself:
    - L229 docstring: "Install lightweight SQLite compatibility macros for DuckDB execution."
    - L230 expr self._conn.execute("\n            CREATE MACRO IF NOT EXISTS sqlite_datetime(x) AS (\n                CASE\n                    WHEN x IS NULL THEN NULL\n                    WHEN LOWER(CAST(x AS VARCHAR)) = 'now' THEN CURRENT_TIMESTAMP\n                    WHEN TRY_CAST(x AS DOUBLE) IS NOT NULL THEN TO_TIMESTAMP(CAST(x AS DOUBLE))\n                    ELSE TRY_CAST(x AS TIMESTAMP)\n                END\n            )\n            ")
    - L242 expr self._conn.execute('\n            CREATE MACRO IF NOT EXISTS sqlite_strftime(fmt, value, modifier := NULL) AS (\n                CASE\n                    WHEN value IS NULL THEN NULL\n                    ELSE STRFTIME(sqlite_datetime(value), fmt)\n                END\n            )\n            ')
    - L253 expr self._conn.execute('CREATE MACRO IF NOT EXISTS datetime(x) AS sqlite_datetime(x)')
    - L254 expr self._conn.execute('CREATE MACRO IF NOT EXISTS strftime(fmt, value, modifier := NULL) AS sqlite_strftime(fmt, value, modifier)')
  - L258 def executeself, sql: str, params: Any | None=None:
    - L259 assign (prepared_sql, ordered_params) = _normalize_params(sql, params)
    - L260 assign rewritten_sql = _rewrite_sql(prepared_sql)
    - L261 try:
      - L262 assign result = self._conn.execute(rewritten_sql, ordered_params)
      - L263 assign df = result.fetchdf()
      - L264 except duckdb.Error as exc:
        - L265 raise RuntimeError(f'DuckDB execution failed: {exc}')
    - L266 return df
  - L268 def closeself:
    - L269 expr self._conn.close()

## backend\app\services\dataframes\sqlite_shim.py
- L1 from __future__ import annotations
- L3 import re
- L4 from pathlib import Path
- L5 from typing import Any, Sequence
- L7 import pandas as pd
- L9 from .sqlite_loader import DuckDBDataFrameQuery, get_loader
- L12 class Error(Exception):
  - L13 docstring: "Base error matching sqlite3.Error."
- L16 class OperationalError(Error):
  - L17 docstring: "Raised when SQL execution fails."
- L20 class Row:
  - L21 docstring: "Lightweight sqlite3.Row replacement supporting dict + index access."
  - L23 assign __slots__ = ('_columns', '_values', '_mapping')
  - L25 def __init__self, columns: Sequence[str], values: Sequence[Any]:
    - L26 assign self._columns = list(columns)
    - L27 assign self._values = tuple(values)
    - L28 assign self._mapping = {col: value for col, value in zip(self._columns, self._values)}
  - L30 def __getitem__self, key: int | str:
    - L31 if isinstance(key, int):
      - L32 return self._values[key]
    - L33 return self._mapping[key]
  - L35 def keysself:
    - L36 return list(self._columns)
  - L38 def __iter__self:
    - L39 return iter(self._values)
  - L41 def __len__self:
    - L42 return len(self._values)
  - L44 def __repr__self:
    - L45 assign items = ', '.join((f'{col}={self._mapping[col]!r}' for col in self._columns))
    - L46 return f'Row({items})'
- L49 def _apply_row_factorycolumns: list[str], values: Sequence[Any], factory: Any | None:
  - L50 if factory is None:
    - L51 return tuple(values)
  - L52 if factory is Row:
    - L53 return Row(columns, values)
  - L54 return factory(columns, values)
- L57 class DataFrameCursor:
  - L58 def __init__self, connection: 'DataFrameConnection':
    - L59 assign self.connection = connection
    - L60 annotated assign self._df: pd.DataFrame | None = None
    - L61 annotated assign self._columns: list[str] = []
    - L62 assign self._pos = 0
  - L64 def executeself, sql: str, params: Any | None=None:
    - L65 assign meta_df = self._try_meta_query(sql)
    - L66 if meta_df is not None:
      - L67 assign self._df = meta_df
      - L68 assign self._columns = list(meta_df.columns)
      - L69 assign self._pos = 0
      - L70 return self
    - L71 try:
      - L72 assign df = self.connection._query.execute(sql, params)
      - L73 except Exception as exc:
        - L74 raise OperationalError(str(exc))
    - L75 assign self._df = df
    - L76 assign self._columns = list(df.columns)
    - L77 assign self._pos = 0
    - L78 return self
  - L80 def _try_meta_queryself, sql: str:
    - L81 assign sql_clean = (sql or '').strip()
    - L82 assign pragma_match = re.match('(?is)^PRAGMA\\s+table_info\\([\'\\"]?(?P<table>[^\'\\")]+)[\'\\"]?\\)\\s*;?$', sql_clean)
    - L83 if pragma_match:
      - L84 assign table_name = pragma_match.group('table')
      - L85 try:
        - L86 assign rows = self.connection._loader.pragma_table_info(table_name)
        - L87 except Exception:
          - L88 assign rows = []
      - L89 assign data = [(int(row.get('cid', 0)), str(row.get('name', '')), str(row.get('type', '')), int(row.get('notnull', 0)), row.get('dflt_value'), int(row.get('pk', 0))) for row in rows]
      - L100 return pd.DataFrame(data, columns=['cid', 'name', 'type', 'notnull', 'dflt_value', 'pk'])
    - L102 assign fk_match = re.match('(?is)^PRAGMA\\s+foreign_key_list\\([\'\\"]?(?P<table>[^\'\\")]+)[\'\\"]?\\)\\s*;?$', sql_clean)
    - L103 if fk_match:
      - L104 assign table_name = fk_match.group('table')
      - L105 try:
        - L106 assign rows = self.connection._loader.foreign_keys(table_name)
        - L107 except Exception:
          - L108 assign rows = []
      - L109 assign data = [(int(row.get('id', 0)), int(row.get('seq', 0)), str(row.get('table', '')), str(row.get('from', '')), str(row.get('to', '')), str(row.get('on_update', '')), str(row.get('on_delete', '')), str(row.get('match', ''))) for row in rows]
      - L122 return pd.DataFrame(data, columns=['id', 'seq', 'table', 'from', 'to', 'on_update', 'on_delete', 'match'])
    - L126 if 'sqlite_master' in sql_clean.lower():
      - L127 assign names = [name for name in self.connection._loader.table_names() if not name.lower().startswith('sqlite_')]
      - L128 assign data = [(name,) for name in sorted(names)]
      - L129 return pd.DataFrame(data, columns=['name'])
    - L130 return None
  - L132 def fetchoneself:
    - L133 if self._df is None:
      - L134 return None
    - L135 if self._pos >= len(self._df):
      - L136 return None
    - L137 assign row = self._df.iloc[self._pos].tolist()
    - L138 aug assign self._pos Add 1
    - L139 return _apply_row_factory(self._columns, row, self.connection.row_factory)
  - L141 def fetchallself:
    - L142 assign rows = []
    - L143 while True:
      - L144 assign row = self.fetchone()
      - L145 if row is None:
        - L146 break
      - L147 expr rows.append(row)
    - L148 return rows
  - L150 def __iter__self:
    - L151 while True:
      - L152 assign row = self.fetchone()
      - L153 if row is None:
        - L154 return None
      - L155 expr (yield row)
- L158 class DataFrameConnection:
  - L159 def __init__self, db_path: Path:
    - L160 assign self.db_path = Path(db_path)
    - L161 assign self._loader = get_loader(self.db_path)
    - L162 assign self._query = DuckDBDataFrameQuery(self._loader.frames())
    - L163 annotated assign self.row_factory: Any | None = None
  - L165 def cursorself:
    - L166 return DataFrameCursor(self)
  - L168 def executeself, sql: str, params: Any | None=None:
    - L169 return self.cursor().execute(sql, params)
  - L171 def closeself:
    - L172 expr self._query.close()
  - L174 def commitself:
    - L175 return None
  - L177 def rollbackself:
    - L178 return None
  - L180 def __enter__self:
    - L181 return self
  - L183 def __exit__self, exc_type, exc, tb:
    - L184 expr self.close()
- L187 def connectdb_path: str | Path, **_kwargs:
  - L188 docstring: "sqlite3.connect-compatible entrypoint backed by pandas DataFrames."
  - L189 return DataFrameConnection(Path(db_path))

## backend\app\services\dataframes\store.py
- L1 docstring: "\nCentralized DataFrame Store for connection-based DataFrame management.\n\nThis..."
- L11 from __future__ import annotations
- L13 import logging
- L14 import os
- L15 import threading
- L16 from pathlib import Path
- L17 from typing import Any, Dict, Optional
- L19 import pandas as pd
- L21 from .sqlite_loader import DuckDBDataFrameQuery, SQLiteDataFrameLoader, get_loader
- L23 assign logger = logging.getLogger('neura.dataframes.store')
- L26 class DataFrameStore:
  - L27 docstring: "\n    Centralized store for managing DataFrames by connection.\n\n    All databa..."
  - L36 annotated assign _instance: Optional['DataFrameStore'] = None
  - L37 assign _lock = threading.Lock()
  - L39 def __new__cls:
    - L40 if cls._instance is None:
      - L41 with cls._lock:
        - L42 if cls._instance is None:
          - L43 assign cls._instance = super().__new__(cls)
          - L44 assign cls._instance._initialized = False
    - L45 return cls._instance
  - L47 def __init__self:
    - L48 if self._initialized:
      - L49 return None
    - L50 annotated assign self._loaders: Dict[str, SQLiteDataFrameLoader] = {}
    - L51 annotated assign self._frames_cache: Dict[str, Dict[str, pd.DataFrame]] = {}
    - L52 annotated assign self._db_paths: Dict[str, Path] = {}
    - L53 annotated assign self._query_engines: Dict[str, DuckDBDataFrameQuery] = {}
    - L54 assign self._store_lock = threading.Lock()
    - L55 assign self._initialized = True
    - L56 expr logger.info('DataFrameStore initialized')
  - L58 def register_connectionself, connection_id: str, db_path: Path:
    - L59 docstring: "\n        Register a database connection and load all tables as DataFrames.\n\n ..."
    - L65 assign db_path = Path(db_path).resolve()
    - L67 with self._store_lock:
      - L69 assign existing_path = self._db_paths.get(connection_id)
      - L70 if existing_path and existing_path == db_path:
        - L72 assign loader = self._loaders.get(connection_id)
        - L73 if loader:
          - L74 assign current_mtime = os.path.getmtime(db_path) if db_path.exists() else 0.0
          - L75 if loader._mtime == current_mtime:
            - L76 expr logger.debug(f'Connection {connection_id} already registered and up to date')
            - L77 return None
      - L80 expr logger.info(f'Loading DataFrames for connection {connection_id} from {db_path}')
      - L81 assign loader = get_loader(db_path)
      - L82 assign frames = loader.frames()
      - L85 assign existing_engine = self._query_engines.get(connection_id)
      - L86 if existing_engine:
        - L87 try:
          - L88 expr existing_engine.close()
          - L89 except Exception:
            - L90 pass
      - L93 assign self._loaders[connection_id] = loader
      - L94 assign self._frames_cache[connection_id] = frames
      - L95 assign self._db_paths[connection_id] = db_path
      - L96 assign self._query_engines[connection_id] = DuckDBDataFrameQuery(frames)
      - L98 expr logger.info(f'Loaded {len(frames)} tables for connection {connection_id}: {list(frames.keys())}')
  - L100 def get_loaderself, connection_id: str:
    - L101 docstring: "Get the loader for a connection."
    - L102 with self._store_lock:
      - L103 return self._loaders.get(connection_id)
  - L105 def get_framesself, connection_id: str:
    - L106 docstring: "Get all DataFrames for a connection."
    - L107 with self._store_lock:
      - L108 return self._frames_cache.get(connection_id, {})
  - L110 def get_frameself, connection_id: str, table_name: str:
    - L111 docstring: "Get a specific DataFrame for a connection."
    - L112 assign frames = self.get_frames(connection_id)
    - L113 return frames.get(table_name)
  - L115 def get_query_engineself, connection_id: str:
    - L116 docstring: "Get the DuckDB query engine for a connection."
    - L117 with self._store_lock:
      - L118 return self._query_engines.get(connection_id)
  - L120 def execute_queryself, connection_id: str, sql: str, params: Any=None:
    - L126 docstring: "\n        Execute a SQL query against the DataFrames for a connection.\n\n      ..."
    - L131 assign engine = self.get_query_engine(connection_id)
    - L132 if engine is None:
      - L133 raise ValueError(f'Connection {connection_id} not registered in DataFrameStore')
    - L134 return engine.execute(sql, params)
  - L136 def execute_query_to_dictsself, connection_id: str, sql: str, params: Any=None:
    - L142 docstring: "\n        Execute a SQL query and return results as list of dicts.\n\n        Th..."
    - L147 assign df = self.execute_query(connection_id, sql, params)
    - L148 return df.to_dict('records')
  - L150 def get_table_namesself, connection_id: str:
    - L151 docstring: "Get list of table names for a connection."
    - L152 assign loader = self.get_loader(connection_id)
    - L153 if loader is None:
      - L154 return []
    - L155 return loader.table_names()
  - L157 def get_table_infoself, connection_id: str, table_name: str:
    - L158 docstring: "Get PRAGMA table_info equivalent for a table."
    - L159 assign loader = self.get_loader(connection_id)
    - L160 if loader is None:
      - L161 return []
    - L162 return loader.pragma_table_info(table_name)
  - L164 def get_foreign_keysself, connection_id: str, table_name: str:
    - L165 docstring: "Get foreign keys for a table."
    - L166 assign loader = self.get_loader(connection_id)
    - L167 if loader is None:
      - L168 return []
    - L169 return loader.foreign_keys(table_name)
  - L171 def invalidate_connectionself, connection_id: str:
    - L172 docstring: "\n        Invalidate cached DataFrames for a connection.\n\n        Call this wh..."
    - L177 with self._store_lock:
      - L178 assign engine = self._query_engines.pop(connection_id, None)
      - L179 if engine:
        - L180 try:
          - L181 expr engine.close()
          - L182 except Exception:
            - L183 pass
      - L184 expr self._loaders.pop(connection_id, None)
      - L185 expr self._frames_cache.pop(connection_id, None)
      - L186 expr self._db_paths.pop(connection_id, None)
      - L187 expr logger.info(f'Invalidated DataFrames for connection {connection_id}')
  - L189 def is_registeredself, connection_id: str:
    - L190 docstring: "Check if a connection is registered."
    - L191 with self._store_lock:
      - L192 return connection_id in self._loaders
  - L194 def get_db_pathself, connection_id: str:
    - L195 docstring: "Get the database path for a connection."
    - L196 with self._store_lock:
      - L197 return self._db_paths.get(connection_id)
  - L199 def statusself:
    - L200 docstring: "Get store status."
    - L201 with self._store_lock:
      - L202 return {'connections': list(self._loaders.keys()), 'total_connections': len(self._loaders), 'tables_per_connection': {conn_id: len(frames) for conn_id, frames in self._frames_cache.items()}}
  - L211 def clearself:
    - L212 docstring: "Clear all cached DataFrames."
    - L213 with self._store_lock:
      - L214 for engine in self._query_engines.values():
        - L215 try:
          - L216 expr engine.close()
          - L217 except Exception:
            - L218 pass
      - L219 expr self._loaders.clear()
      - L220 expr self._frames_cache.clear()
      - L221 expr self._db_paths.clear()
      - L222 expr self._query_engines.clear()
      - L223 expr logger.info('DataFrameStore cleared')
- L227 assign dataframe_store = DataFrameStore()
- L230 def get_dataframe_store:
  - L231 docstring: "Get the singleton DataFrameStore instance."
  - L232 return dataframe_store
- L235 def ensure_connection_loadedconnection_id: str, db_path: Path:
  - L236 docstring: "\n    Ensure a connection's DataFrames are loaded in the store.\n\n    Convenien..."
  - L241 assign store = get_dataframe_store()
  - L242 if not store.is_registered(connection_id):
    - L243 expr store.register_connection(connection_id, db_path)
  - L244 return store

## backend\app\services\excel\__init__.py
- L1 docstring: "Excel pipeline helpers."
- L3 from .ExcelVerify import xlsx_to_html_preview

## backend\app\services\excel\ExcelVerify.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\extraction\__init__.py
- L2 docstring: "\nAdvanced Document Extraction Module.\n\nIntegrates multiple extraction tools:\..."
- L18 from .pdf_extractors import PDFExtractor, TabulaExtractor, CamelotExtractor, PyMuPDFExtractor, PDFPlumberExtractor, ExtractedTable, ExtractionResult, extract_pdf_tables, extract_with_best_method, compare_extractors, get_available_extractors
- L31 from .excel_extractors import ExcelExtractor, ExcelSheet, ExcelExtractionResult, extract_excel_data
- L38 assign __all__ = ['PDFExtractor', 'TabulaExtractor', 'CamelotExtractor', 'PyMuPDFExtractor', 'PDFPlumberExtractor', 'ExtractedTable', 'ExtractionResult', 'extract_pdf_tables', 'extract_with_best_method', 'compare_extractors', 'get_available_extractors', 'ExcelExtractor', 'ExcelSheet', 'ExcelExtractionResult', 'extract_excel_data']

## backend\app\services\extraction\excel_extractors.py
- L2 docstring: "\nExcel/Spreadsheet Extraction Module.\n\nSupports:\n- openpyxl (.xlsx, .xlsm)\n..."
- L22 from __future__ import annotations
- L24 import csv
- L25 import io
- L26 import logging
- L27 import re
- L28 import time
- L29 from dataclasses import dataclass, field
- L30 from datetime import datetime
- L31 from enum import Enum
- L32 from pathlib import Path
- L33 from typing import Any, Callable, Dict, Generator, Iterator, List, Optional, Tuple, Union
- L35 assign logger = logging.getLogger('neura.extraction.excel')
- L42 class DataType(Enum):
  - L43 docstring: "Detected data types for columns."
  - L44 assign STRING = 'string'
  - L45 assign INTEGER = 'integer'
  - L46 assign FLOAT = 'float'
  - L47 assign DATE = 'date'
  - L48 assign DATETIME = 'datetime'
  - L49 assign BOOLEAN = 'boolean'
  - L50 assign CURRENCY = 'currency'
  - L51 assign PERCENTAGE = 'percentage'
  - L52 assign EMPTY = 'empty'
  - L53 assign MIXED = 'mixed'
- L57 class ColumnStats:
  - L58 docstring: "Statistics for a column."
  - L59 annotated assign name: str
  - L60 annotated assign data_type: DataType
  - L61 annotated assign non_empty_count: int
  - L62 annotated assign empty_count: int
  - L63 annotated assign unique_count: int
  - L64 annotated assign min_value: Optional[Any] = None
  - L65 annotated assign max_value: Optional[Any] = None
  - L66 annotated assign sample_values: List[Any] = field(default_factory=list)
  - L69 def fill_rateself:
    - L70 docstring: "Percentage of non-empty values."
    - L71 assign total = self.non_empty_count + self.empty_count
    - L72 return self.non_empty_count / total if total > 0 else 0.0
- L76 class ExtractionConfig:
  - L77 docstring: "Configuration for Excel extraction."
  - L78 annotated assign max_rows: int = 50000
  - L79 annotated assign max_sheets: int = 50
  - L80 annotated assign max_columns: int = 500
  - L81 annotated assign detect_headers: bool = True
  - L82 annotated assign infer_types: bool = True
  - L83 annotated assign compute_stats: bool = True
  - L84 annotated assign chunk_size: int = 1000
  - L85 annotated assign encodings_to_try: Tuple[str, ...] = ('utf-8', 'latin-1', 'cp1252', 'iso-8859-1')
  - L86 annotated assign date_formats: Tuple[str, ...] = ('%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%m/%d', '%d-%m-%Y', '%m-%d-%Y')
- L96 assign DEFAULT_CONFIG = ExtractionConfig()
- L100 class ExcelSheet:
  - L101 docstring: "Data from a single Excel sheet."
  - L102 annotated assign name: str
  - L103 annotated assign headers: List[str]
  - L104 annotated assign rows: List[List[Any]]
  - L105 annotated assign row_count: int
  - L106 annotated assign column_count: int
  - L107 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L108 annotated assign column_stats: List[ColumnStats] = field(default_factory=list)
  - L109 annotated assign header_confidence: float = 1.0
  - L111 def __post_init__self:
    - L112 docstring: "Validate sheet data."
    - L114 assign self.headers = [str(h) if h is not None else '' for h in self.headers]
  - L116 def get_columnself, col_name: str:
    - L117 docstring: "Get all values from a column by header name."
    - L118 try:
      - L119 assign idx = self.headers.index(col_name)
      - L120 return [row[idx] if idx < len(row) else None for row in self.rows]
      - L121 except ValueError:
        - L122 return []
  - L124 def get_column_by_indexself, idx: int:
    - L125 docstring: "Get all values from a column by index."
    - L126 if idx < 0 or idx >= self.column_count:
      - L127 return []
    - L128 return [row[idx] if idx < len(row) else None for row in self.rows]
  - L130 def to_dictself:
    - L131 docstring: "Convert to dictionary."
    - L132 return {'name': self.name, 'headers': self.headers, 'rows': self.rows, 'row_count': self.row_count, 'column_count': self.column_count, 'metadata': self.metadata, 'header_confidence': self.header_confidence, 'column_stats': [{'name': s.name, 'data_type': s.data_type.value, 'non_empty_count': s.non_empty_count, 'fill_rate': s.fill_rate} for s in self.column_stats] if self.column_stats else []}
  - L151 def iter_rowsself:
    - L152 docstring: "Iterate over rows as dictionaries."
    - L153 for row in self.rows:
      - L154 expr (yield {self.headers[i]: row[i] if i < len(row) else None for i in range(len(self.headers))})
- L161 class ExcelExtractionResult:
  - L162 docstring: "Result of Excel extraction."
  - L163 annotated assign sheets: List[ExcelSheet]
  - L164 annotated assign filename: str
  - L165 annotated assign format: str
  - L166 annotated assign errors: List[str] = field(default_factory=list)
  - L167 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L168 annotated assign extraction_time_ms: float = 0.0
  - L171 def total_rowsself:
    - L172 docstring: "Total rows across all sheets."
    - L173 return sum((s.row_count for s in self.sheets))
  - L176 def has_dataself:
    - L177 docstring: "Check if any data was extracted."
    - L178 return any((s.row_count > 0 for s in self.sheets))
  - L180 def get_sheetself, name: str:
    - L181 docstring: "Get sheet by name."
    - L182 for sheet in self.sheets:
      - L183 if sheet.name == name:
        - L184 return sheet
    - L185 return None
  - L187 def to_dictself:
    - L188 docstring: "Convert to dictionary."
    - L189 return {'sheets': [s.to_dict() for s in self.sheets], 'filename': self.filename, 'format': self.format, 'errors': self.errors, 'metadata': self.metadata, 'extraction_time_ms': self.extraction_time_ms, 'total_rows': self.total_rows, 'has_data': self.has_data}
- L205 def _detect_data_typevalue: Any:
  - L206 docstring: "Detect the data type of a value."
  - L207 if value is None or (isinstance(value, str) and (not value.strip())):
    - L208 return DataType.EMPTY
  - L210 if isinstance(value, bool):
    - L211 return DataType.BOOLEAN
  - L213 if isinstance(value, int):
    - L214 return DataType.INTEGER
  - L216 if isinstance(value, float):
    - L217 return DataType.FLOAT
  - L219 if isinstance(value, datetime):
    - L220 return DataType.DATETIME
  - L222 if not isinstance(value, str):
    - L223 return DataType.STRING
  - L225 assign text = value.strip()
  - L228 if text.lower() in ('true', 'false', 'yes', 'no', '1', '0'):
    - L229 return DataType.BOOLEAN
  - L232 if re.match('^[$\u20ac\xa3\xa5\u20b9]?\\s*-?\\d{1,3}(,\\d{3})*(\\.\\d{2})?$', text):
    - L233 return DataType.CURRENCY
  - L236 if re.match('^-?\\d+(\\.\\d+)?%$', text):
    - L237 return DataType.PERCENTAGE
  - L240 if re.match('^-?\\d+$', text):
    - L241 return DataType.INTEGER
  - L244 if re.match('^-?\\d+\\.\\d+$', text):
    - L245 return DataType.FLOAT
  - L248 assign date_patterns = ['^\\d{4}-\\d{2}-\\d{2}$', '^\\d{2}/\\d{2}/\\d{4}$', '^\\d{2}-\\d{2}-\\d{4}$']
  - L253 for pattern in date_patterns:
    - L254 if re.match(pattern, text):
      - L255 return DataType.DATE
  - L258 if re.match('^\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}', text):
    - L259 return DataType.DATETIME
  - L261 return DataType.STRING
- L264 def _infer_column_typevalues: List[Any]:
  - L265 docstring: "Infer the predominant data type for a column."
  - L266 annotated assign type_counts: Dict[DataType, int] = {}
  - L268 for value in values:
    - L269 assign dtype = _detect_data_type(value)
    - L270 if dtype != DataType.EMPTY:
      - L271 assign type_counts[dtype] = type_counts.get(dtype, 0) + 1
  - L273 if not type_counts:
    - L274 return DataType.EMPTY
  - L277 assign most_common = max(type_counts.items(), key=lambda x: x[1])
  - L278 assign total_non_empty = sum(type_counts.values())
  - L281 if most_common[1] / total_non_empty >= 0.8:
    - L282 return most_common[0]
  - L284 return DataType.MIXED
- L287 def _compute_column_statsheader: str, values: List[Any], config: ExtractionConfig:
  - L292 docstring: "Compute statistics for a column."
  - L293 assign non_empty = [v for v in values if v is not None and (not isinstance(v, str) or v.strip())]
  - L294 assign empty_count = len(values) - len(non_empty)
  - L297 assign data_type = _infer_column_type(values) if config.infer_types else DataType.STRING
  - L300 assign sample_for_unique = non_empty[:1000] if len(non_empty) > 1000 else non_empty
  - L301 try:
    - L302 assign unique_count = len(set((str(v) for v in sample_for_unique)))
    - L303 except Exception as e:
      - L304 expr logger.debug(f"Could not compute unique count for column '{header}': {e}")
      - L305 assign unique_count = 0
  - L308 assign min_val = None
  - L309 assign max_val = None
  - L310 if data_type in (DataType.INTEGER, DataType.FLOAT, DataType.CURRENCY):
    - L311 assign numeric_vals = []
    - L312 for v in non_empty:
      - L313 try:
        - L314 if isinstance(v, (int, float)):
          - L315 expr numeric_vals.append(v)
          - L316 else:
            - L316 if isinstance(v, str):
              - L317 assign cleaned = v.replace('$', '').replace(',', '').replace('%', '').strip()
              - L318 expr numeric_vals.append(float(cleaned))
        - L319 except (ValueError, TypeError):
          - L321 pass
    - L322 if numeric_vals:
      - L323 assign min_val = min(numeric_vals)
      - L324 assign max_val = max(numeric_vals)
  - L327 assign sample_values = non_empty[:5] if non_empty else []
  - L329 return ColumnStats(name=header, data_type=data_type, non_empty_count=len(non_empty), empty_count=empty_count, unique_count=unique_count, min_value=min_val, max_value=max_val, sample_values=sample_values)
- L345 def _calculate_header_confidencerow: List[Any], data_rows: List[List[Any]]:
  - L346 docstring: "\n    Calculate confidence that a row is a header row.\n\n    Uses multiple heur..."
  - L354 if not row:
    - L355 return 0.0
  - L357 assign confidence = 0.5
  - L360 assign text_count = sum((1 for cell in row if isinstance(cell, str) and cell.strip()))
  - L361 assign num_count = sum((1 for cell in row if isinstance(cell, (int, float))))
  - L362 assign non_empty = text_count + num_count
  - L364 if non_empty == 0:
    - L365 return 0.0
  - L367 assign text_ratio = text_count / non_empty
  - L368 if text_ratio > 0.8:
    - L369 aug assign confidence Add 0.2
    - L370 else:
      - L370 if text_ratio < 0.3:
        - L371 aug assign confidence Sub 0.2
  - L374 if data_rows:
    - L375 assign data_text_ratios = []
    - L376 for data_row in data_rows[:10]:
      - L377 assign data_text = sum((1 for cell in data_row if isinstance(cell, str) and cell.strip()))
      - L378 assign data_num = sum((1 for cell in data_row if isinstance(cell, (int, float))))
      - L379 assign data_total = data_text + data_num
      - L380 if data_total > 0:
        - L381 expr data_text_ratios.append(data_text / data_total)
    - L383 if data_text_ratios:
      - L384 assign avg_data_text_ratio = sum(data_text_ratios) / len(data_text_ratios)
      - L385 if text_ratio > avg_data_text_ratio + 0.3:
        - L386 aug assign confidence Add 0.2
  - L389 assign header_keywords = {'id', 'name', 'date', 'time', 'amount', 'total', 'qty', 'quantity', 'price', 'description', 'status', 'type', 'category', 'email', 'phone', 'address', 'city', 'country', 'code', 'number', '#', 'no', 'count'}
  - L394 assign keyword_matches = sum((1 for cell in row if isinstance(cell, str) and cell.strip().lower() in header_keywords))
  - L398 if keyword_matches > 0:
    - L399 aug assign confidence Add min(0.2, keyword_matches * 0.05)
  - L402 assign title_or_caps = sum((1 for cell in row if isinstance(cell, str) and (cell.istitle() or cell.isupper())))
  - L406 if title_or_caps > len(row) / 2:
    - L407 aug assign confidence Add 0.1
  - L409 return min(1.0, max(0.0, confidence))
- L412 class ExcelExtractor:
  - L413 docstring: "\n    Excel/spreadsheet data extractor.\n\n    Supports multiple formats and han..."
  - L425 def __init__self, config: Optional[ExtractionConfig]=None, max_rows: int=10000, max_sheets: int=20, detect_headers: bool=True:
    - L432 assign self.config = config or ExtractionConfig(max_rows=max_rows, max_sheets=max_sheets, detect_headers=detect_headers)
  - L438 def extractself, file_path: Union[str, Path], sheet_names: Optional[List[str]]=None:
    - L443 docstring: "\n        Extract data from an Excel file.\n\n        Args:\n            file_pa..."
    - L453 assign start_time = time.time()
    - L454 assign file_path = Path(file_path)
    - L455 assign suffix = file_path.suffix.lower()
    - L457 annotated assign result: ExcelExtractionResult
    - L459 if suffix == '.csv':
      - L460 assign result = self._extract_csv(file_path)
      - L461 else:
        - L461 if suffix == '.tsv':
          - L462 assign result = self._extract_csv(file_path, delimiter='\t')
          - L463 else:
            - L463 if suffix in ('.xlsx', '.xlsm'):
              - L464 assign result = self._extract_xlsx(file_path, sheet_names)
              - L465 else:
                - L465 if suffix == '.xls':
                  - L466 assign result = self._extract_xls(file_path, sheet_names)
                  - L467 else:
                    - L467 if suffix == '.ods':
                      - L468 assign result = self._extract_ods(file_path, sheet_names)
                      - L470 else:
                        - L470 assign result = ExcelExtractionResult(sheets=[], filename=file_path.name, format='unknown', errors=[f'Unsupported format: {suffix}'])
    - L477 assign result.extraction_time_ms = (time.time() - start_time) * 1000
    - L478 return result
  - L480 def _extract_odsself, file_path: Path, sheet_names: Optional[List[str]]=None:
    - L485 docstring: "Extract from ODS (LibreOffice/OpenOffice) files."
    - L486 try:
      - L487 import pandas as pd
      - L488 except ImportError:
        - L489 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='ods', errors=['pandas and odfpy required. Run: pip install pandas odfpy'])
    - L496 annotated assign sheets: List[ExcelSheet] = []
    - L497 annotated assign errors: List[str] = []
    - L499 try:
      - L501 assign ods_data = pd.read_excel(file_path, engine='odf', sheet_name=None)
      - L503 assign sheets_to_process = sheet_names or list(ods_data.keys())[:self.config.max_sheets]
      - L505 for sheet_name in sheets_to_process:
        - L506 if sheet_name not in ods_data:
          - L507 expr errors.append(f"Sheet '{sheet_name}' not found")
          - L508 continue
        - L510 assign df = ods_data[sheet_name]
        - L511 if df.empty:
          - L512 continue
        - L515 if len(df) > self.config.max_rows:
          - L516 expr errors.append(f"Sheet '{sheet_name}' truncated at {self.config.max_rows} rows")
          - L517 assign df = df.head(self.config.max_rows)
        - L520 assign headers = [str(col) for col in df.columns.tolist()]
        - L521 assign rows = df.fillna('').values.tolist()
        - L524 assign column_stats = []
        - L525 if self.config.compute_stats:
          - L526 for (i, header) in enumerate(headers):
            - L527 assign col_values = [row[i] if i < len(row) else None for row in rows]
            - L528 expr column_stats.append(_compute_column_stats(header, col_values, self.config))
        - L530 expr sheets.append(ExcelSheet(name=sheet_name, headers=headers, rows=rows, row_count=len(rows), column_count=len(headers), column_stats=column_stats))
      - L539 return ExcelExtractionResult(sheets=sheets, filename=file_path.name, format='ods', errors=errors)
      - L546 except Exception as e:
        - L547 expr logger.error(f'ODS extraction failed: {e}')
        - L548 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='ods', errors=[f'Extraction failed: {str(e)}'])
  - L555 def _extract_xlsxself, file_path: Path, sheet_names: Optional[List[str]]=None:
    - L560 docstring: "Extract from .xlsx/.xlsm files using openpyxl."
    - L561 try:
      - L562 import openpyxl
      - L563 except ImportError:
        - L564 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='xlsx', errors=['openpyxl not installed. Run: pip install openpyxl'])
    - L571 annotated assign sheets: List[ExcelSheet] = []
    - L572 annotated assign errors: List[str] = []
    - L574 try:
      - L575 assign workbook = openpyxl.load_workbook(file_path, data_only=True, read_only=True)
      - L577 assign sheets_to_process = sheet_names or workbook.sheetnames[:self.config.max_sheets]
      - L579 if len(workbook.sheetnames) > self.config.max_sheets and (not sheet_names):
        - L580 expr errors.append(f'File has {len(workbook.sheetnames)} sheets, processing first {self.config.max_sheets}')
      - L582 for sheet_name in sheets_to_process:
        - L583 if sheet_name not in workbook.sheetnames:
          - L584 expr errors.append(f"Sheet '{sheet_name}' not found")
          - L585 continue
        - L587 assign sheet = workbook[sheet_name]
        - L590 annotated assign rows_data: List[List[Any]] = []
        - L591 for (row_idx, row) in enumerate(sheet.iter_rows(values_only=True)):
          - L592 if row_idx >= self.config.max_rows:
            - L593 expr errors.append(f"Sheet '{sheet_name}' truncated at {self.config.max_rows} rows")
            - L594 break
          - L597 assign cleaned_row = []
          - L598 for cell in row:
            - L599 if cell is None:
              - L600 expr cleaned_row.append('')
              - L601 else:
                - L601 if isinstance(cell, datetime):
                  - L602 expr cleaned_row.append(cell)
                  - L603 else:
                    - L603 if isinstance(cell, (int, float)):
                      - L604 expr cleaned_row.append(cell)
                      - L606 else:
                        - L606 expr cleaned_row.append(str(cell))
          - L607 expr rows_data.append(cleaned_row)
        - L609 if not rows_data:
          - L610 continue
        - L613 assign header_confidence = 1.0
        - L614 if self.config.detect_headers and rows_data:
          - L615 assign header_confidence = _calculate_header_confidence(rows_data[0], rows_data[1:])
          - L617 if header_confidence >= 0.5:
            - L618 assign headers = [str(h) if h else f'Column_{i + 1}' for i, h in enumerate(rows_data[0])]
            - L619 assign data_rows = rows_data[1:]
            - L621 else:
              - L621 assign headers = [f'Column_{i + 1}' for i in range(len(rows_data[0]))]
              - L622 assign data_rows = rows_data
          - L624 else:
            - L624 assign headers = [f'Column_{i + 1}' for i in range(len(rows_data[0]))]
            - L625 assign data_rows = rows_data
        - L628 assign max_cols = min(len(headers), self.config.max_columns)
        - L629 assign headers = headers[:max_cols]
        - L630 assign normalized_rows = []
        - L631 for row in data_rows:
          - L632 assign normalized = list(row)
          - L633 while len(normalized) < max_cols:
            - L634 expr normalized.append('')
          - L635 expr normalized_rows.append(normalized[:max_cols])
        - L638 assign column_stats = []
        - L639 if self.config.compute_stats and normalized_rows:
          - L640 for (i, header) in enumerate(headers):
            - L641 assign col_values = [row[i] if i < len(row) else None for row in normalized_rows]
            - L642 expr column_stats.append(_compute_column_stats(header, col_values, self.config))
        - L644 expr sheets.append(ExcelSheet(name=sheet_name, headers=headers, rows=normalized_rows, row_count=len(normalized_rows), column_count=len(headers), header_confidence=header_confidence, column_stats=column_stats))
      - L654 expr workbook.close()
      - L656 return ExcelExtractionResult(sheets=sheets, filename=file_path.name, format='xlsx', errors=errors, metadata={'total_sheets': len(workbook.sheetnames)})
      - L664 except Exception as e:
        - L665 expr logger.error(f'XLSX extraction failed: {e}')
        - L666 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='xlsx', errors=[f'Extraction failed: {str(e)}'])
  - L673 def _extract_xlsself, file_path: Path, sheet_names: Optional[List[str]]=None:
    - L678 docstring: "Extract from .xls files using xlrd."
    - L679 try:
      - L680 import xlrd
      - L681 except ImportError:
        - L682 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='xls', errors=['xlrd not installed. Run: pip install xlrd'])
    - L689 annotated assign sheets: List[ExcelSheet] = []
    - L690 annotated assign errors: List[str] = []
    - L692 try:
      - L693 assign workbook = xlrd.open_workbook(file_path)
      - L695 assign sheet_list = sheet_names or workbook.sheet_names()[:self.max_sheets]
      - L697 for sheet_name in sheet_list:
        - L698 try:
          - L699 assign sheet = workbook.sheet_by_name(sheet_name)
          - L700 except xlrd.XLRDError:
            - L701 expr errors.append(f"Sheet '{sheet_name}' not found")
            - L702 continue
        - L704 annotated assign rows_data: List[List[Any]] = []
        - L705 for row_idx in range(min(sheet.nrows, self.max_rows)):
          - L706 assign row = []
          - L707 for col_idx in range(sheet.ncols):
            - L708 assign cell = sheet.cell(row_idx, col_idx)
            - L709 if cell.ctype == xlrd.XL_CELL_EMPTY:
              - L710 expr row.append('')
              - L711 else:
                - L711 if cell.ctype == xlrd.XL_CELL_NUMBER:
                  - L712 expr row.append(cell.value)
                  - L714 else:
                    - L714 expr row.append(str(cell.value))
          - L715 expr rows_data.append(row)
        - L717 if not rows_data:
          - L718 continue
        - L721 if self.detect_headers and rows_data:
          - L722 assign headers = [str(h) if h else f'Column_{i + 1}' for i, h in enumerate(rows_data[0])]
          - L723 assign data_rows = rows_data[1:]
          - L725 else:
            - L725 assign headers = [f'Column_{i + 1}' for i in range(len(rows_data[0]))]
            - L726 assign data_rows = rows_data
        - L728 expr sheets.append(ExcelSheet(name=sheet_name, headers=headers, rows=data_rows, row_count=len(data_rows), column_count=len(headers)))
      - L736 return ExcelExtractionResult(sheets=sheets, filename=file_path.name, format='xls', errors=errors)
      - L743 except Exception as e:
        - L744 expr logger.error(f'XLS extraction failed: {e}')
        - L745 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='xls', errors=[f'Extraction failed: {str(e)}'])
  - L752 def _detect_encodingself, file_path: Path:
    - L753 docstring: "Detect file encoding by trying multiple encodings."
    - L755 try:
      - L756 import chardet
      - L757 with open(file_path, 'rb') as f:
        - L758 assign raw = f.read(10000)
      - L759 assign result = chardet.detect(raw)
      - L760 if result['confidence'] > 0.7:
        - L761 return result['encoding']
      - L762 except ImportError:
        - L763 pass
    - L766 for encoding in self.config.encodings_to_try:
      - L767 try:
        - L768 with open(file_path, 'r', encoding=encoding) as f:
          - L769 expr f.read(1000)
        - L770 return encoding
        - L771 except (UnicodeDecodeError, LookupError):
          - L772 continue
    - L774 return 'utf-8'
  - L776 def _extract_csvself, file_path: Path, delimiter: str=',':
    - L781 docstring: "Extract from CSV/TSV files with smart encoding detection."
    - L782 annotated assign errors: List[str] = []
    - L785 assign encoding = self._detect_encoding(file_path)
    - L787 try:
      - L788 with open(file_path, 'r', encoding=encoding, errors='replace') as f:
        - L790 assign sample = f.read(8192)
        - L791 expr f.seek(0)
        - L793 try:
          - L794 assign dialect = csv.Sniffer().sniff(sample, delimiters=',\t;|')
          - L795 assign delimiter = dialect.delimiter
          - L796 except csv.Error:
            - L797 pass
        - L799 assign reader = csv.reader(f, delimiter=delimiter)
        - L800 assign rows_data = list(reader)
      - L802 if not rows_data:
        - L803 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='csv', errors=['CSV file is empty'])
      - L811 if len(rows_data) > self.config.max_rows:
        - L812 expr errors.append(f'CSV truncated at {self.config.max_rows} rows')
        - L813 assign rows_data = rows_data[:self.config.max_rows]
      - L816 assign header_confidence = 1.0
      - L817 if self.config.detect_headers and rows_data:
        - L818 assign header_confidence = _calculate_header_confidence(rows_data[0], rows_data[1:])
        - L820 if header_confidence >= 0.5:
          - L821 assign headers = [str(h) if h else f'Column_{i + 1}' for i, h in enumerate(rows_data[0])]
          - L822 assign data_rows = rows_data[1:]
          - L824 else:
            - L824 assign max_cols = max((len(row) for row in rows_data)) if rows_data else 0
            - L825 assign headers = [f'Column_{i + 1}' for i in range(max_cols)]
            - L826 assign data_rows = rows_data
        - L828 else:
          - L828 assign max_cols = max((len(row) for row in rows_data)) if rows_data else 0
          - L829 assign headers = [f'Column_{i + 1}' for i in range(max_cols)]
          - L830 assign data_rows = rows_data
      - L833 assign max_cols = min(len(headers), self.config.max_columns)
      - L834 assign headers = headers[:max_cols]
      - L835 assign normalized_rows = []
      - L836 for row in data_rows:
        - L837 assign normalized = list(row)
        - L838 while len(normalized) < max_cols:
          - L839 expr normalized.append('')
        - L840 expr normalized_rows.append(normalized[:max_cols])
      - L843 assign column_stats = []
      - L844 if self.config.compute_stats and normalized_rows:
        - L845 for (i, header) in enumerate(headers):
          - L846 assign col_values = [row[i] if i < len(row) else None for row in normalized_rows]
          - L847 expr column_stats.append(_compute_column_stats(header, col_values, self.config))
      - L849 assign sheets = [ExcelSheet(name=file_path.stem, headers=headers, rows=normalized_rows, row_count=len(normalized_rows), column_count=len(headers), header_confidence=header_confidence, column_stats=column_stats, metadata={'encoding': encoding, 'delimiter': delimiter})]
      - L860 return ExcelExtractionResult(sheets=sheets, filename=file_path.name, format='csv', errors=errors, metadata={'encoding': encoding})
      - L868 except Exception as e:
        - L869 expr logger.error(f'CSV extraction failed: {e}')
        - L870 return ExcelExtractionResult(sheets=[], filename=file_path.name, format='csv', errors=[f'Extraction failed: {str(e)}'])
- L878 def extract_excel_datafile_path: Union[str, Path], sheet_names: Optional[List[str]]=None, max_rows: int=10000:
  - L883 docstring: "\n    Quick function to extract data from Excel/CSV files.\n\n    Args:\n       ..."
  - L894 assign extractor = ExcelExtractor(max_rows=max_rows)
  - L895 return extractor.extract(file_path, sheet_names)

## backend\app\services\extraction\pdf_extractors.py
- L2 docstring: "\nPDF Table Extraction using Multiple Tools.\n\nSupports:\n- Tabula (tabula-py):..."
- L22 from __future__ import annotations
- L24 import concurrent.futures
- L25 import hashlib
- L26 import json
- L27 import logging
- L28 import os
- L29 import re
- L30 import tempfile
- L31 import threading
- L32 from abc import ABC, abstractmethod
- L33 from dataclasses import dataclass, field
- L34 from pathlib import Path
- L35 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
- L37 assign logger = logging.getLogger('neura.extraction.pdf')
- L45 class ExtractionConfig:
  - L46 docstring: "Configuration for PDF extraction."
  - L47 annotated assign max_pages: int = 100
  - L48 annotated assign max_tables_per_page: int = 10
  - L49 annotated assign min_rows_for_table: int = 2
  - L50 annotated assign min_cols_for_table: int = 2
  - L51 annotated assign ocr_enabled: bool = True
  - L52 annotated assign ocr_language: str = 'eng'
  - L53 annotated assign parallel_pages: bool = True
  - L54 annotated assign max_workers: int = 4
  - L55 annotated assign confidence_threshold: float = 0.5
  - L56 annotated assign detect_headers: bool = True
  - L57 annotated assign preserve_layout: bool = True
- L60 assign DEFAULT_CONFIG = ExtractionConfig()
- L64 class ExtractedTable:
  - L65 docstring: "A table extracted from a PDF."
  - L66 annotated assign id: str
  - L67 annotated assign page: int
  - L68 annotated assign headers: List[str]
  - L69 annotated assign rows: List[List[str]]
  - L70 annotated assign confidence: float = 1.0
  - L71 annotated assign method: str = 'unknown'
  - L72 annotated assign bbox: Optional[Tuple[float, float, float, float]] = None
  - L73 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L75 def __post_init__self:
    - L76 docstring: "Validate and clean up table data."
    - L78 assign self.headers = [str(h).strip() if h else '' for h in self.headers]
    - L80 assign self.rows = [[str(cell).strip() if cell else '' for cell in row] for row in self.rows]
  - L86 def row_countself:
    - L87 docstring: "Number of data rows."
    - L88 return len(self.rows)
  - L91 def col_countself:
    - L92 docstring: "Number of columns."
    - L93 return len(self.headers) if self.headers else len(self.rows[0]) if self.rows else 0
  - L95 def to_dictself:
    - L96 docstring: "Convert to dictionary."
    - L97 return {'id': self.id, 'page': self.page, 'headers': self.headers, 'rows': self.rows, 'confidence': self.confidence, 'method': self.method, 'bbox': self.bbox, 'metadata': self.metadata, 'row_count': self.row_count, 'col_count': self.col_count}
  - L110 def get_columnself, col_name: str:
    - L111 docstring: "Get all values from a column by header name."
    - L112 try:
      - L113 assign idx = self.headers.index(col_name)
      - L114 return [row[idx] if idx < len(row) else '' for row in self.rows]
      - L115 except ValueError:
        - L116 return []
- L120 class ExtractionResult:
  - L121 docstring: "Result of PDF extraction."
  - L122 annotated assign tables: List[ExtractedTable]
  - L123 annotated assign text: str
  - L124 annotated assign page_count: int
  - L125 annotated assign method: str
  - L126 annotated assign errors: List[str] = field(default_factory=list)
  - L127 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L128 annotated assign extraction_time_ms: float = 0.0
  - L129 annotated assign ocr_used: bool = False
  - L132 def has_tablesself:
    - L133 docstring: "Check if any tables were extracted."
    - L134 return len(self.tables) > 0
  - L137 def total_rowsself:
    - L138 docstring: "Total number of rows across all tables."
    - L139 return sum((t.row_count for t in self.tables))
  - L141 def get_table_by_pageself, page: int:
    - L142 docstring: "Get all tables from a specific page."
    - L143 return [t for t in self.tables if t.page == page]
  - L145 def to_dictself:
    - L146 docstring: "Convert to dictionary."
    - L147 return {'tables': [t.to_dict() for t in self.tables], 'text': self.text, 'page_count': self.page_count, 'method': self.method, 'errors': self.errors, 'metadata': self.metadata, 'extraction_time_ms': self.extraction_time_ms, 'ocr_used': self.ocr_used, 'has_tables': self.has_tables, 'total_rows': self.total_rows}
- L165 class OCREngine:
  - L166 docstring: "OCR engine abstraction supporting multiple backends."
  - L168 def __init__self, language: str='eng':
    - L169 assign self.language = language
    - L170 annotated assign self._engine: Optional[str] = None
    - L171 assign self._lock = threading.Lock()
  - L173 def _detect_engineself:
    - L174 docstring: "Detect available OCR engine."
    - L175 if self._engine:
      - L176 return self._engine
    - L178 with self._lock:
      - L180 try:
        - L181 import easyocr
        - L182 assign self._engine = 'easyocr'
        - L183 return self._engine
        - L184 except ImportError:
          - L185 pass
      - L188 try:
        - L189 import pytesseract
        - L191 expr pytesseract.get_tesseract_version()
        - L192 assign self._engine = 'tesseract'
        - L193 return self._engine
        - L194 except ImportError:
          - L195 expr logger.debug('pytesseract not installed')
        - L196 except Exception as e:
          - L197 expr logger.debug(f'Tesseract not available: {e}')
      - L199 assign self._engine = 'none'
      - L200 return self._engine
  - L202 def extract_textself, image_bytes: bytes:
    - L203 docstring: "Extract text from image bytes using OCR."
    - L204 assign engine = self._detect_engine()
    - L206 if engine == 'none':
      - L207 return ''
    - L209 try:
      - L210 if engine == 'easyocr':
        - L211 return self._ocr_easyocr(image_bytes)
        - L212 else:
          - L212 if engine == 'tesseract':
            - L213 return self._ocr_tesseract(image_bytes)
      - L214 except Exception as e:
        - L215 expr logger.warning(f'OCR extraction failed: {e}')
        - L216 return ''
    - L218 return ''
  - L220 def _ocr_easyocrself, image_bytes: bytes:
    - L221 docstring: "Extract text using EasyOCR."
    - L222 import easyocr
    - L223 import numpy as np
    - L224 from PIL import Image
    - L225 import io
    - L228 assign image = Image.open(io.BytesIO(image_bytes))
    - L229 assign image_array = np.array(image)
    - L232 assign reader = easyocr.Reader([self.language[:2]], gpu=False)
    - L233 assign results = reader.readtext(image_array)
    - L236 return ' '.join([text for _, text, _ in results])
  - L238 def _ocr_tesseractself, image_bytes: bytes:
    - L239 docstring: "Extract text using Tesseract."
    - L240 import pytesseract
    - L241 from PIL import Image
    - L242 import io
    - L244 assign image = Image.open(io.BytesIO(image_bytes))
    - L245 return pytesseract.image_to_string(image, lang=self.language)
  - L247 def is_availableself:
    - L248 docstring: "Check if OCR is available."
    - L249 return self._detect_engine() != 'none'
- L253 annotated assign _ocr_engine: Optional[OCREngine] = None
- L256 def get_ocr_enginelanguage: str='eng':
  - L257 docstring: "Get or create OCR engine."
  - L258 Global
  - L259 if _ocr_engine is None:
    - L260 assign _ocr_engine = OCREngine(language)
  - L261 return _ocr_engine
- L268 def _is_header_rowrow: List[str], all_rows: List[List[str]]:
  - L269 docstring: "\n    Detect if a row is likely a header row.\n\n    Uses heuristics:\n    - Hea..."
  - L277 if not row or not all_rows:
    - L278 return False
  - L281 assign num_numeric = sum((1 for cell in row if _is_numeric(cell)))
  - L282 assign num_total = len([c for c in row if c.strip()])
  - L284 if num_total == 0:
    - L285 return False
  - L287 assign numeric_ratio = num_numeric / num_total
  - L290 if numeric_ratio < 0.3:
    - L292 if len(all_rows) > 1:
      - L293 assign other_numeric_ratios = []
      - L294 for other_row in all_rows[1:min(5, len(all_rows))]:
        - L295 assign other_num = sum((1 for cell in other_row if _is_numeric(cell)))
        - L296 assign other_total = len([c for c in other_row if c.strip()])
        - L297 if other_total > 0:
          - L298 expr other_numeric_ratios.append(other_num / other_total)
      - L300 if other_numeric_ratios:
        - L301 assign avg_other_ratio = sum(other_numeric_ratios) / len(other_numeric_ratios)
        - L302 if avg_other_ratio > numeric_ratio + 0.2:
          - L303 return True
  - L306 assign header_patterns = [lambda c: c.isupper(), lambda c: c.istitle(), lambda c: c.lower() in ('id', 'name', 'date', 'amount', 'total', 'qty', 'price', 'description')]
  - L312 assign pattern_matches = sum((1 for cell in row if cell.strip() and any((p(cell.strip()) for p in header_patterns))))
  - L317 return pattern_matches >= len(row) // 2
- L320 def _is_numericvalue: str:
  - L321 docstring: "Check if a string represents a numeric value."
  - L322 if not value or not value.strip():
    - L323 return False
  - L325 assign cleaned = value.strip().replace(',', '').replace('$', '').replace('%', '')
  - L326 assign cleaned = cleaned.lstrip('-+')
  - L328 try:
    - L329 expr float(cleaned)
    - L330 return True
    - L331 except ValueError:
      - L332 return False
- L335 def _clean_cell_valuevalue: Any:
  - L336 docstring: "Clean and normalize a cell value."
  - L337 if value is None:
    - L338 return ''
  - L340 assign text = str(value).strip()
  - L343 assign text = re.sub('\\s+', ' ', text)
  - L346 assign text = text.replace('\x00', '')
  - L348 return text
- L351 def _calculate_table_confidencetable: ExtractedTable, config: ExtractionConfig:
  - L355 docstring: "Calculate confidence score for extracted table."
  - L356 assign confidence = 1.0
  - L359 if table.row_count < config.min_rows_for_table:
    - L360 aug assign confidence Mult 0.5
  - L363 if table.col_count < config.min_cols_for_table:
    - L364 aug assign confidence Mult 0.5
  - L367 assign empty_cells = sum((1 for row in table.rows for cell in row if not cell.strip()))
  - L372 assign total_cells = table.row_count * table.col_count
  - L373 if total_cells > 0:
    - L374 assign empty_ratio = empty_cells / total_cells
    - L375 if empty_ratio > 0.5:
      - L376 aug assign confidence Mult 1 - empty_ratio
  - L379 assign expected_cols = table.col_count
  - L380 assign inconsistent_rows = sum((1 for row in table.rows if len(row) != expected_cols))
  - L384 if table.row_count > 0:
    - L385 assign inconsistent_ratio = inconsistent_rows / table.row_count
    - L386 aug assign confidence Mult 1 - inconsistent_ratio * 0.5
  - L389 if table.headers and all((h.strip() for h in table.headers)):
    - L390 aug assign confidence Mult 1.1
  - L392 return min(1.0, max(0.0, confidence))
- L395 class PDFExtractor(ABC):
  - L396 docstring: "Abstract base class for PDF extractors."
  - L398 annotated assign name: str = 'base'
  - L401 def is_availableself:
    - L402 docstring: "Check if this extractor is available."
    - L403 pass
  - L406 def extract_tablesself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None:
    - L411 docstring: "Extract tables from a PDF."
    - L412 pass
  - L414 def extract_textself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None:
    - L419 docstring: "Extract text from a PDF."
    - L421 assign result = self.extract_tables(pdf_path, pages)
    - L422 return result.text
- L425 class TabulaExtractor(PDFExtractor):
  - L426 docstring: "\n    Tabula-based PDF table extraction.\n\n    Best for:\n    - Well-structured..."
  - L437 assign name = 'tabula'
  - L439 def is_availableself:
    - L440 try:
      - L441 import tabula
      - L443 return True
      - L444 except ImportError:
        - L445 return False
  - L447 def extract_tablesself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None:
    - L452 try:
      - L453 import tabula
      - L454 except ImportError:
        - L455 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=['tabula-py not installed. Run: pip install tabula-py'])
    - L463 assign pdf_path = Path(pdf_path)
    - L464 annotated assign tables: List[ExtractedTable] = []
    - L465 annotated assign errors: List[str] = []
    - L467 try:
      - L469 assign page_spec = 'all' if pages is None else [p + 1 for p in pages]
      - L472 assign dfs = tabula.read_pdf(str(pdf_path), pages=page_spec, multiple_tables=True, pandas_options={'header': None})
      - L479 for (i, df) in enumerate(dfs):
        - L480 if df is None or df.empty:
          - L481 continue
        - L484 assign data = df.fillna('').astype(str).values.tolist()
        - L486 if len(data) < 1:
          - L487 continue
        - L490 assign headers = [str(h).strip() for h in data[0]]
        - L491 assign rows = [[str(cell).strip() for cell in row] for row in data[1:]]
        - L494 if not any((h for h in headers)):
          - L495 if rows:
            - L496 assign headers = [f'Column_{j + 1}' for j in range(len(rows[0]))]
            - L498 else:
              - L498 continue
        - L500 expr tables.append(ExtractedTable(id=f'tabula_table_{i + 1}', page=1, headers=headers, rows=rows, confidence=0.85, method=self.name))
      - L510 try:
        - L511 import fitz
        - L512 assign doc = fitz.open(pdf_path)
        - L513 assign page_count = doc.page_count
        - L514 expr doc.close()
        - L515 except ImportError:
          - L516 expr logger.debug('PyMuPDF not available for page count, using table count estimate')
          - L517 assign page_count = len(dfs) if dfs else 0
        - L518 except Exception as e:
          - L519 expr logger.debug(f'Could not get page count via PyMuPDF: {e}')
          - L520 assign page_count = len(dfs) if dfs else 0
      - L522 return ExtractionResult(tables=tables, text='', page_count=page_count, method=self.name, errors=errors)
      - L530 except Exception as e:
        - L531 expr logger.error(f'Tabula extraction failed: {e}')
        - L532 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=[f'Tabula extraction failed: {str(e)}'])
- L541 class CamelotExtractor(PDFExtractor):
  - L542 docstring: "\n    Camelot-based PDF table extraction.\n\n    Best for:\n    - Complex table ..."
  - L554 assign name = 'camelot'
  - L556 def is_availableself:
    - L557 try:
      - L558 import camelot
      - L559 return True
      - L560 except ImportError:
        - L561 return False
  - L563 def extract_tablesself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None, flavor: str='lattice':
    - L569 try:
      - L570 import camelot
      - L571 except ImportError:
        - L572 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=['camelot-py not installed. Run: pip install camelot-py[cv]'])
    - L580 assign pdf_path = Path(pdf_path)
    - L581 annotated assign tables: List[ExtractedTable] = []
    - L582 annotated assign errors: List[str] = []
    - L584 try:
      - L586 assign page_spec = 'all' if pages is None else ','.join((str(p + 1) for p in pages))
      - L589 try:
        - L590 assign camelot_tables = camelot.read_pdf(str(pdf_path), pages=page_spec, flavor=flavor)
        - L595 except Exception as e:
          - L596 expr logger.warning(f'Camelot {flavor} mode failed, trying alternative: {e}')
          - L597 assign alt_flavor = 'stream' if flavor == 'lattice' else 'lattice'
          - L598 assign camelot_tables = camelot.read_pdf(str(pdf_path), pages=page_spec, flavor=alt_flavor)
      - L604 for (i, ct) in enumerate(camelot_tables):
        - L605 assign df = ct.df
        - L606 if df is None or df.empty:
          - L607 continue
        - L610 assign data = df.fillna('').astype(str).values.tolist()
        - L612 if len(data) < 1:
          - L613 continue
        - L616 assign headers = [str(h).strip() for h in data[0]]
        - L617 assign rows = [[str(cell).strip() for cell in row] for row in data[1:]]
        - L620 assign bbox = None
        - L621 if hasattr(ct, '_bbox'):
          - L622 assign bbox = ct._bbox
        - L624 expr tables.append(ExtractedTable(id=f'camelot_table_{i + 1}', page=ct.page if hasattr(ct, 'page') else 1, headers=headers, rows=rows, confidence=ct.accuracy / 100.0 if hasattr(ct, 'accuracy') else 0.8, method=self.name, bbox=bbox, metadata={'flavor': flavor}))
      - L636 try:
        - L637 import fitz
        - L638 assign doc = fitz.open(pdf_path)
        - L639 assign page_count = doc.page_count
        - L640 expr doc.close()
        - L641 except ImportError:
          - L642 expr logger.debug('PyMuPDF not available for page count in Camelot extractor')
          - L643 assign page_count = 0
        - L644 except Exception as e:
          - L645 expr logger.debug(f'Could not get page count in Camelot extractor: {e}')
          - L646 assign page_count = 0
      - L648 return ExtractionResult(tables=tables, text='', page_count=page_count, method=self.name, errors=errors)
      - L656 except Exception as e:
        - L657 expr logger.error(f'Camelot extraction failed: {e}')
        - L658 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=[f'Camelot extraction failed: {str(e)}'])
- L667 class PyMuPDFExtractor(PDFExtractor):
  - L668 docstring: "\n    PyMuPDF (fitz) based PDF extraction.\n\n    Best for:\n    - Fast extracti..."
  - L679 assign name = 'pymupdf'
  - L681 def is_availableself:
    - L682 try:
      - L683 import fitz
      - L684 return True
      - L685 except ImportError:
        - L686 return False
  - L688 def extract_tablesself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None:
    - L693 try:
      - L694 import fitz
      - L695 except ImportError:
        - L696 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=['pymupdf not installed. Run: pip install pymupdf'])
    - L704 assign pdf_path = Path(pdf_path)
    - L705 annotated assign tables: List[ExtractedTable] = []
    - L706 annotated assign text_parts: List[str] = []
    - L707 annotated assign errors: List[str] = []
    - L709 try:
      - L710 assign doc = fitz.open(pdf_path)
      - L711 assign page_count = doc.page_count
      - L713 assign pages_to_process = pages if pages else range(page_count)
      - L715 for page_num in pages_to_process:
        - L716 if page_num >= page_count:
          - L717 continue
        - L719 assign page = doc[page_num]
        - L722 assign page_text = page.get_text('text')
        - L723 expr text_parts.append(f'--- Page {page_num + 1} ---\n{page_text}')
        - L726 try:
          - L727 assign page_tables = page.find_tables()
          - L729 for (i, table) in enumerate(page_tables):
            - L730 if table.row_count == 0:
              - L731 continue
            - L733 assign data = table.extract()
            - L734 if not data or len(data) < 2:
              - L735 continue
            - L738 assign headers = [str(cell or '').strip() for cell in data[0]]
            - L739 assign rows = []
            - L741 for row in data[1:]:
              - L742 assign normalized_row = []
              - L743 for (j, cell) in enumerate(row):
                - L744 if j < len(headers):
                  - L745 expr normalized_row.append(str(cell or '').strip())
              - L746 while len(normalized_row) < len(headers):
                - L747 expr normalized_row.append('')
              - L748 expr rows.append(normalized_row)
            - L750 expr tables.append(ExtractedTable(id=f'pymupdf_p{page_num + 1}_t{i + 1}', page=page_num + 1, headers=headers, rows=rows, confidence=0.9, method=self.name))
          - L759 except Exception as e:
            - L760 expr errors.append(f'Table extraction failed on page {page_num + 1}: {e}')
      - L762 expr doc.close()
      - L764 return ExtractionResult(tables=tables, text='\n\n'.join(text_parts), page_count=page_count, method=self.name, errors=errors)
      - L772 except Exception as e:
        - L773 expr logger.error(f'PyMuPDF extraction failed: {e}')
        - L774 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=[f'PyMuPDF extraction failed: {str(e)}'])
- L783 class PDFPlumberExtractor(PDFExtractor):
  - L784 docstring: "\n    pdfplumber based PDF extraction.\n\n    Best for:\n    - Detailed layout a..."
  - L795 assign name = 'pdfplumber'
  - L797 def is_availableself:
    - L798 try:
      - L799 import pdfplumber
      - L800 return True
      - L801 except ImportError:
        - L802 return False
  - L804 def extract_tablesself, pdf_path: Union[str, Path], pages: Optional[List[int]]=None:
    - L809 try:
      - L810 import pdfplumber
      - L811 except ImportError:
        - L812 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=['pdfplumber not installed. Run: pip install pdfplumber'])
    - L820 assign pdf_path = Path(pdf_path)
    - L821 annotated assign tables: List[ExtractedTable] = []
    - L822 annotated assign text_parts: List[str] = []
    - L823 annotated assign errors: List[str] = []
    - L825 try:
      - L826 with pdfplumber.open(pdf_path) as pdf:
        - L827 assign page_count = len(pdf.pages)
        - L828 assign pages_to_process = pages if pages else range(page_count)
        - L830 for page_num in pages_to_process:
          - L831 if page_num >= page_count:
            - L832 continue
          - L834 assign page = pdf.pages[page_num]
          - L837 assign page_text = page.extract_text() or ''
          - L838 expr text_parts.append(f'--- Page {page_num + 1} ---\n{page_text}')
          - L841 try:
            - L842 assign page_tables = page.extract_tables()
            - L844 for (i, table_data) in enumerate(page_tables):
              - L845 if not table_data or len(table_data) < 2:
                - L846 continue
              - L849 assign headers = [str(cell or '').strip() for cell in table_data[0]]
              - L850 assign rows = []
              - L852 for row in table_data[1:]:
                - L853 assign normalized_row = [str(cell or '').strip() for cell in row]
                - L854 while len(normalized_row) < len(headers):
                  - L855 expr normalized_row.append('')
                - L856 expr rows.append(normalized_row[:len(headers)])
              - L858 expr tables.append(ExtractedTable(id=f'pdfplumber_p{page_num + 1}_t{i + 1}', page=page_num + 1, headers=headers, rows=rows, confidence=0.85, method=self.name))
            - L867 except Exception as e:
              - L868 expr errors.append(f'Table extraction failed on page {page_num + 1}: {e}')
      - L870 return ExtractionResult(tables=tables, text='\n\n'.join(text_parts), page_count=page_count, method=self.name, errors=errors)
      - L878 except Exception as e:
        - L879 expr logger.error(f'pdfplumber extraction failed: {e}')
        - L880 return ExtractionResult(tables=[], text='', page_count=0, method=self.name, errors=[f'pdfplumber extraction failed: {str(e)}'])
- L890 annotated assign EXTRACTORS: Dict[str, type] = {'tabula': TabulaExtractor, 'camelot': CamelotExtractor, 'pymupdf': PyMuPDFExtractor, 'pdfplumber': PDFPlumberExtractor}
- L898 def get_available_extractors:
  - L899 docstring: "Get list of available extractors."
  - L900 assign available = []
  - L901 for (name, cls) in EXTRACTORS.items():
    - L902 try:
      - L903 assign extractor = cls()
      - L904 if extractor.is_available():
        - L905 expr available.append(name)
      - L906 except Exception as e:
        - L907 expr logger.debug(f"Extractor '{name}' not available: {e}")
  - L908 return available
- L911 def extract_pdf_tablespdf_path: Union[str, Path], method: str='auto', pages: Optional[List[int]]=None:
  - L916 docstring: "\n    Extract tables from a PDF using the specified method.\n\n    Args:\n      ..."
  - L927 if method == 'auto':
    - L928 return extract_with_best_method(pdf_path, pages)
  - L930 if method not in EXTRACTORS:
    - L931 return ExtractionResult(tables=[], text='', page_count=0, method=method, errors=[f'Unknown extraction method: {method}. Available: {list(EXTRACTORS.keys())}'])
  - L939 assign extractor = EXTRACTORS[method]()
  - L940 if not extractor.is_available():
    - L941 return ExtractionResult(tables=[], text='', page_count=0, method=method, errors=[f"Extractor '{method}' is not available. Check dependencies."])
  - L949 return extractor.extract_tables(pdf_path, pages)
- L952 def extract_with_best_methodpdf_path: Union[str, Path], pages: Optional[List[int]]=None:
  - L956 docstring: "\n    Try multiple extractors and return the best result.\n\n    Priority order:..."
  - L961 assign pdf_path = Path(pdf_path)
  - L962 annotated assign best_result: Optional[ExtractionResult] = None
  - L965 assign priority = ['pymupdf', 'pdfplumber', 'camelot', 'tabula']
  - L967 for method in priority:
    - L968 if method not in EXTRACTORS:
      - L969 continue
    - L971 assign extractor = EXTRACTORS[method]()
    - L972 if not extractor.is_available():
      - L973 continue
    - L975 try:
      - L976 assign result = extractor.extract_tables(pdf_path, pages)
      - L978 if result.errors and (not result.tables):
        - L979 continue
      - L981 if best_result is None:
        - L982 assign best_result = result
        - L983 else:
          - L983 if len(result.tables) > len(best_result.tables):
            - L984 assign best_result = result
            - L985 else:
              - L985 if len(result.tables) == len(best_result.tables) and result.tables and best_result.tables and (result.tables[0].confidence > best_result.tables[0].confidence):
                - L988 assign best_result = result
      - L991 if result.tables and (not result.errors):
        - L992 break
      - L994 except Exception as e:
        - L995 expr logger.warning(f'Extractor {method} failed: {e}')
        - L996 continue
  - L998 if best_result is None:
    - L999 return ExtractionResult(tables=[], text='', page_count=0, method='auto', errors=['No extractors were able to process this PDF'])
  - L1007 return best_result
- L1010 def compare_extractorspdf_path: Union[str, Path], pages: Optional[List[int]]=None:
  - L1014 docstring: "\n    Compare results from all available extractors.\n\n    Useful for finding t..."
  - L1022 assign results = {}
  - L1024 for (name, cls) in EXTRACTORS.items():
    - L1025 try:
      - L1026 assign extractor = cls()
      - L1027 if extractor.is_available():
        - L1028 assign results[name] = extractor.extract_tables(pdf_path, pages)
      - L1029 except Exception as e:
        - L1030 assign results[name] = ExtractionResult(tables=[], text='', page_count=0, method=name, errors=[str(e)])
  - L1038 return results

## backend\app\services\generator\__init__.py
- L1 docstring: "\nGenerator services responsible for producing downstream runtime assets.\n"
- L5 from .GeneratorAssetsV1 import GeneratorAssetsError, build_generator_assets_from_payload, load_generator_assets_bundle
- L11 assign __all__ = ['GeneratorAssetsError', 'build_generator_assets_from_payload', 'load_generator_assets_bundle']

## backend\app\services\generator\GeneratorAssetsV1.py
- L2 from __future__ import annotations
- L4 import json
- L5 import logging
- L6 import re
- L7 from pathlib import Path
- L8 from typing import Any, Callable, Iterable, Mapping, Optional, Sequence
- L10 from ..prompts.llm_prompts import get_prompt_generator_assets as get_pdf_prompt_generator_assets
- L13 from ..templates.TemplateVerify import get_openai_client
- L14 from ..utils import write_artifact_manifest, write_json_atomic, write_text_atomic
- L15 from ..utils.llm import call_chat_completion
- L16 from ..utils.validation import validate_contract_v2, validate_generator_output_schemas, validate_generator_sql_pack
- L22 assign logger = logging.getLogger(__name__)
- L24 assign DEFAULT_MODEL = 'gpt-5'
- L27 class GeneratorAssetsError(RuntimeError):
  - L28 docstring: "Raised when generator asset creation fails."
- L31 def _ensure_itervalues: Iterable[Any] | None:
  - L32 if not values:
    - L33 return []
  - L34 return list(values)
- L37 def _normalized_tokenstokens: Iterable[str] | None:
  - L38 annotated assign cleaned: list[str] = []
  - L39 if not tokens:
    - L40 return cleaned
  - L41 annotated assign seen: set[str] = set()
  - L42 for raw in tokens:
    - L43 assign text = str(raw or '').strip()
    - L44 if not text or text in seen:
      - L45 continue
    - L46 expr seen.add(text)
    - L47 expr cleaned.append(text)
  - L48 return cleaned
- L51 def _normalize_sql_dialectvalue: str | None:
  - L52 docstring: "\n    Normalize dialect names so the runtime consistently receives DuckDB-friend..."
  - L56 assign text = str(value or '').strip().lower()
  - L57 if text in ('', 'sqlite', 'duckdb'):
    - L58 return 'duckdb'
  - L59 if text in ('postgres', 'postgresql'):
    - L60 return 'postgres'
  - L61 return text or 'duckdb'
- L64 assign _SQL_PLACEHOLDER_PATTERNS = ('\\.\\.\\.', '\\bTBD\\b', '\\bTODO\\b', 'ADD_SQL_HERE', '<add_sql_here>')
- L73 def _extract_aliasessql: str | None:
  - L74 if not sql:
    - L75 return []
  - L76 assign pattern = re.compile('\\bAS\\s+([A-Za-z_][\\w]*)', re.IGNORECASE)
  - L77 return pattern.findall(sql)
- L80 def _sql_contains_keywordsql: str, keyword: str:
  - L81 return bool(re.search(f'\\b{re.escape(keyword)}\\b', sql, re.IGNORECASE))
- L84 def _sql_uses_table_referencesql: str:
  - L85 return bool(re.search('[A-Za-z_][\\w]*\\.[A-Za-z_][\\w]*', sql))
- L88 def _validate_entrypoint_sql_shapeentrypoints: Mapping[str, str]:
  - L89 annotated assign issues: list[str] = []
  - L90 for section in ('header', 'rows', 'totals'):
    - L91 assign sql = (entrypoints.get(section) or '').strip()
    - L92 if not sql:
      - L93 continue
    - L94 if not _sql_contains_keyword(sql, 'select'):
      - L95 expr issues.append(f'missing_select:{section}')
    - L96 assign needs_from = _sql_uses_table_reference(sql)
    - L97 if needs_from and (not _sql_contains_keyword(sql, 'from')):
      - L98 expr issues.append(f'missing_from:{section}')
    - L99 for pattern in _SQL_PLACEHOLDER_PATTERNS:
      - L100 if re.search(pattern, sql, re.IGNORECASE):
        - L101 expr issues.append(f'incomplete_sql:{section}')
        - L102 break
  - L103 return issues
- L106 def _derive_output_schemascontract: Mapping[str, Any] | None:
  - L107 docstring: "\n    Build header/rows/totals token lists from the contract when the LLM respon..."
  - L111 assign contract = contract or {}
  - L112 assign tokens_section = contract.get('tokens') if isinstance(contract, Mapping) else {}
  - L114 assign header_tokens = _normalized_tokens(contract.get('header_tokens') if isinstance(contract, Mapping) else None)
  - L115 assign row_tokens = _normalized_tokens(contract.get('row_tokens') if isinstance(contract, Mapping) else None)
  - L116 assign totals_tokens = _normalized_tokens(list((contract.get('totals') or {}).keys()) if isinstance(contract, Mapping) else None)
  - L120 if isinstance(tokens_section, Mapping):
    - L121 assign header_tokens = header_tokens or _normalized_tokens(tokens_section.get('scalars'))
    - L122 assign row_tokens = row_tokens or _normalized_tokens(tokens_section.get('row_tokens'))
    - L123 assign totals_tokens = totals_tokens or _normalized_tokens(tokens_section.get('totals'))
  - L125 return {'header': header_tokens, 'rows': row_tokens, 'totals': totals_tokens}
- L132 def _validate_entrypoints_against_schemaentrypoints: Mapping[str, str], output_schemas: Mapping[str, Sequence[str]]:
  - L136 annotated assign issues: list[str] = []
  - L137 for (section, expected) in output_schemas.items():
    - L138 assign expected_tokens = [str(token) for token in expected or []]
    - L139 assign sql = entrypoints.get(section, '') or ''
    - L140 if expected_tokens:
      - L141 assign aliases = _extract_aliases(sql)
      - L142 if aliases:
        - L143 assign alias_set = {alias.strip() for alias in aliases if alias}
        - L144 assign mismatch = [token for token in expected_tokens if token not in alias_set]
        - L145 if mismatch:
          - L146 expr issues.append(f'schema_mismatch:{section}')
        - L149 else:
          - L149 expr issues.append(f'schema_ambiguous:{section}')
  - L150 return issues
- L153 def _default_entrypointsexisting: Mapping[str, str] | None:
  - L154 assign normalized = {}
  - L155 assign existing = existing or {}
  - L156 for name in ('header', 'rows', 'totals'):
    - L157 assign sql = existing.get(name)
    - L158 if sql:
      - L159 assign normalized[name] = str(sql)
      - L161 else:
        - L161 assign normalized[name] = 'SELECT 1;'
  - L162 return normalized
- L165 def _render_sql_scriptscript: str | None, entrypoints: Mapping[str, str]:
  - L166 if script and script.strip():
    - L167 return script.strip() + ('\n' if not script.strip().endswith('\n') else '')
  - L168 assign sections = []
  - L169 for name in ('header', 'rows', 'totals'):
    - L170 assign sql = entrypoints.get(name, '').strip() or 'SELECT 1;'
    - L171 assign section = f'-- {name.upper()} --\n{sql.strip()}'
    - L172 expr sections.append(section)
  - L173 return '\n\n'.join(sections) + '\n'
- L176 def _json_safevalue: Any:
  - L177 docstring: "Convert Paths and other non-serialisable types into JSON-friendly shapes."
  - L178 if isinstance(value, Path):
    - L179 return value.as_posix()
  - L180 if isinstance(value, dict):
    - L181 return {str(k): _json_safe(v) for k, v in value.items()}
  - L182 if isinstance(value, (list, tuple)):
    - L183 return [_json_safe(v) for v in value]
  - L184 if isinstance(value, set):
    - L185 return [_json_safe(v) for v in value]
  - L186 return value
- L189 def _prepare_step4_for_promptstep4_output: Mapping[str, Any]:
  - L190 assign allowed_keys = ('contract', 'overview_md', 'step5_requirements', 'assumptions', 'warnings', 'validation')
  - L198 annotated assign payload: dict[str, Any] = {}
  - L199 for key in allowed_keys:
    - L200 if key in step4_output and step4_output[key] is not None:
      - L201 assign payload[key] = step4_output[key]
  - L202 return _json_safe(payload)
- L205 assign _JSON_FENCE_RE = re.compile('```(?:json)?\\s*(\\{.*\\})\\s*```', re.DOTALL | re.IGNORECASE)
- L206 annotated assign _JSON_ARRAY_CLOSURE_FIXES: tuple[tuple[str, str], ...] = (('\n    },\n    "row_computed"', '\n    ],\n    "row_computed"'), ('\n    },\n    "header_tokens"', '\n    ],\n    "header_tokens"'), ('\n    },\n    "row_tokens"', '\n    ],\n    "row_tokens"'), ('\n    },\n    "row_order"', '\n    ],\n    "row_order"'))
- L214 def _repair_generator_jsontext: str:
  - L215 docstring: "\n    Attempt to repair simple LLM mistakes where arrays are closed with `}` ins..."
  - L218 assign working = text
  - L219 assign max_attempts = len(_JSON_ARRAY_CLOSURE_FIXES) + 1
  - L220 for _ in range(max_attempts):
    - L221 try:
      - L222 return json.loads(working)
      - L223 except json.JSONDecodeError:
        - L224 assign replaced = False
        - L225 for (needle, replacement) in _JSON_ARRAY_CLOSURE_FIXES:
          - L226 if needle in working:
            - L227 assign working = working.replace(needle, replacement, 1)
            - L228 assign replaced = True
            - L229 break
        - L230 if not replaced:
          - L231 return None
  - L232 try:
    - L233 return json.loads(working)
    - L234 except json.JSONDecodeError:
      - L235 return None
- L238 def _ensure_reshape_rule_purposecontract: Mapping[str, Any]:
  - L239 docstring: "\n    Backfill reshape rule purpose strings when the LLM omits them.\n    "
  - L242 assign reshape_rules = contract.get('reshape_rules')
  - L243 if not isinstance(reshape_rules, list):
    - L244 return None
  - L245 for (idx, rule) in enumerate(reshape_rules):
    - L246 if not isinstance(rule, dict):
      - L247 continue
    - L248 assign purpose = rule.get('purpose')
    - L249 if isinstance(purpose, str) and purpose.strip():
      - L250 continue
    - L251 assign alias = str(rule.get('alias') or '').strip()
    - L252 assign strategy = str(rule.get('strategy') or '').strip()
    - L253 if alias and strategy:
      - L254 assign summary = f'{alias} {strategy} rule'
      - L255 else:
        - L255 if alias:
          - L256 assign summary = f'{alias} reshape rule'
          - L257 else:
            - L257 if strategy:
              - L258 assign summary = f'{strategy} reshape rule'
              - L260 else:
                - L260 assign summary = f'Reshape rule {idx + 1}'
    - L261 assign rule['purpose'] = summary[:120]
- L264 def _ensure_row_ordercontract: Mapping[str, Any]:
  - L265 docstring: "\n    Normalise row_order to a non-empty list, deriving it from order_by when om..."
  - L268 assign row_order_raw = contract.get('row_order')
  - L269 annotated assign cleaned: list[str] = []
  - L270 if isinstance(row_order_raw, str):
    - L271 assign text = row_order_raw.strip()
    - L272 if text:
      - L273 assign cleaned = [text]
    - L274 else:
      - L274 if isinstance(row_order_raw, list):
        - L275 assign cleaned = [str(item).strip() for item in row_order_raw if str(item or '').strip()]
  - L277 assign order_block = contract.get('order_by')
  - L278 annotated assign rows_spec: Any = None
  - L279 if isinstance(order_block, Mapping):
    - L280 assign rows_spec = order_block.get('rows')
    - L281 else:
      - L281 if isinstance(order_block, list):
        - L282 assign rows_spec = list(order_block)
        - L283 assign contract['order_by'] = {'rows': list(rows_spec)}
        - L284 else:
          - L284 if isinstance(order_block, str) and order_block.strip():
            - L285 assign rows_spec = [order_block.strip()]
            - L286 assign contract['order_by'] = {'rows': list(rows_spec)}
            - L288 else:
              - L288 assign contract['order_by'] = {'rows': []}
  - L289 if isinstance(rows_spec, list):
    - L290 assign rows_order = [str(item).strip() for item in rows_spec if str(item or '').strip()]
    - L291 else:
      - L291 if isinstance(rows_spec, str) and rows_spec.strip():
        - L292 assign rows_order = [rows_spec.strip()]
        - L293 assign contract['order_by']['rows'] = rows_order
        - L295 else:
          - L295 assign rows_order = []
  - L297 assign contract['row_order'] = cleaned or rows_order or ['ROWID']
- L300 def _normalize_contract_joincontract: Mapping[str, Any]:
  - L301 docstring: "\n    Drop or sanitise join blocks that the LLM emits with blank keys so schema ..."
  - L305 assign join = contract.get('join')
  - L306 if not isinstance(join, Mapping):
    - L307 return None
  - L309 assign parent_table = str(join.get('parent_table') or '').strip()
  - L310 assign parent_key = str(join.get('parent_key') or '').strip()
  - L311 assign child_table = str(join.get('child_table') or '').strip()
  - L312 assign child_key = str(join.get('child_key') or '').strip()
  - L314 if not parent_table or not parent_key:
    - L315 if any((parent_table, parent_key, child_table, child_key)):
      - L316 expr logger.info('generator_contract_join_dropped', extra={'event': 'generator_contract_join_dropped', 'parent_table': parent_table, 'parent_key': parent_key, 'child_table': child_table, 'child_key': child_key})
    - L326 expr contract.pop('join', None)
    - L327 return None
  - L329 annotated assign normalized_join: dict[str, str] = {'parent_table': parent_table, 'parent_key': parent_key}
  - L333 if child_table and child_key:
    - L334 assign normalized_join['child_table'] = child_table
    - L335 assign normalized_join['child_key'] = child_key
  - L337 assign contract['join'] = normalized_join
- L340 def _parse_generator_responseraw_text: str:
  - L341 assign text = (raw_text or '').strip()
  - L342 if not text:
    - L343 raise GeneratorAssetsError('Generator response was empty.')
  - L344 assign match = _JSON_FENCE_RE.search(text)
  - L345 if match:
    - L346 assign text = match.group(1).strip()
  - L347 try:
    - L348 return json.loads(text)
    - L349 except json.JSONDecodeError as exc:
      - L350 assign repaired_payload = _repair_generator_json(text)
      - L351 if repaired_payload is not None:
        - L352 return repaired_payload
      - L353 assign start = text.find('{')
      - L354 assign end = text.rfind('}')
      - L355 if start != -1 and end != -1 and (end > start):
        - L356 assign snippet = text[start:end + 1]
        - L357 try:
          - L358 return json.loads(snippet)
          - L359 except Exception:
            - L360 pass
      - L361 raise GeneratorAssetsError(f'Generator response was not valid JSON: {exc}')
- L364 def _prepare_messagespayload: dict[str, Any], prompt_getter: Callable[[], dict[str, str]]:
  - L368 assign prompts = prompt_getter() or {}
  - L369 assign system_text = prompts.get('system') or 'You generate SQL packs.'
  - L370 assign user_template = prompts.get('user')
  - L371 assign user_payload = json.dumps(payload, ensure_ascii=False, indent=2)
  - L372 if user_template and '{payload}' in user_template:
    - L373 assign user_text = user_template.replace('{payload}', user_payload)
    - L375 else:
      - L375 assign user_text = f"{user_template or ''}\n{user_payload}"
  - L376 assign user_text = f'{user_text.strip()}\n\nIMPORTANT: Output strictly valid JSON. Use double quotes for every key and string value. Do not include trailing commas or comments.'
  - L380 return [{'role': 'system', 'content': system_text}, {'role': 'user', 'content': user_text.strip()}]
- L386 def _write_outputstemplate_dir: Path, contract: Mapping[str, Any], entrypoints: Mapping[str, str], output_schemas: Mapping[str, Sequence[str]], params: dict[str, list[str]], dialect: str, needs_user_fix: list[str], invalid: bool, summary: Mapping[str, Any], key_tokens: Iterable[str] | None, script: str | None:
  - L399 assign generator_dir = template_dir / 'generator'
  - L400 expr generator_dir.mkdir(parents=True, exist_ok=True)
  - L401 assign contract_path = template_dir / 'contract.json'
  - L402 expr write_json_atomic(contract_path, contract, ensure_ascii=False, indent=2, step='generator_contract')
  - L404 assign sql_path = generator_dir / 'sql_pack.sql'
  - L405 assign script_text = _render_sql_script(script, entrypoints)
  - L406 expr write_text_atomic(sql_path, script_text, encoding='utf-8', step='generator_sql_pack')
  - L408 assign output_schemas_path = generator_dir / 'output_schemas.json'
  - L409 expr write_json_atomic(output_schemas_path, output_schemas, ensure_ascii=False, indent=2, step='generator_output_schemas')
  - L417 assign meta_payload = {'dialect': dialect, 'entrypoints': entrypoints, 'params': params, 'needs_user_fix': needs_user_fix, 'invalid': invalid, 'summary': summary, 'cached': False, 'key_tokens': _normalized_tokens(key_tokens)}
  - L427 assign meta_path = generator_dir / 'generator_assets.json'
  - L428 expr write_json_atomic(meta_path, meta_payload, ensure_ascii=False, indent=2, step='generator_assets_meta')
  - L430 expr write_artifact_manifest(template_dir, step='generator_assets_v1', files={'contract.json': contract_path, 'sql_pack.sql': sql_path, 'output_schemas.json': output_schemas_path, 'generator_assets.json': meta_path}, inputs=['generator_assets_v1'], correlation_id=None)
  - L443 return {'contract': contract_path, 'sql_pack': sql_path, 'output_schemas': output_schemas_path, 'generator_assets': meta_path}
- L451 def build_generator_assets_from_payload*, template_dir: Path, step4_output: Mapping[str, Any], final_template_html: str, reference_pdf_image: Any=None, reference_worksheet_html: str | None=None, catalog_allowlist: Iterable[str] | None=None, params_spec: Sequence[str] | None=None, sample_params: Mapping[str, Any] | None=None, force_rebuild: bool=False, dialect: str | None=None, key_tokens: Iterable[str] | None=None, prompt_getter: Optional[Callable[[], dict[str, str]]]=None, require_contract_join: bool=True:
  - L467 assign template_dir = Path(template_dir)
  - L468 expr template_dir.mkdir(parents=True, exist_ok=True)
  - L470 assign catalog_list = [str(item) for item in catalog_allowlist or [] if str(item).strip()]
  - L471 assign params_list = list(params_spec or [])
  - L472 assign sample_params_dict = dict(sample_params or {})
  - L474 assign step4_prompt_payload = _prepare_step4_for_prompt(step4_output)
  - L476 assign request_payload = {'final_template_html': final_template_html, 'reference_pdf_image': reference_pdf_image, 'step4_output': step4_prompt_payload, 'catalog_allowlist': catalog_list, 'params_spec': params_list, 'sample_params': _json_safe(sample_params_dict), 'force_rebuild': bool(force_rebuild), 'key_tokens': _normalized_tokens(key_tokens)}
  - L488 if isinstance(reference_worksheet_html, str) and reference_worksheet_html.strip():
    - L489 assign request_payload['reference_worksheet_html'] = reference_worksheet_html
  - L491 assign client = get_openai_client()
  - L492 if client is None:
    - L493 raise GeneratorAssetsError('OpenAI client is not configured.')
  - L495 assign prompt_factory = prompt_getter or get_pdf_prompt_generator_assets
  - L496 assign messages = _prepare_messages(request_payload, prompt_factory)
  - L497 try:
    - L498 assign raw_response = call_chat_completion(client, model=DEFAULT_MODEL, messages=messages, description='generator_assets_v1')
    - L504 except Exception as exc:
      - L505 raise GeneratorAssetsError(f'Generator LLM call failed: {exc}')
  - L507 try:
    - L508 assign content = raw_response.choices[0].message.content or ''
    - L509 assign response_payload = _parse_generator_response(content)
    - L510 except GeneratorAssetsError:
      - L511 raise
    - L512 except Exception as exc:
      - L513 raise GeneratorAssetsError(f'Generator response was not valid JSON: {exc}')
  - L515 assign sql_pack_raw = response_payload.get('sql_pack') or {}
  - L516 assign contract = response_payload.get('contract')
  - L517 if not isinstance(contract, Mapping) or not contract:
    - L518 raise GeneratorAssetsError('Generator LLM response did not include a contract payload.')
  - L519 try:
    - L520 expr _ensure_reshape_rule_purpose(contract)
    - L521 expr _ensure_row_order(contract)
    - L522 expr _normalize_contract_join(contract)
    - L523 expr validate_contract_v2(contract, require_join=require_contract_join)
    - L524 except Exception as exc:
      - L525 raise GeneratorAssetsError(f'Generator contract failed validation: {exc}')
  - L527 assign output_schemas_payload = response_payload.get('output_schemas')
  - L528 if isinstance(output_schemas_payload, Mapping):
    - L529 assign output_schemas = {'header': _normalized_tokens(output_schemas_payload.get('header')), 'rows': _normalized_tokens(output_schemas_payload.get('rows')), 'totals': _normalized_tokens(output_schemas_payload.get('totals'))}
    - L535 else:
      - L535 assign output_schemas = _derive_output_schemas(contract)
  - L538 try:
    - L539 expr validate_generator_output_schemas(output_schemas)
    - L540 except Exception as exc:
      - L541 raise GeneratorAssetsError(f'Invalid output schemas: {exc}')
  - L544 assign entrypoints_raw = sql_pack_raw.get('entrypoints')
  - L545 if isinstance(entrypoints_raw, Mapping) and entrypoints_raw:
    - L546 assign entrypoints = _default_entrypoints(entrypoints_raw)
    - L548 else:
      - L548 assign legacy_entrypoints = {key: value for key, value in {'header': sql_pack_raw.get('header'), 'rows': sql_pack_raw.get('rows'), 'totals': sql_pack_raw.get('totals')}.items() if isinstance(value, str)}
      - L557 assign entrypoints = _default_entrypoints(legacy_entrypoints)
  - L559 assign script_text = sql_pack_raw.get('script')
  - L560 if isinstance(script_text, str) and script_text.strip():
    - L561 assign script_for_validation = script_text
    - L563 else:
      - L563 assign script_for_validation = _render_sql_script(script_text, entrypoints)
  - L565 assign params_section = sql_pack_raw.get('params')
  - L566 annotated assign required_params: list[str] = []
  - L567 annotated assign optional_params: list[str] = []
  - L568 if isinstance(params_section, Mapping):
    - L569 expr required_params.extend(params_section.get('required') or [])
    - L570 expr optional_params.extend(params_section.get('optional') or [])
    - L571 else:
      - L571 if isinstance(params_section, Sequence):
        - L572 expr required_params.extend(params_section)
  - L574 assign base_params = _normalized_tokens(params_list)
  - L575 assign key_param_tokens = _normalized_tokens(key_tokens)
  - L576 for token in key_param_tokens:
    - L577 if token not in base_params:
      - L578 expr base_params.append(token)
  - L579 for item in base_params:
    - L580 if item not in required_params:
      - L581 expr required_params.append(item)
  - L582 assign optional_params = [p for p in optional_params if p not in required_params]
  - L583 assign params_normalized = {'required': required_params, 'optional': optional_params}
  - L585 assign sql_pack_normalized = {'dialect': _normalize_sql_dialect(sql_pack_raw.get('dialect') or response_payload.get('dialect') or dialect), 'script': script_for_validation, 'entrypoints': entrypoints, 'params': params_normalized}
  - L592 try:
    - L593 expr validate_generator_sql_pack(sql_pack_normalized)
    - L594 except Exception as exc:
      - L595 raise GeneratorAssetsError(f'Invalid SQL pack: {exc}')
  - L597 assign schema_issues = _validate_entrypoints_against_schema(entrypoints, output_schemas)
  - L598 assign shape_issues = _validate_entrypoint_sql_shape(entrypoints)
  - L599 if schema_issues:
    - L600 expr logger.warning('generator_assets_schema_issues', extra={'event': 'generator_assets_schema_issues', 'issues': schema_issues})
  - L603 if shape_issues:
    - L604 expr logger.warning('generator_assets_sql_shape_issues', extra={'event': 'generator_assets_sql_shape_issues', 'issues': shape_issues})
  - L609 assign needs_user_fix = _ensure_iter(response_payload.get('needs_user_fix')) + schema_issues + shape_issues
  - L610 assign invalid = bool(response_payload.get('invalid')) or bool(schema_issues) or bool(shape_issues)
  - L611 assign summary = response_payload.get('summary') or {}
  - L612 assign selected_dialect = _normalize_sql_dialect(response_payload.get('dialect') or dialect)
  - L614 assign artifacts = _write_outputs(template_dir=template_dir, contract=contract, entrypoints=entrypoints, output_schemas=output_schemas, params=params_normalized, dialect=selected_dialect, needs_user_fix=needs_user_fix, invalid=invalid, summary=summary, key_tokens=key_param_tokens, script=script_text)
  - L628 assign result = {'artifacts': artifacts, 'needs_user_fix': needs_user_fix, 'invalid': invalid, 'dialect': selected_dialect, 'params': params_normalized, 'dry_run': response_payload.get('dry_run'), 'summary': summary, 'cached': False}
  - L638 return result
- L641 def load_generator_assets_bundletemplate_dir: Path:
  - L642 assign generator_dir = Path(template_dir) / 'generator'
  - L643 assign meta_path = generator_dir / 'generator_assets.json'
  - L644 if not meta_path.exists():
    - L645 return None
  - L647 try:
    - L648 assign meta = json.loads(meta_path.read_text(encoding='utf-8'))
    - L649 except Exception:
      - L650 return None
  - L652 assign sql_path = generator_dir / 'sql_pack.sql'
  - L653 assign output_schemas_path = generator_dir / 'output_schemas.json'
  - L654 assign contract_path = Path(template_dir) / 'contract.json'
  - L656 annotated assign artifacts: dict[str, Path] = {}
  - L657 if contract_path.exists():
    - L658 assign artifacts['contract'] = contract_path
  - L659 if sql_path.exists():
    - L660 assign artifacts['sql_pack'] = sql_path
  - L661 if output_schemas_path.exists():
    - L662 assign artifacts['output_schemas'] = output_schemas_path
  - L663 if meta_path.exists():
    - L664 assign artifacts['generator_assets'] = meta_path
  - L666 assign bundle = {'artifacts': artifacts, 'meta': meta, 'needs_user_fix': _ensure_iter(meta.get('needs_user_fix')), 'invalid': bool(meta.get('invalid')), 'dialect': meta.get('dialect'), 'params': meta.get('params') or {'required': [], 'optional': []}, 'summary': meta.get('summary') or {}, 'dry_run': None, 'cached': True, 'key_tokens': meta.get('key_tokens') or []}
  - L678 return bundle

## backend\app\services\jobs\__init__.py
- L1 docstring: "Jobs & logs placeholder."

## backend\app\services\jobs\report_scheduler.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import logging
- L5 from datetime import datetime, timedelta, timezone
- L6 from typing import Any, Callable
- L8 from ..state import state_store
- L9 from src.services.report_service import JobRunTracker, _build_job_steps, _step_progress_from_steps
- L10 from backend.app.features.generate.schemas.reports import RunPayload
- L12 assign logger = logging.getLogger('neura.scheduler')
- L15 def _parse_isots: str | None:
  - L16 if not ts:
    - L17 return None
  - L18 try:
    - L19 assign value = datetime.fromisoformat(ts)
    - L20 except ValueError:
      - L21 return None
  - L22 if value.tzinfo is None:
    - L23 return value.replace(tzinfo=timezone.utc)
  - L24 return value.astimezone(timezone.utc)
- L27 def _now_utc:
  - L28 return datetime.now(timezone.utc)
- L31 def _next_run_datetimeschedule: dict, baseline: datetime:
  - L32 assign minutes = schedule.get('interval_minutes') or 0
  - L33 assign minutes = max(int(minutes), 1)
  - L34 return baseline + timedelta(minutes=minutes)
- L37 class ReportScheduler:
  - L38 def __init__self, runner: Callable[..., dict], *, poll_seconds: int=60:
    - L44 docstring: "Initialize the report scheduler.\n\n        Args:\n            runner: Callable ..."
    - L51 assign self._runner = runner
    - L52 assign self._poll_seconds = max(poll_seconds, 5)
    - L53 annotated assign self._task: asyncio.Task | None = None
    - L54 assign self._stop_event = asyncio.Event()
    - L55 annotated assign self._inflight: set[str] = set()
  - L57 async def startself:
    - L58 if self._task and (not self._task.done()):
      - L59 return None
    - L60 expr self._stop_event.clear()
    - L61 assign self._task = asyncio.create_task(self._run_loop(), name='report-scheduler-loop')
    - L62 expr logger.info('scheduler_started', extra={'event': 'scheduler_started'})
  - L64 async def stopself:
    - L65 if not self._task:
      - L66 return None
    - L67 expr self._stop_event.set()
    - L68 expr self._task.cancel()
    - L69 try:
      - L70 expr await self._task
      - L71 except asyncio.CancelledError:
        - L72 pass
      - L74 finally:
        - L74 assign self._task = None
    - L75 expr logger.info('scheduler_stopped', extra={'event': 'scheduler_stopped'})
  - L77 async def _run_loopself:
    - L78 try:
      - L79 while not self._stop_event.is_set():
        - L80 try:
          - L81 expr await self._dispatch_due_jobs()
          - L82 except Exception:
            - L83 expr logger.exception('scheduler_tick_failed', extra={'event': 'scheduler_tick_failed'})
        - L84 try:
          - L85 expr await asyncio.wait_for(self._stop_event.wait(), timeout=self._poll_seconds)
          - L86 except asyncio.TimeoutError:
            - L87 continue
      - L88 except asyncio.CancelledError:
        - L89 raise
  - L91 async def _dispatch_due_jobsself:
    - L92 assign schedules = state_store.list_schedules()
    - L93 assign now = _now_utc()
    - L94 for schedule in schedules:
      - L95 if not schedule.get('active', True):
        - L96 continue
      - L97 assign sid = schedule.get('id')
      - L98 if not sid or sid in self._inflight:
        - L99 continue
      - L102 assign end_date = _parse_iso(schedule.get('end_date'))
      - L103 if end_date is not None and now > end_date:
        - L104 expr logger.info('schedule_past_end_date', extra={'event': 'schedule_past_end_date', 'schedule_id': sid, 'end_date': schedule.get('end_date')})
        - L112 continue
      - L115 assign start_date = _parse_iso(schedule.get('start_date'))
      - L116 if start_date is not None and now < start_date:
        - L117 continue
      - L119 assign next_run_at = _parse_iso(schedule.get('next_run_at'))
      - L120 if next_run_at is None or next_run_at <= now:
        - L121 expr self._inflight.add(sid)
        - L122 expr asyncio.create_task(self._run_schedule(schedule), name=f'schedule-{sid}')
  - L124 async def _run_scheduleself, schedule: dict:
    - L125 assign schedule_id = schedule.get('id')
    - L126 assign started = _now_utc()
    - L127 assign correlation_id = f"sched-{schedule_id or 'job'}-{started.timestamp():.0f}"
    - L128 annotated assign job_tracker: JobRunTracker | None = None
    - L129 try:
      - L130 assign payload = {'template_id': schedule.get('template_id'), 'connection_id': schedule.get('connection_id'), 'start_date': schedule.get('start_date'), 'end_date': schedule.get('end_date'), 'batch_ids': schedule.get('batch_ids') or None, 'key_values': schedule.get('key_values') or None, 'docx': bool(schedule.get('docx')), 'xlsx': bool(schedule.get('xlsx')), 'email_recipients': schedule.get('email_recipients') or None, 'email_subject': schedule.get('email_subject') or f"[Scheduled] {schedule.get('template_name') or schedule.get('template_id')}", 'email_message': schedule.get('email_message') or f"Scheduled run '{schedule.get('name')}' completed.\nWindow: {schedule.get('start_date')} - {schedule.get('end_date')}.", 'schedule_id': schedule_id, 'schedule_name': schedule.get('name')}
      - L150 assign kind = schedule.get('template_kind') or 'pdf'
      - L151 try:
        - L152 assign run_payload = RunPayload(**payload)
        - L153 except Exception:
          - L154 assign run_payload = None
      - L155 if run_payload is not None:
        - L156 assign steps = _build_job_steps(run_payload, kind=kind)
        - L157 assign meta = {'start_date': payload.get('start_date'), 'end_date': payload.get('end_date'), 'schedule_id': schedule_id, 'schedule_name': schedule.get('name'), 'docx': bool(payload.get('docx')), 'xlsx': bool(payload.get('xlsx'))}
        - L165 assign job_record = state_store.create_job(job_type='run_report', template_id=run_payload.template_id, connection_id=run_payload.connection_id, template_name=schedule.get('template_name') or run_payload.template_id, template_kind=kind, schedule_id=schedule_id, correlation_id=correlation_id, steps=steps, meta=meta)
        - L176 assign step_progress = _step_progress_from_steps(steps)
        - L177 assign job_tracker = JobRunTracker(job_record.get('id'), correlation_id=correlation_id, step_progress=step_progress)
        - L178 expr job_tracker.start()
      - L179 assign result = await asyncio.to_thread(self._runner, payload, kind, job_tracker=job_tracker)
      - L180 except Exception as exc:
        - L181 assign finished = _now_utc()
        - L182 assign next_run = _next_run_datetime(schedule, finished).isoformat()
        - L183 expr state_store.record_schedule_run(schedule_id, started_at=started.isoformat(), finished_at=finished.isoformat(), status='failed', next_run_at=next_run, error=str(exc), artifacts=None)
        - L192 expr logger.exception('schedule_run_failed', extra={'event': 'schedule_run_failed', 'schedule_id': schedule_id, 'error': str(exc)})
        - L200 if job_tracker:
          - L201 expr job_tracker.fail(str(exc))
      - L203 else:
        - L203 assign finished = _now_utc()
        - L204 assign next_run = _next_run_datetime(schedule, finished).isoformat()
        - L205 assign artifacts = {'html_url': result.get('html_url'), 'pdf_url': result.get('pdf_url'), 'docx_url': result.get('docx_url'), 'xlsx_url': result.get('xlsx_url')}
        - L211 expr state_store.record_schedule_run(schedule_id, started_at=started.isoformat(), finished_at=finished.isoformat(), status='success', next_run_at=next_run, error=None, artifacts=artifacts)
        - L220 expr logger.info('schedule_run_complete', extra={'event': 'schedule_run_complete', 'schedule_id': schedule_id, 'html': artifacts.get('html_url'), 'pdf': artifacts.get('pdf_url')})
        - L229 if job_tracker:
          - L230 expr job_tracker.succeed(result)
      - L232 finally:
        - L232 if schedule_id in self._inflight:
          - L233 expr self._inflight.remove(schedule_id)

## backend\app\services\llm\__init__.py
- L2 docstring: "\nMulti-provider LLM abstraction layer.\n\nSupports:\n- OpenAI (GPT-4, GPT-4o, e..."
- L49 from .client import LLMClient, get_llm_client, call_completion, call_completion_with_vision, get_available_models, health_check
- L57 from .config import LLMConfig, LLMProvider, get_llm_config
- L58 from .providers import BaseProvider, OpenAIProvider, OllamaProvider, DeepSeekProvider, AnthropicProvider, AzureOpenAIProvider, GoogleGeminiProvider, get_provider
- L68 from .vision import VisionLanguageModel, DocumentAnalysisResult, TableExtractionResult, get_vlm, analyze_document_image, extract_tables_from_image
- L76 from .agents import Agent, AgentConfig, AgentRole, Task, TaskResult, Crew, Tool, create_document_analyzer_agent, create_data_extractor_agent, create_sql_generator_agent, create_chart_suggester_agent, create_template_mapper_agent, create_quality_reviewer_agent, create_document_processing_crew, create_report_generation_crew
- L93 from .text_to_sql import TextToSQL, TableSchema, SQLGenerationResult, get_text_to_sql, generate_sql
- L100 from .rag import RAGRetriever, Document, RetrievalResult, BM25Index, create_retriever, quick_rag_query
- L108 from .document_extractor import EnhancedDocumentExtractor, ExtractedContent, ExtractedTable, FieldSchema, extract_document, extract_tables
- L117 assign __all__ = ['LLMClient', 'get_llm_client', 'call_completion', 'call_completion_with_vision', 'get_available_models', 'health_check', 'LLMConfig', 'LLMProvider', 'get_llm_config', 'BaseProvider', 'OpenAIProvider', 'OllamaProvider', 'DeepSeekProvider', 'AnthropicProvider', 'AzureOpenAIProvider', 'GoogleGeminiProvider', 'get_provider', 'VisionLanguageModel', 'DocumentAnalysisResult', 'TableExtractionResult', 'get_vlm', 'analyze_document_image', 'extract_tables_from_image', 'Agent', 'AgentConfig', 'AgentRole', 'Task', 'TaskResult', 'Crew', 'Tool', 'create_document_analyzer_agent', 'create_data_extractor_agent', 'create_sql_generator_agent', 'create_chart_suggester_agent', 'create_template_mapper_agent', 'create_quality_reviewer_agent', 'create_document_processing_crew', 'create_report_generation_crew', 'TextToSQL', 'TableSchema', 'SQLGenerationResult', 'get_text_to_sql', 'generate_sql', 'RAGRetriever', 'Document', 'RetrievalResult', 'BM25Index', 'create_retriever', 'quick_rag_query', 'EnhancedDocumentExtractor', 'ExtractedContent', 'ExtractedTable', 'FieldSchema', 'extract_document', 'extract_tables']

## backend\app\services\llm\agents.py
- L2 docstring: "\nMulti-Agent Orchestration System.\n\nInspired by CrewAI, this module provides:..."
- L11 from __future__ import annotations
- L13 import json
- L14 import logging
- L15 import time
- L16 from abc import ABC, abstractmethod
- L17 from dataclasses import dataclass, field
- L18 from enum import Enum
- L19 from typing import Any, Callable, Dict, List, Optional, Union
- L21 from .client import LLMClient, get_llm_client
- L22 from .config import LLMConfig
- L24 assign logger = logging.getLogger('neura.llm.agents')
- L27 class AgentRole(str, Enum):
  - L28 docstring: "Predefined agent roles."
  - L29 assign DOCUMENT_ANALYZER = 'document_analyzer'
  - L30 assign DATA_EXTRACTOR = 'data_extractor'
  - L31 assign SQL_GENERATOR = 'sql_generator'
  - L32 assign CHART_SUGGESTER = 'chart_suggester'
  - L33 assign TEMPLATE_MAPPER = 'template_mapper'
  - L34 assign REPORT_GENERATOR = 'report_generator'
  - L35 assign QUALITY_REVIEWER = 'quality_reviewer'
  - L36 assign COORDINATOR = 'coordinator'
- L40 class AgentConfig:
  - L41 docstring: "Configuration for an agent."
  - L42 annotated assign role: str
  - L43 annotated assign goal: str
  - L44 annotated assign backstory: str
  - L45 annotated assign model: Optional[str] = None
  - L46 annotated assign temperature: float = 0.7
  - L47 annotated assign max_tokens: Optional[int] = None
  - L48 annotated assign tools: List[str] = field(default_factory=list)
  - L49 annotated assign allow_delegation: bool = False
  - L50 annotated assign verbose: bool = False
- L54 class Task:
  - L55 docstring: "A task to be executed by an agent."
  - L56 annotated assign description: str
  - L57 annotated assign agent_role: str
  - L58 annotated assign expected_output: str
  - L59 annotated assign context: Dict[str, Any] = field(default_factory=dict)
  - L60 annotated assign dependencies: List[str] = field(default_factory=list)
  - L61 annotated assign tools: List[str] = field(default_factory=list)
- L65 class TaskResult:
  - L66 docstring: "Result of a task execution."
  - L67 annotated assign task_id: str
  - L68 annotated assign agent_role: str
  - L69 annotated assign output: Any
  - L70 annotated assign success: bool
  - L71 annotated assign error: Optional[str] = None
  - L72 annotated assign execution_time: float = 0.0
  - L73 annotated assign token_usage: Dict[str, int] = field(default_factory=dict)
- L76 class Tool(ABC):
  - L77 docstring: "Base class for agent tools."
  - L78 annotated assign name: str
  - L79 annotated assign description: str
  - L82 def executeself, **kwargs: Any:
    - L83 docstring: "Execute the tool with given arguments."
    - L84 pass
- L87 class Agent:
  - L88 docstring: "\n    An AI agent with a specific role and capabilities.\n\n    Agents can:\n   ..."
  - L97 def __init__self, config: AgentConfig, client: Optional[LLMClient]=None, tools: Optional[Dict[str, Tool]]=None:
    - L103 assign self.config = config
    - L104 assign self.client = client or get_llm_client()
    - L105 assign self.tools = tools or {}
    - L106 annotated assign self._conversation_history: List[Dict[str, Any]] = []
  - L109 def roleself:
    - L110 return self.config.role
  - L112 def execute_taskself, task: Task, context: Optional[Dict[str, Any]]=None:
    - L117 docstring: "\n        Execute a task and return the result.\n\n        Args:\n            ta..."
    - L127 assign start_time = time.time()
    - L128 assign task_id = f'{self.role}_{int(start_time)}'
    - L130 try:
      - L132 assign prompt = self._build_task_prompt(task, context)
      - L135 assign response = self.client.complete(messages=[{'role': 'system', 'content': self._build_system_prompt()}, {'role': 'user', 'content': prompt}], model=self.config.model, description=f'agent_{self.role}_{task_id}', temperature=self.config.temperature, max_tokens=self.config.max_tokens)
      - L146 assign output = response['choices'][0]['message']['content']
      - L147 assign token_usage = response.get('usage', {})
      - L150 assign output = self._process_tool_calls(output, task)
      - L153 expr self._conversation_history.append({'task': task.description, 'output': output})
      - L158 assign execution_time = time.time() - start_time
      - L160 expr logger.info('agent_task_complete', extra={'event': 'agent_task_complete', 'agent_role': self.role, 'task_id': task_id, 'execution_time': execution_time})
      - L170 return TaskResult(task_id=task_id, agent_role=self.role, output=output, success=True, execution_time=execution_time, token_usage=token_usage)
      - L179 except Exception as e:
        - L180 expr logger.error('agent_task_failed', extra={'event': 'agent_task_failed', 'agent_role': self.role, 'task_id': task_id, 'error': str(e)})
        - L189 return TaskResult(task_id=task_id, agent_role=self.role, output=None, success=False, error=str(e), execution_time=time.time() - start_time)
  - L198 def _build_system_promptself:
    - L199 docstring: "Build the system prompt for the agent."
    - L200 assign prompt = f'You are a {self.config.role} agent.\n\nGOAL: {self.config.goal}\n\nBACKSTORY: {self.config.backstory}\n\nGUIDELINES:\n- Focus on your specific role and expertise\n- Provide clear, structured outputs\n- If you need to use a tool, indicate it clearly\n- Be thorough but concise\n'
    - L213 if self.tools:
      - L214 aug assign prompt Add '\n\nAVAILABLE TOOLS:\n'
      - L215 for (name, tool) in self.tools.items():
        - L216 aug assign prompt Add f'- {name}: {tool.description}\n'
      - L217 aug assign prompt Add '\nTo use a tool, respond with: TOOL_CALL: tool_name(arg1=value1, arg2=value2)'
    - L219 return prompt
  - L221 def _build_task_promptself, task: Task, context: Optional[Dict[str, Any]]:
    - L226 docstring: "Build the task prompt."
    - L227 assign prompt = f'TASK: {task.description}\n\nEXPECTED OUTPUT: {task.expected_output}\n'
    - L232 if task.context:
      - L233 aug assign prompt Add f'\nTASK CONTEXT:\n{json.dumps(task.context, indent=2)}\n'
    - L235 if context:
      - L236 aug assign prompt Add f'\nPREVIOUS RESULTS:\n{json.dumps(context, indent=2)}\n'
    - L238 return prompt
  - L240 def _process_tool_callsself, output: str, task: Task:
    - L241 docstring: "Process any tool calls in the output."
    - L242 import re
    - L244 assign tool_pattern = 'TOOL_CALL:\\s*(\\w+)\\((.*?)\\)'
    - L245 assign matches = re.findall(tool_pattern, output)
    - L247 for (tool_name, args_str) in matches:
      - L248 if tool_name in self.tools:
        - L249 try:
          - L251 assign args = {}
          - L252 if args_str:
            - L253 for arg in args_str.split(','):
              - L254 if '=' in arg:
                - L255 assign (key, value) = arg.split('=', 1)
                - L256 assign args[key.strip()] = value.strip().strip('\'"')
          - L259 assign result = self.tools[tool_name].execute(**args)
          - L262 assign tool_call = f'TOOL_CALL: {tool_name}({args_str})'
          - L263 assign output = output.replace(tool_call, f'TOOL_RESULT ({tool_name}): {result}')
          - L265 except Exception as e:
            - L266 expr logger.warning('agent_tool_call_failed', extra={'tool': tool_name, 'error': str(e)})
    - L274 return output
- L277 class Crew:
  - L278 docstring: "\n    A crew of agents working together on tasks.\n\n    Manages agent coordinat..."
  - L284 def __init__self, agents: List[Agent], tasks: List[Task], verbose: bool=False:
    - L290 assign self.agents = {agent.role: agent for agent in agents}
    - L291 assign self.tasks = tasks
    - L292 assign self.verbose = verbose
    - L293 annotated assign self._results: Dict[str, TaskResult] = {}
  - L295 def kickoffself, inputs: Optional[Dict[str, Any]]=None:
    - L296 docstring: "\n        Start the crew's work on all tasks.\n\n        Args:\n            inpu..."
    - L305 assign context = inputs or {}
    - L307 expr logger.info('crew_kickoff', extra={'event': 'crew_kickoff', 'num_agents': len(self.agents), 'num_tasks': len(self.tasks)})
    - L316 for task in self.tasks:
      - L318 for dep in task.dependencies:
        - L319 if dep not in self._results or not self._results[dep].success:
          - L320 expr logger.warning('crew_task_dependency_not_met', extra={'task': task.description[:50], 'dependency': dep})
          - L327 continue
      - L330 assign agent = self.agents.get(task.agent_role)
      - L331 if not agent:
        - L332 expr logger.error('crew_agent_not_found', extra={'agent_role': task.agent_role, 'available_agents': list(self.agents.keys())})
        - L339 continue
      - L342 assign task_context = {**context}
      - L343 for dep in task.dependencies:
        - L344 if dep in self._results:
          - L345 assign task_context[dep] = self._results[dep].output
      - L348 assign result = agent.execute_task(task, task_context)
      - L349 assign self._results[task.description[:50]] = result
      - L351 if result.success:
        - L352 assign context[task.description[:50]] = result.output
      - L354 if self.verbose:
        - L355 expr print(f"[{task.agent_role}] {task.description[:50]}: {('SUCCESS' if result.success else 'FAILED')}")
    - L357 return {'results': {k: v.output for k, v in self._results.items() if v.success}, 'errors': {k: v.error for k, v in self._results.items() if not v.success}, 'execution_summary': self._get_execution_summary()}
  - L363 def _get_execution_summaryself:
    - L364 docstring: "Get a summary of the execution."
    - L365 assign total_time = sum((r.execution_time for r in self._results.values()))
    - L366 assign total_tokens = sum((r.token_usage.get('total_tokens', 0) for r in self._results.values()))
    - L371 return {'total_tasks': len(self.tasks), 'successful_tasks': sum((1 for r in self._results.values() if r.success)), 'failed_tasks': sum((1 for r in self._results.values() if not r.success)), 'total_execution_time': total_time, 'total_tokens_used': total_tokens}
- L382 def create_document_analyzer_agentclient: Optional[LLMClient]=None:
  - L383 docstring: "Create a document analysis agent."
  - L384 assign config = AgentConfig(role=AgentRole.DOCUMENT_ANALYZER.value, goal='Analyze documents to extract structure, content, and metadata', backstory='You are an expert document analyst with years of experience\n        analyzing various document types including PDFs, spreadsheets, and reports.\n        You excel at understanding document structure, identifying key sections,\n        and extracting meaningful information.', temperature=0.3)
  - L393 return Agent(config, client)
- L396 def create_data_extractor_agentclient: Optional[LLMClient]=None:
  - L397 docstring: "Create a data extraction agent."
  - L398 assign config = AgentConfig(role=AgentRole.DATA_EXTRACTOR.value, goal='Extract structured data from documents accurately', backstory='You are a meticulous data extraction specialist.\n        You can identify tables, lists, and structured data in any format\n        and convert them into clean, well-organized data structures.\n        Accuracy is your top priority.', temperature=0.2)
  - L407 return Agent(config, client)
- L410 def create_sql_generator_agentclient: Optional[LLMClient]=None:
  - L411 docstring: "Create a SQL generation agent."
  - L412 assign config = AgentConfig(role=AgentRole.SQL_GENERATOR.value, goal='Generate accurate and efficient SQL queries', backstory="You are a database expert specializing in SQL query generation.\n        You understand complex data relationships and can translate natural language\n        requirements into precise SQL queries. You're proficient in DuckDB, SQLite,\n        and standard SQL.", temperature=0.1)
  - L421 return Agent(config, client)
- L424 def create_chart_suggester_agentclient: Optional[LLMClient]=None:
  - L425 docstring: "Create a chart suggestion agent."
  - L426 assign config = AgentConfig(role=AgentRole.CHART_SUGGESTER.value, goal='Suggest optimal visualizations for data', backstory='You are a data visualization expert who understands\n        how to present data effectively. You know when to use bar charts,\n        line graphs, pie charts, and other visualizations based on the\n        data characteristics and the story to be told.', temperature=0.5)
  - L435 return Agent(config, client)
- L438 def create_template_mapper_agentclient: Optional[LLMClient]=None:
  - L439 docstring: "Create a template mapping agent."
  - L440 assign config = AgentConfig(role=AgentRole.TEMPLATE_MAPPER.value, goal='Map data fields to template placeholders accurately', backstory='You are an expert at understanding document templates\n        and mapping data fields to placeholders. You can identify patterns,\n        handle edge cases, and ensure data is correctly positioned in reports.', temperature=0.2)
  - L448 return Agent(config, client)
- L451 def create_quality_reviewer_agentclient: Optional[LLMClient]=None:
  - L452 docstring: "Create a quality review agent."
  - L453 assign config = AgentConfig(role=AgentRole.QUALITY_REVIEWER.value, goal='Review outputs for accuracy and quality', backstory='You are a meticulous quality assurance specialist.\n        You review work products for accuracy, completeness, and adherence\n        to requirements. You catch errors others might miss.', temperature=0.3)
  - L461 return Agent(config, client)
- L466 def create_document_processing_crewclient: Optional[LLMClient]=None, verbose: bool=False:
  - L470 docstring: "Create a crew for document processing workflow."
  - L471 assign agents = [create_document_analyzer_agent(client), create_data_extractor_agent(client), create_quality_reviewer_agent(client)]
  - L477 assign tasks = [Task(description='Analyze the document structure and identify key sections', agent_role=AgentRole.DOCUMENT_ANALYZER.value, expected_output='JSON with document structure, sections, and content overview'), Task(description='Extract all tables and structured data from the document', agent_role=AgentRole.DATA_EXTRACTOR.value, expected_output='JSON with extracted tables, their headers, and row data', dependencies=['Analyze the document structure and identify key sections'[:50]]), Task(description='Review extracted data for accuracy and completeness', agent_role=AgentRole.QUALITY_REVIEWER.value, expected_output='Quality report with any issues found and suggestions', dependencies=['Extract all tables and structured data from the document'[:50]])]
  - L497 return Crew(agents, tasks, verbose)
- L500 def create_report_generation_crewclient: Optional[LLMClient]=None, verbose: bool=False:
  - L504 docstring: "Create a crew for report generation workflow."
  - L505 assign agents = [create_data_extractor_agent(client), create_sql_generator_agent(client), create_template_mapper_agent(client), create_chart_suggester_agent(client)]
  - L512 assign tasks = [Task(description='Extract and prepare data for the report', agent_role=AgentRole.DATA_EXTRACTOR.value, expected_output='Clean, structured data ready for report generation'), Task(description='Generate SQL queries for data retrieval', agent_role=AgentRole.SQL_GENERATOR.value, expected_output='DuckDB-compatible SQL queries for report data', dependencies=['Extract and prepare data for the report'[:50]]), Task(description='Map data fields to template placeholders', agent_role=AgentRole.TEMPLATE_MAPPER.value, expected_output='Field mapping configuration JSON', dependencies=['Extract and prepare data for the report'[:50]]), Task(description='Suggest visualizations for the report data', agent_role=AgentRole.CHART_SUGGESTER.value, expected_output='Chart recommendations with configurations', dependencies=['Extract and prepare data for the report'[:50]])]
  - L538 return Crew(agents, tasks, verbose)

## backend\app\services\llm\client.py
- L2 docstring: "\nUnified LLM Client.\n\nProvides a single interface for all LLM providers with:..."
- L15 from __future__ import annotations
- L17 import hashlib
- L18 import json
- L19 import logging
- L20 import os
- L21 import threading
- L22 import time
- L23 from collections import deque
- L24 from dataclasses import dataclass, field
- L25 from datetime import datetime, timezone
- L26 from enum import Enum
- L27 from pathlib import Path
- L28 from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union
- L30 from .config import LLMConfig, LLMProvider, get_llm_config
- L31 from .providers import BaseProvider, get_provider
- L33 assign logger = logging.getLogger('neura.llm.client')
- L40 class CircuitState(Enum):
  - L41 docstring: "Circuit breaker states."
  - L42 assign CLOSED = 'closed'
  - L43 assign OPEN = 'open'
  - L44 assign HALF_OPEN = 'half_open'
- L48 class CircuitBreakerConfig:
  - L49 docstring: "Configuration for circuit breaker."
  - L50 annotated assign failure_threshold: int = 5
  - L51 annotated assign success_threshold: int = 2
  - L52 annotated assign timeout_seconds: float = 60.0
  - L53 annotated assign failure_window_seconds: float = 120.0
- L56 class CircuitBreaker:
  - L57 docstring: "\n    Circuit breaker for fault tolerance.\n\n    Prevents cascading failures by..."
  - L64 def __init__self, name: str, config: Optional[CircuitBreakerConfig]=None:
    - L65 assign self.name = name
    - L66 assign self.config = config or CircuitBreakerConfig()
    - L67 assign self._state = CircuitState.CLOSED
    - L68 assign self._failure_count = 0
    - L69 assign self._success_count = 0
    - L70 annotated assign self._last_failure_time: Optional[float] = None
    - L71 assign self._state_changed_at = time.time()
    - L72 annotated assign self._failure_timestamps: deque = deque()
    - L73 assign self._lock = threading.Lock()
  - L76 def stateself:
    - L77 docstring: "Get current circuit state, potentially transitioning from OPEN to HALF_OPEN."
    - L78 with self._lock:
      - L79 if self._state == CircuitState.OPEN:
        - L80 if time.time() - self._state_changed_at >= self.config.timeout_seconds:
          - L81 expr self._transition_to(CircuitState.HALF_OPEN)
      - L82 return self._state
  - L84 def _transition_toself, new_state: CircuitState:
    - L85 docstring: "Transition to a new state."
    - L86 assign old_state = self._state
    - L87 assign self._state = new_state
    - L88 assign self._state_changed_at = time.time()
    - L90 if new_state == CircuitState.CLOSED:
      - L91 assign self._failure_count = 0
      - L92 assign self._success_count = 0
      - L93 expr self._failure_timestamps.clear()
      - L94 else:
        - L94 if new_state == CircuitState.HALF_OPEN:
          - L95 assign self._success_count = 0
    - L97 expr logger.info('circuit_breaker_state_change', extra={'event': 'circuit_breaker_state_change', 'name': self.name, 'old_state': old_state.value, 'new_state': new_state.value})
  - L107 def allow_requestself:
    - L108 docstring: "Check if request should be allowed."
    - L109 assign current_state = self.state
    - L111 if current_state == CircuitState.CLOSED:
      - L112 return True
      - L113 else:
        - L113 if current_state == CircuitState.OPEN:
          - L114 return False
          - L116 else:
            - L116 return True
  - L118 def record_successself:
    - L119 docstring: "Record a successful request."
    - L120 with self._lock:
      - L121 if self._state == CircuitState.HALF_OPEN:
        - L122 aug assign self._success_count Add 1
        - L123 if self._success_count >= self.config.success_threshold:
          - L124 expr self._transition_to(CircuitState.CLOSED)
        - L125 else:
          - L125 if self._state == CircuitState.CLOSED:
            - L127 expr self._clean_old_failures()
  - L129 def record_failureself:
    - L130 docstring: "Record a failed request."
    - L131 with self._lock:
      - L132 assign now = time.time()
      - L133 assign self._last_failure_time = now
      - L135 if self._state == CircuitState.HALF_OPEN:
        - L136 expr self._transition_to(CircuitState.OPEN)
        - L137 else:
          - L137 if self._state == CircuitState.CLOSED:
            - L138 expr self._failure_timestamps.append(now)
            - L139 expr self._clean_old_failures()
            - L141 if len(self._failure_timestamps) >= self.config.failure_threshold:
              - L142 expr self._transition_to(CircuitState.OPEN)
  - L144 def _clean_old_failuresself:
    - L145 docstring: "Remove failures outside the failure window."
    - L146 assign cutoff = time.time() - self.config.failure_window_seconds
    - L147 while self._failure_timestamps and self._failure_timestamps[0] < cutoff:
      - L148 expr self._failure_timestamps.popleft()
  - L150 def get_statsself:
    - L151 docstring: "Get circuit breaker statistics."
    - L152 with self._lock:
      - L153 return {'name': self.name, 'state': self._state.value, 'failure_count': len(self._failure_timestamps), 'success_count': self._success_count, 'last_failure': self._last_failure_time, 'state_changed_at': self._state_changed_at}
- L168 class CacheEntry:
  - L169 docstring: "Cache entry with metadata."
  - L170 annotated assign response: Dict[str, Any]
  - L171 annotated assign created_at: float
  - L172 annotated assign expires_at: float
  - L173 annotated assign hit_count: int = 0
  - L174 annotated assign request_hash: str = ''
- L177 class ResponseCache:
  - L178 docstring: "\n    LRU cache for LLM responses with disk persistence.\n\n    Features:\n    -..."
  - L188 def __init__self, max_memory_items: int=100, max_disk_items: int=1000, default_ttl_seconds: float=3600.0, cache_dir: Optional[Path]=None:
    - L195 assign self.max_memory_items = max_memory_items
    - L196 assign self.max_disk_items = max_disk_items
    - L197 assign self.default_ttl_seconds = default_ttl_seconds
    - L198 assign self.cache_dir = cache_dir
    - L200 annotated assign self._memory_cache: Dict[str, CacheEntry] = {}
    - L201 annotated assign self._access_order: deque = deque()
    - L202 assign self._lock = threading.Lock()
    - L203 assign self._stats = {'hits': 0, 'misses': 0, 'evictions': 0}
    - L205 if cache_dir:
      - L206 expr cache_dir.mkdir(parents=True, exist_ok=True)
  - L208 def _compute_keyself, messages: List[Dict[str, Any]], model: str, **kwargs: Any:
    - L214 docstring: "Compute cache key from request parameters."
    - L216 assign key_data = {'messages': messages, 'model': model, 'kwargs': {k: v for k, v in sorted(kwargs.items()) if k not in ('stream',)}}
    - L221 assign key_json = json.dumps(key_data, sort_keys=True, default=str)
    - L222 return hashlib.sha256(key_json.encode()).hexdigest()[:32]
  - L224 def getself, messages: List[Dict[str, Any]], model: str, **kwargs: Any:
    - L230 docstring: "Get cached response if available."
    - L231 assign key = self._compute_key(messages, model, **kwargs)
    - L233 with self._lock:
      - L235 assign entry = self._memory_cache.get(key)
      - L236 if entry:
        - L237 if time.time() < entry.expires_at:
          - L238 aug assign entry.hit_count Add 1
          - L239 aug assign self._stats['hits'] Add 1
          - L241 if key in self._access_order:
            - L242 expr self._access_order.remove(key)
          - L243 expr self._access_order.append(key)
          - L244 return entry.response
          - L247 else:
            - L247 delete self._memory_cache[key]
            - L248 if key in self._access_order:
              - L249 expr self._access_order.remove(key)
      - L252 if self.cache_dir:
        - L253 assign disk_response = self._read_from_disk(key)
        - L254 if disk_response:
          - L255 aug assign self._stats['hits'] Add 1
          - L257 expr self._set_memory(key, disk_response)
          - L258 return disk_response
      - L260 aug assign self._stats['misses'] Add 1
      - L261 return None
  - L263 def setself, messages: List[Dict[str, Any]], model: str, response: Dict[str, Any], ttl_seconds: Optional[float]=None, **kwargs: Any:
    - L271 docstring: "Cache a response."
    - L272 assign key = self._compute_key(messages, model, **kwargs)
    - L273 assign ttl = ttl_seconds or self.default_ttl_seconds
    - L275 with self._lock:
      - L276 expr self._set_memory(key, response, ttl)
      - L279 if self.cache_dir:
        - L280 expr self._write_to_disk(key, response, ttl)
  - L282 def _set_memoryself, key: str, response: Dict[str, Any], ttl_seconds: Optional[float]=None:
    - L288 docstring: "Set entry in memory cache."
    - L289 assign ttl = ttl_seconds or self.default_ttl_seconds
    - L290 assign now = time.time()
    - L293 while len(self._memory_cache) >= self.max_memory_items and self._access_order:
      - L294 assign oldest_key = self._access_order.popleft()
      - L295 if oldest_key in self._memory_cache:
        - L296 delete self._memory_cache[oldest_key]
        - L297 aug assign self._stats['evictions'] Add 1
    - L299 assign self._memory_cache[key] = CacheEntry(response=response, created_at=now, expires_at=now + ttl, request_hash=key)
    - L305 expr self._access_order.append(key)
  - L307 def _read_from_diskself, key: str:
    - L308 docstring: "Read cached response from disk."
    - L309 if not self.cache_dir:
      - L310 return None
    - L312 assign cache_file = self.cache_dir / f'{key}.json'
    - L313 if not cache_file.exists():
      - L314 return None
    - L316 try:
      - L317 assign data = json.loads(cache_file.read_text(encoding='utf-8'))
      - L318 if time.time() < data.get('expires_at', 0):
        - L319 return data.get('response')
        - L322 else:
          - L322 expr cache_file.unlink(missing_ok=True)
          - L323 return None
      - L324 except Exception:
        - L325 return None
  - L327 def _write_to_diskself, key: str, response: Dict[str, Any], ttl_seconds: float:
    - L333 docstring: "Write cached response to disk."
    - L334 if not self.cache_dir:
      - L335 return None
    - L337 assign cache_file = self.cache_dir / f'{key}.json'
    - L338 try:
      - L339 assign data = {'response': response, 'created_at': time.time(), 'expires_at': time.time() + ttl_seconds}
      - L344 expr cache_file.write_text(json.dumps(data), encoding='utf-8')
      - L345 except Exception as e:
        - L346 expr logger.debug(f'Failed to write cache to disk: {e}')
  - L348 def clearself:
    - L349 docstring: "Clear all cached entries."
    - L350 with self._lock:
      - L351 expr self._memory_cache.clear()
      - L352 expr self._access_order.clear()
      - L353 assign self._stats = {'hits': 0, 'misses': 0, 'evictions': 0}
      - L355 if self.cache_dir:
        - L356 for f in self.cache_dir.glob('*.json'):
          - L357 try:
            - L358 expr f.unlink()
            - L359 except Exception:
              - L360 pass
  - L362 def get_statsself:
    - L363 docstring: "Get cache statistics."
    - L364 with self._lock:
      - L365 assign total = self._stats['hits'] + self._stats['misses']
      - L366 assign hit_rate = self._stats['hits'] / total if total > 0 else 0
      - L367 return {**self._stats, 'hit_rate': hit_rate, 'memory_size': len(self._memory_cache)}
- L379 annotated assign TOKEN_COSTS: Dict[str, Dict[str, float]] = {'gpt-4o': {'input': 0.0025, 'output': 0.01}, 'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006}, 'gpt-4-turbo': {'input': 0.01, 'output': 0.03}, 'gpt-3.5-turbo': {'input': 0.0005, 'output': 0.0015}, 'claude-3-5-sonnet-20241022': {'input': 0.003, 'output': 0.015}, 'claude-3-opus-20240229': {'input': 0.015, 'output': 0.075}, 'deepseek-chat': {'input': 0.00014, 'output': 0.00028}, 'deepseek-reasoner': {'input': 0.00055, 'output': 0.00219}}
- L391 def estimate_tokenstext: str:
  - L392 docstring: "\n    Estimate token count for text.\n\n    Uses a simple heuristic: ~4 characte..."
  - L398 try:
    - L399 import tiktoken
    - L400 assign enc = tiktoken.get_encoding('cl100k_base')
    - L401 return len(enc.encode(text))
    - L402 except ImportError:
      - L404 return max(1, len(text) // 4)
- L407 def estimate_costmodel: str, input_tokens: int, output_tokens: int:
  - L412 docstring: "Estimate cost for a completion."
  - L413 assign costs = TOKEN_COSTS.get(model, TOKEN_COSTS.get('gpt-4o', {'input': 0.01, 'output': 0.03}))
  - L414 assign input_cost = input_tokens / 1000 * costs['input']
  - L415 assign output_cost = output_tokens / 1000 * costs['output']
  - L416 return input_cost + output_cost
- L420 class UsageTracker:
  - L421 docstring: "Track token usage and costs."
  - L422 annotated assign total_input_tokens: int = 0
  - L423 annotated assign total_output_tokens: int = 0
  - L424 annotated assign total_cost: float = 0.0
  - L425 annotated assign request_count: int = 0
  - L426 annotated assign _lock: threading.Lock = field(default_factory=threading.Lock)
  - L428 def recordself, model: str, input_tokens: int, output_tokens: int:
    - L429 docstring: "Record token usage from a request."
    - L430 with self._lock:
      - L431 aug assign self.total_input_tokens Add input_tokens
      - L432 aug assign self.total_output_tokens Add output_tokens
      - L433 aug assign self.total_cost Add estimate_cost(model, input_tokens, output_tokens)
      - L434 aug assign self.request_count Add 1
  - L436 def get_statsself:
    - L437 docstring: "Get usage statistics."
    - L438 with self._lock:
      - L439 return {'total_input_tokens': self.total_input_tokens, 'total_output_tokens': self.total_output_tokens, 'total_tokens': self.total_input_tokens + self.total_output_tokens, 'estimated_cost_usd': round(self.total_cost, 4), 'request_count': self.request_count}
  - L447 def resetself:
    - L448 docstring: "Reset usage statistics."
    - L449 with self._lock:
      - L450 assign self.total_input_tokens = 0
      - L451 assign self.total_output_tokens = 0
      - L452 assign self.total_cost = 0.0
      - L453 assign self.request_count = 0
- L457 assign _usage_tracker = UsageTracker()
- L460 assign _LOG_PATH_ENV = os.getenv('LLM_RAW_OUTPUT_PATH')
- L461 if _LOG_PATH_ENV:
  - L462 assign _RAW_OUTPUT_PATH = Path(_LOG_PATH_ENV).expanduser()
  - L464 else:
    - L464 assign _RAW_OUTPUT_PATH = Path(__file__).resolve().parents[3] / 'llm_raw_outputs.md'
- L465 assign _RAW_OUTPUT_LOCK = threading.Lock()
- L468 class LLMClient:
  - L469 docstring: "\n    Unified LLM client supporting multiple providers.\n\n    Features:\n    - ..."
  - L487 def __init__self, config: Optional[LLMConfig]=None, provider: Optional[BaseProvider]=None, enable_cache: bool=True, enable_circuit_breaker: bool=True, cache_dir: Optional[Path]=None:
    - L495 assign self.config = config or get_llm_config()
    - L496 assign self._provider = provider or get_provider(self.config)
    - L497 annotated assign self._fallback_provider: Optional[BaseProvider] = None
    - L500 annotated assign self._circuit_breaker: Optional[CircuitBreaker] = None
    - L501 if enable_circuit_breaker:
      - L502 assign self._circuit_breaker = CircuitBreaker(name=f'llm_{self.config.provider.value}', config=CircuitBreakerConfig(failure_threshold=self.config.max_retries + 2, timeout_seconds=60.0))
    - L511 annotated assign self._cache: Optional[ResponseCache] = None
    - L512 if enable_cache:
      - L513 assign default_cache_dir = cache_dir or Path(os.getenv('LLM_CACHE_DIR', '')) if os.getenv('LLM_CACHE_DIR') else None
      - L516 assign self._cache = ResponseCache(max_memory_items=int(os.getenv('LLM_CACHE_MAX_ITEMS', '100')), default_ttl_seconds=float(os.getenv('LLM_CACHE_TTL_SECONDS', '3600')), cache_dir=default_cache_dir)
    - L523 assign self._usage_tracker = UsageTracker()
    - L525 if self.config.fallback_provider:
      - L526 assign fallback_config = LLMConfig(provider=self.config.fallback_provider, model=self.config.fallback_model or self.config.model, api_key=os.getenv(f'{self.config.fallback_provider.value.upper()}_API_KEY'), timeout_seconds=self.config.timeout_seconds, max_retries=self.config.max_retries)
      - L533 assign self._fallback_provider = get_provider(fallback_config)
  - L536 def providerself:
    - L537 docstring: "Get the current provider."
    - L538 return self._provider
  - L541 def modelself:
    - L542 docstring: "Get the current model name."
    - L543 return self.config.model
  - L545 def completeself, messages: List[Dict[str, Any]], model: Optional[str]=None, description: str='llm_call', use_cache: bool=True, cache_ttl: Optional[float]=None, **kwargs: Any:
    - L554 docstring: "\n        Execute a chat completion with retries, caching, and fallback.\n\n    ..."
    - L568 assign model = model or self.config.model
    - L569 assign delay = self.config.retry_delay
    - L570 annotated assign last_exc: Optional[Exception] = None
    - L573 if use_cache and self._cache and (not kwargs.get('stream')):
      - L574 assign cached = self._cache.get(messages, model, **kwargs)
      - L575 if cached:
        - L576 expr logger.debug('llm_cache_hit', extra={'event': 'llm_cache_hit', 'description': description, 'model': model})
        - L584 return cached
    - L587 if self._circuit_breaker and (not self._circuit_breaker.allow_request()):
      - L588 expr logger.warning('llm_circuit_open', extra={'event': 'llm_circuit_open', 'description': description, 'provider': self.config.provider.value})
      - L597 if self._fallback_provider:
        - L598 return self._try_fallback(messages, model, description, **kwargs)
      - L599 raise RuntimeError('AI service is temporarily unavailable due to repeated failures. Please try again in a few minutes. If the problem persists, check your API configuration.')
    - L604 for attempt in range(1, self.config.max_retries + 1):
      - L605 try:
        - L606 expr logger.info('llm_call_start', extra={'event': 'llm_call_start', 'description': description, 'attempt': attempt, 'model': model, 'provider': self.config.provider.value})
        - L617 assign start_time = time.time()
        - L618 assign response = self._provider.chat_completion(messages=messages, model=model, **kwargs)
        - L623 assign latency_ms = (time.time() - start_time) * 1000
        - L625 expr _append_raw_output(description, response)
        - L628 if self._circuit_breaker:
          - L629 expr self._circuit_breaker.record_success()
        - L632 assign usage = response.get('usage', {})
        - L633 assign input_tokens = usage.get('prompt_tokens', 0)
        - L634 assign output_tokens = usage.get('completion_tokens', 0)
        - L635 expr self._usage_tracker.record(model, input_tokens, output_tokens)
        - L636 expr _usage_tracker.record(model, input_tokens, output_tokens)
        - L638 expr logger.info('llm_call_success', extra={'event': 'llm_call_success', 'description': description, 'attempt': attempt, 'model': model, 'provider': self.config.provider.value, 'latency_ms': round(latency_ms, 2), 'input_tokens': input_tokens, 'output_tokens': output_tokens})
        - L653 if use_cache and self._cache and (not kwargs.get('stream')):
          - L654 expr self._cache.set(messages, model, response, cache_ttl, **kwargs)
        - L656 return response
        - L658 except Exception as exc:
          - L659 assign last_exc = exc
          - L662 if self._circuit_breaker:
            - L663 expr self._circuit_breaker.record_failure()
          - L666 if _is_quota_exceeded_error(exc):
            - L667 expr logger.warning('llm_quota_exceeded', extra={'event': 'llm_quota_exceeded', 'description': description, 'provider': self.config.provider.value})
            - L675 break
          - L678 if 'temperature' in kwargs and _is_temperature_unsupported_error(exc):
            - L679 expr logger.info('llm_temperature_override_removed', extra={'event': 'llm_temperature_override_removed', 'description': description, 'model': model})
            - L687 expr kwargs.pop('temperature', None)
            - L688 continue
          - L691 if _is_context_length_error(exc):
            - L692 expr logger.warning('llm_context_length_exceeded', extra={'event': 'llm_context_length_exceeded', 'description': description, 'model': model})
            - L700 break
          - L702 expr logger.warning('llm_call_retry', extra={'event': 'llm_call_retry', 'description': description, 'attempt': attempt, 'max_attempts': self.config.max_retries, 'retry_in': delay if attempt < self.config.max_retries else None, 'error': str(exc), 'error_type': type(exc).__name__})
          - L715 if attempt >= self.config.max_retries:
            - L716 break
          - L718 expr time.sleep(delay)
          - L719 aug assign delay Mult self.config.retry_multiplier
    - L722 annotated assign fallback_exc: Optional[Exception] = None
    - L723 if self._fallback_provider and last_exc:
      - L724 try:
        - L725 return self._try_fallback(messages, model, description, **kwargs)
        - L726 except Exception as fb_exc:
          - L727 assign fallback_exc = fb_exc
          - L728 expr logger.warning('llm_fallback_also_failed', extra={'event': 'llm_fallback_also_failed', 'description': description, 'primary_error': str(last_exc), 'fallback_error': str(fb_exc)})
    - L739 assert last_exc is not None
    - L740 assign fallback_attempted = self._fallback_provider is not None
    - L741 expr logger.error('llm_call_failed', extra={'event': 'llm_call_failed', 'description': description, 'attempts': self.config.max_retries, 'model': model, 'error_type': type(last_exc).__name__, 'fallback_attempted': fallback_attempted, 'fallback_error': str(fallback_exc) if fallback_exc else None}, exc_info=last_exc)
    - L755 if _is_quota_exceeded_error(last_exc):
      - L756 raise RuntimeError('AI service quota exceeded. Please check your API plan and billing details, or wait for the rate limit to reset.')
    - L761 if _is_context_length_error(last_exc):
      - L762 raise RuntimeError('The document is too large for the AI to process. Please try with a smaller document or fewer pages.')
    - L768 assign error_msg = 'AI processing failed. Please try again. If the problem persists, check your API configuration or contact support.'
    - L769 if fallback_exc:
      - L770 assign error_msg = f'AI processing failed (primary and fallback providers both failed). Please try again. If the problem persists, check your API configuration or contact support.'
    - L772 raise RuntimeError(error_msg)
  - L774 def _try_fallbackself, messages: List[Dict[str, Any]], model: str, description: str, **kwargs: Any:
    - L781 docstring: "Try fallback provider."
    - L782 expr logger.info('llm_fallback_attempt', extra={'event': 'llm_fallback_attempt', 'description': description, 'fallback_provider': self.config.fallback_provider.value if self.config.fallback_provider else None})
    - L790 try:
      - L791 assign response = self._fallback_provider.chat_completion(messages=messages, model=self.config.fallback_model, **kwargs)
      - L796 expr _append_raw_output(f'{description}_fallback', response)
      - L799 assign usage = response.get('usage', {})
      - L800 assign input_tokens = usage.get('prompt_tokens', 0)
      - L801 assign output_tokens = usage.get('completion_tokens', 0)
      - L802 assign fallback_model = self.config.fallback_model or model
      - L803 expr self._usage_tracker.record(fallback_model, input_tokens, output_tokens)
      - L804 expr _usage_tracker.record(fallback_model, input_tokens, output_tokens)
      - L806 return response
      - L807 except Exception as fallback_exc:
        - L808 expr logger.error('llm_fallback_failed', extra={'event': 'llm_fallback_failed', 'description': description, 'error': str(fallback_exc)})
        - L816 raise
  - L818 def complete_with_visionself, text: str, images: List[Union[str, bytes, Path]], description: str='vision_call', model: Optional[str]=None, detail: str='auto', **kwargs: Any:
    - L827 docstring: "\n        Execute a chat completion with vision/image inputs.\n\n        Args:\n..."
    - L841 assign model = model or self.config.get_vision_model()
    - L843 assign vision_message = self._provider.prepare_vision_message(text=text, images=images, detail=detail)
    - L849 return self.complete(messages=[vision_message], model=model, description=description, **kwargs)
  - L856 def streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, description: str='llm_stream', **kwargs: Any:
    - L863 docstring: "\n        Execute a streaming chat completion.\n\n        Yields:\n            O..."
    - L869 assign model = model or self.config.model
    - L871 expr logger.info('llm_stream_start', extra={'event': 'llm_stream_start', 'description': description, 'model': model, 'provider': self.config.provider.value})
    - L881 try:
      - L882 for chunk in self._provider.chat_completion_stream(messages=messages, model=model, **kwargs):
        - L887 expr (yield chunk)
      - L889 expr logger.info('llm_stream_complete', extra={'event': 'llm_stream_complete', 'description': description, 'model': model})
      - L897 except Exception as exc:
        - L898 expr logger.error('llm_stream_failed', extra={'event': 'llm_stream_failed', 'description': description, 'error': str(exc)})
        - L906 raise
  - L908 def list_modelsself:
    - L909 docstring: "List available models from the current provider."
    - L910 return self._provider.list_models()
  - L912 def health_checkself:
    - L913 docstring: "Check if the provider is available."
    - L914 return self._provider.health_check()
- L918 annotated assign _client: Optional[LLMClient] = None
- L919 assign _client_lock = threading.Lock()
- L922 def get_llm_clientforce_new: bool=False:
  - L923 docstring: "Get the global LLM client instance."
  - L924 Global
  - L925 with _client_lock:
    - L926 if _client is None or force_new:
      - L927 assign _client = LLMClient()
  - L928 return _client
- L931 def call_completionclient: Any, *, model: str, messages: List[Dict[str, Any]], description: str, timeout: Optional[float]=None, **kwargs: Any:
  - L940 docstring: "\n    Execute a chat completion - backwards compatible with existing code.\n\n  ..."
  - L957 if isinstance(client, LLMClient):
    - L958 return client.complete(messages=messages, model=model, description=description, **kwargs)
  - L967 from ..utils.llm import call_chat_completion as legacy_call
  - L968 return legacy_call(client, model=model, messages=messages, description=description, timeout=timeout, **kwargs)
- L978 def call_completion_with_visionclient: Any, *, text: str, images: List[Union[str, bytes, Path]], model: str, description: str, detail: str='auto', **kwargs: Any:
  - L988 docstring: "\n    Execute a chat completion with vision inputs.\n\n    Args:\n        client..."
  - L1003 if isinstance(client, LLMClient):
    - L1004 return client.complete_with_vision(text=text, images=images, model=model, description=description, detail=detail, **kwargs)
  - L1014 import base64
  - L1016 annotated assign content: List[Dict[str, Any]] = [{'type': 'text', 'text': text}]
  - L1018 for image in images:
    - L1019 if isinstance(image, Path):
      - L1020 assign image_data = base64.b64encode(image.read_bytes()).decode('utf-8')
      - L1021 assign media_type = 'image/png' if image.suffix.lower() == '.png' else 'image/jpeg'
      - L1022 assign image_url = f'data:{media_type};base64,{image_data}'
      - L1023 else:
        - L1023 if isinstance(image, bytes):
          - L1024 assign image_data = base64.b64encode(image).decode('utf-8')
          - L1025 assign image_url = f'data:image/png;base64,{image_data}'
          - L1027 else:
            - L1027 assign image_url = image if image.startswith(('data:', 'http')) else f'data:image/png;base64,{image}'
    - L1029 expr content.append({'type': 'image_url', 'image_url': {'url': image_url, 'detail': detail}})
  - L1034 assign messages = [{'role': 'user', 'content': content}]
  - L1036 from ..utils.llm import call_chat_completion as legacy_call
  - L1037 return legacy_call(client, model=model, messages=messages, description=description, **kwargs)
- L1046 def get_available_models:
  - L1047 docstring: "Get list of available models from the current provider."
  - L1048 assign client = get_llm_client()
  - L1049 return client.list_models()
- L1052 def health_check:
  - L1053 docstring: "Check health of the LLM provider."
  - L1054 assign client = get_llm_client()
  - L1055 assign config = client.config
  - L1057 assign result = {'provider': config.provider.value, 'model': config.model, 'healthy': False, 'fallback_available': config.fallback_provider is not None}
  - L1064 try:
    - L1065 assign result['healthy'] = client.health_check()
    - L1066 if result['healthy']:
      - L1067 assign result['available_models'] = client.list_models()[:5]
    - L1068 except Exception as e:
      - L1069 assign result['error'] = str(e)
  - L1071 return result
- L1076 def _append_raw_outputdescription: str, response: Any:
  - L1077 docstring: "Append the raw LLM response to a Markdown log file."
  - L1078 assign timestamp = datetime.utcnow().isoformat(timespec='seconds') + 'Z'
  - L1079 assign entry = _coerce_jsonable(response)
  - L1081 try:
    - L1082 with _RAW_OUTPUT_LOCK:
      - L1083 expr _RAW_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
      - L1084 with _RAW_OUTPUT_PATH.open('a', encoding='utf-8') as handle:
        - L1085 expr handle.write(f'## {timestamp} - {description}\n\n')
        - L1086 expr handle.write('```json\n')
        - L1087 expr handle.write(json.dumps(entry, indent=2))
        - L1088 expr handle.write('\n```\n\n')
    - L1089 except Exception as exc:
      - L1090 expr logger.debug('llm_raw_output_log_failed', extra={'event': 'llm_raw_output_log_failed'}, exc_info=(type(exc), exc, exc.__traceback__))
- L1097 def _coerce_jsonablevalue: Any:
  - L1098 docstring: "Best-effort conversion to JSON-serializable data."
  - L1099 if isinstance(value, (str, int, float, bool)) or value is None:
    - L1100 return value
  - L1102 if isinstance(value, dict):
    - L1103 return {str(k): _coerce_jsonable(v) for k, v in value.items()}
  - L1105 if isinstance(value, (list, tuple, set)):
    - L1106 return [_coerce_jsonable(v) for v in value]
  - L1108 for attr in ('model_dump', 'to_dict', 'dict'):
    - L1109 assign method = getattr(value, attr, None)
    - L1110 if callable(method):
      - L1111 try:
        - L1112 return _coerce_jsonable(method())
        - L1113 except Exception:
          - L1114 continue
  - L1116 return repr(value)
- L1119 def _is_quota_exceeded_errorexc: BaseException:
  - L1120 docstring: "Check if exception represents a quota/rate limit error."
  - L1121 assign detail = str(exc).lower()
  - L1123 if 'insufficient_quota' in detail:
    - L1124 return True
  - L1125 if 'quota' in detail and ('exceeded' in detail or 'insufficient' in detail):
    - L1126 return True
  - L1127 if 'rate_limit' in detail or 'ratelimit' in detail:
    - L1128 return True
  - L1131 assign body = getattr(exc, 'body', None)
  - L1132 if isinstance(body, dict):
    - L1133 assign error = body.get('error', {})
    - L1134 if isinstance(error, dict):
      - L1135 assign code = str(error.get('code', '')).lower()
      - L1136 if code in ('insufficient_quota', 'rate_limit_exceeded'):
        - L1137 return True
  - L1139 return False
- L1142 def _is_temperature_unsupported_errorexc: BaseException:
  - L1143 docstring: "Check if exception is about unsupported temperature."
  - L1144 assign detail = str(exc).lower()
  - L1145 return 'temperature' in detail and 'unsupported' in detail
- L1148 def _is_context_length_errorexc: BaseException:
  - L1149 docstring: "Check if exception is about context length exceeded."
  - L1150 assign detail = str(exc).lower()
  - L1152 assign context_indicators = ['context_length_exceeded', 'maximum context length', 'token limit', 'too many tokens', 'context window', 'max_tokens', 'input too long']
  - L1162 return any((indicator in detail for indicator in context_indicators))
- L1165 def _is_invalid_request_errorexc: BaseException:
  - L1166 docstring: "Check if exception is an invalid request error."
  - L1167 assign detail = str(exc).lower()
  - L1169 assign invalid_indicators = ['invalid_request', 'invalid request', 'bad request', 'malformed', 'invalid_api_key', 'invalid api key']
  - L1178 return any((indicator in detail for indicator in invalid_indicators))
- L1181 def get_global_usage_stats:
  - L1182 docstring: "Get global token usage statistics."
  - L1183 return _usage_tracker.get_stats()
- L1186 def reset_global_usage_stats:
  - L1187 docstring: "Reset global token usage statistics."
  - L1188 expr _usage_tracker.reset()

## backend\app\services\llm\config.py
- L2 docstring: "\nLLM Configuration Module.\n\nSupports multiple providers via environment varia..."
- L18 from __future__ import annotations
- L20 import os
- L21 import logging
- L22 from dataclasses import dataclass, field
- L23 from enum import Enum
- L24 from typing import Any, Dict, List, Optional
- L26 assign logger = logging.getLogger('neura.llm.config')
- L29 class LLMProvider(str, Enum):
  - L30 docstring: "Supported LLM providers."
  - L31 assign OPENAI = 'openai'
  - L32 assign OLLAMA = 'ollama'
  - L33 assign DEEPSEEK = 'deepseek'
  - L34 assign ANTHROPIC = 'anthropic'
  - L35 assign AZURE = 'azure'
  - L36 assign GEMINI = 'gemini'
  - L37 assign CUSTOM = 'custom'
- L41 annotated assign DEFAULT_MODELS: Dict[LLMProvider, str] = {LLMProvider.OPENAI: 'gpt-5', LLMProvider.OLLAMA: 'llama3.2', LLMProvider.DEEPSEEK: 'deepseek-chat', LLMProvider.ANTHROPIC: 'claude-3-5-sonnet-20241022', LLMProvider.AZURE: 'gpt-4o', LLMProvider.GEMINI: 'gemini-1.5-pro', LLMProvider.CUSTOM: 'gpt-5'}
- L52 annotated assign VISION_MODELS: Dict[LLMProvider, List[str]] = {LLMProvider.OPENAI: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4-vision-preview', 'gpt-5'], LLMProvider.OLLAMA: ['llava', 'llava:13b', 'llava:34b', 'bakllava', 'moondream', 'llama3.2-vision'], LLMProvider.DEEPSEEK: ['deepseek-vl', 'deepseek-vl2'], LLMProvider.ANTHROPIC: ['claude-3-5-sonnet-20241022', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'], LLMProvider.AZURE: ['gpt-4o', 'gpt-4-turbo', 'gpt-4-vision-preview'], LLMProvider.GEMINI: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-2.0-flash-exp'], LLMProvider.CUSTOM: []}
- L63 annotated assign DOCUMENT_ANALYSIS_MODELS: Dict[LLMProvider, str] = {LLMProvider.OPENAI: 'gpt-5', LLMProvider.OLLAMA: 'qwen2.5:32b', LLMProvider.DEEPSEEK: 'deepseek-chat', LLMProvider.ANTHROPIC: 'claude-3-5-sonnet-20241022', LLMProvider.AZURE: 'gpt-4o', LLMProvider.GEMINI: 'gemini-1.5-pro', LLMProvider.CUSTOM: 'gpt-5'}
- L74 annotated assign CODE_GENERATION_MODELS: Dict[LLMProvider, str] = {LLMProvider.OPENAI: 'gpt-5', LLMProvider.OLLAMA: 'deepseek-coder-v2:16b', LLMProvider.DEEPSEEK: 'deepseek-coder', LLMProvider.ANTHROPIC: 'claude-3-5-sonnet-20241022', LLMProvider.AZURE: 'gpt-4o', LLMProvider.GEMINI: 'gemini-1.5-pro', LLMProvider.CUSTOM: 'gpt-5'}
- L86 class LLMConfig:
  - L87 docstring: "Configuration for LLM provider."
  - L89 annotated assign provider: LLMProvider
  - L90 annotated assign model: str
  - L91 annotated assign api_key: Optional[str] = None
  - L92 annotated assign base_url: Optional[str] = None
  - L95 annotated assign azure_endpoint: Optional[str] = None
  - L96 annotated assign azure_deployment: Optional[str] = None
  - L97 annotated assign azure_api_version: str = '2024-02-15-preview'
  - L100 annotated assign timeout_seconds: float = 120.0
  - L101 annotated assign max_retries: int = 3
  - L102 annotated assign retry_delay: float = 1.5
  - L103 annotated assign retry_multiplier: float = 2.0
  - L106 annotated assign temperature: Optional[float] = None
  - L107 annotated assign max_tokens: Optional[int] = None
  - L110 annotated assign supports_vision: bool = False
  - L111 annotated assign supports_function_calling: bool = True
  - L112 annotated assign supports_streaming: bool = True
  - L115 annotated assign fallback_provider: Optional[LLMProvider] = None
  - L116 annotated assign fallback_model: Optional[str] = None
  - L119 annotated assign extra_options: Dict[str, Any] = field(default_factory=dict)
  - L121 def __post_init__self:
    - L122 docstring: "Validate and finalize configuration."
    - L124 assign vision_models = VISION_MODELS.get(self.provider, [])
    - L125 assign self.supports_vision = any((vm in self.model or self.model in vm for vm in vision_models))
    - L130 expr logger.info('llm_config_initialized', extra={'event': 'llm_config_initialized', 'provider': self.provider.value, 'model': self.model, 'base_url': self.base_url, 'supports_vision': self.supports_vision})
  - L142 def from_envcls:
    - L143 docstring: "Create configuration from environment variables."
    - L145 assign provider_str = os.getenv('LLM_PROVIDER', '').lower().strip()
    - L148 if not provider_str:
      - L149 if os.getenv('OPENAI_API_KEY'):
        - L150 assign provider_str = 'openai'
        - L151 else:
          - L151 if os.getenv('OLLAMA_BASE_URL') or _check_ollama_running():
            - L152 assign provider_str = 'ollama'
            - L153 else:
              - L153 if os.getenv('DEEPSEEK_API_KEY'):
                - L154 assign provider_str = 'deepseek'
                - L155 else:
                  - L155 if os.getenv('ANTHROPIC_API_KEY'):
                    - L156 assign provider_str = 'anthropic'
                    - L157 else:
                      - L157 if os.getenv('AZURE_OPENAI_KEY'):
                        - L158 assign provider_str = 'azure'
                        - L159 else:
                          - L159 if os.getenv('GOOGLE_API_KEY'):
                            - L160 assign provider_str = 'gemini'
                            - L163 else:
                              - L163 assign provider_str = 'openai'
    - L165 try:
      - L166 assign provider = LLMProvider(provider_str)
      - L167 except ValueError:
        - L168 expr logger.warning('invalid_llm_provider', extra={'event': 'invalid_llm_provider', 'provider': provider_str})
        - L172 assign provider = LLMProvider.OPENAI
    - L175 assign model = os.getenv('LLM_MODEL') or os.getenv(f'{provider.value.upper()}_MODEL') or os.getenv('OPENAI_MODEL') or DEFAULT_MODELS.get(provider, 'gpt-4o')
    - L183 assign api_key = os.getenv('LLM_API_KEY') or os.getenv(f'{provider.value.upper()}_API_KEY') or os.getenv('OPENAI_API_KEY')
    - L190 assign base_url = os.getenv('LLM_BASE_URL')
    - L191 if not base_url:
      - L192 if provider == LLMProvider.OLLAMA:
        - L193 assign base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        - L194 else:
          - L194 if provider == LLMProvider.DEEPSEEK:
            - L195 assign base_url = 'https://api.deepseek.com'
            - L196 else:
              - L196 if provider == LLMProvider.CUSTOM:
                - L197 assign base_url = os.getenv('CUSTOM_LLM_BASE_URL')
    - L200 assign azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
    - L201 assign azure_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT')
    - L202 assign azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')
    - L203 if provider == LLMProvider.AZURE:
      - L204 assign api_key = os.getenv('AZURE_OPENAI_KEY') or api_key
    - L207 assign timeout = float(os.getenv('LLM_TIMEOUT_SECONDS', os.getenv('OPENAI_REQUEST_TIMEOUT_SECONDS', '120')))
    - L208 assign max_retries = int(os.getenv('LLM_MAX_RETRIES', os.getenv('OPENAI_MAX_ATTEMPTS', '3')))
    - L209 assign retry_delay = float(os.getenv('LLM_RETRY_DELAY', os.getenv('OPENAI_BACKOFF_SECONDS', '1.5')))
    - L210 assign retry_multiplier = float(os.getenv('LLM_RETRY_MULTIPLIER', os.getenv('OPENAI_BACKOFF_MULTIPLIER', '2.0')))
    - L213 assign temperature = os.getenv('LLM_TEMPERATURE')
    - L214 assign max_tokens = os.getenv('LLM_MAX_TOKENS')
    - L217 assign fallback_provider_str = os.getenv('LLM_FALLBACK_PROVIDER', '').lower().strip()
    - L218 assign fallback_provider = None
    - L219 assign fallback_model = None
    - L220 if fallback_provider_str:
      - L221 try:
        - L222 assign fallback_provider = LLMProvider(fallback_provider_str)
        - L223 assign fallback_model = os.getenv('LLM_FALLBACK_MODEL', DEFAULT_MODELS.get(fallback_provider))
        - L224 except ValueError:
          - L225 pass
    - L227 assign force_gpt5 = os.getenv('NEURA_FORCE_GPT5', 'true').lower().strip() in {'1', 'true', 'yes'}
    - L228 if force_gpt5:
      - L229 if provider != LLMProvider.OPENAI:
        - L230 expr logger.warning('llm_provider_overridden', extra={'event': 'llm_provider_overridden', 'requested': provider.value, 'forced': LLMProvider.OPENAI.value})
      - L238 assign provider = LLMProvider.OPENAI
      - L239 assign model = 'gpt-5'
      - L240 assign api_key = os.getenv('OPENAI_API_KEY') or api_key
      - L241 assign base_url = os.getenv('OPENAI_BASE_URL') or os.getenv('LLM_BASE_URL')
      - L242 assign azure_endpoint = None
      - L243 assign azure_deployment = None
      - L244 assign fallback_provider = None
      - L245 assign fallback_model = None
    - L247 return cls(provider=provider, model=model, api_key=api_key, base_url=base_url, azure_endpoint=azure_endpoint, azure_deployment=azure_deployment, azure_api_version=azure_api_version, timeout_seconds=timeout, max_retries=max_retries, retry_delay=retry_delay, retry_multiplier=retry_multiplier, temperature=float(temperature) if temperature else None, max_tokens=int(max_tokens) if max_tokens else None, fallback_provider=fallback_provider, fallback_model=fallback_model)
  - L265 def get_vision_modelself:
    - L266 docstring: "Get the recommended vision model for this provider."
    - L267 assign vision_models = VISION_MODELS.get(self.provider, [])
    - L268 if self.model in vision_models or any((self.model in vm for vm in vision_models)):
      - L269 return self.model
    - L270 return vision_models[0] if vision_models else self.model
  - L272 def get_document_analysis_modelself:
    - L273 docstring: "Get the recommended model for document analysis."
    - L274 return DOCUMENT_ANALYSIS_MODELS.get(self.provider, self.model)
  - L276 def get_code_generation_modelself:
    - L277 docstring: "Get the recommended model for code/SQL generation."
    - L278 return CODE_GENERATION_MODELS.get(self.provider, self.model)
- L281 def _check_ollama_running:
  - L282 docstring: "Check if Ollama is running locally."
  - L283 try:
    - L284 import urllib.request
    - L285 assign req = urllib.request.Request('http://localhost:11434/api/tags', method='GET')
    - L289 with urllib.request.urlopen(req, timeout=2) as resp:
      - L290 return resp.status == 200
    - L291 except Exception:
      - L292 return False
- L296 annotated assign _config: Optional[LLMConfig] = None
- L299 def get_llm_configforce_reload: bool=False:
  - L300 docstring: "Get the global LLM configuration."
  - L301 Global
  - L302 if _config is None or force_reload:
    - L303 assign _config = LLMConfig.from_env()
  - L304 return _config

## backend\app\services\llm\document_extractor.py
- L2 docstring: "\nEnhanced Document Extraction Module.\n\nCombines traditional extraction method..."
- L12 from __future__ import annotations
- L14 import json
- L15 import logging
- L16 import re
- L17 from dataclasses import dataclass, field
- L18 from pathlib import Path
- L19 from typing import Any, Dict, List, Optional, Tuple, Union
- L21 assign logger = logging.getLogger('neura.llm.document_extractor')
- L25 class ExtractedTable:
  - L26 docstring: "Extracted table from a document."
  - L27 annotated assign id: str
  - L28 annotated assign title: Optional[str]
  - L29 annotated assign headers: List[str]
  - L30 annotated assign rows: List[List[Any]]
  - L31 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L32 annotated assign confidence: float = 1.0
- L36 class ExtractedContent:
  - L37 docstring: "Complete extracted content from a document."
  - L38 annotated assign text: str
  - L39 annotated assign tables: List[ExtractedTable]
  - L40 annotated assign metadata: Dict[str, Any]
  - L41 annotated assign structure: Dict[str, Any]
  - L42 annotated assign warnings: List[str] = field(default_factory=list)
- L46 class FieldSchema:
  - L47 docstring: "Schema for an extracted field."
  - L48 annotated assign name: str
  - L49 annotated assign data_type: str
  - L50 annotated assign sample_values: List[Any]
  - L51 annotated assign nullable: bool = True
  - L52 annotated assign description: Optional[str] = None
- L55 class EnhancedDocumentExtractor:
  - L56 docstring: "\n    Enhanced document extractor with AI-powered understanding.\n\n    Combines..."
  - L66 def __init__self, use_vlm: bool=True, use_ocr: bool=True, max_pages: int=50, max_tables: int=100:
    - L73 assign self.use_vlm = use_vlm
    - L74 assign self.use_ocr = use_ocr
    - L75 assign self.max_pages = max_pages
    - L76 assign self.max_tables = max_tables
    - L79 assign self._vlm = None
  - L82 def vlmself:
    - L83 docstring: "Get VLM instance (lazy loaded)."
    - L84 if self._vlm is None and self.use_vlm:
      - L85 try:
        - L86 from .vision import get_vlm
        - L87 assign self._vlm = get_vlm()
        - L88 except Exception as e:
          - L89 expr logger.warning(f'VLM not available: {e}')
          - L90 assign self._vlm = None
    - L91 return self._vlm
  - L93 def extractself, file_path: Union[str, Path], extraction_mode: str='auto':
    - L98 docstring: "\n        Extract content from a document.\n\n        Args:\n            file_pa..."
    - L108 assign file_path = Path(file_path)
    - L110 if not file_path.exists():
      - L111 raise FileNotFoundError(f'File not found: {file_path}')
    - L113 assign suffix = file_path.suffix.lower()
    - L115 if suffix == '.pdf':
      - L116 return self._extract_pdf(file_path, extraction_mode)
      - L117 else:
        - L117 if suffix in ('.xlsx', '.xls', '.xlsm'):
          - L118 return self._extract_excel(file_path, extraction_mode)
          - L119 else:
            - L119 if suffix == '.csv':
              - L120 return self._extract_csv(file_path)
              - L121 else:
                - L121 if suffix in ('.png', '.jpg', '.jpeg', '.tiff', '.bmp'):
                  - L122 return self._extract_image(file_path, extraction_mode)
                  - L124 else:
                    - L124 raise ValueError(f'Unsupported file format: {suffix}')
  - L126 def _extract_pdfself, file_path: Path, mode: str:
    - L131 docstring: "Extract content from PDF."
    - L132 try:
      - L133 import fitz
      - L134 except ImportError:
        - L135 raise RuntimeError('PyMuPDF is required for PDF extraction. Install with: pip install pymupdf')
    - L137 assign doc = fitz.open(file_path)
    - L138 assign warnings = []
    - L140 if doc.page_count > self.max_pages:
      - L141 expr warnings.append(f'PDF has {doc.page_count} pages, only processing first {self.max_pages}')
    - L143 assign text_content = []
    - L144 annotated assign tables: List[ExtractedTable] = []
    - L145 assign metadata = {'filename': file_path.name, 'page_count': doc.page_count, 'format': 'pdf'}
    - L151 for (page_num, page) in enumerate(doc):
      - L152 if page_num >= self.max_pages:
        - L153 break
      - L156 assign page_text = page.get_text('text')
      - L157 expr text_content.append(f'--- Page {page_num + 1} ---\n{page_text}')
      - L160 assign page_tables = self._extract_pdf_tables(page, page_num)
      - L161 expr tables.extend(page_tables)
      - L164 if self.vlm and mode in ('auto', 'comprehensive'):
        - L165 try:
          - L166 assign pix = page.get_pixmap(dpi=150)
          - L167 assign img_bytes = pix.tobytes('png')
          - L168 assign vlm_result = self.vlm.extract_tables(img_bytes)
          - L171 for (i, vt) in enumerate(vlm_result.tables):
            - L172 assign table_id = f'page{page_num + 1}_vlm_{i + 1}'
            - L173 if not self._table_exists(tables, vt.get('headers', [])):
              - L174 expr tables.append(ExtractedTable(id=table_id, title=vt.get('title'), headers=vt.get('headers', []), rows=vt.get('rows', []), metadata={'source': 'vlm', 'page': page_num + 1}, confidence=vlm_result.confidence))
          - L182 except Exception as e:
            - L183 expr logger.warning(f'VLM extraction failed for page {page_num + 1}: {e}')
      - L185 if len(tables) >= self.max_tables:
        - L186 expr warnings.append(f'Table limit ({self.max_tables}) reached')
        - L187 break
    - L189 expr doc.close()
    - L192 assign structure = self._analyze_structure('\n\n'.join(text_content))
    - L194 return ExtractedContent(text='\n\n'.join(text_content), tables=tables, metadata=metadata, structure=structure, warnings=warnings)
  - L202 def _extract_pdf_tablesself, page, page_num: int:
    - L207 docstring: "Extract tables from a PDF page."
    - L208 assign tables = []
    - L210 try:
      - L212 assign page_tables = page.find_tables()
      - L214 for (i, table) in enumerate(page_tables):
        - L215 if table.row_count == 0:
          - L216 continue
        - L219 assign data = table.extract()
        - L220 if not data or len(data) < 2:
          - L221 continue
        - L224 assign headers = [str(cell or '').strip() for cell in data[0]]
        - L227 assign rows = []
        - L228 for row in data[1:]:
          - L229 assign normalized_row = []
          - L230 for (j, cell) in enumerate(row):
            - L231 if j < len(headers):
              - L232 expr normalized_row.append(str(cell or '').strip())
          - L234 while len(normalized_row) < len(headers):
            - L235 expr normalized_row.append('')
          - L236 expr rows.append(normalized_row)
        - L238 assign table_id = f'page{page_num + 1}_table_{i + 1}'
        - L239 expr tables.append(ExtractedTable(id=table_id, title=None, headers=headers, rows=rows, metadata={'source': 'pymupdf', 'page': page_num + 1}, confidence=0.9))
      - L248 except Exception as e:
        - L249 expr logger.warning(f'Table extraction failed for page {page_num + 1}: {e}')
    - L251 return tables
  - L253 def _extract_excelself, file_path: Path, mode: str:
    - L258 docstring: "Extract content from Excel file."
    - L259 try:
      - L260 import openpyxl
      - L261 except ImportError:
        - L262 raise RuntimeError('openpyxl is required for Excel extraction. Install with: pip install openpyxl')
    - L264 assign workbook = openpyxl.load_workbook(file_path, data_only=True)
    - L265 assign warnings = []
    - L267 annotated assign tables: List[ExtractedTable] = []
    - L268 assign text_parts = []
    - L269 assign metadata = {'filename': file_path.name, 'sheet_count': len(workbook.sheetnames), 'format': 'excel', 'sheets': workbook.sheetnames}
    - L276 for sheet_name in workbook.sheetnames:
      - L277 assign sheet = workbook[sheet_name]
      - L279 if sheet.max_row == 0 or sheet.max_column == 0:
        - L280 continue
      - L283 assign headers = []
      - L284 assign rows = []
      - L286 for (row_num, row) in enumerate(sheet.iter_rows(values_only=True)):
        - L288 assign cleaned_row = [str(cell) if cell is not None else '' for cell in row]
        - L290 if row_num == 0:
          - L291 assign headers = cleaned_row
          - L294 else:
            - L294 if any((cell.strip() for cell in cleaned_row)):
              - L296 while len(cleaned_row) < len(headers):
                - L297 expr cleaned_row.append('')
              - L298 expr rows.append(cleaned_row[:len(headers)])
      - L300 if headers and rows:
        - L301 expr tables.append(ExtractedTable(id=f'sheet_{sheet_name}', title=sheet_name, headers=headers, rows=rows, metadata={'source': 'openpyxl', 'sheet': sheet_name}, confidence=1.0))
      - L311 expr text_parts.append(f'=== Sheet: {sheet_name} ===')
      - L312 expr text_parts.append('\t'.join(headers))
      - L313 for row in rows[:10]:
        - L314 expr text_parts.append('\t'.join(row))
      - L315 if len(rows) > 10:
        - L316 expr text_parts.append(f'... ({len(rows) - 10} more rows)')
    - L318 expr workbook.close()
    - L320 return ExtractedContent(text='\n'.join(text_parts), tables=tables, metadata=metadata, structure={'type': 'spreadsheet', 'sheets': workbook.sheetnames}, warnings=warnings)
  - L328 def _extract_csvself, file_path: Path:
    - L329 docstring: "Extract content from CSV file."
    - L330 import csv
    - L332 assign tables = []
    - L333 assign warnings = []
    - L335 with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
      - L337 assign sample = f.read(4096)
      - L338 expr f.seek(0)
      - L340 try:
        - L341 assign dialect = csv.Sniffer().sniff(sample)
        - L342 except csv.Error:
          - L343 assign dialect = csv.excel
      - L345 assign reader = csv.reader(f, dialect)
      - L346 assign rows_list = list(reader)
    - L348 if not rows_list:
      - L349 return ExtractedContent(text='', tables=[], metadata={'filename': file_path.name, 'format': 'csv'}, structure={'type': 'empty'}, warnings=['CSV file is empty'])
    - L357 assign headers = rows_list[0] if rows_list else []
    - L358 assign rows = rows_list[1:] if len(rows_list) > 1 else []
    - L361 assign max_cols = max((len(row) for row in rows_list)) if rows_list else 0
    - L362 while len(headers) < max_cols:
      - L363 expr headers.append(f'Column_{len(headers) + 1}')
    - L365 assign normalized_rows = []
    - L366 for row in rows:
      - L367 assign normalized = list(row)
      - L368 while len(normalized) < len(headers):
        - L369 expr normalized.append('')
      - L370 expr normalized_rows.append(normalized[:len(headers)])
    - L372 expr tables.append(ExtractedTable(id='csv_table', title=file_path.stem, headers=headers, rows=normalized_rows, metadata={'source': 'csv'}, confidence=1.0))
    - L382 assign text_parts = ['\t'.join(headers)]
    - L383 for row in normalized_rows[:20]:
      - L384 expr text_parts.append('\t'.join(row))
    - L385 if len(normalized_rows) > 20:
      - L386 expr text_parts.append(f'... ({len(normalized_rows) - 20} more rows)')
    - L388 return ExtractedContent(text='\n'.join(text_parts), tables=tables, metadata={'filename': file_path.name, 'format': 'csv', 'row_count': len(normalized_rows), 'column_count': len(headers)}, structure={'type': 'tabular'}, warnings=warnings)
  - L401 def _extract_imageself, file_path: Path, mode: str:
    - L406 docstring: "Extract content from image using VLM."
    - L407 if not self.vlm:
      - L408 raise RuntimeError('VLM is required for image extraction but not available')
    - L411 assign result = self.vlm.analyze_document(file_path, analysis_type=mode)
    - L413 assign tables = []
    - L414 for (i, table_data) in enumerate(result.tables):
      - L415 expr tables.append(ExtractedTable(id=f'image_table_{i + 1}', title=table_data.get('title'), headers=table_data.get('headers', []), rows=table_data.get('rows', []), metadata={'source': 'vlm'}, confidence=0.8))
    - L424 return ExtractedContent(text=result.text_content, tables=tables, metadata={'filename': file_path.name, 'format': 'image', **result.metadata}, structure=result.structure, warnings=[])
  - L436 def _analyze_structureself, text: str:
    - L437 docstring: "Analyze document structure from text."
    - L438 assign structure = {'type': 'document', 'has_headers': False, 'has_lists': False, 'has_tables': False, 'sections': []}
    - L447 assign header_pattern = re.compile('^[A-Z][A-Za-z\\s]+:?\\s*$', re.MULTILINE)
    - L448 assign headers = header_pattern.findall(text)
    - L449 assign structure['has_headers'] = len(headers) > 0
    - L450 assign structure['sections'] = [h.strip().rstrip(':') for h in headers[:10]]
    - L453 assign list_pattern = re.compile('^[\\s]*[-\u2022*]\\s+.+$', re.MULTILINE)
    - L454 assign structure['has_lists'] = bool(list_pattern.search(text))
    - L457 assign table_pattern = re.compile('\\|.+\\|', re.MULTILINE)
    - L458 assign structure['has_tables'] = bool(table_pattern.search(text))
    - L460 return structure
  - L462 def _table_existsself, tables: List[ExtractedTable], headers: List[str]:
    - L467 docstring: "Check if a table with similar headers already exists."
    - L468 if not headers:
      - L469 return True
    - L471 for table in tables:
      - L472 if len(table.headers) == len(headers):
        - L474 if all((h1.lower().strip() == h2.lower().strip() for h1, h2 in zip(table.headers, headers))):
          - L478 return True
    - L479 return False
  - L481 def infer_schemaself, table: ExtractedTable:
    - L485 docstring: "Infer schema for a table's columns."
    - L486 assign schemas = []
    - L488 for (col_idx, header) in enumerate(table.headers):
      - L490 assign sample_values = []
      - L491 for row in table.rows[:100]:
        - L492 if col_idx < len(row) and row[col_idx]:
          - L493 expr sample_values.append(row[col_idx])
      - L496 assign data_type = self._infer_column_type(sample_values)
      - L498 expr schemas.append(FieldSchema(name=header, data_type=data_type, sample_values=sample_values[:5], nullable=any((not v for v in sample_values))))
    - L505 return schemas
  - L507 def _infer_column_typeself, values: List[Any]:
    - L508 docstring: "Infer the data type of a column from sample values."
    - L509 if not values:
      - L510 return 'text'
    - L513 assign numeric_count = 0
    - L514 assign date_count = 0
    - L515 assign bool_count = 0
    - L517 assign date_patterns = ['^\\d{4}-\\d{2}-\\d{2}$', '^\\d{2}/\\d{2}/\\d{4}$', '^\\d{2}-\\d{2}-\\d{4}$', '^\\d{1,2}/\\d{1,2}/\\d{2,4}$']
    - L524 for value in values:
      - L525 assign value_str = str(value).strip()
      - L527 if not value_str:
        - L528 continue
      - L531 try:
        - L532 assign cleaned = re.sub('[$,% ]', '', value_str)
        - L533 expr float(cleaned)
        - L534 aug assign numeric_count Add 1
        - L535 continue
        - L536 except (ValueError, TypeError):
          - L537 pass
      - L540 for pattern in date_patterns:
        - L541 if re.match(pattern, value_str):
          - L542 aug assign date_count Add 1
          - L543 break
      - L546 if value_str.lower() in ('true', 'false', 'yes', 'no', '1', '0'):
        - L547 aug assign bool_count Add 1
    - L549 assign total = len([v for v in values if str(v).strip()])
    - L550 if total == 0:
      - L551 return 'text'
    - L554 assign threshold = 0.7
    - L556 if date_count / total >= threshold:
      - L557 return 'datetime'
    - L558 if numeric_count / total >= threshold:
      - L559 return 'numeric'
    - L560 if bool_count / total >= threshold:
      - L561 return 'boolean'
    - L563 return 'text'
- L568 def extract_documentfile_path: Union[str, Path], use_vlm: bool=True:
  - L572 docstring: "Quick function to extract content from a document."
  - L573 assign extractor = EnhancedDocumentExtractor(use_vlm=use_vlm)
  - L574 return extractor.extract(file_path)
- L577 def extract_tablesfile_path: Union[str, Path]:
  - L580 docstring: "Quick function to extract tables from a document."
  - L581 assign extractor = EnhancedDocumentExtractor(use_vlm=True)
  - L582 assign result = extractor.extract(file_path, extraction_mode='tables_only')
  - L583 return result.tables

## backend\app\services\llm\providers.py
- L2 docstring: "\nLLM Provider Implementations.\n\nEach provider implements a common interface f..."
- L11 from __future__ import annotations
- L13 import base64
- L14 import json
- L15 import logging
- L16 import os
- L17 import time
- L18 from abc import ABC, abstractmethod
- L19 from pathlib import Path
- L20 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
- L22 from .config import LLMConfig, LLMProvider
- L24 assign logger = logging.getLogger('neura.llm.providers')
- L25 assign _FORCE_GPT5 = os.getenv('NEURA_FORCE_GPT5', 'true').lower() in {'1', 'true', 'yes'}
- L28 def _force_gpt5model_name: Optional[str]:
  - L29 if not _FORCE_GPT5:
    - L30 return str(model_name or 'gpt-5').strip() or 'gpt-5'
  - L31 assign normalized = str(model_name or '').strip()
  - L32 if normalized.lower().startswith('gpt-5'):
    - L33 return normalized
  - L34 if normalized:
    - L35 expr logger.warning('llm_model_overridden', extra={'event': 'llm_model_overridden', 'requested': normalized, 'forced': 'gpt-5'})
  - L39 return 'gpt-5'
- L42 class BaseProvider(ABC):
  - L43 docstring: "Abstract base class for LLM providers."
  - L45 def __init__self, config: LLMConfig:
    - L46 assign self.config = config
    - L47 annotated assign self._client: Any = None
  - L50 def get_clientself:
    - L51 docstring: "Get or create the provider client."
    - L52 pass
  - L55 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L61 docstring: "Execute a chat completion request."
    - L62 pass
  - L65 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L71 docstring: "Execute a streaming chat completion request."
    - L72 pass
  - L75 def list_modelsself:
    - L76 docstring: "List available models."
    - L77 pass
  - L80 def health_checkself:
    - L81 docstring: "Check if the provider is available."
    - L82 pass
  - L84 def supports_visionself, model: Optional[str]=None:
    - L85 docstring: "Check if the model supports vision inputs."
    - L86 return self.config.supports_vision
  - L88 def prepare_vision_messageself, text: str, images: List[Union[str, bytes, Path]], detail: str='auto':
    - L94 docstring: "Prepare a message with vision content."
    - L95 annotated assign content: List[Dict[str, Any]] = [{'type': 'text', 'text': text}]
    - L97 for image in images:
      - L98 if isinstance(image, Path):
        - L99 assign image_data = base64.b64encode(image.read_bytes()).decode('utf-8')
        - L100 assign media_type = 'image/png' if image.suffix.lower() == '.png' else 'image/jpeg'
        - L101 assign image_url = f'data:{media_type};base64,{image_data}'
        - L102 else:
          - L102 if isinstance(image, bytes):
            - L103 assign image_data = base64.b64encode(image).decode('utf-8')
            - L104 assign image_url = f'data:image/png;base64,{image_data}'
            - L107 else:
              - L107 if image.startswith('data:') or image.startswith('http'):
                - L108 assign image_url = image
                - L110 else:
                  - L110 assign image_url = f'data:image/png;base64,{image}'
      - L112 expr content.append({'type': 'image_url', 'image_url': {'url': image_url, 'detail': detail}})
    - L117 return {'role': 'user', 'content': content}
- L120 class OpenAIProvider(BaseProvider):
  - L121 docstring: "OpenAI API provider (also works with OpenAI-compatible endpoints)."
  - L123 def _use_responsesself, model: str:
    - L124 assign force = os.getenv('OPENAI_USE_RESPONSES', '').lower() in {'1', 'true', 'yes'}
    - L125 return force or str(model or '').lower().startswith('gpt-5')
  - L127 def get_clientself:
    - L128 if self._client is not None:
      - L129 return self._client
    - L131 try:
      - L132 from openai import OpenAI
      - L133 except ImportError:
        - L134 raise RuntimeError('openai package is required. Install with: pip install openai>=1.0.0')
    - L136 annotated assign client_kwargs: Dict[str, Any] = {}
    - L138 if self.config.api_key:
      - L139 assign client_kwargs['api_key'] = self.config.api_key
    - L141 if self.config.base_url:
      - L142 assign client_kwargs['base_url'] = self.config.base_url
    - L144 if self.config.timeout_seconds:
      - L145 assign client_kwargs['timeout'] = self.config.timeout_seconds
    - L147 assign self._client = OpenAI(**client_kwargs)
    - L148 return self._client
  - L150 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L156 assign client = self.get_client()
    - L157 assign model = _force_gpt5(model or self.config.model)
    - L160 if self.config.temperature is not None and 'temperature' not in kwargs:
      - L161 assign kwargs['temperature'] = self.config.temperature
    - L162 if self.config.max_tokens is not None and 'max_tokens' not in kwargs:
      - L163 assign kwargs['max_tokens'] = self.config.max_tokens
    - L165 if self._use_responses(model):
      - L166 assign payload_kwargs = _openai_prepare_responses_kwargs(kwargs)
      - L167 assign response = client.responses.create(model=model, input=_openai_messages_to_responses_input(messages), **payload_kwargs)
      - L172 return _openai_responses_to_dict(response)
    - L174 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L180 return _openai_response_to_dict(response)
  - L182 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L188 assign client = self.get_client()
    - L189 assign model = _force_gpt5(model or self.config.model)
    - L191 if self._use_responses(model):
      - L192 assign payload_kwargs = _openai_prepare_responses_kwargs(kwargs)
      - L193 assign response = client.responses.create(model=model, input=_openai_messages_to_responses_input(messages), **payload_kwargs)
      - L198 assign response_dict = _openai_responses_to_dict(response)
      - L199 assign content = response_dict.get('choices', [{}])[0].get('message', {}).get('content', '')
      - L204 if content:
        - L205 expr (yield {'id': response_dict.get('id', ''), 'model': response_dict.get('model', model), 'choices': [{'index': 0, 'delta': {'content': content}, 'finish_reason': 'stop'}]})
      - L214 return None
    - L216 assign kwargs['stream'] = True
    - L218 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L224 for chunk in response:
      - L225 expr (yield _openai_chunk_to_dict(chunk))
  - L227 def list_modelsself:
    - L228 try:
      - L229 assign client = self.get_client()
      - L230 assign models = client.models.list()
      - L231 return [m.id for m in models.data]
      - L232 except Exception as e:
        - L233 expr logger.warning('list_models_failed', extra={'error': str(e)})
        - L234 return []
  - L236 def health_checkself:
    - L237 try:
      - L238 assign client = self.get_client()
      - L239 expr client.models.list()
      - L240 return True
      - L241 except Exception:
        - L242 return False
- L245 class OllamaProvider(BaseProvider):
  - L246 docstring: "Ollama local LLM provider."
  - L248 def get_clientself:
    - L249 if self._client is not None:
      - L250 return self._client
    - L253 assign base_url = self.config.base_url or 'http://localhost:11434'
    - L255 try:
      - L256 from openai import OpenAI
      - L258 assign self._client = OpenAI(base_url=f'{base_url}/v1', api_key='ollama', timeout=self.config.timeout_seconds)
      - L263 assign self._use_openai_compat = True
      - L264 except ImportError:
        - L266 assign self._client = {'base_url': base_url}
        - L267 assign self._use_openai_compat = False
    - L269 return self._client
  - L271 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L277 assign client = self.get_client()
    - L278 assign model = model or self.config.model
    - L280 if getattr(self, '_use_openai_compat', False):
      - L282 if self.config.temperature is not None and 'temperature' not in kwargs:
        - L283 assign kwargs['temperature'] = self.config.temperature
      - L285 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
      - L290 return _openai_response_to_dict(response)
      - L293 else:
        - L293 return self._native_chat_completion(messages, model, **kwargs)
  - L295 def _native_chat_completionself, messages: List[Dict[str, Any]], model: str, **kwargs: Any:
    - L301 docstring: "Use native Ollama HTTP API."
    - L302 import urllib.request
    - L303 import json
    - L305 assign base_url = self._client['base_url']
    - L306 assign url = f'{base_url}/api/chat'
    - L309 assign ollama_messages = []
    - L310 for msg in messages:
      - L311 assign ollama_msg = {'role': msg['role']}
      - L312 assign content = msg.get('content', '')
      - L314 if isinstance(content, list):
        - L316 assign text_parts = []
        - L317 assign images = []
        - L318 for part in content:
          - L319 if part.get('type') == 'text':
            - L320 expr text_parts.append(part.get('text', ''))
            - L321 else:
              - L321 if part.get('type') == 'image_url':
                - L322 assign img_url = part.get('image_url', {}).get('url', '')
                - L323 if img_url.startswith('data:'):
                  - L325 assign (_, b64_data) = img_url.split(',', 1)
                  - L326 expr images.append(b64_data)
        - L327 assign ollama_msg['content'] = '\n'.join(text_parts)
        - L328 if images:
          - L329 assign ollama_msg['images'] = images
        - L331 else:
          - L331 assign ollama_msg['content'] = content
      - L333 expr ollama_messages.append(ollama_msg)
    - L335 assign payload = {'model': model, 'messages': ollama_messages, 'stream': False}
    - L341 if self.config.temperature is not None:
      - L342 assign payload['options'] = {'temperature': self.config.temperature}
    - L344 assign data = json.dumps(payload).encode('utf-8')
    - L345 assign req = urllib.request.Request(url, data=data, headers={'Content-Type': 'application/json'}, method='POST')
    - L352 with urllib.request.urlopen(req, timeout=self.config.timeout_seconds) as resp:
      - L353 assign result = json.loads(resp.read().decode('utf-8'))
    - L355 return {'id': 'ollama-' + str(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': result.get('message', {}).get('content', '')}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': result.get('prompt_eval_count', 0), 'completion_tokens': result.get('eval_count', 0), 'total_tokens': result.get('prompt_eval_count', 0) + result.get('eval_count', 0)}}
  - L373 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L379 assign client = self.get_client()
    - L380 assign model = model or self.config.model
    - L382 if getattr(self, '_use_openai_compat', False):
      - L383 assign kwargs['stream'] = True
      - L384 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
      - L389 for chunk in response:
        - L390 expr (yield _openai_chunk_to_dict(chunk))
      - L393 else:
        - L393 assign response = self.chat_completion(messages, model, **kwargs)
        - L394 expr (yield {'id': response['id'], 'model': model, 'choices': [{'index': 0, 'delta': {'content': response['choices'][0]['message']['content']}, 'finish_reason': 'stop'}]})
  - L404 def list_modelsself:
    - L405 try:
      - L406 import urllib.request
      - L407 import json
      - L409 assign base_url = self.config.base_url or 'http://localhost:11434'
      - L410 assign url = f'{base_url}/api/tags'
      - L412 assign req = urllib.request.Request(url, method='GET')
      - L413 with urllib.request.urlopen(req, timeout=10) as resp:
        - L414 assign result = json.loads(resp.read().decode('utf-8'))
      - L416 return [m['name'] for m in result.get('models', [])]
      - L417 except Exception as e:
        - L418 expr logger.warning('ollama_list_models_failed', extra={'error': str(e)})
        - L419 return []
  - L421 def health_checkself:
    - L422 try:
      - L423 assign models = self.list_models()
      - L424 return len(models) > 0 or self._check_api_endpoint()
      - L425 except Exception:
        - L426 return False
  - L428 def _check_api_endpointself:
    - L429 try:
      - L430 import urllib.request
      - L431 assign base_url = self.config.base_url or 'http://localhost:11434'
      - L432 assign req = urllib.request.Request(f'{base_url}/api/tags', method='GET')
      - L433 with urllib.request.urlopen(req, timeout=5) as resp:
        - L434 return resp.status == 200
      - L435 except Exception:
        - L436 return False
  - L438 def pull_modelself, model: str:
    - L439 docstring: "Pull a model from Ollama library."
    - L440 try:
      - L441 import urllib.request
      - L442 import json
      - L444 assign base_url = self.config.base_url or 'http://localhost:11434'
      - L445 assign url = f'{base_url}/api/pull'
      - L447 assign payload = json.dumps({'name': model}).encode('utf-8')
      - L448 assign req = urllib.request.Request(url, data=payload, headers={'Content-Type': 'application/json'}, method='POST')
      - L455 with urllib.request.urlopen(req, timeout=600) as resp:
        - L457 for line in resp:
          - L458 assign data = json.loads(line.decode('utf-8'))
          - L459 if data.get('status') == 'success':
            - L460 return True
          - L461 expr logger.info('ollama_pull_progress', extra={'status': data.get('status')})
      - L463 return True
      - L464 except Exception as e:
        - L465 expr logger.error('ollama_pull_failed', extra={'model': model, 'error': str(e)})
        - L466 return False
- L469 class DeepSeekProvider(BaseProvider):
  - L470 docstring: "DeepSeek API provider."
  - L472 def get_clientself:
    - L473 if self._client is not None:
      - L474 return self._client
    - L476 try:
      - L477 from openai import OpenAI
      - L478 except ImportError:
        - L479 raise RuntimeError('openai package is required for DeepSeek. Install with: pip install openai>=1.0.0')
    - L481 assign base_url = self.config.base_url or 'https://api.deepseek.com'
    - L483 assign self._client = OpenAI(api_key=self.config.api_key, base_url=base_url, timeout=self.config.timeout_seconds)
    - L488 return self._client
  - L490 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L496 assign client = self.get_client()
    - L497 assign model = model or self.config.model
    - L499 if self.config.temperature is not None and 'temperature' not in kwargs:
      - L500 assign kwargs['temperature'] = self.config.temperature
    - L501 if self.config.max_tokens is not None and 'max_tokens' not in kwargs:
      - L502 assign kwargs['max_tokens'] = self.config.max_tokens
    - L504 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L510 return _openai_response_to_dict(response)
  - L512 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L518 assign client = self.get_client()
    - L519 assign model = model or self.config.model
    - L520 assign kwargs['stream'] = True
    - L522 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L528 for chunk in response:
      - L529 expr (yield _openai_chunk_to_dict(chunk))
  - L531 def list_modelsself:
    - L532 return ['deepseek-chat', 'deepseek-coder', 'deepseek-reasoner']
  - L534 def health_checkself:
    - L535 try:
      - L536 assign client = self.get_client()
      - L537 expr client.models.list()
      - L538 return True
      - L539 except Exception:
        - L540 return False
- L543 class AnthropicProvider(BaseProvider):
  - L544 docstring: "Anthropic Claude API provider."
  - L546 def get_clientself:
    - L547 if self._client is not None:
      - L548 return self._client
    - L550 try:
      - L551 import anthropic
      - L552 except ImportError:
        - L553 raise RuntimeError('anthropic package is required. Install with: pip install anthropic')
    - L555 assign self._client = anthropic.Anthropic(api_key=self.config.api_key, timeout=self.config.timeout_seconds)
    - L559 return self._client
  - L561 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L567 assign client = self.get_client()
    - L568 assign model = model or self.config.model
    - L571 assign anthropic_messages = self._convert_messages(messages)
    - L574 assign system_message = None
    - L575 for msg in messages:
      - L576 if msg.get('role') == 'system':
        - L577 assign system_message = msg.get('content', '')
        - L578 break
    - L580 annotated assign create_kwargs: Dict[str, Any] = {'model': model, 'messages': anthropic_messages, 'max_tokens': kwargs.get('max_tokens', self.config.max_tokens or 4096)}
    - L586 if system_message:
      - L587 assign create_kwargs['system'] = system_message
    - L589 if self.config.temperature is not None and 'temperature' not in kwargs:
      - L590 assign create_kwargs['temperature'] = self.config.temperature
      - L591 else:
        - L591 if 'temperature' in kwargs:
          - L592 assign create_kwargs['temperature'] = kwargs['temperature']
    - L594 assign response = client.messages.create(**create_kwargs)
    - L596 return self._convert_response(response)
  - L598 def _convert_messagesself, messages: List[Dict[str, Any]]:
    - L599 docstring: "Convert OpenAI message format to Anthropic format."
    - L600 assign anthropic_messages = []
    - L602 for msg in messages:
      - L603 if msg.get('role') == 'system':
        - L604 continue
      - L606 assign anthropic_msg = {'role': msg['role']}
      - L607 assign content = msg.get('content', '')
      - L609 if isinstance(content, list):
        - L611 assign anthropic_content = []
        - L612 for part in content:
          - L613 if part.get('type') == 'text':
            - L614 expr anthropic_content.append({'type': 'text', 'text': part.get('text', '')})
            - L618 else:
              - L618 if part.get('type') == 'image_url':
                - L619 assign img_url = part.get('image_url', {}).get('url', '')
                - L620 if img_url.startswith('data:'):
                  - L622 assign (meta, b64_data) = img_url.split(',', 1)
                  - L623 assign media_type = meta.split(';')[0].replace('data:', '')
                  - L624 expr anthropic_content.append({'type': 'image', 'source': {'type': 'base64', 'media_type': media_type, 'data': b64_data}})
        - L632 assign anthropic_msg['content'] = anthropic_content
        - L634 else:
          - L634 assign anthropic_msg['content'] = content
      - L636 expr anthropic_messages.append(anthropic_msg)
    - L638 return anthropic_messages
  - L640 def _convert_responseself, response: Any:
    - L641 docstring: "Convert Anthropic response to OpenAI-compatible format."
    - L642 assign content = ''
    - L643 for block in response.content:
      - L644 if hasattr(block, 'text'):
        - L645 aug assign content Add block.text
    - L647 return {'id': response.id, 'model': response.model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': response.stop_reason or 'stop'}], 'usage': {'prompt_tokens': response.usage.input_tokens, 'completion_tokens': response.usage.output_tokens, 'total_tokens': response.usage.input_tokens + response.usage.output_tokens}}
  - L665 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L671 assign client = self.get_client()
    - L672 assign model = model or self.config.model
    - L674 assign anthropic_messages = self._convert_messages(messages)
    - L676 assign system_message = None
    - L677 for msg in messages:
      - L678 if msg.get('role') == 'system':
        - L679 assign system_message = msg.get('content', '')
        - L680 break
    - L682 annotated assign create_kwargs: Dict[str, Any] = {'model': model, 'messages': anthropic_messages, 'max_tokens': kwargs.get('max_tokens', self.config.max_tokens or 4096), 'stream': True}
    - L689 if system_message:
      - L690 assign create_kwargs['system'] = system_message
    - L692 with client.messages.stream(**create_kwargs) as stream:
      - L693 for text in stream.text_stream:
        - L694 expr (yield {'id': 'anthropic-stream', 'model': model, 'choices': [{'index': 0, 'delta': {'content': text}, 'finish_reason': None}]})
  - L704 def list_modelsself:
    - L705 return ['claude-3-5-sonnet-20241022', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307']
  - L712 def health_checkself:
    - L713 try:
      - L714 assign client = self.get_client()
      - L716 return client is not None
      - L717 except Exception:
        - L718 return False
- L721 class AzureOpenAIProvider(BaseProvider):
  - L722 docstring: "Azure OpenAI provider."
  - L724 def get_clientself:
    - L725 if self._client is not None:
      - L726 return self._client
    - L728 try:
      - L729 from openai import AzureOpenAI
      - L730 except ImportError:
        - L731 raise RuntimeError('openai package is required. Install with: pip install openai>=1.0.0')
    - L733 assign self._client = AzureOpenAI(api_key=self.config.api_key, api_version=self.config.azure_api_version, azure_endpoint=self.config.azure_endpoint, timeout=self.config.timeout_seconds)
    - L739 return self._client
  - L741 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L747 assign client = self.get_client()
    - L748 assign model = model or self.config.azure_deployment or self.config.model
    - L750 if self.config.temperature is not None and 'temperature' not in kwargs:
      - L751 assign kwargs['temperature'] = self.config.temperature
    - L752 if self.config.max_tokens is not None and 'max_tokens' not in kwargs:
      - L753 assign kwargs['max_tokens'] = self.config.max_tokens
    - L755 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L761 return _openai_response_to_dict(response)
  - L763 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L769 assign client = self.get_client()
    - L770 assign model = model or self.config.azure_deployment or self.config.model
    - L771 assign kwargs['stream'] = True
    - L773 assign response = client.chat.completions.create(model=model, messages=messages, **kwargs)
    - L779 for chunk in response:
      - L780 expr (yield _openai_chunk_to_dict(chunk))
  - L782 def list_modelsself:
    - L784 return [self.config.azure_deployment] if self.config.azure_deployment else []
  - L786 def health_checkself:
    - L787 try:
      - L788 assign client = self.get_client()
      - L789 return client is not None
      - L790 except Exception:
        - L791 return False
- L794 class GoogleGeminiProvider(BaseProvider):
  - L795 docstring: "Google Gemini API provider."
  - L797 def get_clientself:
    - L798 if self._client is not None:
      - L799 return self._client
    - L801 try:
      - L802 import google.generativeai as genai
      - L803 except ImportError:
        - L804 raise RuntimeError('google-generativeai package is required. Install with: pip install google-generativeai')
    - L806 expr genai.configure(api_key=self.config.api_key)
    - L807 assign self._client = genai
    - L808 return self._client
  - L810 def chat_completionself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L816 assign genai = self.get_client()
    - L817 assign model_name = model or self.config.model
    - L820 assign gemini_messages = self._convert_messages(messages)
    - L822 assign generation_config = {}
    - L823 if self.config.temperature is not None:
      - L824 assign generation_config['temperature'] = self.config.temperature
    - L825 if self.config.max_tokens is not None:
      - L826 assign generation_config['max_output_tokens'] = self.config.max_tokens
    - L828 assign model_instance = genai.GenerativeModel(model_name)
    - L829 assign response = model_instance.generate_content(gemini_messages, generation_config=generation_config if generation_config else None)
    - L834 return self._convert_response(response, model_name)
  - L836 def _convert_messagesself, messages: List[Dict[str, Any]]:
    - L837 docstring: "Convert OpenAI message format to Gemini format."
    - L838 assign gemini_parts = []
    - L840 for msg in messages:
      - L841 assign content = msg.get('content', '')
      - L842 assign role = msg.get('role', 'user')
      - L844 if isinstance(content, list):
        - L846 for part in content:
          - L847 if part.get('type') == 'text':
            - L848 expr gemini_parts.append(part.get('text', ''))
            - L849 else:
              - L849 if part.get('type') == 'image_url':
                - L850 assign img_url = part.get('image_url', {}).get('url', '')
                - L851 if img_url.startswith('data:'):
                  - L853 try:
                    - L854 from PIL import Image
                    - L855 import io
                    - L856 import base64
                    - L858 assign (_, b64_data) = img_url.split(',', 1)
                    - L859 assign image_bytes = base64.b64decode(b64_data)
                    - L860 assign image = Image.open(io.BytesIO(image_bytes))
                    - L861 expr gemini_parts.append(image)
                    - L862 except Exception:
                      - L863 pass
        - L865 else:
          - L865 if role == 'system':
            - L866 expr gemini_parts.insert(0, f'System: {content}\n\n')
            - L868 else:
              - L868 expr gemini_parts.append(content)
    - L870 return gemini_parts
  - L872 def _convert_responseself, response: Any, model: str:
    - L873 docstring: "Convert Gemini response to OpenAI-compatible format."
    - L874 assign content = response.text if hasattr(response, 'text') else ''
    - L876 return {'id': 'gemini-' + str(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}
  - L894 def chat_completion_streamself, messages: List[Dict[str, Any]], model: Optional[str]=None, **kwargs: Any:
    - L900 assign genai = self.get_client()
    - L901 assign model_name = model or self.config.model
    - L903 assign gemini_messages = self._convert_messages(messages)
    - L904 assign model_instance = genai.GenerativeModel(model_name)
    - L906 assign response = model_instance.generate_content(gemini_messages, stream=True)
    - L911 for chunk in response:
      - L912 if hasattr(chunk, 'text'):
        - L913 expr (yield {'id': 'gemini-stream', 'model': model_name, 'choices': [{'index': 0, 'delta': {'content': chunk.text}, 'finish_reason': None}]})
  - L923 def list_modelsself:
    - L924 return ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-2.0-flash-exp', 'gemini-pro']
  - L931 def health_checkself:
    - L932 try:
      - L933 assign genai = self.get_client()
      - L934 return genai is not None
      - L935 except Exception:
        - L936 return False
- L941 def _openai_prepare_responses_kwargskwargs: Dict[str, Any]:
  - L942 assign payload = dict(kwargs)
  - L943 expr payload.pop('stream', None)
  - L944 if 'max_tokens' in payload and 'max_output_tokens' not in payload:
    - L945 assign payload['max_output_tokens'] = payload.pop('max_tokens')
  - L946 return payload
- L949 def _openai_messages_to_responses_inputmessages: List[Dict[str, Any]]:
  - L950 annotated assign converted: List[Dict[str, Any]] = []
  - L951 for message in messages:
    - L952 if not isinstance(message, dict):
      - L953 continue
    - L954 assign role = message.get('role') or 'user'
    - L955 assign content = message.get('content', '')
    - L956 if isinstance(content, list):
      - L957 annotated assign parts: List[Dict[str, Any]] = []
      - L958 for part in content:
        - L959 if isinstance(part, dict):
          - L960 assign part_type = part.get('type')
          - L961 if part_type == 'text':
            - L962 expr parts.append({'type': 'input_text', 'text': part.get('text', '')})
            - L963 continue
          - L964 if part_type == 'image_url':
            - L965 assign image_url = part.get('image_url')
            - L966 if isinstance(image_url, dict):
              - L967 assign image_url = image_url.get('url') or image_url.get('image_url')
            - L968 expr parts.append({'type': 'input_image', 'image_url': image_url})
            - L969 continue
          - L970 expr parts.append(part)
          - L972 else:
            - L972 expr parts.append({'type': 'input_text', 'text': str(part)})
      - L973 assign content = parts
    - L974 expr converted.append({'role': role, 'content': content})
  - L975 return converted
- L978 def _openai_responses_output_textresponse: Any:
  - L979 if isinstance(response, dict):
    - L980 assign output_text = response.get('output_text')
    - L981 if isinstance(output_text, str) and output_text.strip():
      - L982 return output_text
    - L983 assign output = response.get('output')
    - L985 else:
      - L985 assign output_text = getattr(response, 'output_text', None)
      - L986 if isinstance(output_text, str) and output_text.strip():
        - L987 return output_text
      - L988 assign output = getattr(response, 'output', None)
  - L990 if isinstance(output, list):
    - L991 annotated assign texts: List[str] = []
    - L992 for item in output:
      - L993 if isinstance(item, dict):
        - L994 assign item_type = item.get('type')
        - L995 assign content = item.get('content') or []
        - L997 else:
          - L997 assign item_type = getattr(item, 'type', None)
          - L998 assign content = getattr(item, 'content', None) or []
      - L999 if item_type != 'message':
        - L1000 continue
      - L1001 for segment in content:
        - L1002 if isinstance(segment, dict):
          - L1003 assign seg_type = segment.get('type')
          - L1004 assign text = segment.get('text')
          - L1006 else:
            - L1006 assign seg_type = getattr(segment, 'type', None)
            - L1007 assign text = getattr(segment, 'text', None)
        - L1008 if seg_type in {'output_text', 'text'} and isinstance(text, str):
          - L1009 expr texts.append(text)
    - L1010 if texts:
      - L1011 return '\n'.join(texts)
  - L1012 return ''
- L1015 def _openai_responses_usageresponse: Any:
  - L1016 assign usage = response.get('usage') if isinstance(response, dict) else getattr(response, 'usage', None)
  - L1017 if usage is None:
    - L1018 return {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
  - L1019 if isinstance(usage, dict):
    - L1020 assign input_tokens = usage.get('input_tokens') or usage.get('prompt_tokens') or 0
    - L1021 assign output_tokens = usage.get('output_tokens') or usage.get('completion_tokens') or 0
    - L1023 else:
      - L1023 assign input_tokens = getattr(usage, 'input_tokens', None)
      - L1024 if input_tokens is None:
        - L1025 assign input_tokens = getattr(usage, 'prompt_tokens', 0)
      - L1026 assign output_tokens = getattr(usage, 'output_tokens', None)
      - L1027 if output_tokens is None:
        - L1028 assign output_tokens = getattr(usage, 'completion_tokens', 0)
  - L1029 assign total_tokens = int(input_tokens or 0) + int(output_tokens or 0)
  - L1030 return {'prompt_tokens': int(input_tokens or 0), 'completion_tokens': int(output_tokens or 0), 'total_tokens': int(total_tokens)}
- L1037 def _openai_responses_to_dictresponse: Any:
  - L1038 assign output_text = _openai_responses_output_text(response)
  - L1039 assign response_id = response.get('id') if isinstance(response, dict) else getattr(response, 'id', '')
  - L1040 assign model = response.get('model') if isinstance(response, dict) else getattr(response, 'model', '')
  - L1041 return {'id': response_id, 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': output_text}, 'finish_reason': 'stop'}], 'usage': _openai_responses_usage(response)}
- L1052 def _openai_response_to_dictresponse: Any:
  - L1053 docstring: "Convert OpenAI response object to dictionary."
  - L1054 if hasattr(response, 'model_dump'):
    - L1055 return response.model_dump()
    - L1056 else:
      - L1056 if hasattr(response, 'to_dict'):
        - L1057 return response.to_dict()
        - L1059 else:
          - L1059 return {'id': getattr(response, 'id', ''), 'model': getattr(response, 'model', ''), 'choices': [{'index': c.index, 'message': {'role': c.message.role, 'content': c.message.content}, 'finish_reason': c.finish_reason} for c in response.choices], 'usage': {'prompt_tokens': response.usage.prompt_tokens if response.usage else 0, 'completion_tokens': response.usage.completion_tokens if response.usage else 0, 'total_tokens': response.usage.total_tokens if response.usage else 0}}
- L1081 def _openai_chunk_to_dictchunk: Any:
  - L1082 docstring: "Convert OpenAI streaming chunk to dictionary."
  - L1083 if hasattr(chunk, 'model_dump'):
    - L1084 return chunk.model_dump()
  - L1086 return {'id': getattr(chunk, 'id', ''), 'model': getattr(chunk, 'model', ''), 'choices': [{'index': c.index, 'delta': {'role': getattr(c.delta, 'role', None), 'content': getattr(c.delta, 'content', None)}, 'finish_reason': c.finish_reason} for c in chunk.choices] if chunk.choices else []}
- L1105 annotated assign PROVIDERS: Dict[LLMProvider, type] = {LLMProvider.OPENAI: OpenAIProvider, LLMProvider.OLLAMA: OllamaProvider, LLMProvider.DEEPSEEK: DeepSeekProvider, LLMProvider.ANTHROPIC: AnthropicProvider, LLMProvider.AZURE: AzureOpenAIProvider, LLMProvider.GEMINI: GoogleGeminiProvider, LLMProvider.CUSTOM: OpenAIProvider}
- L1116 def get_providerconfig: LLMConfig:
  - L1117 docstring: "Get the appropriate provider for the configuration."
  - L1118 assign provider_class = PROVIDERS.get(config.provider)
  - L1119 if provider_class is None:
    - L1120 raise ValueError(f'Unknown provider: {config.provider}')
  - L1121 return provider_class(config)

## backend\app\services\llm\rag.py
- L2 docstring: "\nLightweight RAG (Retrieval-Augmented Generation) Module.\n\nProvides document ..."
- L11 from __future__ import annotations
- L13 import hashlib
- L14 import json
- L15 import logging
- L16 import math
- L17 import re
- L18 from collections import Counter
- L19 from dataclasses import dataclass, field
- L20 from pathlib import Path
- L21 from typing import Any, Callable, Dict, List, Optional, Tuple
- L23 from .client import LLMClient, get_llm_client
- L25 assign logger = logging.getLogger('neura.llm.rag')
- L29 class Document:
  - L30 docstring: "A document chunk for retrieval."
  - L31 annotated assign id: str
  - L32 annotated assign content: str
  - L33 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L34 annotated assign embedding: Optional[List[float]] = None
- L38 class RetrievalResult:
  - L39 docstring: "Result of document retrieval."
  - L40 annotated assign documents: List[Document]
  - L41 annotated assign scores: List[float]
  - L42 annotated assign query: str
  - L43 annotated assign method: str
- L46 class BM25Index:
  - L47 docstring: "\n    BM25 keyword-based retrieval index.\n\n    Efficient for exact and fuzzy k..."
  - L53 def __init__self, k1: float=1.5, b: float=0.75:
    - L54 assign self.k1 = k1
    - L55 assign self.b = b
    - L56 annotated assign self._documents: List[Document] = []
    - L57 annotated assign self._doc_freqs: Dict[str, int] = Counter()
    - L58 annotated assign self._doc_lens: List[int] = []
    - L59 annotated assign self._avg_doc_len: float = 0.0
    - L60 annotated assign self._inverted_index: Dict[str, List[Tuple[int, int]]] = {}
  - L62 def add_documentsself, documents: List[Document]:
    - L63 docstring: "Add documents to the index."
    - L64 for doc in documents:
      - L65 expr self._add_document(doc)
    - L66 assign self._avg_doc_len = sum(self._doc_lens) / len(self._doc_lens) if self._doc_lens else 0
  - L68 def _add_documentself, doc: Document:
    - L69 docstring: "Add a single document to the index."
    - L70 assign doc_idx = len(self._documents)
    - L71 expr self._documents.append(doc)
    - L74 assign tokens = self._tokenize(doc.content)
    - L75 expr self._doc_lens.append(len(tokens))
    - L78 assign term_freqs = Counter(tokens)
    - L81 for (term, freq) in term_freqs.items():
      - L82 if term not in self._inverted_index:
        - L83 assign self._inverted_index[term] = []
      - L84 expr self._inverted_index[term].append((doc_idx, freq))
      - L85 aug assign self._doc_freqs[term] Add 1
  - L87 def searchself, query: str, top_k: int=5:
    - L88 docstring: "Search the index and return top-k documents with scores."
    - L89 if not self._documents:
      - L90 return []
    - L92 assign query_tokens = self._tokenize(query)
    - L93 annotated assign scores: Dict[int, float] = {}
    - L94 assign n_docs = len(self._documents)
    - L96 for term in query_tokens:
      - L97 if term not in self._inverted_index:
        - L98 continue
      - L101 assign df = self._doc_freqs[term]
      - L102 assign idf = math.log((n_docs - df + 0.5) / (df + 0.5) + 1)
      - L105 for (doc_idx, tf) in self._inverted_index[term]:
        - L106 assign doc_len = self._doc_lens[doc_idx]
        - L108 assign numerator = tf * (self.k1 + 1)
        - L109 assign denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self._avg_doc_len)
        - L110 assign score = idf * numerator / denominator
        - L112 assign scores[doc_idx] = scores.get(doc_idx, 0) + score
    - L115 assign sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
    - L116 return [(self._documents[idx], score) for idx, score in sorted_results]
  - L118 def _tokenizeself, text: str:
    - L119 docstring: "Simple tokenization."
    - L121 assign tokens = re.findall('\\b\\w+\\b', text.lower())
    - L123 assign stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which', 'who', 'whom', 'whose', 'where', 'when', 'why', 'how', 'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just', 'and', 'but', 'or', 'if', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once'}
    - L136 return [t for t in tokens if len(t) > 2 and t not in stopwords]
- L139 class SimpleVectorStore:
  - L140 docstring: "\n    Simple in-memory vector store using cosine similarity.\n\n    For producti..."
  - L147 def __init__self:
    - L148 annotated assign self._documents: List[Document] = []
  - L150 def add_documentsself, documents: List[Document]:
    - L151 docstring: "Add documents with embeddings to the store."
    - L152 for doc in documents:
      - L153 if doc.embedding is not None:
        - L154 expr self._documents.append(doc)
  - L156 def searchself, query_embedding: List[float], top_k: int=5:
    - L161 docstring: "Search for similar documents using cosine similarity."
    - L162 if not self._documents or not query_embedding:
      - L163 return []
    - L165 assign scores = []
    - L166 for doc in self._documents:
      - L167 if doc.embedding:
        - L168 assign similarity = self._cosine_similarity(query_embedding, doc.embedding)
        - L169 expr scores.append((doc, similarity))
    - L172 expr scores.sort(key=lambda x: x[1], reverse=True)
    - L173 return scores[:top_k]
  - L175 def _cosine_similarityself, a: List[float], b: List[float]:
    - L176 docstring: "Calculate cosine similarity between two vectors."
    - L177 if len(a) != len(b):
      - L178 return 0.0
    - L180 assign dot_product = sum((x * y for x, y in zip(a, b)))
    - L181 assign norm_a = math.sqrt(sum((x * x for x in a)))
    - L182 assign norm_b = math.sqrt(sum((x * x for x in b)))
    - L184 if norm_a == 0 or norm_b == 0:
      - L185 return 0.0
    - L187 return dot_product / (norm_a * norm_b)
- L190 class RAGRetriever:
  - L191 docstring: "\n    RAG Retriever combining keyword and semantic search.\n\n    Features:\n   ..."
  - L201 def __init__self, client: Optional[LLMClient]=None, use_embeddings: bool=False, max_context_tokens: int=4000:
    - L207 assign self.client = client or get_llm_client()
    - L208 assign self.use_embeddings = use_embeddings
    - L209 assign self.max_context_tokens = max_context_tokens
    - L211 assign self._bm25_index = BM25Index()
    - L212 assign self._vector_store = SimpleVectorStore() if use_embeddings else None
    - L213 annotated assign self._documents: Dict[str, Document] = {}
  - L215 def add_documentself, content: str, doc_id: Optional[str]=None, metadata: Optional[Dict[str, Any]]=None, chunk_size: int=500, chunk_overlap: int=50:
    - L223 docstring: "\n        Add a document to the retriever.\n\n        Args:\n            content..."
    - L236 if not doc_id:
      - L237 assign doc_id = hashlib.md5(content.encode()).hexdigest()[:12]
    - L240 assign chunks = self._chunk_text(content, chunk_size, chunk_overlap)
    - L241 assign chunk_ids = []
    - L243 for (i, chunk_content) in enumerate(chunks):
      - L244 assign chunk_id = f'{doc_id}_chunk_{i}'
      - L245 assign chunk_metadata = {**(metadata or {}), 'parent_doc_id': doc_id, 'chunk_index': i, 'total_chunks': len(chunks)}
      - L252 assign doc = Document(id=chunk_id, content=chunk_content, metadata=chunk_metadata)
      - L259 if self.use_embeddings:
        - L260 assign doc.embedding = self._get_embedding(chunk_content)
      - L262 assign self._documents[chunk_id] = doc
      - L263 expr chunk_ids.append(chunk_id)
    - L266 expr self._rebuild_indices()
    - L268 return chunk_ids
  - L270 def add_documents_bulkself, documents: List[Dict[str, Any]], chunk_size: int=500:
    - L275 docstring: "\n        Add multiple documents in bulk.\n\n        Args:\n            document..."
    - L282 for doc in documents:
      - L283 expr self.add_document(content=doc['content'], doc_id=doc.get('id'), metadata=doc.get('metadata'), chunk_size=chunk_size)
  - L290 def retrieveself, query: str, top_k: int=5, method: str='hybrid':
    - L296 docstring: "\n        Retrieve relevant documents for a query.\n\n        Args:\n           ..."
    - L307 if method == 'bm25' or not self.use_embeddings:
      - L308 assign results = self._bm25_index.search(query, top_k)
      - L309 return RetrievalResult(documents=[doc for doc, _ in results], scores=[score for _, score in results], query=query, method='bm25')
    - L316 if method == 'vector' and self._vector_store:
      - L317 assign query_embedding = self._get_embedding(query)
      - L318 assign results = self._vector_store.search(query_embedding, top_k)
      - L319 return RetrievalResult(documents=[doc for doc, _ in results], scores=[score for _, score in results], query=query, method='vector')
    - L327 if method == 'hybrid' and self._vector_store:
      - L328 assign bm25_results = self._bm25_index.search(query, top_k * 2)
      - L329 assign query_embedding = self._get_embedding(query)
      - L330 assign vector_results = self._vector_store.search(query_embedding, top_k * 2)
      - L333 assign fused_scores = self._reciprocal_rank_fusion([r[0].id for r in bm25_results], [r[0].id for r in vector_results])
      - L339 assign sorted_ids = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
      - L340 assign documents = [self._documents[doc_id] for doc_id, _ in sorted_ids if doc_id in self._documents]
      - L341 assign scores = [score for _, score in sorted_ids[:len(documents)]]
      - L343 return RetrievalResult(documents=documents, scores=scores, query=query, method='hybrid')
    - L351 assign results = self._bm25_index.search(query, top_k)
    - L352 return RetrievalResult(documents=[doc for doc, _ in results], scores=[score for _, score in results], query=query, method='bm25')
  - L359 def query_with_contextself, question: str, top_k: int=5, include_sources: bool=True:
    - L365 docstring: "\n        Answer a question using retrieved context.\n\n        Args:\n         ..."
    - L377 assign retrieval = self.retrieve(question, top_k, method='hybrid')
    - L379 if not retrieval.documents:
      - L380 return {'answer': "I couldn't find relevant information to answer this question.", 'sources': [], 'context_used': False}
    - L387 assign context = self._build_context(retrieval.documents)
    - L390 assign prompt = f"Answer the following question based on the provided context.\n\nContext:\n{context}\n\nQuestion: {question}\n\nInstructions:\n- Answer based ONLY on the information provided in the context\n- If the context doesn't contain enough information, say so\n- Be concise but complete\n- Cite specific parts of the context when relevant\n\nAnswer:"
    - L405 assign response = self.client.complete(messages=[{'role': 'user', 'content': prompt}], description='rag_query', temperature=0.3)
    - L411 assign answer = response['choices'][0]['message']['content']
    - L413 assign sources = []
    - L414 if include_sources:
      - L415 for doc in retrieval.documents:
        - L416 expr sources.append({'id': doc.id, 'content_preview': doc.content[:200] + '...' if len(doc.content) > 200 else doc.content, 'metadata': doc.metadata})
    - L422 return {'answer': answer, 'sources': sources, 'context_used': True, 'documents_retrieved': len(retrieval.documents)}
  - L429 def _chunk_textself, text: str, chunk_size: int, chunk_overlap: int:
    - L435 docstring: "Split text into overlapping chunks."
    - L436 if len(text) <= chunk_size:
      - L437 return [text]
    - L439 assign chunks = []
    - L440 assign start = 0
    - L442 while start < len(text):
      - L443 assign end = start + chunk_size
      - L446 if end < len(text):
        - L448 for punct in ['. ', '! ', '? ', '\n\n', '\n']:
          - L449 assign boundary = text.rfind(punct, start + chunk_size // 2, end)
          - L450 if boundary != -1:
            - L451 assign end = boundary + len(punct)
            - L452 break
      - L454 expr chunks.append(text[start:end].strip())
      - L455 assign start = end - chunk_overlap
    - L457 return [c for c in chunks if c]
  - L459 def _build_contextself, documents: List[Document]:
    - L460 docstring: "Build context string from documents, respecting token limit."
    - L461 assign context_parts = []
    - L462 assign estimated_tokens = 0
    - L464 for (i, doc) in enumerate(documents):
      - L466 assign doc_tokens = len(doc.content) // 4
      - L468 if estimated_tokens + doc_tokens > self.max_context_tokens:
        - L470 assign remaining_tokens = self.max_context_tokens - estimated_tokens
        - L471 assign truncated_content = doc.content[:remaining_tokens * 4]
        - L472 expr context_parts.append(f'[Document {i + 1}]\n{truncated_content}...')
        - L473 break
      - L475 expr context_parts.append(f'[Document {i + 1}]\n{doc.content}')
      - L476 aug assign estimated_tokens Add doc_tokens
    - L478 return '\n\n'.join(context_parts)
  - L480 def _rebuild_indicesself:
    - L481 docstring: "Rebuild search indices."
    - L482 assign documents = list(self._documents.values())
    - L483 assign self._bm25_index = BM25Index()
    - L484 expr self._bm25_index.add_documents(documents)
    - L486 if self._vector_store:
      - L487 assign self._vector_store = SimpleVectorStore()
      - L488 expr self._vector_store.add_documents(documents)
  - L490 def _get_embeddingself, text: str:
    - L491 docstring: "\n        Get embedding for text.\n\n        Uses LLM to generate a simple embed..."
    - L503 assign words = re.findall('\\b\\w+\\b', text.lower())
    - L504 assign word_hashes = [hash(word) % 1000 / 1000.0 for word in words[:256]]
    - L507 assign embedding_size = 256
    - L508 if len(word_hashes) < embedding_size:
      - L509 expr word_hashes.extend([0.0] * (embedding_size - len(word_hashes)))
    - L511 return word_hashes[:embedding_size]
  - L513 def _reciprocal_rank_fusionself, ranking1: List[str], ranking2: List[str], k: int=60:
    - L519 docstring: "Combine two rankings using Reciprocal Rank Fusion."
    - L520 annotated assign fused_scores: Dict[str, float] = {}
    - L522 for (rank, doc_id) in enumerate(ranking1):
      - L523 assign fused_scores[doc_id] = fused_scores.get(doc_id, 0) + 1 / (k + rank + 1)
    - L525 for (rank, doc_id) in enumerate(ranking2):
      - L526 assign fused_scores[doc_id] = fused_scores.get(doc_id, 0) + 1 / (k + rank + 1)
    - L528 return fused_scores
- L533 def create_retrieveruse_embeddings: bool=False:
  - L534 docstring: "Create a RAG retriever instance."
  - L535 return RAGRetriever(use_embeddings=use_embeddings)
- L538 def quick_rag_queryquestion: str, documents: List[str], top_k: int=3:
  - L543 docstring: "\n    Quick RAG query over a list of documents.\n\n    Args:\n        question: ..."
  - L554 assign retriever = RAGRetriever(use_embeddings=False)
  - L556 for (i, doc) in enumerate(documents):
    - L557 expr retriever.add_document(doc, doc_id=f'doc_{i}')
  - L559 assign result = retriever.query_with_context(question, top_k=top_k)
  - L560 return result['answer']

## backend\app\services\llm\text_to_sql.py
- L2 docstring: "\nText-to-SQL Generation Module.\n\nImplements SQLCoder-style prompting for accu..."
- L11 from __future__ import annotations
- L13 import json
- L14 import logging
- L15 import re
- L16 from dataclasses import dataclass, field
- L17 from typing import Any, Dict, List, Optional, Tuple
- L19 from .client import LLMClient, get_llm_client
- L20 from .config import LLMConfig, LLMProvider
- L22 assign logger = logging.getLogger('neura.llm.text_to_sql')
- L26 class TableSchema:
  - L27 docstring: "Schema information for a database table."
  - L28 annotated assign name: str
  - L29 annotated assign columns: List[Dict[str, str]]
  - L30 annotated assign primary_key: Optional[str] = None
  - L31 annotated assign foreign_keys: List[Dict[str, str]] = field(default_factory=list)
  - L32 annotated assign sample_values: Dict[str, List[Any]] = field(default_factory=dict)
  - L33 annotated assign description: Optional[str] = None
- L37 class SQLGenerationResult:
  - L38 docstring: "Result of SQL generation."
  - L39 annotated assign sql: str
  - L40 annotated assign explanation: str
  - L41 annotated assign confidence: float
  - L42 annotated assign dialect: str
  - L43 annotated assign warnings: List[str] = field(default_factory=list)
  - L44 annotated assign raw_response: str = ''
- L47 class TextToSQL:
  - L48 docstring: "\n    Text-to-SQL generation using LLM with SQLCoder-style prompting.\n\n    Sup..."
  - L60 assign SQL_MODELS = {LLMProvider.OPENAI: 'gpt-5', LLMProvider.OLLAMA: 'sqlcoder:7b', LLMProvider.DEEPSEEK: 'deepseek-coder', LLMProvider.ANTHROPIC: 'claude-3-5-sonnet-20241022'}
  - L67 def __init__self, client: Optional[LLMClient]=None, dialect: str='duckdb', model: Optional[str]=None:
    - L73 assign self.client = client or get_llm_client()
    - L74 assign self.dialect = dialect.lower()
    - L75 assign self._model = model
    - L76 annotated assign self._schemas: Dict[str, TableSchema] = {}
  - L79 def modelself:
    - L80 docstring: "Get the model for SQL generation."
    - L81 if self._model:
      - L82 return self._model
    - L83 assign provider = self.client.config.provider
    - L84 return self.SQL_MODELS.get(provider, self.client.config.model)
  - L86 def add_table_schemaself, schema: TableSchema:
    - L87 docstring: "Add a table schema for context."
    - L88 assign self._schemas[schema.name] = schema
  - L90 def add_schemas_from_catalogself, catalog: Dict[str, Any]:
    - L91 docstring: "Add schemas from a database catalog dictionary."
    - L92 for (table_name, table_info) in catalog.items():
      - L93 assign columns = []
      - L94 for col in table_info.get('columns', []):
        - L95 if isinstance(col, dict):
          - L96 expr columns.append({'name': col.get('name', ''), 'type': col.get('type', 'TEXT'), 'description': col.get('description', '')})
          - L101 else:
            - L101 if isinstance(col, str):
              - L102 expr columns.append({'name': col, 'type': 'TEXT', 'description': ''})
      - L104 assign schema = TableSchema(name=table_name, columns=columns, primary_key=table_info.get('primary_key'), foreign_keys=table_info.get('foreign_keys', []), sample_values=table_info.get('sample_values', {}), description=table_info.get('description'))
      - L112 assign self._schemas[table_name] = schema
  - L114 def generate_sqlself, question: str, tables: Optional[List[str]]=None, context: Optional[str]=None:
    - L120 docstring: "\n        Generate SQL from a natural language question.\n\n        Args:\n     ..."
    - L132 assign prompt = self._build_sqlcoder_prompt(question, tables, context)
    - L135 assign response = self.client.complete(messages=[{'role': 'user', 'content': prompt}], model=self.model, description='text_to_sql', temperature=0.0)
    - L142 assign raw_content = response['choices'][0]['message']['content']
    - L143 return self._parse_sql_response(raw_content)
  - L145 def generate_sql_with_decompositionself, question: str, tables: Optional[List[str]]=None:
    - L150 docstring: "\n        Generate SQL using query decomposition for complex questions.\n\n     ..."
    - L157 assign analysis_prompt = f'Analyze this question and determine if it needs to be decomposed:\n\nQuestion: {question}\n\nRespond in JSON format:\n```json\n{{\n  "is_complex": true/false,\n  "sub_questions": ["sub-question 1", "sub-question 2"],\n  "combination_strategy": "join|union|subquery|none"\n}}\n```'
    - L170 assign analysis_response = self.client.complete(messages=[{'role': 'user', 'content': analysis_prompt}], model=self.model, description='sql_decomposition_analysis', temperature=0.0)
    - L177 assign analysis = self._parse_json_response(analysis_response['choices'][0]['message']['content'], {'is_complex': False, 'sub_questions': [], 'combination_strategy': 'none'})
    - L182 if not analysis.get('is_complex') or not analysis.get('sub_questions'):
      - L184 return self.generate_sql(question, tables)
    - L187 assign sub_queries = []
    - L188 for sub_q in analysis['sub_questions']:
      - L189 assign result = self.generate_sql(sub_q, tables)
      - L190 expr sub_queries.append({'question': sub_q, 'sql': result.sql})
    - L196 assign combination_prompt = f"Combine these sub-queries to answer the original question.\n\nOriginal Question: {question}\n\nSub-queries:\n{json.dumps(sub_queries, indent=2)}\n\nCombination Strategy: {analysis.get('combination_strategy', 'join')}\n\nGenerate the final combined SQL query for {self.dialect.upper()}.\nReturn ONLY the SQL, no explanation."
    - L208 assign final_response = self.client.complete(messages=[{'role': 'user', 'content': combination_prompt}], model=self.model, description='sql_combination', temperature=0.0)
    - L215 assign final_sql = self._extract_sql(final_response['choices'][0]['message']['content'])
    - L217 return SQLGenerationResult(sql=final_sql, explanation=f'Combined from {len(sub_queries)} sub-queries', confidence=0.8, dialect=self.dialect, warnings=['Query was decomposed and combined'], raw_response=final_response['choices'][0]['message']['content'])
  - L226 def validate_and_fix_sqlself, sql: str, error_message: Optional[str]=None:
    - L231 docstring: "\n        Validate SQL and attempt to fix any errors.\n\n        Args:\n        ..."
    - L241 assign schema_context = self._build_schema_context()
    - L243 assign prompt = f"Review and fix this SQL query.\n\nSchema:\n{schema_context}\n\nOriginal SQL:\n```sql\n{sql}\n```\n\n{(f'Error encountered: {error_message}' if error_message else '')}\n\nTasks:\n1. Check for syntax errors\n2. Verify table and column names match the schema\n3. Fix any issues found\n4. Ensure the query is valid {self.dialect.upper()}\n\nReturn the corrected SQL in a code block, followed by an explanation of changes made."
    - L263 assign response = self.client.complete(messages=[{'role': 'user', 'content': prompt}], model=self.model, description='sql_validation', temperature=0.0)
    - L270 assign raw_content = response['choices'][0]['message']['content']
    - L271 assign result = self._parse_sql_response(raw_content)
    - L273 if error_message:
      - L274 expr result.warnings.append(f'Fixed error: {error_message}')
    - L276 return result
  - L278 def explain_sqlself, sql: str:
    - L279 docstring: "\n        Generate a natural language explanation of SQL.\n\n        Args:\n    ..."
    - L288 assign prompt = f'Explain this SQL query in simple terms:\n\n```sql\n{sql}\n```\n\nProvide a clear, non-technical explanation of:\n1. What data is being retrieved\n2. What filters/conditions are applied\n3. How the results are organized\n4. Any calculations or aggregations performed'
    - L300 assign response = self.client.complete(messages=[{'role': 'user', 'content': prompt}], model=self.model, description='sql_explanation', temperature=0.3)
    - L307 return response['choices'][0]['message']['content']
  - L309 def _build_sqlcoder_promptself, question: str, tables: Optional[List[str]], context: Optional[str]:
    - L315 docstring: "Build a SQLCoder-style prompt."
    - L316 assign schema_context = self._build_schema_context(tables)
    - L319 assign prompt = f'### Task\nGenerate a SQL query to answer [QUESTION]{question}[/QUESTION]\n\n### Database Schema\nThe query will run on a database with the following schema:\n{schema_context}\n\n### SQL Dialect\nUse {self.dialect.upper()} syntax.\n\n'
    - L330 if context:
      - L331 aug assign prompt Add f'### Additional Context\n{context}\n\n'
    - L336 aug assign prompt Add '### Guidelines\n- Use proper table aliases\n- Handle NULL values appropriately\n- Use appropriate JOIN types\n- Include only necessary columns\n- Use appropriate aggregation functions\n- Ensure the query is efficient\n\n### Answer\nGiven the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]:\n```sql\n'
    - L348 return prompt
  - L350 def _build_schema_contextself, tables: Optional[List[str]]=None:
    - L354 docstring: "Build schema context string for the prompt."
    - L355 if not self._schemas:
      - L356 return 'No schema information available.'
    - L358 assign schemas_to_use = {name: self._schemas[name] for name in tables if name in self._schemas} if tables else self._schemas
    - L364 assign context_parts = []
    - L366 for (table_name, schema) in schemas_to_use.items():
      - L368 assign columns_def = []
      - L369 for col in schema.columns:
        - L370 assign col_def = f"  {col['name']} {col.get('type', 'TEXT')}"
        - L371 if col.get('description'):
          - L372 aug assign col_def Add f" -- {col['description']}"
        - L373 expr columns_def.append(col_def)
      - L375 assign table_def = f'CREATE TABLE {table_name} (\n'
      - L376 aug assign table_def Add ',\n'.join(columns_def)
      - L378 if schema.primary_key:
        - L379 aug assign table_def Add f',\n  PRIMARY KEY ({schema.primary_key})'
      - L381 for fk in schema.foreign_keys:
        - L382 aug assign table_def Add f",\n  FOREIGN KEY ({fk.get('column')}) REFERENCES {fk.get('references')}"
      - L384 aug assign table_def Add '\n);'
      - L386 if schema.description:
        - L387 assign table_def = f'-- {schema.description}\n{table_def}'
      - L389 expr context_parts.append(table_def)
      - L392 if schema.sample_values:
        - L393 assign samples = []
        - L394 for (col, values) in schema.sample_values.items():
          - L395 if values:
            - L396 expr samples.append(f"  {col}: {', '.join((str(v) for v in values[:3]))}")
        - L397 if samples:
          - L398 expr context_parts.append(f'-- Sample values for {table_name}:\n' + '\n'.join(samples))
    - L400 return '\n\n'.join(context_parts)
  - L402 def _parse_sql_responseself, raw_content: str:
    - L403 docstring: "Parse the LLM response to extract SQL."
    - L404 assign sql = self._extract_sql(raw_content)
    - L407 assign explanation = ''
    - L408 assign sql_end = raw_content.rfind('```')
    - L409 if sql_end != -1:
      - L410 assign explanation = raw_content[sql_end + 3:].strip()
    - L413 assign confidence = 0.9
    - L414 assign warnings = []
    - L416 if not sql:
      - L417 assign confidence = 0.0
      - L418 expr warnings.append('No SQL found in response')
      - L419 else:
        - L419 if 'SELECT *' in sql.upper():
          - L420 aug assign confidence Sub 0.1
          - L421 expr warnings.append('Using SELECT * may be inefficient')
    - L422 if '-- TODO' in sql or '-- FIXME' in sql:
      - L423 aug assign confidence Sub 0.2
      - L424 expr warnings.append('Query contains TODO/FIXME comments')
    - L426 return SQLGenerationResult(sql=sql, explanation=explanation, confidence=min(max(confidence, 0.0), 1.0), dialect=self.dialect, warnings=warnings, raw_response=raw_content)
  - L435 def _extract_sqlself, content: str:
    - L436 docstring: "Extract SQL from LLM response."
    - L438 assign sql_match = re.search('```(?:sql)?\\s*([\\s\\S]*?)```', content, re.IGNORECASE)
    - L439 if sql_match:
      - L440 return sql_match.group(1).strip()
    - L443 assign statement_match = re.search('(SELECT|INSERT|UPDATE|DELETE|WITH|CREATE|ALTER|DROP)\\b[\\s\\S]+?(?:;|$)', content, re.IGNORECASE)
    - L448 if statement_match:
      - L449 return statement_match.group(0).strip().rstrip(';') + ';'
    - L451 return content.strip()
  - L453 def _parse_json_responseself, raw_content: str, default: Dict[str, Any]:
    - L458 docstring: "Extract JSON from LLM response."
    - L459 assign json_match = re.search('```(?:json)?\\s*([\\s\\S]*?)```', raw_content)
    - L460 if json_match:
      - L461 assign json_str = json_match.group(1).strip()
      - L463 else:
        - L463 assign json_str = raw_content.strip()
    - L465 try:
      - L466 return json.loads(json_str)
      - L467 except json.JSONDecodeError:
        - L468 return default
- L473 def get_text_to_sqldialect: str='duckdb':
  - L474 docstring: "Get a TextToSQL instance."
  - L475 return TextToSQL(dialect=dialect)
- L478 def generate_sqlquestion: str, schema: Dict[str, Any], dialect: str='duckdb':
  - L483 docstring: "\n    Quick function to generate SQL from a question.\n\n    Args:\n        ques..."
  - L494 assign t2sql = TextToSQL(dialect=dialect)
  - L495 expr t2sql.add_schemas_from_catalog(schema)
  - L496 assign result = t2sql.generate_sql(question)
  - L497 return result.sql

## backend\app\services\llm\vision.py
- L2 docstring: "\nVision-Language Model Integration for Document Understanding.\n\nProvides spec..."
- L19 from __future__ import annotations
- L21 import base64
- L22 import json
- L23 import logging
- L24 import re
- L25 from dataclasses import dataclass
- L26 from pathlib import Path
- L27 from typing import Any, Dict, List, Optional, Tuple, Union
- L29 from .client import LLMClient, get_llm_client
- L30 from .config import LLMConfig, LLMProvider, VISION_MODELS
- L32 assign logger = logging.getLogger('neura.llm.vision')
- L36 class DocumentAnalysisResult:
  - L37 docstring: "Result of VLM document analysis."
  - L38 annotated assign text_content: str
  - L39 annotated assign tables: List[Dict[str, Any]]
  - L40 annotated assign structure: Dict[str, Any]
  - L41 annotated assign metadata: Dict[str, Any]
  - L42 annotated assign raw_response: str
- L46 class TableExtractionResult:
  - L47 docstring: "Result of table extraction from image."
  - L48 annotated assign tables: List[Dict[str, Any]]
  - L49 annotated assign confidence: float
  - L50 annotated assign raw_response: str
- L53 class VisionLanguageModel:
  - L54 docstring: "\n    Vision-Language Model service for document understanding.\n\n    Provides ..."
  - L64 def __init__self, client: Optional[LLMClient]=None, model: Optional[str]=None:
    - L69 assign self.client = client or get_llm_client()
    - L70 assign self._model = model
  - L73 def modelself:
    - L74 docstring: "Get the vision model to use."
    - L75 if self._model:
      - L76 return self._model
    - L77 return self.client.config.get_vision_model()
  - L79 def analyze_documentself, image: Union[str, bytes, Path], analysis_type: str='comprehensive', language: str='auto':
    - L85 docstring: "\n        Analyze a document image using VLM.\n\n        Args:\n            imag..."
    - L96 assign prompt = self._build_document_analysis_prompt(analysis_type, language)
    - L98 assign response = self.client.complete_with_vision(text=prompt, images=[image], model=self.model, description=f'vlm_document_analysis_{analysis_type}')
    - L105 assign raw_content = response['choices'][0]['message']['content']
    - L106 return self._parse_document_analysis(raw_content, analysis_type)
  - L108 def extract_tablesself, image: Union[str, bytes, Path], expected_columns: Optional[List[str]]=None:
    - L113 docstring: "\n        Extract tables from a document image.\n\n        Args:\n            im..."
    - L123 assign prompt = self._build_table_extraction_prompt(expected_columns)
    - L125 assign response = self.client.complete_with_vision(text=prompt, images=[image], model=self.model, description='vlm_table_extraction')
    - L132 assign raw_content = response['choices'][0]['message']['content']
    - L133 return self._parse_table_extraction(raw_content)
  - L135 def extract_text_with_layoutself, image: Union[str, bytes, Path], preserve_formatting: bool=True:
    - L140 docstring: "\n        Extract text from image while preserving layout structure.\n\n        ..."
    - L150 assign prompt = f"""Analyze this document image and extract all text content.\n\n{('Preserve the original formatting, spacing, and layout as much as possible.' if preserve_formatting else 'Extract the text content in reading order.')}\n\nReturn your response in the following JSON format:\n```json\n{{\n  "title": "Document title if present",\n  "sections": [\n    {{\n      "type": "header|paragraph|list|table|footer",\n      "level": 1,\n      "content": "Text content",\n      "formatting": {{\n        "bold": false,\n        "italic": false,\n        "alignment": "left|center|right"\n      }}\n    }}\n  ],\n  "page_number": null,\n  "reading_order_text": "Full text in reading order"\n}}\n```\n\nAnalyze the image carefully and extract all visible text."""
    - L177 assign response = self.client.complete_with_vision(text=prompt, images=[image], model=self.model, description='vlm_text_extraction')
    - L184 assign raw_content = response['choices'][0]['message']['content']
    - L185 return self._parse_json_response(raw_content, {'title': None, 'sections': [], 'page_number': None, 'reading_order_text': raw_content})
  - L192 def analyze_chartself, image: Union[str, bytes, Path]:
    - L196 docstring: "\n        Analyze a chart/graph image and extract data.\n\n        Args:\n      ..."
    - L205 assign prompt = 'Analyze this chart/graph image and extract the data.\n\nReturn your response in the following JSON format:\n```json\n{\n  "chart_type": "bar|line|pie|scatter|area|other",\n  "title": "Chart title if visible",\n  "x_axis": {\n    "label": "X axis label",\n    "values": ["value1", "value2"]\n  },\n  "y_axis": {\n    "label": "Y axis label",\n    "min": 0,\n    "max": 100\n  },\n  "data_series": [\n    {\n      "name": "Series name",\n      "values": [10, 20, 30],\n      "color": "blue"\n    }\n  ],\n  "legend": ["Item 1", "Item 2"],\n  "insights": "Brief description of what the chart shows"\n}\n```\n\nExtract as much data as you can accurately determine from the image.'
    - L235 assign response = self.client.complete_with_vision(text=prompt, images=[image], model=self.model, description='vlm_chart_analysis')
    - L242 assign raw_content = response['choices'][0]['message']['content']
    - L243 return self._parse_json_response(raw_content, {'chart_type': 'unknown', 'data_series': [], 'insights': raw_content})
  - L249 def extract_form_fieldsself, image: Union[str, bytes, Path]:
    - L253 docstring: "\n        Extract form fields and their values from an image.\n\n        Args:\n..."
    - L262 assign prompt = 'Analyze this form image and extract all form fields with their values.\n\nReturn your response in the following JSON format:\n```json\n{\n  "form_title": "Form title if visible",\n  "fields": [\n    {\n      "label": "Field label",\n      "value": "Filled value or null if empty",\n      "type": "text|checkbox|radio|date|signature|other",\n      "required": true\n    }\n  ],\n  "sections": [\n    {\n      "name": "Section name",\n      "fields": ["field_label_1", "field_label_2"]\n    }\n  ]\n}\n```\n\nExtract all visible form fields, whether filled or empty.'
    - L287 assign response = self.client.complete_with_vision(text=prompt, images=[image], model=self.model, description='vlm_form_extraction')
    - L294 assign raw_content = response['choices'][0]['message']['content']
    - L295 return self._parse_json_response(raw_content, {'form_title': None, 'fields': []})
  - L300 def compare_documentsself, image1: Union[str, bytes, Path], image2: Union[str, bytes, Path]:
    - L305 docstring: "\n        Compare two document images and identify differences.\n\n        Args:..."
    - L315 assign prompt = 'Compare these two document images and identify any differences.\n\nThe first image is the reference, and the second is the version to compare.\n\nReturn your response in the following JSON format:\n```json\n{\n  "identical": false,\n  "similarity_score": 0.95,\n  "differences": [\n    {\n      "type": "text_change|layout_change|missing_element|added_element",\n      "location": "Description of where the difference is",\n      "reference_content": "Content in first image",\n      "compared_content": "Content in second image"\n    }\n  ],\n  "summary": "Brief summary of the comparison"\n}\n```\n\nBe thorough but focus on meaningful differences, not minor formatting variations.'
    - L338 assign response = self.client.complete_with_vision(text=prompt, images=[image1, image2], model=self.model, description='vlm_document_comparison')
    - L345 assign raw_content = response['choices'][0]['message']['content']
    - L346 return self._parse_json_response(raw_content, {'identical': False, 'differences': [], 'summary': raw_content})
  - L352 def _build_document_analysis_promptself, analysis_type: str, language: str:
    - L357 docstring: "Build the prompt for document analysis."
    - L358 assign base_prompt = 'Analyze this document image comprehensively.\n\n'
    - L361 if analysis_type == 'text_only':
      - L362 aug assign base_prompt Add 'Focus on extracting all text content accurately.\n\nReturn your response in JSON format:\n```json\n{\n  "text_content": "Full extracted text",\n  "paragraphs": ["paragraph 1", "paragraph 2"],\n  "headers": ["header 1"],\n  "language": "detected language"\n}\n```'
      - L373 else:
        - L373 if analysis_type == 'tables_only':
          - L374 aug assign base_prompt Add 'Focus on extracting any tables present in the document.\n\nReturn your response in JSON format:\n```json\n{\n  "tables": [\n    {\n      "id": 1,\n      "title": "Table title if present",\n      "headers": ["Column 1", "Column 2"],\n      "rows": [\n        ["Value 1", "Value 2"],\n        ["Value 3", "Value 4"]\n      ]\n    }\n  ]\n}\n```'
          - L392 else:
            - L392 if analysis_type == 'structure':
              - L393 aug assign base_prompt Add 'Focus on understanding the document structure and layout.\n\nReturn your response in JSON format:\n```json\n{\n  "document_type": "invoice|report|form|letter|other",\n  "sections": [\n    {"name": "Section name", "type": "header|body|footer|sidebar"}\n  ],\n  "has_tables": true,\n  "has_images": false,\n  "layout": "single_column|multi_column|mixed"\n}\n```'
              - L408 else:
                - L408 aug assign base_prompt Add 'Extract all content including text, tables, and structure.\n\nReturn your response in JSON format:\n```json\n{\n  "document_type": "invoice|report|form|letter|other",\n  "title": "Document title if present",\n  "text_content": "Full text content in reading order",\n  "tables": [\n    {\n      "id": 1,\n      "title": "Table title",\n      "headers": ["Col1", "Col2"],\n      "rows": [["Val1", "Val2"]]\n    }\n  ],\n  "structure": {\n    "sections": ["Section 1", "Section 2"],\n    "has_headers": true,\n    "has_footers": true\n  },\n  "metadata": {\n    "language": "en",\n    "date_found": "2024-01-01",\n    "page_number": 1\n  }\n}\n```'
    - L437 if language != 'auto':
      - L438 aug assign base_prompt Add f'\n\nThe document is in {language}.'
    - L440 return base_prompt
  - L442 def _build_table_extraction_promptself, expected_columns: Optional[List[str]]:
    - L446 docstring: "Build the prompt for table extraction."
    - L447 assign prompt = 'Extract all tables from this document image.\n\nReturn your response in JSON format:\n```json\n{\n  "tables": [\n    {\n      "id": 1,\n      "title": "Table title if visible",\n      "headers": ["Column 1", "Column 2", "Column 3"],\n      "rows": [\n        ["Row 1 Col 1", "Row 1 Col 2", "Row 1 Col 3"],\n        ["Row 2 Col 1", "Row 2 Col 2", "Row 2 Col 3"]\n      ],\n      "notes": "Any footnotes or notes about the table"\n    }\n  ],\n  "confidence": 0.95\n}\n```\n\nBe accurate with the data extraction. If a cell is empty, use an empty string.\nIf you cannot read a value clearly, indicate it with "[unclear]".\n'
    - L472 if expected_columns:
      - L473 aug assign prompt Add f"\n\nExpected columns: {', '.join(expected_columns)}"
      - L474 aug assign prompt Add '\nMap extracted columns to these expected names if they match.'
    - L476 return prompt
  - L478 def _parse_document_analysisself, raw_content: str, analysis_type: str:
    - L483 docstring: "Parse the document analysis response."
    - L484 assign parsed = self._parse_json_response(raw_content, {})
    - L486 assign text_content = parsed.get('text_content', '')
    - L487 if not text_content and 'paragraphs' in parsed:
      - L488 assign text_content = '\n\n'.join(parsed.get('paragraphs', []))
    - L490 assign tables = parsed.get('tables', [])
    - L491 assign structure = parsed.get('structure', {})
    - L493 if 'document_type' in parsed:
      - L494 assign structure['document_type'] = parsed['document_type']
    - L495 if 'title' in parsed:
      - L496 assign structure['title'] = parsed['title']
    - L498 assign metadata = parsed.get('metadata', {})
    - L499 if 'language' in parsed:
      - L500 assign metadata['language'] = parsed['language']
    - L502 return DocumentAnalysisResult(text_content=text_content or raw_content, tables=tables, structure=structure, metadata=metadata, raw_response=raw_content)
  - L510 def _parse_table_extractionself, raw_content: str:
    - L514 docstring: "Parse the table extraction response."
    - L515 assign parsed = self._parse_json_response(raw_content, {'tables': [], 'confidence': 0.5})
    - L517 return TableExtractionResult(tables=parsed.get('tables', []), confidence=parsed.get('confidence', 0.5), raw_response=raw_content)
  - L523 def _parse_json_responseself, raw_content: str, default: Dict[str, Any]:
    - L528 docstring: "Extract JSON from LLM response."
    - L530 assign json_match = re.search('```(?:json)?\\s*([\\s\\S]*?)```', raw_content)
    - L531 if json_match:
      - L532 assign json_str = json_match.group(1).strip()
      - L535 else:
        - L535 assign json_str = raw_content.strip()
    - L538 assign start = json_str.find('{')
    - L539 if start == -1:
      - L540 return default
    - L543 assign depth = 0
    - L544 assign in_string = False
    - L545 assign escape_next = False
    - L547 for (i, char) in enumerate(json_str[start:], start):
      - L548 if escape_next:
        - L549 assign escape_next = False
        - L550 continue
      - L552 if char == '\\':
        - L553 assign escape_next = True
        - L554 continue
      - L556 if char == '"' and (not escape_next):
        - L557 assign in_string = not in_string
        - L558 continue
      - L560 if in_string:
        - L561 continue
      - L563 if char == '{':
        - L564 aug assign depth Add 1
        - L565 else:
          - L565 if char == '}':
            - L566 aug assign depth Sub 1
            - L567 if depth == 0:
              - L568 assign json_str = json_str[start:i + 1]
              - L569 break
    - L571 try:
      - L572 return json.loads(json_str)
      - L573 except json.JSONDecodeError as e:
        - L574 expr logger.warning('vlm_json_parse_failed', extra={'event': 'vlm_json_parse_failed', 'error': str(e), 'snippet': json_str[:200]})
        - L582 return default
- L587 def get_vlmmodel: Optional[str]=None:
  - L588 docstring: "Get a VisionLanguageModel instance."
  - L589 return VisionLanguageModel(model=model)
- L592 def analyze_document_imageimage: Union[str, bytes, Path], analysis_type: str='comprehensive':
  - L596 docstring: "Quick function to analyze a document image."
  - L597 assign vlm = get_vlm()
  - L598 return vlm.analyze_document(image, analysis_type)
- L601 def extract_tables_from_imageimage: Union[str, bytes, Path]:
  - L604 docstring: "Quick function to extract tables from an image."
  - L605 assign vlm = get_vlm()
  - L606 return vlm.extract_tables(image)

## backend\app\services\mapping\__init__.py
- L1 docstring: "Mapping assistant placeholder."

## backend\app\services\mapping\auto_fill.py
- L1 from __future__ import annotations
- L3 import hashlib
- L4 import json
- L5 import logging
- L6 from ..dataframes.sqlite_loader import get_loader
- L7 from pathlib import Path
- L8 from typing import Optional
- L10 assign logger = logging.getLogger('neura.auto_fill')
- L13 def _compute_db_signaturedb_path: Path:
  - L14 docstring: "\n    Build a stable fingerprint of the SQLite schema (user tables only).\n    C..."
  - L18 annotated assign schema: dict[str, dict[str, list[dict[str, object]]]] = {}
  - L19 try:
    - L20 assign loader = get_loader(db_path)
    - L21 except Exception as exc:
      - L22 expr logger.warning('db_signature_connect_failed', extra={'event': 'db_signature_connect_failed', 'db_path': str(db_path)}, exc_info=exc)
      - L30 return None
  - L32 try:
    - L33 assign tables = loader.table_names()
    - L34 for table in tables:
      - L35 annotated assign table_entry: dict[str, list[dict[str, object]]] = {'columns': [], 'foreign_keys': []}
      - L36 try:
        - L37 assign columns = loader.pragma_table_info(table)
        - L38 assign table_entry['columns'] = [{'name': str(col.get('name') or ''), 'type': str(col.get('type') or ''), 'notnull': int(col.get('notnull') or 0), 'pk': int(col.get('pk') or 0)} for col in columns]
        - L47 except Exception:
          - L48 assign table_entry['columns'] = []
      - L50 try:
        - L51 assign fks = loader.foreign_keys(table)
        - L52 assign table_entry['foreign_keys'] = [{'id': int(fk.get('id', 0)), 'seq': int(fk.get('seq', 0)), 'table': str(fk.get('table') or ''), 'from': str(fk.get('from') or ''), 'to': str(fk.get('to') or '')} for fk in fks]
        - L62 except Exception:
          - L63 assign table_entry['foreign_keys'] = []
      - L65 assign schema[table] = table_entry
    - L66 except Exception as exc:
      - L67 expr logger.warning('db_signature_pragmas_failed', extra={'event': 'db_signature_pragmas_failed', 'db_path': str(db_path)}, exc_info=exc)
      - L75 return None
  - L77 assign payload = json.dumps(schema, sort_keys=True, separators=(',', ':')).encode('utf-8')
  - L78 return hashlib.sha256(payload).hexdigest()

## backend\app\services\mapping\AutoMapInline.py
- L1 from __future__ import annotations
- L3 import base64
- L4 import copy
- L5 import hashlib
- L6 import json
- L7 import logging
- L8 import re
- L9 from dataclasses import dataclass
- L10 from pathlib import Path
- L11 from typing import Any, Iterable, Mapping, Sequence
- L13 from ..prompts import llm_prompts
- L14 from ..templates.TemplateVerify import MODEL, get_openai_client
- L15 from ..utils import call_chat_completion, extract_tokens, strip_code_fences, validate_mapping_inline_v4
- L21 from ..utils.validation import SchemaValidationError, normalize_mapping_inline_payload
- L22 from .HeaderMapping import REPORT_SELECTED_VALUE
- L24 assign logger = logging.getLogger('neura.mapping.inline')
- L26 assign MAPPING_INLINE_MAX_ATTEMPTS = 5
- L28 assign ALLOWED_SPECIAL_VALUES = {'UNRESOLVED', 'INPUT_SAMPLE', REPORT_SELECTED_VALUE}
- L29 assign LEGACY_WRAPPER_RE = re.compile('(?i)\\b(DERIVED\\s*:|TABLE_COLUMNS\\s*\\[|COLUMN_EXP\\s*\\[|PARAM\\s*:)')
- L30 assign PARAM_REF_RE = re.compile('^params\\.[A-Za-z_][\\w]*$')
- L31 assign _TOKEN_DATE_RE = re.compile('(date|time|month|year)', re.IGNORECASE)
- L32 assign _REPORT_DATE_PREFIXES = {'from', 'to', 'start', 'end', 'begin', 'finish', 'through', 'thru'}
- L42 assign _REPORT_DATE_KEYWORDS = {'date', 'dt', 'day', 'period', 'range', 'time', 'timestamp', 'window', 'month', 'year'}
- L54 assign _REPORT_SELECTED_EXACT = {'page_info', 'page_number', 'page_no', 'page_num', 'page_count', 'page_total', 'page_total_count'}
- L63 assign _REPORT_SELECTED_KEYWORDS = {'page', 'sheet'}
- L67 assign _REPORT_SELECTED_SUFFIXES = {'info', 'number', 'no', 'num', 'count', 'label', 'total'}
- L76 assign _COLUMN_REF_RE = re.compile('\n    ["`\\[]?\n    (?P<table>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    \\.\n    ["`\\[]?\n    (?P<column>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    ', re.VERBOSE)
- L88 assign _SQL_EXPR_HINT_RE = re.compile('\n    [()+\\-*/%]|\n    ::|\n    \\b(\n        SUM|AVG|COUNT|MIN|MAX|\n        CASE|COALESCE|NULLIF|\n        ROW_NUMBER|DENSE_RANK|RANK|NTILE|OVER|\n        LEAD|LAG|\n        ABS|ROUND|TRIM|UPPER|LOWER|SUBSTR|CAST|\n        DATE|DATETIME|IFNULL|IIF|\n        CURRENT_DATE|CURRENT_TIME|CURRENT_TIMESTAMP|\n        LOCALTIME|LOCALTIMESTAMP|NOW|GETDATE|SYSDATE|STRFTIME\n    )\\b\n    ', re.IGNORECASE | re.VERBOSE)
- L107 def _normalized_token_partstoken: str:
  - L108 assign normalized = re.sub('[^a-z0-9]+', '_', str(token or '').lower())
  - L109 return [part for part in normalized.split('_') if part]
- L112 def _is_report_generator_date_tokentoken: str:
  - L113 assign parts = _normalized_token_parts(token)
  - L114 if not parts:
    - L115 return False
  - L116 assign lowered_token = (token or '').lower()
  - L117 if lowered_token in _REPORT_SELECTED_EXACT:
    - L118 return True
  - L119 if any((part in _REPORT_SELECTED_KEYWORDS for part in parts)) and any((part in _REPORT_SELECTED_SUFFIXES for part in parts)):
    - L122 return True
  - L124 assign has_prefix = any((part in _REPORT_DATE_PREFIXES for part in parts))
  - L125 assign has_keyword = any((part in _REPORT_DATE_KEYWORDS for part in parts))
  - L126 if has_prefix and has_keyword:
    - L127 return True
  - L130 if parts[0] in _REPORT_DATE_KEYWORDS and any((part in _REPORT_DATE_PREFIXES for part in parts[1:])):
    - L131 return True
  - L132 if parts[-1] in _REPORT_DATE_KEYWORDS and any((part in _REPORT_DATE_PREFIXES for part in parts[:-1])):
    - L133 return True
  - L135 return False
- L138 def _normalize_report_date_mappingmapping: dict[str, str]:
  - L139 docstring: "Coerce report date tokens to INPUT_SAMPLE so the UI can treat them as report fil..."
  - L140 for (key, value) in list(mapping.items()):
    - L141 if not _is_report_generator_date_token(key):
      - L142 continue
    - L143 assign normalized_value = (value or '').strip()
    - L144 if not normalized_value:
      - L145 continue
    - L146 assign lowered = normalized_value.lower()
    - L147 if PARAM_REF_RE.match(normalized_value) or lowered.startswith('to be selected'):
      - L148 assign mapping[key] = REPORT_SELECTED_VALUE
- L151 class MappingInlineValidationError(RuntimeError):
  - L152 docstring: "Raised when the LLM output fails validation."
- L156 class MappingInlineResult:
  - L157 annotated assign html_constants_applied: str
  - L158 annotated assign mapping: dict[str, str]
  - L159 annotated assign constant_replacements: dict[str, str]
  - L160 annotated assign token_samples: dict[str, str]
  - L161 annotated assign meta: dict[str, Any]
  - L162 annotated assign prompt_meta: dict[str, Any]
  - L163 annotated assign raw_payload: dict[str, Any]
- L166 def _read_png_as_data_uripng_path: Path:
  - L167 if not png_path.exists():
    - L168 return None
  - L169 try:
    - L170 assign data = base64.b64encode(png_path.read_bytes()).decode('utf-8')
    - L171 except Exception:
      - L172 expr logger.exception('mapping_inline_png_read_failed', extra={'path': str(png_path)})
      - L173 return None
  - L174 return f'data:image/png;base64,{data}'
- L177 def _mapping_allowlist_errorsmapping: dict[str, str], catalog: Iterable[str]:
  - L178 assign allowed_catalog = {val.strip() for val in catalog if val}
  - L179 assign allowed = set(allowed_catalog)
  - L180 expr allowed.update(ALLOWED_SPECIAL_VALUES)
  - L181 annotated assign errors: list[str] = []
  - L182 for (key, value) in mapping.items():
    - L183 assign normalized = (value or '').strip()
    - L184 if not normalized:
      - L185 expr errors.append(f'{key!r} -> {value!r}')
      - L186 continue
    - L187 if LEGACY_WRAPPER_RE.search(normalized):
      - L188 expr errors.append(f'{key!r} -> uses legacy wrapper (DERIVED/TABLE_COLUMNS/COLUMN_EXP)')
      - L189 continue
    - L190 if normalized in allowed:
      - L191 continue
    - L192 if PARAM_REF_RE.match(normalized):
      - L193 continue
    - L194 annotated assign referenced: list[str] = [f"{match.group('table')}.{match.group('column')}" for match in _COLUMN_REF_RE.finditer(normalized)]
    - L197 if not referenced and (not _SQL_EXPR_HINT_RE.search(normalized)):
      - L198 expr errors.append(f'{key!r} -> value is not a catalog column, params reference, or recognizable DuckDB SQL expression')
      - L201 continue
    - L202 assign invalid = [col for col in referenced if col not in allowed_catalog]
    - L203 if invalid:
      - L204 expr errors.append(f'{key!r} -> references columns outside catalog: {sorted(invalid)}')
  - L205 return errors
- L208 def _alias_lookup_keyslowered_key: str:
  - L209 annotated assign candidates: list[str] = []
  - L210 if not lowered_key:
    - L211 return candidates
  - L213 if lowered_key.endswith('_wt_kg'):
    - L214 assign base = lowered_key[:-len('_wt_kg')]
    - L215 if base:
      - L216 expr candidates.append(f'{base}_wt')
  - L218 if lowered_key.endswith('_kg'):
    - L219 assign base = lowered_key[:-len('_kg')].rstrip('_')
    - L220 if base:
      - L221 expr candidates.append(base)
  - L223 annotated assign filtered: list[str] = []
  - L224 for candidate in candidates:
    - L225 assign candidate = candidate.strip('_')
    - L226 if candidate and candidate != lowered_key:
      - L227 expr filtered.append(candidate)
  - L228 return filtered
- L231 def _align_mapping_to_template_tokensmapping: Mapping[str, str], template_tokens: Iterable[str]:
  - L235 docstring: "\n    Remap LLM-provided mapping keys onto the actual placeholders that exist in..."
  - L239 annotated assign token_lookup: dict[str, str] = {}
  - L240 annotated assign row_suffix_lookup: dict[str, str] = {}
  - L241 for raw_token in template_tokens:
    - L242 assign token = str(raw_token or '')
    - L243 if not token:
      - L244 continue
    - L245 assign lowered = token.lower()
    - L246 expr token_lookup.setdefault(lowered, token)
    - L247 if lowered.startswith('row_') and len(token) > 4:
      - L248 assign suffix = lowered[4:]
      - L249 if suffix:
        - L250 expr row_suffix_lookup.setdefault(suffix, token)
  - L252 annotated assign aligned: dict[str, str] = {}
  - L253 annotated assign remapped: dict[str, str] = {}
  - L254 for (raw_key, value) in mapping.items():
    - L255 assign key = str(raw_key or '')
    - L256 assign lowered = key.lower()
    - L257 assign target = token_lookup.get(lowered) or row_suffix_lookup.get(lowered)
    - L258 if target is None:
      - L259 assign alias_target = None
      - L260 for alias_key in _alias_lookup_keys(lowered):
        - L261 assign alias_target = token_lookup.get(alias_key) or row_suffix_lookup.get(alias_key)
        - L262 if alias_target:
          - L263 break
      - L264 assign target = alias_target
    - L266 if target is None:
      - L267 if key not in aligned:
        - L268 assign aligned[key] = value
      - L269 continue
    - L270 if target in aligned:
      - L271 continue
    - L272 assign aligned[target] = value
    - L273 if target != key:
      - L274 assign remapped[key] = target
  - L276 return (aligned, remapped)
- L279 def _validate_constant_replacementstemplate_html: str, replacements: Mapping[str, Any], schema: dict[str, Any] | None:
  - L284 if replacements is None:
    - L285 return set()
  - L286 if not isinstance(replacements, Mapping):
    - L287 raise MappingInlineValidationError('constant_replacements must be an object')
  - L289 assign available_tokens = set(extract_tokens(template_html))
  - L290 if not available_tokens and replacements:
    - L291 raise MappingInlineValidationError('Template does not contain any placeholders to replace')
  - L293 annotated assign schema_tokens: set[str] = set()
  - L294 if isinstance(schema, dict):
    - L295 for key in ('row_tokens', 'totals'):
      - L296 assign values = schema.get(key)
      - L297 if isinstance(values, list):
        - L298 expr schema_tokens.update((str(v).strip() for v in values if v))
  - L300 annotated assign seen: set[str] = set()
  - L301 annotated assign inline_tokens: set[str] = set()
  - L302 for (raw_token, raw_value) in replacements.items():
    - L303 assign token = str(raw_token or '').strip()
    - L304 if not token:
      - L305 raise MappingInlineValidationError('constant_replacements keys must be non-empty strings')
    - L306 if token in seen:
      - L307 raise MappingInlineValidationError(f'Duplicate constant token recorded: {token}')
    - L308 expr seen.add(token)
    - L310 if token not in available_tokens:
      - L311 raise MappingInlineValidationError(f"Token '{token}' not present in template HTML")
    - L313 if token.lower().startswith('row_'):
      - L314 raise MappingInlineValidationError(f"Token '{token}' is a row-level placeholder and cannot be treated as a constant")
    - L317 if token in schema_tokens:
      - L318 raise MappingInlineValidationError(f"Token '{token}' is defined as dynamic in the schema")
    - L319 if _TOKEN_DATE_RE.search(token):
      - L320 raise MappingInlineValidationError(f"Date-like token '{token}' cannot be treated as a constant")
    - L322 if raw_value is None:
      - L323 raise MappingInlineValidationError(f"constant_replacements['{token}'] cannot be null")
    - L325 expr inline_tokens.add(token)
  - L327 return inline_tokens
- L330 def _replace_tokenhtml: str, token: str, value: str:
  - L331 assign patterns = [re.compile(f'\\{{\\{{\\s*{re.escape(token)}\\s*\\}}\\}}'), re.compile(f'\\{{\\s*{re.escape(token)}\\s*\\}}')]
  - L335 for pattern in patterns:
    - L336 assign html = pattern.sub(value, html)
  - L337 return html
- L340 def _apply_constant_replacementshtml: str, replacements: Mapping[str, Any]:
  - L341 assign updated = html
  - L342 for (token, raw_value) in replacements.items():
    - L343 assign value = str(raw_value)
    - L344 assign updated = _replace_token(updated, str(token), value)
  - L345 return updated
- L348 def _normalize_token_samplestoken_samples_raw: Mapping[str, Any] | None, expected_tokens: set[str], *, allow_missing_tokens: bool=False:
  - L354 if not isinstance(token_samples_raw, Mapping):
    - L355 raise MappingInlineValidationError('token_samples must be an object with token -> literal value')
  - L357 annotated assign normalized: dict[str, str] = {}
  - L358 for (raw_key, raw_value) in token_samples_raw.items():
    - L359 assign token = str(raw_key or '').strip()
    - L360 if not token:
      - L361 raise MappingInlineValidationError('token_samples keys must be non-empty token names')
    - L362 if token in normalized:
      - L363 raise MappingInlineValidationError(f"Duplicate token_samples entry for '{token}'")
    - L365 if raw_value is None:
      - L366 assign value = ''
      - L368 else:
        - L368 assign value = str(raw_value)
    - L369 if not value.strip():
      - L370 raise MappingInlineValidationError(f"token_samples['{token}'] must be a non-empty literal string (use NOT_VISIBLE/UNREADABLE when necessary)")
    - L374 assign normalized[token] = value
  - L376 assign missing = sorted(expected_tokens - set(normalized))
  - L377 if missing:
    - L378 raise MappingInlineValidationError(f'token_samples missing entries for tokens: {missing}')
  - L380 assign extras = sorted(set(normalized) - expected_tokens)
  - L381 if extras:
    - L382 if allow_missing_tokens:
      - L383 for extra in extras:
        - L384 expr normalized.pop(extra, None)
      - L386 else:
        - L386 raise MappingInlineValidationError(f'token_samples contains unknown tokens: {extras}')
  - L388 return normalized
- L391 def _ensure_dictvalue: Any, label: str:
  - L392 if not isinstance(value, dict):
    - L393 raise MappingInlineValidationError(f'{label} must be an object')
  - L394 return value
- L397 def _build_messagessystem_text: str, user_text: str, attachments: Sequence[dict[str, Any]], png_data_uri: str | None, validation_feedback: str | None:
  - L404 annotated assign messages: list[dict[str, Any]] = []
  - L405 if system_text:
    - L406 expr messages.append({'role': 'system', 'content': [{'type': 'text', 'text': system_text}]})
  - L408 annotated assign user_content: list[dict[str, Any]] = [{'type': 'text', 'text': user_text}]
  - L409 expr user_content.extend(attachments or [])
  - L410 if png_data_uri:
    - L411 expr user_content.append({'type': 'image_url', 'image_url': {'url': png_data_uri}})
  - L412 if validation_feedback:
    - L413 expr user_content.append({'type': 'text', 'text': f'VALIDATION_FEEDBACK:\n{validation_feedback}\nPlease correct the issues above and resend a compliant JSON response.'})
  - L423 expr messages.append({'role': 'user', 'content': user_content})
  - L424 return messages
- L427 def catalog_sha256catalog: Sequence[str]:
  - L428 assign normalized = sorted({str(item).strip() for item in catalog if item})
  - L429 assign serialized = '\n'.join(normalized).encode('utf-8')
  - L430 return hashlib.sha256(serialized).hexdigest()
- L433 def schema_sha256schema: dict[str, Any] | None:
  - L434 if schema is None:
    - L435 return hashlib.sha256(b'null').hexdigest()
  - L436 assign payload = json.dumps(schema, ensure_ascii=False, sort_keys=True).encode('utf-8')
  - L437 return hashlib.sha256(payload).hexdigest()
- L440 def prompt_sha256system_text: str, user_text: str:
  - L441 assign combined = f'{system_text.strip()}\n---\n{user_text.strip()}'.encode('utf-8')
  - L442 return hashlib.sha256(combined).hexdigest()
- L445 def run_llm_call_3template_html: str, catalog: Sequence[str], schema: dict[str, Any] | None, prompt_version: str, png_path: str, cache_key: str, prompt_builder=None, *, allow_missing_tokens: bool=False:
  - L456 assign builder = prompt_builder or llm_prompts.build_llm_call_3_prompt
  - L457 assign prompt_payload = builder(template_html, catalog, schema)
  - L458 assign system_text = prompt_payload.get('system', '')
  - L459 assign user_text = prompt_payload.get('user', '')
  - L460 assign attachments = prompt_payload.get('attachments', [])
  - L462 assign prompt_hash = prompt_sha256(system_text, user_text)
  - L463 assign catalog_hash = catalog_sha256(catalog)
  - L464 assign schema_hash = schema_sha256(schema)
  - L465 assign pre_html_hash = hashlib.sha256((template_html or '').encode('utf-8')).hexdigest()
  - L467 assign png_uri = _read_png_as_data_uri(Path(png_path)) if png_path else None
  - L469 assign client = get_openai_client()
  - L470 annotated assign validation_feedback: str | None = None
  - L471 annotated assign last_error: Exception | None = None
  - L473 for attempt in range(1, MAPPING_INLINE_MAX_ATTEMPTS + 1):
    - L474 assign messages = _build_messages(system_text, user_text, attachments, png_uri, validation_feedback)
    - L475 try:
      - L476 expr logger.info('mapping_inline_call_start', extra={'event': 'mapping_inline_call_start', 'attempt': attempt, 'prompt_version': prompt_version, 'prompt_sha256': prompt_hash, 'catalog_sha256': catalog_hash, 'schema_sha256': schema_hash, 'cache_key': cache_key})
      - L488 assign response = call_chat_completion(client, model=MODEL, messages=messages, description=f'{prompt_version}')
      - L494 except Exception as exc:
        - L495 expr logger.exception('mapping_inline_call_failed', extra={'event': 'mapping_inline_call_failed', 'attempt': attempt, 'prompt_version': prompt_version, 'cache_key': cache_key})
        - L504 raise RuntimeError(f'LLM call failed for {prompt_version}')
    - L506 assign raw_text = (response.choices[0].message.content or '').strip()
    - L507 assign parsed_text = strip_code_fences(raw_text)
    - L508 try:
      - L509 assign payload = json.loads(parsed_text)
      - L510 except Exception as exc:
        - L511 assign last_error = MappingInlineValidationError(f'Invalid JSON response: {exc}')
        - L512 expr logger.warning('mapping_inline_json_parse_failed', extra={'event': 'mapping_inline_json_parse_failed', 'attempt': attempt, 'prompt_version': prompt_version, 'cache_key': cache_key})
        - L521 assign validation_feedback = str(last_error)
        - L522 continue
    - L524 assign payload = normalize_mapping_inline_payload(payload)
    - L525 assign raw_payload = copy.deepcopy(payload)
    - L527 try:
      - L528 try:
        - L529 expr validate_mapping_inline_v4(payload)
        - L530 except SchemaValidationError as exc:
          - L531 raise MappingInlineValidationError(str(exc))
      - L532 assign mapping_raw = _ensure_dict(payload.get('mapping'), 'mapping')
      - L533 assign mapping = {str(k): str(v) for k, v in mapping_raw.items()}
      - L534 expr _normalize_report_date_mapping(mapping)
      - L536 assign original_tokens = set(extract_tokens(template_html))
      - L537 assign (mapping, remapped_aliases) = _align_mapping_to_template_tokens(mapping, original_tokens)
      - L538 if remapped_aliases:
        - L539 expr logger.info('mapping_inline_row_token_aligned', extra={'event': 'mapping_inline_row_token_aligned', 'attempt': attempt, 'aliases': remapped_aliases, 'prompt_version': prompt_version, 'cache_key': cache_key})
      - L550 assign allowlist_errors = _mapping_allowlist_errors(mapping, catalog)
      - L551 if allowlist_errors:
        - L552 raise MappingInlineValidationError('Mapping values outside allow-list: ' + ', '.join(allowlist_errors))
      - L559 assign row_like_tokens = {tok for tok in original_tokens if str(tok).lower().startswith('row_')}
      - L561 assign token_samples = _normalize_token_samples(payload.get('token_samples'), original_tokens, allow_missing_tokens=allow_missing_tokens)
      - L566 assign constant_tokens = original_tokens - set(mapping.keys()) - row_like_tokens
      - L567 assign constant_entries = {token: token_samples[token] for token in constant_tokens}
      - L568 assign inline_token_set = _validate_constant_replacements(template_html, constant_entries, schema)
      - L570 assign missing_tokens = [token for token in list(mapping.keys()) if token not in original_tokens]
      - L571 if missing_tokens:
        - L572 assign log_event = 'mapping_inline_missing_tokens_allowed' if allow_missing_tokens else 'mapping_inline_missing_tokens'
        - L575 assign log_level = logger.info if allow_missing_tokens else logger.warning
        - L576 expr log_level(log_event, extra={'event': log_event, 'attempt': attempt, 'tokens': sorted(missing_tokens), 'prompt_version': prompt_version, 'cache_key': cache_key})
        - L586 if not allow_missing_tokens:
          - L587 for token in missing_tokens:
            - L588 expr mapping.pop(token, None)
      - L590 assign overlap = inline_token_set.intersection(set(mapping.keys()))
      - L591 if overlap:
        - L592 raise MappingInlineValidationError(f'Constant tokens still present in mapping: {sorted(overlap)}')
      - L594 assign html_constants_applied = _apply_constant_replacements(template_html, constant_entries)
      - L596 assign updated_tokens = set(extract_tokens(html_constants_applied))
      - L597 assign added_tokens = updated_tokens - original_tokens
      - L598 if added_tokens:
        - L599 raise MappingInlineValidationError(f'New tokens introduced: {sorted(added_tokens)}')
      - L600 assign removed_tokens = original_tokens - updated_tokens
      - L601 if removed_tokens != inline_token_set:
        - L602 raise MappingInlineValidationError(f'Token removal mismatch. Expected removal {sorted(inline_token_set)}, observed {sorted(removed_tokens)}')
      - L607 assign meta = _ensure_dict(payload.get('meta'), 'meta')
      - L608 if missing_tokens:
        - L609 assign dropped_tokens = meta.get('dropped_tokens')
        - L610 if isinstance(dropped_tokens, list):
          - L611 expr dropped_tokens.extend(sorted(missing_tokens))
          - L613 else:
            - L613 assign meta['dropped_tokens'] = sorted(missing_tokens)
      - L615 assign unresolved = meta.get('unresolved')
      - L616 if isinstance(unresolved, list):
        - L617 assign meta['unresolved'] = [tok for tok in unresolved if tok in updated_tokens]
      - L619 assign ambiguous = meta.get('ambiguous')
      - L620 if isinstance(ambiguous, list):
        - L621 assign meta['ambiguous'] = [entry for entry in ambiguous if isinstance(entry, dict) and entry.get('header') in mapping]
      - L625 assign hints = meta.get('hints')
      - L626 if isinstance(hints, dict):
        - L627 assign meta['hints'] = {key: value for key, value in hints.items() if key in mapping}
      - L629 assign confidence = meta.get('confidence')
      - L630 if isinstance(confidence, dict):
        - L631 assign meta['confidence'] = {key: value for key, value in confidence.items() if key in mapping}
      - L633 assign replacements_clean = {str(k): str(v) for k, v in constant_entries.items()}
      - L634 assign raw_payload['token_samples'] = token_samples
      - L635 assign raw_payload['constant_replacements'] = replacements_clean
      - L636 assign post_html_hash = hashlib.sha256(html_constants_applied.encode('utf-8')).hexdigest()
      - L638 expr logger.info('mapping_inline_call_success', extra={'event': 'mapping_inline_call_success', 'attempt': attempt, 'prompt_version': prompt_version, 'prompt_sha256': prompt_hash, 'pre_html_sha256': pre_html_hash, 'post_html_sha256': post_html_hash, 'cache_key': cache_key})
      - L651 return MappingInlineResult(html_constants_applied=html_constants_applied, mapping=mapping, constant_replacements=replacements_clean, token_samples=token_samples, meta=meta, prompt_meta={'version': prompt_version, 'prompt_sha256': prompt_hash, 'catalog_sha256': catalog_hash, 'schema_sha256': schema_hash, 'cache_key': cache_key, 'pre_html_sha256': pre_html_hash, 'post_html_sha256': post_html_hash}, raw_payload=raw_payload)
      - L668 except MappingInlineValidationError as exc:
        - L669 assign last_error = exc
        - L670 expr logger.warning('mapping_inline_validation_failed', extra={'event': 'mapping_inline_validation_failed', 'attempt': attempt, 'error': str(exc), 'prompt_version': prompt_version, 'cache_key': cache_key})
        - L680 assign validation_feedback = str(exc)
        - L681 continue
  - L683 assert last_error is not None
  - L684 expr logger.error('mapping_inline_failed', extra={'event': 'mapping_inline_failed', 'prompt_version': prompt_version, 'cache_key': cache_key})
  - L692 raise MappingInlineValidationError(str(last_error))
- L695 assign __all__ = ['MappingInlineResult', 'run_llm_call_3', 'catalog_sha256', 'schema_sha256', 'prompt_sha256', 'MappingInlineValidationError']

## backend\app\services\mapping\CorrectionsPreview.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\mapping\HeaderMapping.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\prompts\__init__.py
- L1 docstring: "\nPrompt builders for backend LLM calls.\n\nCurrently exposes:\n    - build_llm_..."
- L8 from .llm_prompts import build_llm_call_3_prompt
- L10 assign __all__ = ['build_llm_call_3_prompt']

## backend\app\services\prompts\llm_prompts.py
- L2 from __future__ import annotations
- L4 import base64
- L5 import json
- L6 import re
- L7 from functools import lru_cache
- L8 from pathlib import Path
- L9 from textwrap import dedent
- L10 from typing import Any, Dict, Iterable, Mapping
- L12 assign PROMPT_VERSION = 'llm_call_3_v7'
- L13 assign PROMPT_VERSION_3_5 = 'v4'
- L14 assign PROMPT_VERSION_4 = 'v2'
- L15 annotated assign LLM_CALL_PROMPTS: Dict[str, str] = {'llm_call_1': dedent('        Produce a COMPLETE, self-contained HTML document (<!DOCTYPE html> ...) with inline <style>. It must visually photocopy the given PDF page image as closely as possible. Mirror fonts, spacing, borders, alignment, and table layouts. Tables must use border-collapse, 1px borders, and table-layout: fixed for neat alignment.\n\n        SCHEMA USAGE\n        - If a SCHEMA is provided below, use ONLY placeholders from that SCHEMA exactly as written (same names).\n        - If SCHEMA is NOT provided, FIRST infer a compact schema (see OUTPUT FORMAT for SCHEMA_JSON) and then use ONLY those tokens in the HTML.\n        - In HTML, placeholders must be written as {token} (single braces). In SCHEMA_JSON they must appear WITHOUT braces (e.g., "report_title").\n        - If a value is not in SCHEMA/SCHEMA_JSON, render it as literal text. If a token exists in SCHEMA/SCHEMA_JSON but does not appear on this page, omit it.\n\n        REPEATABLE BLOCK (edge case)\n        - If the page clearly contains repeating sections (visually identical blocks stacked vertically), output ONE prototype of that block wrapped exactly as:\n        <!-- BEGIN:BLOCK_REPEAT batches -->\n        <section class=\'batch-block\'>...</section>\n        <!-- END:BLOCK_REPEAT -->\n        - Place header/footer OUTSIDE these markers. Do NOT clone or duplicate multiple blocks.\n\n        ROW PROTOTYPES\n        - For tables with repeating rows, output headers plus a single <tbody><tr>...</tr></tbody> row prototype.\n        - Keep any final summary/total row if it exists.\n\n        STRUCTURE & CSS\n        - The result must be printable: use @page size A4 with sensible margins.\n        - Prefer flowing layout (avoid fixed heights). Avoid absolute positioning except for persistent header/footer if clearly present.\n        - Reproduce what is visible - draw ONLY the rules/lines that exist in the image. Default to no borders and transparent backgrounds; add borders per edge only where a line is visible.\n        - Use table markup ONLY for true grids and structured data (never div-based). Use borderless tables or simple divs for key/value areas. Avoid unnecessary nested tables or enclosing frames.\n        - Right-align numeric columns where appropriate; keep typographic rhythm tight enough to match the PDF.\n\n        PROJECT-SPECIFIC ADDITIONS (to aid later steps while keeping fidelity)\n        - Add a minimal set of CSS custom properties to make column widths and key spacings easy to refine later (e.g., :root { --col-1-w: 24mm; --row-gap: 2mm; }). Use these variables inside the CSS you produce.\n        - Add stable IDs for major zones only (no extra wrappers): #report-header, #data-table (main grid), #report-totals (if present). Do NOT add decorative containers.\n        - For table header cells, include a data-label attribute with the visible header text normalized (e.g., <th data-label="header_label">Material Name</th>). Visible text must remain unchanged.\n\n        OUTPUT RULES\n        - No lorem ipsum or sample values. No external resources.\n        - No comments except the repeat markers if applicable.\n        - Do NOT rename or invent tokens beyond SCHEMA/SCHEMA_JSON.\n        - Return ONLY the outputs described below - no markdown fences, no explanations.\n\n        OUTPUT FORMAT\n        1) First, the RAW HTML between these exact markers:\n        <!--BEGIN_HTML-->\n        ...full html...\n        <!--END_HTML-->\n\n        2) Then, the SCHEMA JSON between these markers (EXAMPLE ONLY - replace with actual tokens discovered from THIS page; do NOT output these example names):\n        <!--BEGIN_SCHEMA_JSON-->\n        {\n          "scalars": ["scalar_token_1","scalar_token_2","scalar_token_3"],\n          "row_tokens": ["row_token_1","row_token_2","row_token_3"],\n          "totals": ["total_token_1","total_token_2","total_token_3"],\n          "notes": ""\n        }\n        <!--END_SCHEMA_JSON-->\n\n        If SCHEMA is provided below, ensure SCHEMA_JSON you output matches it exactly (names and membership). If SCHEMA is not provided, infer SCHEMA_JSON consistently with the placeholders you used in the HTML (one-to-one, no extras).\n\n        [INPUTS]\n        - PDF page image is attached.\n        - SCHEMA (may be absent):\n        SCHEMA:\n        {schema_str}\n        ').strip(), 'llm_call_2': dedent('        Compare these images: REFERENCE (PDF page) vs RENDER (current HTML). SSIM={{ssim_value:.4f}}.\n        Goal: refine the provided HTML/CSS so the render becomes a near-perfect PHOTOCOPY of the reference.\n\n        STRICT RULES (unchanged core)\n        - Do NOT rename, add, remove, or move SCHEMA placeholders; keep all tokens exactly as in the current HTML.\n        - Do NOT change the number of repeating sections or table rows that currently exist in the HTML.\n        - If repeat markers (e.g., <!-- BEGIN:BLOCK_REPEAT ... -->) are present, keep them unchanged with exactly one prototype inside.\n        - Prefer CSS edits; only introduce minimal HTML wrappers (e.g., structural containers/colgroups) if strictly necessary to achieve alignment - never alter tokens.\n\n        PROJECT-SPECIFIC ADDITIONS (minimal, to suit our pipeline)\n        - First adjust existing CSS custom properties if present (e.g., --col-1-w, --col-2-w, --row-gap, --header-h); only if insufficient, then edit rules.\n        - Respect stable IDs/zones if present (#report-header, #data-table, #report-totals). Do not add decorative wrappers.\n        - Use millimetre-based sizing where practical for print parity (e.g., widths, padding, margins in mm). Snap adjustments to small increments (~0.1mm / 0.25px) to avoid jitter.\n        - For numeric columns, ensure right alignment and enable tabular figures (font-variant-numeric: tabular-nums) so digits align vertically.\n        - Borders/lines: match only what is visible in the reference; prefer per-edge borders to mimic single ruled lines. Avoid box-shadows, rounded corners, blurs, gradients, or tints unless clearly present in the reference.\n        - Pagination stability: keep .batch-block printable (break-inside: avoid; page-break-inside: avoid); do not fix heights unless the reference shows fixed frames.\n        - Do NOT scale the entire page via CSS transforms to "cheat" alignment. Correct geometry via widths, margins, paddings, line-height, letter-spacing, and colgroup widths.\n\n        VISUAL MATCHING (unchanged intent)\n        Identify and correct EVERY visible discrepancy between reference and render at any scale. Infer and adjust geometry, proportions, typography and line metrics, borders/line weights, grid/column structure, text/number alignment, intra/inter-block spacing, pagination behavior, page frame presence, and header/footer placement. Derive all values from the reference image; do not assume defaults. The result should be indistinguishable from the reference when printed.\n\n        OUTPUT (tightened for our tooling)\n        - Return FULL HTML (<!DOCTYPE html> ...) with inline <style> only - no external resources.\n        - No markdown, no commentary, no sample data.\n        - Preserve existing IDs/classes/markers; add only what is minimally required for fidelity.\n        - Wrap output exactly between these markers:\n        <!--BEGIN_HTML-->\n        ...full refined html...\n        <!--END_HTML-->\n\n        [INPUTS]\n        SCHEMA:\n        {schema_str}\n\n        [REFERENCE_IMAGE]\n        (embedded image URL)\n\n        [RENDER_IMAGE]\n        (embedded image URL)\n\n        [CURRENT_HTML]\n        {current_html}\n        ').strip()}
- L128 assign _INPUT_MARKER = '[INPUTS]'
- L129 assign _CALL3_PROMPT_SECTION = '\nYou are given the FULL HTML of a report template and a strict DB CATALOG. Your job now has TWO parts:\nA) AUTO-MAPPING:\nIdentify all visible header/label texts that correspond to data fields (table headers, field labels, totals, footer labels, etc.) and map each token/label to exactly one database column from the allow-list CATALOG.\nB) CONSTANT VALUE DISCOVERY:\nDetect placeholders/tokens that are visually constant on the reference PDF (e.g., report title, company/brand name, static section captions) and record their literal values WITHOUT editing the HTML. Only capture constants when you are 100% confident the value is not per-run data (i.e., will not vary by date/batch/user). Leave truly dynamic tokens untouched.\n--------------------------------------------------------------------------------\nGOALS\n- Produce a JSON object that (1) proposes a strict mapping, and (2) lists constant placeholders you can safely replace using the sample PDF values.\n- Treat the provided HTML as read-only context; do not attempt to rewrite or return it.\n- Ignore join/date speculation entirely.\nCORE RULES (unchanged)\n- Choose strictly from CATALOG (fully-qualified "table.column") when a token maps directly to a single source column.\n- If the value should be passed through from request parameters, return params.param_name (lower snake_case).\n- For report filter or paging tokens (e.g., from_date, to_date, start_date, end_date, date_from, date_to, range_start, range_end, page_info, page_number, page_no), return the literal string "To Be Selected..." in the mapping so the report generator can populate them later (these surface to users as "To Be Selected in Report generator"). Treat any similar date-range or page metadata fields the same. Do NOT map these to params.* or table columns.\n- If no clear source exists, set the mapping value to UNRESOLVED.\n- RUNTIME CONTEXT: Every SQL fragment you output is executed by DuckDB against pandas DataFrames (each catalog table is already materialised as a DataFrame). Use portable DuckDB-compatible SQL only; reference columns exactly as they appear in the catalog and avoid engine-specific extensions.\n- If a header requires combining multiple columns, return a DuckDB-compatible SQL expression that references only catalog columns.\n- Never emit legacy wrappers such as DERIVED:, TABLE_COLUMNS[\u2026], or COLUMN_EXP[\u2026]; the raw SQL fragment itself is required.\n- Do not invent headers, tokens, tables, columns, or duplicate mappings.\n- Prefer concise, human-visible labels (strip punctuation/colons) for header keys.\nHEADER KEYING (same as before)\n- If a <th> has data-label, only use that value (converted to lowercase snake_case) when the same token name also appears as a {placeholder} in the HTML or is listed in SCHEMA. Otherwise treat the data-label as decorative and fall back to the visible header text.\nSYNONYMS/SHORTHANDS TO NORMALIZE\n- set/set_wt -> set_weight\n- ach/achieved -> achieved_weight\n- err -> error_kg\n- err%/error% -> error_percent\n- sl/serial -> sl_no\n- name/material -> material_name\nENUMERATED/AGGREGATE HEADERS\n- If a header clearly represents an aggregate across enumerated columns (e.g., bin1_sp..bin12_sp, bin1_act..bin12_act), do NOT guess a single column. Return a valid DuckDB-compatible SQL expression that sums/averages/etc. using only catalog columns (e.g., `SUM(recipes.bin1_sp + ... + recipes.bin12_sp)` or a CASE expression). Record the contributing columns under meta.hints[header], for example:\n  { "op": "SUM", "over": ["qualified col1","qualified col2","..."] } derived from CATALOG.\n- If you cannot confidently enumerate the contributing columns, keep it UNRESOLVED.\nCONSTANT PLACEHOLDERS (UPDATED)\n- Report ONLY tokens that are truly constant across runs (e.g., page titles, company name/logo text, static captions).\n- NEVER mark tokens that are per-run or DB-driven: dates, row values, totals, page numbers, or anything under schema.row_tokens/totals/date-like fields.\n- You may ONLY record a constant when that placeholder actually exists in the provided HTML. If the schema lists a token but you do not see its placeholder in the template, leave it unmapped (and call it out under `meta.unresolved`/`meta.unresolved_missing_tokens`) instead of inventing a constant entry.\n- Remove constant tokens from the "mapping" object so downstream steps treat them as literals.\nTOKEN SNAPSHOT (UPDATED)\n- Emit a "token_samples" object that enumerates EVERY placeholder token from the HTML (exact token name, no braces).\n- For each token output the literal string you see on the PDF (best effort). Never leave it blank\u2014if the token is absent or unreadable, return a descriptive fallback such as "NOT_VISIBLE" or "UNREADABLE".\n- Match capitalization, punctuation, and spacing exactly when the text is clear.\nINPUTS\n[FULL_HTML]\n{html_for_llm}\n[CATALOG]\n{catalog_json}\nOptional:\n[SCHEMA_JSON]\n{schema_json_if_any}\n[REFERENCE_PNG_HINT]\n"A screenshot of the reference PDF was used to create this template; treat visible page titles/branding as likely constants."\nOUTPUT -- STRICT JSON ONLY (v7)\n{\n  "mapping": {\n    "<header_or_token>": "<table.column | params.param_name | UNRESOLVED | DuckDB SQL expression using only catalog columns>"\n  },\n  "token_samples": {\n    "<token>": "<literal string>"\n  },\n  "meta": {\n    "unresolved": ["<token>", "..."],\n    "hints": {\n      "<token>": { "op": "SUM", "over": ["table.bin1_sp","...","table.bin12_sp"] }\n    }\n  }\n}\nVALIDATION & FORMATTING\n- Output ONE JSON object only. No markdown, no commentary.\n- Every mapping value must either (a) match a catalog entry exactly, (b) use the params.param_name form for request parameters, (c) be the literal UNRESOLVED, or (d) be a DuckDB-compatible SQL expression that references only catalog columns.\n- Reject / avoid legacy wrappers such as DERIVED:, TABLE_COLUMNS[\u2026], COLUMN_EXP[\u2026]; if you detect that pattern, resolve it into the raw SQL or fall back to UNRESOLVED.\n- Any token you remove from "mapping" (because it is constant) must still appear in "token_samples" with the literal string you inlined.\n- Do not add or rename remaining tokens. Do not alter repeat markers/tbody row prototypes.\n- "token_samples" must include every placeholder exactly once and each value must be a non-empty string (use "NOT_VISIBLE"/"UNREADABLE" instead of leaving blanks).\n'.strip()
- L208 def _load_llm_call_3_section:
  - L209 assign section = _CALL3_PROMPT_SECTION
  - L210 if _INPUT_MARKER in section:
    - L211 assign (system, remainder) = section.split(_INPUT_MARKER, 1)
    - L212 assign system_text = system.strip()
    - L213 assign user_template = f'{_INPUT_MARKER}{remainder}'.strip()
    - L215 else:
      - L215 assign system_text = section.strip()
      - L216 assign user_template = ''
  - L218 return (system_text, user_template)
- L221 def _sanitize_htmlhtml: str:
  - L222 docstring: "\n    Strip comments, scripts, and excessive whitespace to keep prompts compact...."
  - L225 assign html = html or ''
  - L227 assign comment_re = re.compile('(?is)<!--(?!\\s*(BEGIN:BLOCK_REPEAT|END:BLOCK_REPEAT)).*?-->')
  - L228 assign script_re = re.compile('(?is)<script\\b[^>]*>.*?</script>')
  - L229 assign collapsed = script_re.sub('', comment_re.sub('', html))
  - L230 assign collapsed = re.sub('[ \\t]{2,}', ' ', collapsed)
  - L231 assign collapsed = re.sub('\\n{3,}', '\n\n', collapsed)
  - L232 return collapsed.strip()
- L235 def _format_catalogcatalog: Iterable[str]:
  - L236 assign catalog_list = [str(item).strip() for item in catalog]
  - L237 return json.dumps(catalog_list, ensure_ascii=False, indent=2)
- L240 def _normalize_schema_payloadschema: Mapping[str, Any] | None:
  - L241 assign base = {'scalars': [], 'row_tokens': [], 'totals': [], 'notes': ''}
  - L247 if not isinstance(schema, Mapping):
    - L248 return base
  - L250 def _collectkey: str:
    - L251 assign values = schema.get(key, [])
    - L252 if isinstance(values, Iterable) and (not isinstance(values, (str, bytes))):
      - L253 return [str(item).strip() for item in values if str(item).strip()]
    - L254 return []
  - L256 assign base['scalars'] = _collect('scalars')
  - L257 assign base['row_tokens'] = _collect('row_tokens')
  - L258 assign base['totals'] = _collect('totals')
  - L260 assign notes = schema.get('notes')
  - L261 if notes is not None:
    - L262 assign base['notes'] = str(notes)
  - L263 return base
- L266 def _format_schemaschema: Dict[str, Any] | None:
  - L267 assign normalized = _normalize_schema_payload(schema)
  - L268 return json.dumps(normalized, ensure_ascii=False, indent=2, sort_keys=True)
- L271 def _row_token_hintschema: Mapping[str, Any] | None:
  - L272 assign normalized = _normalize_schema_payload(schema)
  - L273 assign row_tokens = [tok for tok in normalized.get('row_tokens', []) if str(tok).lower().startswith('row_')]
  - L274 if not row_tokens:
    - L275 return ''
  - L277 assign preview = ', '.join(row_tokens[:8])
  - L278 if len(row_tokens) > 8:
    - L279 aug assign preview Add ', ...'
  - L281 assign hint_lines = ['ROW TOKEN NAMING', '- The HTML template exposes repeating-row placeholders that already include the `row_` prefix.', '- When producing the mapping, reference those tokens verbatim (including the prefix and casing).', f'- Example row tokens: {preview}']
  - L287 return '\n'.join(hint_lines)
- L290 def build_llm_call_3_prompthtml: str, catalog: Iterable[str], schema_json: Dict[str, Any] | None=None:
  - L295 docstring: "\n    Build the system/user payload for LLM Call 3 (auto-map + constant discover..."
  - L298 assign (system_template, user_template) = _load_llm_call_3_section()
  - L299 if not user_template:
    - L300 assign user_template = system_template
    - L301 assign system_template = 'You are a meticulous analyst that performs report auto-mapping and constant inlining. Follow the subsequent instructions strictly.'
  - L306 assign html_block = _sanitize_html(html)
  - L307 assign catalog_block = _format_catalog(catalog)
  - L308 assign schema_block = _format_schema(schema_json)
  - L309 assign row_hint = _row_token_hint(schema_json)
  - L311 assign user_payload = user_template
  - L312 for (placeholder, value) in (('{html_for_llm}', html_block), ('{catalog_json}', catalog_block), ('{schema_json_if_any}', schema_block)):
    - L317 assign user_payload = user_payload.replace(placeholder, value)
  - L319 if row_hint:
    - L320 assign user_payload = f'{user_payload.strip()}\n\n{row_hint}'
  - L322 annotated assign attachments: list[dict[str, Any]] = []
  - L323 if '[REFERENCE_PNG_HINT]' not in user_payload:
    - L324 expr attachments.append({'type': 'text', 'text': '[REFERENCE_PNG_HINT]\n"A screenshot of the reference PDF was used to create this template; treat visible page titles/branding as likely constants."'})
  - L335 return {'system': system_template.strip(), 'user': user_payload.strip(), 'attachments': attachments, 'version': PROMPT_VERSION}
- L343 annotated assign LLM_CALL_3_5_PROMPT: Dict[str, str] = {'system': dedent('        You are the Step 3.5 corrections specialist.\n        Your responsibilities:\n        A) Apply every explicit user instruction to the HTML template. Text edits, structural tweaks, CSS adjustments, and token changes are all allowed when the user asks. Do not invent changes or perform a wholesale redesign unless the user requests it.\n        B) Inline any token whose mapping (or explicit user instruction) marks it as a constant (e.g., mapping value "INPUT_SAMPLE"). Use the literals provided in `mapping_context.token_samples` whenever available\u2014copy the string exactly.\n        C) Produce a `page_summary` that captures the page\'s business/data content for Step 4: list the constants you inlined, key field values, notable numeric totals, dates, codes, unresolved tokens, and uncertainties. Do not rehash layout, typography, or other presentation details unless they directly affect data interpretation.\n\n        Core invariants (must hold unless a user instruction explicitly overrides them):\n        1) Preserve the DOM hierarchy, repeat markers, data-region attributes, and row prototypes; only adjust them when the user explicitly says so.\n        2) Preserve all remaining dynamic tokens exactly (examples: "{token}", "{{ token }}", "<span id=\'tok-x\'>{{token}}</span>"). Only inline tokens you were instructed to convert to constants.\n        3) Keep the HTML self-contained (no external resources or <script> tags). Maintain semantic structure.\n\n        Hints:\n        - The `mapping_context.mapping` object reflects the latest binding state after Step 3 and any overrides. Tokens mapped to "INPUT_SAMPLE" must be inlined; leave tokens mapped to DuckDB SQL expressions or table columns untouched unless instructed otherwise.\n        - The `mapping_context.token_samples` dictionary lists the literal strings extracted in Step 3 for every placeholder. Inline tokens using these values exactly.\n        - `mapping_context.sample_tokens` / `mapping_context.inline_tokens` highlight placeholders the user wants to double-check; use these cues when reporting lingering uncertainties in the page summary.\n        - A reference PNG is attached via `image_url`. Treat it as general context while following all other instructions.\n        - `user_input` contains the authoritative instructions for this pass. Follow it exactly.\n\n        Output (strict JSON, no markdown fences, no extra keys):\n        {\n          "final_template_html": "<string>",  // template after applying user instructions and inlining required constants\n          "page_summary": "<string>"          // thorough prose description of the PDF page; must be non-empty\n        }\n\n        Validation checklist before responding:\n        - Tokens remaining in "final_template_html" match the original tokens minus those explicitly inlined as constants.\n        - Repeat markers, <tbody> counts, row prototypes, and data-region attributes are unchanged unless the user asked to modify them.\n        - HTML stays free of external resources/scripts and contains no accidental literal leak of unresolved token data.\n        - "page_summary" is a detailed narrative (>1 sentence) that reports the exact values you inlined (including any best-guess readings), important metrics, unresolved fields, and uncertainties, without digressing into layout or styling trivia.\n        - JSON is valid (UTF-8), strings escaped properly, and only the two required keys are present.\n\n\n\n\n        ').strip(), 'user': dedent('        {\n          "template_html": "<HTML from step 3 with constants already inlined where obvious>",\n          "schema": {\n            "scalars": ["..."],\n            "row_tokens": ["..."],\n            "totals": ["..."]\n          },\n          "mapping_context": {\n            "mapping": {\n              "recipe_code": "recipes.recipe_code",\n              "set_weight": "SUM(recipes.bin1_sp + recipes.bin2_sp)",\n              "material_name": "INPUT_SAMPLE"\n            },\n            "mapping_override": { "material_name": "INPUT_SAMPLE" },\n            "sample_tokens": ["material_name", "qty"],\n            "token_samples": {\n              "material_name": "Aluminum Oxide 45 Micron",\n              "qty": "12.50 kg"\n            }\n          },\n          "token_samples": {\n            "plant_name": "North Works Plant 05",\n            "location": "Boulder, CO"\n          },\n          "user_input": "Free-form instructions and corrections. Examples: 1) Change \'Reciepe\' -> \'Recipe\'; 2) Inline company_name using what you see on the PDF; 3) Add a bold border to the totals row; 4) Move the logo into the header; 5) Describe visible signatures in the summary."\n        }\n        ').strip()}
- L415 def _build_data_uripath: Path | None:
  - L416 if not path or not path.exists():
    - L417 return None
  - L418 try:
    - L419 assign encoded = base64.b64encode(path.read_bytes()).decode('utf-8')
    - L420 except Exception:
      - L421 return None
  - L422 return f'data:image/png;base64,{encoded}'
- L425 def build_llm_call_3_5_prompttemplate_html: str, schema: Mapping[str, Any] | None, user_input: str, page_png_path: str | None=None, mapping_context: Mapping[str, Any] | None=None:
  - L432 assign schema_payload = dict(schema or {})
  - L433 annotated assign payload: Dict[str, Any] = {'template_html': template_html, 'schema': schema_payload, 'user_input': user_input or ''}
  - L439 if mapping_context:
    - L440 assign mapping_context_clean = dict(mapping_context)
    - L441 assign payload['mapping_context'] = mapping_context_clean
  - L443 assign data_uri = _build_data_uri(Path(page_png_path) if page_png_path else None)
  - L444 assign payload_json = json.dumps(payload, ensure_ascii=False, indent=2)
  - L445 assign user_content = [{'type': 'text', 'text': f'USER (JSON payload):\n{payload_json}'}]
  - L451 if data_uri:
    - L452 expr user_content.append({'type': 'image_url', 'image_url': {'url': data_uri, 'detail': 'high'}})
  - L462 assign messages = [{'role': 'user', 'content': user_content}]
  - L469 return {'system': LLM_CALL_3_5_PROMPT['system'], 'messages': messages, 'version': PROMPT_VERSION_3_5}
- L476 assign LLM_CALL_4_SYSTEM_PROMPT = dedent('    LLM CALL 4 - Step-5 Hand-off Builder\n    You generate the structured payload that LLM Call 5 will consume. Your job is to:\n    1. Interpret the user\'s instructions, template structure, and catalog context to finalize report logic and reshape rules.\n    2. Produce a precise overview plus the Step-5 requirements bundle (datasets, parameter semantics, transformations).\n    3. Emit a fully mapped contract object so Call 5 can compile SQL without guessing.\n    You must:\n    * Use only columns from the provided CATALOG allow-list. If a necessary column is absent, surface it via validation; never invent table/column names.\n    * RUNTIME CONTEXT: Every SQL fragment you emit (mappings, reshape rules, entrypoints) runs through DuckDB against pandas DataFrames that mirror the catalog tables. Stick to portable DuckDB SQL and reference dataset/table aliases exactly as defined.\n    * Preserve every dynamic token from the schema. Do not add, remove, or rename tokens; constants already inlined in HTML are not tokens.\n    * Map every token with exactly one of:\n        - `TABLE.COLUMN` (direct source, obeying the catalog allow-list),\n        - `DATASET.COLUMN` (use dataset aliases produced by Step-5, e.g., `rows.row_token`, `totals.total_token`),\n        - `PARAM:name` (header/parameter passthrough),\n        - a DuckDB SQL expression that references only catalog columns or dataset aliases (no `DERIVED:` prefix).\n    * When you emit a DuckDB SQL expression, reuse the identical expression inside `row_computed` / `totals_math` so the runtime stays consistent.\n    * The auto_mapping_proposal.mapping and mapping_override values may contain SQL expressions; treat them as guidance (mapping_override is authoritative when provided).\n    * Describe reshape rules with exact column ordering, filters to apply before aggregation, and any dedup or ordering requirements. Ensure `step5_requirements.datasets.rows.grouping` / `ordering` mirror those expectations. Every reshape rule must include a non-empty `"purpose"` sentence (plain-English summary, <= 15 words) so downstream validators understand its intent.\n    * Guarantee `order_by.rows` and `row_order` are non-empty arrays. Mirror the stable ordering you describe (typically timestamp ASC) and default to `["ROWID"]` only when no explicit ordering exists. Never leave `row_order` missing or blank.\n      * Spell out parameter semantics: which parameters are pass-through, which drive filters, and the condition for applying optional filters (e.g., "only when non-empty").\n      * Treat `key_tokens` (see payload) as required user filters. Preserve their mappings (typically `PARAM:<name>`), list them under `step5_requirements.parameters.required`, and explain that downstream SQL must apply equality predicates for them in parent/child/totals entrypoints when values are supplied.\n    * Leave `validation.unknown_tokens` and `validation.unknown_columns` empty; by the end of this call every token must be mapped and every column reference resolved.\n    * Return exactly one JSON object that matches the output shape described below. No extra text outside JSON.\n    User message (JSON payload):\n    {\n      "final_template_html": "<HTML from Step 3.5 (constants already inlined; dynamic tokens preserved)>",\n      "page_summary": "Detailed narrative of the PDF page from Step 3.5",\n      "schema": {\n        "scalars": ["..."],\n        "row_tokens": ["..."],\n        "totals": ["..."]\n      },\n      "auto_mapping_proposal": {\n        "mapping": {\n          "set_weight": "SUM(recipes.bin1_sp + recipes.bin2_sp + ... + recipes.bin12_sp)",\n          "material_name": "recipes.bin_content_normalized"\n        },\n        "join": { "parent_table": "...", "parent_key": "...", "child_table": "...", "child_key": "..." },\n        "date_columns": { "table": "date_col" },\n        "unresolved": ["token_a", "token_b"],\n        "warnings": ["..."]\n      },\n      "mapping_override": {\n        "set_weight": "SUM(recipes.bin1_sp + ... + recipes.bin12_sp)",\n        "achieved_weight": "SUM(recipes.bin1_act + ... + recipes.bin12_act)"\n      },\n        "user_instructions": "Free-form explanation from the user. May describe reshaping, grouping keys, aggregations, filters, etc.",\n        "key_tokens": ["plant_name", "recipe_code"],\n        "catalog": ["table.column", "..."],\n      "dialect_hint": "sqlite or postgres"\n    }\n    Expected output (single JSON object):\n    {\n      "overview_md": "STRING (Markdown). A comprehensive, in-depth overview that Step 5 will rely on. Include the sections listed below.",\n      "step5_requirements": {\n        "datasets": {\n          "header": {"description": "One-row header dataset", "columns": ["<scalar tokens in order>"]},\n          "rows": {\n            "description": "Detail rows dataset",\n            "columns": ["<row tokens in order>"],\n            "grouping": ["Describe logical grouping keys, include any trimming or dedup rules."],\n            "ordering": ["Describe stable sort order with tie-breakers and case handling (e.g., ORDER BY material_name ASC, bin_index ASC). Ensure this exact ordering is copied into both `order_by.rows` and `row_order`."]\n          },\n          "totals": {"description": "Aggregate totals dataset", "columns": ["<totals tokens in order>"]}\n        },\n        "semantics": "Explain required filters versus pass-through parameters and when optional filters apply (e.g., only when non-empty).",\n        "parameters": {\n          "required": [{"name": "from_date", "type": "date"}, {"name": "to_date", "type": "date"}],\n          "optional": [{"name": "plant_name", "type": "string"}, {"name": "recipe_code", "type": "string"}]\n        },\n        "transformations": [\n          "Explicit reshaping rules (for example UNION ALL across bin1..bin12 -> material_name/set/ach) with filters applied before grouping.",\n          "Row-level computed columns and staging tables required to produce row tokens (note COALESCE/NULLIF guards).",\n          "Totals math rationale mirroring the contract expressions."\n        ]\n      },\n      "contract": {\n        "tokens": {\n          "scalars": ["..."],\n          "row_tokens": ["..."],\n          "totals": ["..."]\n        },\n        "mapping": {\n          "plant_name": "PARAM:plant_name",\n          "from_date": "PARAM:from_date",\n          "row_material_name": "TRIM(long_bins.material_name)",\n          "row_set_wt": "SUM(long_bins.set_wt_kg)",\n          "total_set_wt": "SUM(rows.row_set_wt)",\n          "print_date": "PARAM:print_date"\n        },\n        "unresolved": [],\n        "join": {\n          "parent_table": "recipes",\n          "parent_key": "id",\n          "child_table": null,\n          "child_key": null\n        },\n        "date_columns": { "recipes": "start_time" },\n        "filters": { "optional": { "recipe_code": "recipes.recipe_name" } },\n        "reshape_rules": [\n          {\n            "purpose": "Produce rows dataset",\n            "strategy": "UNION_ALL",\n            "explain": "Union bin1..bin12 columns into long form, trim material names, drop blanks before grouping",\n            "columns": [\n              {"as": "material_name", "from": ["recipes.bin1_content", "...", "recipes.bin12_content"]},\n              {"as": "set_wt_kg", "from": ["recipes.bin1_sp", "...", "recipes.bin12_sp"]},\n              {"as": "ach_wt_kg", "from": ["recipes.bin1_act", "...", "recipes.bin12_act"]}\n            ]\n          }\n        ],\n        "row_computed": {\n          "row_error_kg": "SUM(long_bins.ach_wt_kg) - SUM(long_bins.set_wt_kg)",\n          "row_error_pct": "CASE WHEN SUM(long_bins.set_wt_kg)=0 THEN NULL ELSE (SUM(long_bins.ach_wt_kg) - SUM(long_bins.set_wt_kg))/SUM(long_bins.set_wt_kg) END"\n        },\n        "totals_math": {\n          "total_set_wt": "SUM(rows.row_set_wt)",\n          "total_ach_wt": "SUM(rows.row_ach_wt)",\n          "total_error_kg": "SUM(rows.row_error_kg)",\n          "total_error_percent": "SUM(rows.row_error_kg) / NULLIF(SUM(rows.row_set_wt), 0)"\n        },\n        "formatters": {\n          "row_error_pct": "percent(2)",\n          "total_error_pct": "percent(2)",\n          "print_date": "date(YYYY-MM-DD)"\n        },\n        "order_by": {"rows": ["row_material_name ASC"]},\n        "notes": "Domain notes from the user that matter in the DuckDB/DataFrame runtime"\n      },\n      "validation": {\n        "unknown_tokens": [],\n        "unknown_columns": [],\n        "token_coverage": {\n          "scalars_mapped_pct": 100,\n          "row_tokens_mapped_pct": 100,\n          "totals_mapped_pct": 100\n        }\n      }\n    }\n    Guidance for overview_md:\n    * Executive Summary: what the user wants; special reshaping, grouping, totals.\n    * Token Inventory: list tokens that remain dynamic.\n    * Mapping Table: Markdown table Token -> source (TABLE.COLUMN / PARAM / DuckDB SQL expression).\n    * Join & Date Rules: tables, joins, filter semantics.\n    * Transformations: explicit unpivot/union shapes, computed fields, totals rationale.\n    * Parameters: required/optional, semantics, examples (note pass-through vs. filter behaviour).\n    * Checklist for Step 5: bullet list of DuckDB SQL requirements (column orders, filters, grouping, NULLIF guards, optional filter behaviour on the DataFrame backend).\n    Model self-check expectations:\n    * `unknown_tokens` must be [] because every token is mapped.\n    * `unknown_columns` must be [] because every column reference is in the catalog allow-list.\n    * `token_coverage` should report 100% across scalars/rows/totals once mapping is complete.\n\n    ').strip()
- L632 annotated assign LLM_CALL_5_PROMPT: Dict[str, str] = {'system': dedent('        LLM CALL 5 - Generator Assets Emitter\n        You are the Step-5 generator bundle author. Given the final template HTML, the complete Step-4 payload, and an optional reference image, you must emit every runtime artifact required by the Python pipeline.\n\n        Canonical sources:\n        * Treat `step4_output.contract` as the authoritative blueprint for tokens, bindings, reshape rules, joins, filters, and math. Do not add/drop/rename tokens.\n        * Mirror the contract exactly in your `contract` output (same ordering, same expressions). If you cannot satisfy a requirement, leave data unchanged and set `"invalid": true`.\n        * When SQL entrypoints produce dataset aliases (`header`, `rows`, `totals`), reference those explicitly in the contract mapping (e.g., `rows.row_token`, `totals.total_token`) so the runtime can hydrate values without re-deriving them.\n        * RUNTIME CONTEXT: The generated SQL runs through DuckDB on top of pandas DataFrames that mirror every catalog table. Stay within DuckDB-compatible syntax even when the dialect hint is "sqlite"; reference dataset/table aliases exactly as emitted.\n\n        Required output structure (output exactly this object shape; no extra keys, no missing keys):\n        {\n          "contract": {\n            "tokens": { "scalars": [...], "row_tokens": [...], "totals": [...] },\n            "mapping": { "<token>": "TABLE.COLUMN|DATASET.COLUMN|PARAM:name|<DuckDB sql expression>" },\n            "join": { "parent_table": "...", "parent_key": "...", "child_table": "...", "child_key": "..." },\n            "date_columns": { "<table>": "<date column>" },\n            "filters": { ...copy of Step-4 filters object... },\n            "reshape_rules": [...],\n            "row_computed": { "<token>": "<DuckDB sql expression>" },\n            "totals_math": { "<token>": "<DuckDB sql expression>" },\n            "formatters": { "<token>": "<formatter spec>" },\n            "order_by": { "rows": ["<order clause>", ...] },\n            "header_tokens": ["<scalar token>", ...],\n            "row_tokens": ["<row token>", ...],\n            "totals": { "<total token>": "DATASET.COLUMN|<DuckDB sql expression>" },\n            "row_order": ["<order clause>", ...]\n          },\n          "sql_pack": {\n            "dialect": "duckdb|postgres",\n            "script": "-- HEADER SELECT --\n<SQL>\n-- ROWS SELECT --\n<SQL>\n-- TOTALS SELECT --\n<SQL>",\n            "entrypoints": { "header": "<SQL>", "rows": "<SQL>", "totals": "<SQL>" },\n            "params": { "required": ["<param>", ...], "optional": ["<param>", ...] }\n          },\n          "dialect": "duckdb|postgres",\n          "invalid": false\n        }\n        Use [] for empty arrays, {} for empty objects, and "" for empty strings.\n\n        Dialect rules (executed via DuckDB in all cases):\n        * sqlite: treat this as the DuckDB subset that mimics SQLite; use named params (:param), avoid FILTER, prefer NULLIF(x, 0) guards.\n        * postgres: DuckDB supports most Postgres syntax; named params allowed; NULLIF/COALESCE allowed; FILTER allowed.\n\n        Contract requirements:\n        * Copy tokens, mappings, reshape rules, row_computed, totals_math, formatters, filters, date_columns, join, `order_by`, `row_order`, and notes from the Step-4 contract. Ensure every DuckDB SQL expression matches between `mapping`, `row_computed`, and `totals_math`.\n        * Mapping values must use only `TABLE.COLUMN`, `PARAM:name`, dataset aliases, or DuckDB SQL expressions built from catalog/dataset columns (no prefixes or prose).\n        * Populate any missing optional sections (e.g., empty objects/arrays) so the JSON validates against `contract_v2.schema.json`.\n        * Join block must remain present with non-empty `parent_table`, `parent_key`, `child_table`, `child_key`. If there is no logical child table, set `child_table` equal to the parent and reuse the same key; never leave keys blank or null.\n        * Mirror Step-4 `reshape_rules` exactly (strategy, datasets, alias ordering) and guarantee every rule carries a non-empty `"purpose"` summary. If Step-4 omitted it, synthesize a concise description (<= 15 words) before returning.\n        * Ensure `order_by.rows` and `row_order` remain aligned (both arrays). If Step-4 provided only one of them, copy it to the other; if neither exists, emit `["ROWID"]` for both instead of leaving blanks.\n        * Every token listed in `contract.tokens` must appear exactly once across the header/rows/totals SELECTs (no duplicate aliases or alternate spellings).\n\n        SQL pack requirements (DuckDB over pandas DataFrames):\n        * Implement `reshape_rules`, joins, filters, parameter semantics, and math exactly as described by the contract and `step5_requirements`.\n        * Provide a single consolidated `script` string that contains the header, rows, and totals SELECT statements in order. Include clear section markers such as:\n          -- HEADER SELECT --\n          ...select...\n          -- ROWS SELECT --\n          ...select...\n          -- TOTALS SELECT --\n          ...select...\n        * Ensure each SELECT projects aliases exactly matching the tokens for that section; do not emit multiple SELECT statements per token or redundant aliases.\n        * Whenever you aggregate the detail rows, materialize that result as a named CTE (e.g., `WITH long_bins AS (...), rows AS (...)`) and reuse it everywhere. Window functions must order by the CTE column names (never aliases defined later in the same SELECT), and the totals SELECT must read directly from that shared `rows` CTE rather than referencing an undefined table.\n        * Provide an `entrypoints` object where each SELECT projects aliases exactly matching the contract token order (header \u2192 scalars, rows \u2192 row tokens, totals \u2192 totals tokens).\n        * Reflect the contract join keys and filters in the SQL. Header, rows, and totals must reference the parent key columns in their FROM/JOIN clauses, respecting optional filter semantics.\n        * Every entrypoint SQL must compile as-is. Return a single finalized SELECT per section; do not leave dangling `WITH` clauses, ellipses, comments like `-- TODO`, or placeholder text.\n        * The header SELECT must read from the parent table (or a CTE derived from it) and apply the same predicates used by the row/totals datasets. `SELECT ...` without `FROM` is only acceptable when you project literals/params exclusively; otherwise include the table reference.\n        * Rows entrypoints must select directly from the aggregated dataset defined in the reshape rules (e.g., the `rows` CTE) and end with a concrete `ORDER BY` that matches `row_order`.\n        * Totals entrypoints must select from a concrete dataset (`rows`, `totals`, etc.) and may not reference undefined aliases or rely on spreadsheet math outside SQL.\n        * Ensure `params.required/optional` aligns with the contract bindings and Step-4 requirements. Optional filters only apply when the parameter is non-null/non-empty.\n        * Treat `key_tokens` (see payload) as mandatory equality filters. Add them to `params.required`, keep their mappings as `PARAM:<name>`, and ensure each entrypoint\'s WHERE clause applies `= :token` tests on the correct table aliases.\n        * Header entrypoints must return exactly one row. If you only project parameters or literals, emit `SELECT ...` with no FROM clause (or aggregate) rather than scanning base tables.\n        * When reshape_rules specify `UNION_ALL`, emit one SELECT per source entry in `columns[*].from`, referencing each catalog column directly and aliasing it with the provided `as` name. Do not replace this pattern with CASE expressions or references to columns that are not enumerated.\n        * Ensure `output_schemas.header`, `output_schemas.rows`, and `output_schemas.totals` list tokens in the exact order defined by the contract; resolve any mismatch yourself instead of flagging schema issues.\n\n        Quality gating before returning:\n        * Ensure SQL aliases align 1:1 with the contract token order for header, rows, and totals sections.\n        * Verify `contract.join.parent_table`, `contract.join.parent_key`, `contract.join.child_table`, and `contract.join.child_key` are non-empty strings (reuse the parent table/key for the child when no separate child exists).\n        * Do not emit `needs_user_fix` entries or set `"invalid": true`; resolve the underlying mismatch before responding so the bundle is production ready.\n\n        Reference HTML/image:\n        * Use only for sanity checks (naming consistency, visual ordering). Do NOT alter tokens or data logic based on appearance alone.\n        ').strip(), 'user': dedent('        {\n          "final_template_html": "<HTML from Step 3.5 (constants already inlined; dynamic tokens preserved)>",\n          "step4_output": {\n            "contract": { /* EXACT Step-4 contract object */ },\n            "overview_md": "OPTIONAL: the Step-4 overview markdown as a single string",\n            "step5_requirements": { /* OPTIONAL: Step-4 step5_requirements payload */ },\n            "assumptions": [/* OPTIONAL strings */],\n            "warnings": [/* OPTIONAL strings */],\n            "validation": { /* OPTIONAL validation report */ }\n          },\n          "reference_pdf_image": "OPTIONAL: data URI or URL to the page PNG/JPG used as visual reference",\n          "key_tokens": ["plant_name", "recipe_code"],\n          "dialect": "sqlite"\n        }\n        ').strip()}
- L738 annotated assign PROMPT_LIBRARY: Dict[str, str] = {**LLM_CALL_PROMPTS, 'llm_call_3_5_system': LLM_CALL_3_5_PROMPT['system'], 'llm_call_3_5_user': LLM_CALL_3_5_PROMPT['user'], 'llm_call_4_system': LLM_CALL_4_SYSTEM_PROMPT, 'llm_call_5_system': LLM_CALL_5_PROMPT['system'], 'llm_call_5_user': LLM_CALL_5_PROMPT['user']}
- L748 def build_llm_call_4_prompt*, final_template_html: str, page_summary: str, schema: Mapping[str, Any] | None, auto_mapping_proposal: Mapping[str, Any], mapping_override: Mapping[str, Any] | None, user_instructions: str, catalog: Iterable[str], dialect_hint: str | None=None, key_tokens: Iterable[str] | None=None:
  - L760 docstring: "\n    Build the payload for LLM Call 4 (contract builder + overview).\n    "
  - L763 assign system_text = LLM_CALL_4_SYSTEM_PROMPT
  - L764 annotated assign key_tokens_list: list[str] = []
  - L765 if key_tokens:
    - L766 annotated assign seen: set[str] = set()
    - L767 for token in key_tokens:
      - L768 assign text = str(token or '').strip()
      - L769 if not text or text in seen:
        - L770 continue
      - L771 expr seen.add(text)
      - L772 expr key_tokens_list.append(text)
  - L773 annotated assign payload: Dict[str, Any] = {'final_template_html': final_template_html, 'page_summary': page_summary, 'schema': dict(schema or {}), 'auto_mapping_proposal': dict(auto_mapping_proposal or {}), 'mapping_override': dict(mapping_override or {}), 'user_instructions': user_instructions or '', 'catalog': [str(item) for item in catalog]}
  - L782 if key_tokens_list:
    - L783 assign payload['key_tokens'] = key_tokens_list
  - L784 if dialect_hint is not None:
    - L785 assign payload['dialect_hint'] = str(dialect_hint)
  - L787 assign payload_json = json.dumps(payload, ensure_ascii=False, indent=2)
  - L788 assign messages = [{'role': 'user', 'content': [{'type': 'text', 'text': payload_json}]}]
  - L800 return {'system': system_text, 'messages': messages, 'version': PROMPT_VERSION_4}
- L808 def get_prompt_generator_assets:
  - L809 docstring: "Return the system and user template strings for LLM CALL 5."
  - L810 return dict(LLM_CALL_5_PROMPT)
- L813 assign __all__ = ['build_llm_call_3_prompt', 'build_llm_call_3_5_prompt', 'PROMPT_VERSION', 'PROMPT_VERSION_3_5', 'PROMPT_VERSION_4', 'build_llm_call_4_prompt', 'LLM_CALL_PROMPTS', 'PROMPT_LIBRARY', 'get_prompt_generator_assets']

## backend\app\services\prompts\llm_prompts_analysis.py
- L2 docstring: "LLM prompts for document analysis and data extraction."
- L3 from __future__ import annotations
- L5 import json
- L6 import logging
- L7 import re
- L8 from typing import Any
- L10 assign logger = logging.getLogger('neura.prompts.analysis')
- L12 assign DOCUMENT_ANALYSIS_PROMPT = 'You are a data extraction specialist analyzing documents for NeuraReport.\n\nDOCUMENT CONTEXT:\n- Document type: {document_type}\n- File name: {file_name}\n- Pages/Sheets: {page_count}\n\nEXTRACTED CONTENT:\n{extracted_content}\n\nTASK:\nExtract all meaningful data from this document and return a structured JSON response.\n\nOUTPUT FORMAT (return ONLY valid JSON, no markdown or commentary):\n{{\n  "summary": "Brief 1-2 sentence description of document contents",\n  "tables": [\n    {{\n      "id": "table_1",\n      "title": "Descriptive name for this table",\n      "headers": ["Column1", "Column2", "Column3"],\n      "rows": [\n        ["value1", "value2", "value3"],\n        ["value4", "value5", "value6"]\n      ],\n      "data_types": ["text", "numeric", "date"]\n    }}\n  ],\n  "key_metrics": [\n    {{\n      "name": "metric_name",\n      "value": 123.45,\n      "unit": "USD" | "%" | "units" | null,\n      "context": "Where this value appears or what it represents"\n    }}\n  ],\n  "time_series_candidates": [\n    {{\n      "date_column": "column_name_with_dates",\n      "value_columns": ["numeric_col1", "numeric_col2"],\n      "frequency": "daily" | "weekly" | "monthly" | "yearly" | null,\n      "table_id": "table_1"\n    }}\n  ],\n  "chart_recommendations": [\n    {{\n      "type": "line" | "bar" | "pie" | "scatter",\n      "title": "Suggested chart title",\n      "x_field": "field_for_x_axis",\n      "y_fields": ["field1", "field2"],\n      "rationale": "Brief explanation of why this chart is useful"\n    }}\n  ]\n}}\n\nRULES:\n1. Extract ALL tables from the document, even small ones\n2. Identify numeric columns vs text/categorical columns\n3. Detect date/time patterns for time series analysis\n4. Suggest meaningful chart visualizations based on the data\n5. Preserve original precision for numeric values\n6. Use "numeric" for data_types when column contains numbers\n7. Use "date" or "datetime" for date columns\n8. Use "text" or "category" for text/categorical columns\n9. Return ONLY valid JSON - no markdown code fences, no explanatory text\n'
- L79 assign CHART_SUGGESTION_PROMPT = 'Based on the extracted data below, suggest the best visualizations.\n\nDATA SUMMARY:\n{data_summary}\n\nFIELD CATALOG:\n{field_catalog}\n\nUSER QUESTION (if provided):\n{user_question}\n\nSuggest 2-5 charts that would best visualize this data. Focus on:\n- Time series trends if date fields exist\n- Comparisons between categories\n- Distributions of numeric values\n- Relationships between numeric fields\n\nReturn ONLY valid JSON in this format:\n{{\n  "charts": [\n    {{\n      "id": "chart_1",\n      "type": "line" | "bar" | "pie" | "scatter",\n      "title": "Chart title",\n      "xField": "field_name_for_x",\n      "yFields": ["field1", "field2"],\n      "groupField": "optional_grouping_field" | null,\n      "aggregation": "sum" | "avg" | "count" | "none" | null,\n      "description": "What insight this chart provides"\n    }}\n  ]\n}}\n'
- L114 def strip_code_fencestext: str:
  - L115 docstring: "Remove markdown code fences from LLM output."
  - L116 if not text:
    - L117 return ''
  - L118 assign text = text.strip()
  - L122 assign text = re.sub('^```(?:json|JSON|js)?\\s*\\n?', '', text)
  - L124 assign text = re.sub('\\n?```\\s*$', '', text)
  - L126 return text.strip()
- L129 def _extract_json_objecttext: str:
  - L130 docstring: "\n    Extract the first complete JSON object from text.\n    Uses bracket counti..."
  - L134 if not text:
    - L135 return None
  - L138 assign start = text.find('{')
  - L139 if start == -1:
    - L140 return None
  - L143 assign depth = 0
  - L144 assign in_string = False
  - L145 assign escape_next = False
  - L147 for (i, char) in enumerate(text[start:], start):
    - L148 if escape_next:
      - L149 assign escape_next = False
      - L150 continue
    - L152 if char == '\\' and in_string:
      - L153 assign escape_next = True
      - L154 continue
    - L156 if char == '"' and (not escape_next):
      - L157 assign in_string = not in_string
      - L158 continue
    - L160 if in_string:
      - L161 continue
    - L163 if char == '{':
      - L164 aug assign depth Add 1
      - L165 else:
        - L165 if char == '}':
          - L166 aug assign depth Sub 1
          - L167 if depth == 0:
            - L168 return text[start:i + 1]
  - L171 expr logger.warning('JSON extraction: unbalanced braces, attempting recovery')
  - L172 return None
- L175 def _try_repair_jsontext: str:
  - L176 docstring: "Attempt to repair common JSON issues from LLM output."
  - L177 if not text:
    - L178 return None
  - L180 assign working = text
  - L183 assign repairs = [(',\\s*([\\]}])', '\\1'), ("'([^']*)':", '"\\1":'), ('([{,]\\s*)(\\w+)(\\s*:)', '\\1"\\2"\\3'), ('//[^\\n]*\\n', '\n'), ('/\\*[\\s\\S]*?\\*/', '')]
  - L195 for (pattern, replacement) in repairs:
    - L196 assign working = re.sub(pattern, replacement, working)
  - L198 try:
    - L199 return json.loads(working)
    - L200 except json.JSONDecodeError:
      - L201 return None
- L204 def parse_analysis_responseraw_response: str:
  - L205 docstring: "Parse LLM response into structured analysis data."
  - L206 assign default_response = {'summary': '', 'tables': [], 'key_metrics': [], 'time_series_candidates': [], 'chart_recommendations': []}
  - L214 if not raw_response:
    - L215 return default_response
  - L217 assign cleaned = strip_code_fences(raw_response)
  - L220 try:
    - L221 assign data = json.loads(cleaned)
    - L222 if isinstance(data, dict):
      - L223 return {'summary': data.get('summary', ''), 'tables': data.get('tables', []), 'key_metrics': data.get('key_metrics', []), 'time_series_candidates': data.get('time_series_candidates', []), 'chart_recommendations': data.get('chart_recommendations', [])}
    - L230 except json.JSONDecodeError:
      - L231 pass
  - L234 assign json_str = _extract_json_object(cleaned)
  - L235 if json_str:
    - L236 try:
      - L237 assign data = json.loads(json_str)
      - L238 if isinstance(data, dict):
        - L239 return {'summary': data.get('summary', ''), 'tables': data.get('tables', []), 'key_metrics': data.get('key_metrics', []), 'time_series_candidates': data.get('time_series_candidates', []), 'chart_recommendations': data.get('chart_recommendations', [])}
      - L246 except json.JSONDecodeError:
        - L247 pass
  - L250 assign repaired = _try_repair_json(cleaned)
  - L251 if repaired:
    - L252 return {'summary': repaired.get('summary', ''), 'tables': repaired.get('tables', []), 'key_metrics': repaired.get('key_metrics', []), 'time_series_candidates': repaired.get('time_series_candidates', []), 'chart_recommendations': repaired.get('chart_recommendations', [])}
  - L260 expr logger.warning('Failed to parse LLM analysis response as JSON')
  - L261 return default_response
- L264 def build_analysis_promptdocument_type: str, file_name: str, page_count: int, extracted_content: str:
  - L270 docstring: "Build the document analysis prompt."
  - L271 return DOCUMENT_ANALYSIS_PROMPT.format(document_type=document_type, file_name=file_name, page_count=page_count, extracted_content=extracted_content)
- L279 def build_chart_suggestion_promptdata_summary: str, field_catalog: str, user_question: str | None=None:
  - L284 docstring: "Build the chart suggestion prompt."
  - L285 return CHART_SUGGESTION_PROMPT.format(data_summary=data_summary, field_catalog=field_catalog, user_question=user_question or 'No specific question provided - suggest generally useful charts.')
- L292 def infer_data_typevalues: list[Any]:
  - L293 docstring: "\n    Infer the data type from a list of sample values.\n    Requires 70%+ match..."
  - L297 if not values:
    - L298 return 'text'
  - L300 assign date_patterns = ['^\\d{4}-\\d{2}-\\d{2}', '^\\d{2}/\\d{2}/\\d{4}', '^\\d{2}-\\d{2}-\\d{4}', '^\\d{1,2}/\\d{1,2}/\\d{2,4}', '^\\d{4}/\\d{2}/\\d{2}']
  - L308 assign numeric_count = 0
  - L309 assign date_count = 0
  - L310 assign total_valid = 0
  - L312 for val in values[:30]:
    - L313 if val is None:
      - L314 continue
    - L315 assign str_val = str(val).strip()
    - L316 if not str_val:
      - L317 continue
    - L319 aug assign total_valid Add 1
    - L322 assign is_date = False
    - L323 for pattern in date_patterns:
      - L324 if re.match(pattern, str_val):
        - L325 aug assign date_count Add 1
        - L326 assign is_date = True
        - L327 break
    - L329 if is_date:
      - L330 continue
    - L333 try:
      - L335 assign cleaned = str_val.replace(',', '').replace('$', '').replace('\u20ac', '').replace('\xa3', '')
      - L336 assign cleaned = cleaned.replace('%', '').replace(' ', '').strip()
      - L337 if cleaned:
        - L338 expr float(cleaned)
        - L339 aug assign numeric_count Add 1
      - L340 except (ValueError, TypeError):
        - L341 pass
  - L343 if total_valid == 0:
    - L344 return 'text'
  - L347 assign date_ratio = date_count / total_valid
  - L348 assign numeric_ratio = numeric_count / total_valid
  - L350 if date_ratio >= 0.7:
    - L351 return 'datetime'
  - L352 if numeric_ratio >= 0.7:
    - L353 return 'numeric'
  - L355 return 'text'
- L358 assign __all__ = ['DOCUMENT_ANALYSIS_PROMPT', 'CHART_SUGGESTION_PROMPT', 'strip_code_fences', 'parse_analysis_response', 'build_analysis_prompt', 'build_chart_suggestion_prompt', 'infer_data_type']

## backend\app\services\prompts\llm_prompts_charts.py
- L2 from __future__ import annotations
- L4 import json
- L5 from textwrap import dedent
- L6 from typing import Any, Iterable, Mapping, Sequence
- L8 assign CHART_SUGGEST_PROMPT_VERSION = 'chart_suggestions_v1'
- L11 annotated assign CHART_TEMPLATE_CATALOG: dict[str, dict[str, Any]] = {'time_series_basic': {'id': 'time_series_basic', 'description': 'Trend over an ordered index (typically time or batch_index) for one or two numeric metrics.', 'recommended_chart_type': 'line', 'recommended_use': 'Use when xField is an ordered index such as time or batch_index and yFields are numeric measures.', 'recharts': {'component': 'LineChart', 'props': {'margin': {'top': 8, 'right': 16, 'bottom': 24, 'left': 0}, 'cartesianGrid': {'strokeDasharray': '3 3'}}}}, 'top_n_categories': {'id': 'top_n_categories', 'description': 'Ranked comparison of the largest categories by a numeric metric (e.g. rows).', 'recommended_chart_type': 'bar', 'recommended_use': 'Use when xField is a categorical label and yFields contains a single numeric metric you want to rank by size.', 'recharts': {'component': 'BarChart', 'props': {'layout': 'vertical', 'margin': {'top': 8, 'right': 16, 'bottom': 16, 'left': 0}}}}, 'distribution_histogram': {'id': 'distribution_histogram', 'description': 'Histogram-style distribution of a numeric metric, approximated with a bar chart.', 'recommended_chart_type': 'bar', 'recommended_use': 'Use when xField is a numeric metric and you conceptually bucket values into ranges to show their distribution.', 'recharts': {'component': 'BarChart', 'props': {'margin': {'top': 8, 'right': 16, 'bottom': 24, 'left': 0}, 'cartesianGrid': {'strokeDasharray': '3 3'}}}}}
- L54 def _to_pretty_jsonvalue: Any:
  - L55 try:
    - L56 return json.dumps(value, ensure_ascii=False, indent=2, sort_keys=isinstance(value, Mapping))
    - L57 except Exception:
      - L58 return json.dumps(str(value), ensure_ascii=False)
- L61 assign CHART_SUGGEST_PROMPT_TEMPLATE = dedent('\n    You are an analytics assistant helping a user explore report batch discovery data in NeuraReport.\n\n    DATA CONTEXT\n    - Each row in the dataset represents a single batch for the selected template and date range.\n    - Available fields are described in FIELD_CATALOG_JSON below. You MUST treat those field names as the only columns\n      you can reference in charts (for xField, yFields, and groupField).\n    - DATA_STATS_JSON provides basic statistics over the dataset (counts, totals, min/max/avg) so you can prioritise\n      interesting views.\n    - KEY_FILTERS_JSON describes any key token filters that have been applied.\n\n    CHART SPEC CONTRACT\n    You must return a single JSON object with this exact shape:\n      {{\n        "charts": [\n          {{\n            "id": "short_unique_id",\n            "type": "bar" | "line" | "pie" | "scatter",\n            "xField": "<field name from FIELD_CATALOG_JSON>",\n            "yFields": ["<field name>", "..."],\n            "groupField": "<field name>" | null,\n            "aggregation": "sum" | "avg" | "count" | "none" | null,\n            "chartTemplateId": "time_series_basic" | "top_n_categories" | "distribution_histogram" | null,\n            "title": "Concise chart title",\n            "description": "Short explanation of what the chart shows and why it is useful"\n          }},\n          ...\n        ]\n      }}\n\n    RULES\n    - Propose between 2 and 5 charts that best answer the user\'s question while remaining faithful to the available fields.\n    - Prefer highlighting metrics that show strong variation (e.g., largest numeric totals, widest min/max range) using DATA_STATS_JSON.\n    - Use only field names that appear in FIELD_CATALOG_JSON for xField, yFields, and groupField.\n    - Prefer using chartTemplateId values as follows when they fit:\n        * "time_series_basic": xField is an ordered index (e.g. "batch_index") and yFields are numeric metrics such as "rows".\n        * "top_n_categories": xField is categorical (e.g. "batch_id") and yFields contains a single numeric metric to compare.\n        * "distribution_histogram": xField is a numeric metric and you conceptually bucket values into ranges to show distribution.\n      It is still valid to omit chartTemplateId for free-form charts.\n    - For "pie" charts, use xField as the category label and yFields[0] as the numeric value.\n    - For "scatter" charts, use xField as the numeric/ordered axis and yFields[0] as the numeric dependent variable.\n    - If the question references measures that are not present in FIELD_CATALOG_JSON, fall back to useful, honest charts\n      based on the available fields (e.g., largest batches by rows, relationship between parent and child rows, distributions).\n    - Do NOT include any commentary, markdown code fences, or extra top-level keys; return only the JSON object.\n\n    TEMPLATE_ID: {template_id}\n    TEMPLATE_KIND: {template_kind}\n    DATE_RANGE:\n      start_date: {start_date}\n      end_date: {end_date}\n\n    KEY_FILTERS_JSON:\n    {key_values_json}\n\n    FIELD_CATALOG_JSON:\n    {field_catalog_json}\n\n    DATA_STATS_JSON:\n    {data_stats_json}\n\n    CHART_TEMPLATE_CATALOG_JSON:\n    {template_catalog_json}\n\n    USER_QUESTION:\n    {user_question}\n    ').strip()
- L131 def build_chart_suggestions_prompt*, template_id: str, kind: str, start_date: str, end_date: str, key_values: Mapping[str, Any] | None, field_catalog: Iterable[Mapping[str, Any]] | Sequence[Mapping[str, Any]], data_stats: Mapping[str, Any] | None=None, question: str | None=None:
  - L142 assign key_values_json = _to_pretty_json(key_values or {})
  - L143 assign field_catalog_json = _to_pretty_json(list(field_catalog or []))
  - L144 assign data_stats_json = _to_pretty_json(data_stats or {})
  - L145 assign template_catalog_json = _to_pretty_json(CHART_TEMPLATE_CATALOG)
  - L146 assign user_question = (question or '').strip() or 'Suggest several informative charts using the available fields.'
  - L148 assign prompt = CHART_SUGGEST_PROMPT_TEMPLATE
  - L149 assign prompt = prompt.replace('{template_id}', template_id)
  - L150 assign prompt = prompt.replace('{template_kind}', (kind or 'pdf').lower())
  - L151 assign prompt = prompt.replace('{start_date}', start_date)
  - L152 assign prompt = prompt.replace('{end_date}', end_date)
  - L153 assign prompt = prompt.replace('{key_values_json}', key_values_json)
  - L154 assign prompt = prompt.replace('{field_catalog_json}', field_catalog_json)
  - L155 assign prompt = prompt.replace('{data_stats_json}', data_stats_json)
  - L156 assign prompt = prompt.replace('{template_catalog_json}', template_catalog_json)
  - L157 assign prompt = prompt.replace('{user_question}', user_question)
  - L158 return prompt

## backend\app\services\prompts\llm_prompts_excel.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\prompts\llm_prompts_template_chat.py
- L2 from __future__ import annotations
- L4 import json
- L5 from textwrap import dedent
- L6 from typing import Any, Dict, List
- L8 assign TEMPLATE_CHAT_PROMPT_VERSION = 'template_chat_v1'
- L11 assign TEMPLATE_CHAT_SYSTEM_PROMPT = dedent('    You are an expert HTML template editing assistant working inside the NeuraReport reporting engine.\n    You help users edit their report templates through an interactive conversation.\n\n    YOUR ROLE\n    - Engage in a helpful conversation to understand what changes the user wants to make to their template.\n    - Ask clarifying questions when the user\'s request is ambiguous or incomplete.\n    - When you have gathered enough information, propose the changes you will make.\n    - Only apply changes when you are confident you understand the user\'s intent.\n\n    TEMPLATE CONTEXT\n    - The template uses dynamic tokens/placeholders like {token}, {{ token }}, {row_token}, etc.\n    - Templates may contain repeat markers like <!-- BEGIN:BLOCK_REPEAT ... --> / <!-- END:BLOCK_REPEAT -->.\n    - Templates include IDs, classes, data-* attributes that should be preserved unless explicitly asked to change.\n\n    CONVERSATION GUIDELINES\n    - Be conversational and helpful, but concise.\n    - If the user\'s request is clear and complete, you can proceed to propose changes immediately.\n    - If you need more information, ask specific questions (limit to 2-3 questions at a time).\n    - When proposing changes, summarize what you will do before showing the result.\n    - Always confirm understanding before making significant structural changes.\n\n    WHAT TO CLARIFY\n    - Vague styling requests (e.g., "make it look better" - ask what style they prefer)\n    - Structural changes without clear scope (e.g., "reorganize" - ask what sections)\n    - Adding new elements without context (e.g., "add a chart" - ask where and what data)\n    - Changes that might affect dynamic tokens (explain the impact and confirm)\n\n    OUTPUT FORMAT (STRICT JSON, no markdown fences, no commentary):\n    {\n      "message": "<string>",              // Your response message to the user\n      "ready_to_apply": <boolean>,        // true if you have enough info and are ready to show changes\n      "proposed_changes": ["change 1", "change 2"] | null,  // List of changes you will make (when ready_to_apply=true)\n      "follow_up_questions": ["q1", "q2"] | null,          // Questions to ask (when ready_to_apply=false)\n      "updated_html": "<string>" | null   // The full updated HTML (only when ready_to_apply=true)\n    }\n\n    IMPORTANT RULES\n    - When ready_to_apply=true, you MUST provide updated_html with the complete modified template.\n    - When ready_to_apply=false, you MUST NOT provide updated_html.\n    - proposed_changes should be short, human-readable descriptions.\n    - follow_up_questions should be specific and actionable.\n    - Preserve all dynamic tokens exactly unless explicitly asked to change them.\n    - Maintain valid HTML structure.\n    ').strip()
- L60 def build_template_chat_prompttemplate_html: str, conversation_history: List[Dict[str, str]]:
  - L64 docstring: "\n    Build a chat-completions payload for conversational template editing.\n\n ..."
  - L79 assign context_message = f"Here is the current template HTML that the user wants to edit:\n\n```html\n{template_html or ''}\n```\n\nThe user will now describe what changes they want. Engage in a conversation to understand their needs fully before making changes."
  - L88 annotated assign messages: List[Dict[str, Any]] = [{'role': 'system', 'content': [{'type': 'text', 'text': TEMPLATE_CHAT_SYSTEM_PROMPT}]}, {'role': 'user', 'content': [{'type': 'text', 'text': context_message}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': json.dumps({'message': "I've reviewed your template. What changes would you like to make? Feel free to describe what you want - whether it's styling updates, layout changes, adding or removing sections, or any other modifications.", 'ready_to_apply': False, 'proposed_changes': None, 'follow_up_questions': None, 'updated_html': None}, ensure_ascii=False)}]}]
  - L118 for msg in conversation_history:
    - L119 assign role = msg.get('role', 'user')
    - L120 assign content = msg.get('content', '')
    - L121 if role in ('user', 'assistant'):
      - L122 expr messages.append({'role': role, 'content': [{'type': 'text', 'text': content}]})
  - L129 return {'system': TEMPLATE_CHAT_SYSTEM_PROMPT, 'messages': messages, 'version': TEMPLATE_CHAT_PROMPT_VERSION}
- L136 assign __all__ = ['TEMPLATE_CHAT_PROMPT_VERSION', 'TEMPLATE_CHAT_SYSTEM_PROMPT', 'build_template_chat_prompt']

## backend\app\services\prompts\llm_prompts_template_edit.py
- L2 from __future__ import annotations
- L4 import json
- L5 from textwrap import dedent
- L6 from typing import Any, Dict, List
- L8 assign TEMPLATE_EDIT_PROMPT_VERSION = 'template_edit_v1'
- L11 assign TEMPLATE_EDIT_SYSTEM_PROMPT = dedent('    You are an expert HTML template editor working inside the NeuraReport reporting engine.\n\n    GOAL\n    - Apply the user\'s natural-language instructions to the existing report template HTML.\n    - Make only the requested changes; do not redesign the template unless explicitly asked.\n\n    CONSTRAINTS\n    - Preserve all dynamic tokens/placeholders exactly as written (examples: {token}, {{ token }}, {row_token}, etc.).\n    - Preserve repeat markers and structural markers such as <!-- BEGIN:BLOCK_REPEAT ... --> / <!-- END:BLOCK_REPEAT -->.\n    - Do not remove or rename IDs, classes, data-* attributes, or comments that look like implementation markers\n      unless the user explicitly asks.\n    - Keep the HTML self-contained: no external CSS/JS, no <script> tags, no external URLs.\n\n    EDITING BEHAVIOUR\n    - Work in-place on the provided HTML.\n    - Prefer minimal structural changes: adjust text, styles, and small layout tweaks unless the instructions clearly\n      request bigger changes.\n    - If an instruction conflicts with token semantics (for example, replacing a token with fixed text) then only do so\n      when the user explicitly asks.\n    - Maintain valid HTML.\n\n    OUTPUT FORMAT (STRICT JSON, no markdown fences, no commentary):\n    {\n      "updated_html": "<string>",          // full HTML after applying the instructions\n      "summary": ["change 1", "change 2"]  // short, human-readable descriptions of the main changes\n    }\n\n    - Always return BOTH keys.\n    - Ensure JSON is valid UTF-8 and properly escaped.\n    ').strip()
- L46 def build_template_edit_prompttemplate_html: str, instructions: str:
  - L47 docstring: "\n    Build a chat-completions payload for editing an existing template HTML usi..."
  - L57 annotated assign payload: Dict[str, Any] = {'template_html': template_html or '', 'instructions': instructions or ''}
  - L61 assign payload_json = json.dumps(payload, ensure_ascii=False, indent=2)
  - L62 assign user_text = 'Apply the instructions in this JSON payload:\n' + payload_json
  - L64 annotated assign messages: List[Dict[str, Any]] = [{'role': 'system', 'content': [{'type': 'text', 'text': TEMPLATE_EDIT_SYSTEM_PROMPT}]}, {'role': 'user', 'content': [{'type': 'text', 'text': user_text}]}]
  - L75 return {'system': TEMPLATE_EDIT_SYSTEM_PROMPT, 'messages': messages, 'version': TEMPLATE_EDIT_PROMPT_VERSION}
- L82 assign __all__ = ['TEMPLATE_EDIT_PROMPT_VERSION', 'TEMPLATE_EDIT_SYSTEM_PROMPT', 'build_template_edit_prompt']

## backend\app\services\prompts\llm_prompts_templates.py
- L1 from __future__ import annotations
- L3 import json
- L4 import logging
- L5 from typing import Any, Dict, Iterable, List, Mapping, Sequence
- L7 from ..templates.TemplateVerify import get_openai_client
- L8 from ..utils import call_chat_completion, strip_code_fences
- L10 assign logger = logging.getLogger('neura.template_recommender')
- L12 assign DEFAULT_MODEL = 'gpt-5'
- L15 def _summarise_catalogcatalog: Sequence[Mapping[str, Any]]:
  - L16 docstring: "\n    Convert the unified catalog into a compact form suitable for the LLM.\n\n ..."
  - L21 annotated assign summary: list[dict] = []
  - L22 for item in catalog:
    - L23 assign template_id = str(item.get('id') or '').strip()
    - L24 if not template_id:
      - L25 continue
    - L26 expr summary.append({'id': template_id, 'name': item.get('name') or '', 'kind': item.get('kind') or '', 'domain': item.get('domain') or '', 'tags': list(item.get('tags') or []), 'useCases': list(item.get('useCases') or []), 'primaryMetrics': list(item.get('primaryMetrics') or []), 'source': item.get('source') or ''})
  - L38 return summary
- L41 def _build_messagescatalog: Sequence[Mapping[str, Any]], requirement: str, hints: Mapping[str, Any] | None, max_results: int:
  - L47 assign catalog_json = json.dumps(_summarise_catalog(catalog), ensure_ascii=False)
  - L48 assign hints = hints or {}
  - L49 assign hints_json = json.dumps(hints, ensure_ascii=False)
  - L51 assign system_text = 'You are a template recommendation engine for an automated reporting tool. Given a catalog of report templates and a user\'s free-text requirement, you must select the best matching templates.\n\nReturn results as STRICT JSON with this shape:\n{\n  "recommendations": [\n    {"id": "<template_id>", "explanation": "<short reason>", "score": 0.0-1.0},\n    ... up to the requested max_results\n  ]\n}\n\n- score must be a number between 0 and 1 where higher means better match.\n- explanation must be a short, user-facing sentence fragment (no markdown).\n- Only use template IDs that appear in the catalog.\n- Use HINTS_JSON (domains, kinds, schema_snapshot, tables, etc.) to bias the ranking when relevant.'
  - L68 assign user_text = f'USER_REQUIREMENT:\n{requirement.strip()}\n\nMAX_RESULTS: {max_results}\nHINTS_JSON: {hints_json}\n\nTEMPLATE_CATALOG_JSON:\n{catalog_json}\n'
  - L77 return [{'role': 'system', 'content': system_text}, {'role': 'user', 'content': user_text}]
- L83 def _coerce_scorevalue: Any:
  - L84 docstring: "Coerce a value to a score between 0.0 and 1.0."
  - L85 try:
    - L86 assign score = float(value)
    - L87 except (TypeError, ValueError):
      - L88 return 0.0
  - L90 if score != score or score == float('inf') or score == float('-inf'):
    - L91 return 0.0
  - L92 if score < 0.0:
    - L93 return 0.0
  - L94 if score > 1.0:
    - L95 return 1.0
  - L96 return score
- L99 def _extract_json_objecttext: str:
  - L100 docstring: "\n    Extract the first complete JSON object from text.\n    Uses bracket counti..."
  - L104 if not text:
    - L105 return None
  - L108 assign start = text.find('{')
  - L109 if start == -1:
    - L110 return None
  - L113 assign depth = 0
  - L114 assign in_string = False
  - L115 assign escape_next = False
  - L117 for (i, char) in enumerate(text[start:], start):
    - L118 if escape_next:
      - L119 assign escape_next = False
      - L120 continue
    - L122 if char == '\\' and in_string:
      - L123 assign escape_next = True
      - L124 continue
    - L126 if char == '"' and (not escape_next):
      - L127 assign in_string = not in_string
      - L128 continue
    - L130 if in_string:
      - L131 continue
    - L133 if char == '{':
      - L134 aug assign depth Add 1
      - L135 else:
        - L135 if char == '}':
          - L136 aug assign depth Sub 1
          - L137 if depth == 0:
            - L138 return text[start:i + 1]
  - L140 return None
- L143 def _parse_recommendationsraw_text: str:
  - L144 assign text = strip_code_fences(raw_text or '').strip()
  - L145 if not text:
    - L146 return []
  - L149 try:
    - L150 assign payload = json.loads(text)
    - L151 except json.JSONDecodeError:
      - L153 assign json_str = _extract_json_object(text)
      - L154 if not json_str:
        - L155 return []
      - L156 try:
        - L157 assign payload = json.loads(json_str)
        - L158 except json.JSONDecodeError:
          - L159 expr logger.warning('Failed to parse template recommendation JSON')
          - L160 return []
  - L162 if not isinstance(payload, dict):
    - L163 return []
  - L164 assign raw_recs = payload.get('recommendations')
  - L165 if not isinstance(raw_recs, Iterable):
    - L166 return []
  - L169 annotated assign results: list[dict] = []
  - L170 for item in raw_recs:
    - L171 if not isinstance(item, Mapping):
      - L172 continue
    - L173 assign template_id = str(item.get('id') or '').strip()
    - L174 if not template_id:
      - L175 continue
    - L176 assign explanation = str(item.get('explanation') or '').strip()
    - L177 assign score = _coerce_score(item.get('score'))
    - L178 expr results.append({'id': template_id, 'explanation': explanation, 'score': score})
  - L185 return results
- L188 def recommend_templates_from_catalogcatalog: Sequence[Mapping[str, Any]], *, requirement: str, hints: Mapping[str, Any] | None=None, max_results: int=6:
  - L195 docstring: "\n    Call the LLM to obtain a ranked list of template IDs with explanations.\n\..."
  - L201 assign requirement = (requirement or '').strip()
  - L202 if not requirement or not catalog:
    - L203 return []
  - L205 assign messages = _build_messages(catalog, requirement=requirement, hints=hints, max_results=max_results)
  - L206 assign client = get_openai_client()
  - L208 try:
    - L209 assign response = call_chat_completion(client, model=DEFAULT_MODEL, messages=messages, description='template_recommendations', temperature=0.2, max_tokens=512)
    - L217 except Exception as exc:
      - L218 expr logger.warning('template_recommend_llm_failed', extra={'event': 'template_recommend_llm_failed', 'error': str(exc)})
      - L222 return []
  - L224 try:
    - L226 assign content = response.choices[0].message.content or ''
    - L227 except Exception:
      - L228 expr logger.warning('template_recommend_response_shape_unexpected', extra={'event': 'template_recommend_response_shape_unexpected'})
      - L232 return []
  - L234 assign recommendations = _parse_recommendations(content)
  - L235 if not recommendations:
    - L236 expr logger.info('template_recommend_no_results', extra={'event': 'template_recommend_no_results'})
    - L240 return []
  - L243 return recommendations[:max_results]

## backend\app\services\render\__init__.py
- L1 docstring: "HTML rendering utilities."

## backend\app\services\render\html_raster.py
- L1 from __future__ import annotations
- L3 import os
- L4 from typing import Literal
- L6 import fitz
- L7 from playwright.sync_api import sync_playwright
- L9 assign MM_PER_INCH = 25.4
- L11 assign A4_MM_W = 210.0
- L12 assign A4_MM_H = 297.0
- L15 def _a4_enforcing_css:
  - L17 return '\n    @page { size: A4; margin: 0 }\n    html, body { margin: 0; padding: 0; background: #fff; }\n    .page { width: 210mm; min-height: 297mm; box-sizing: border-box; background: #fff; }\n    '
- L24 def _wait_for_fontspage:
  - L26 try:
    - L27 expr page.wait_for_function("document.fonts && document.fonts.status === 'loaded'", timeout=10000)
    - L28 except Exception:
      - L29 pass
- L32 def _html_to_pdf_bytes_with_playwrighthtml: str:
  - L33 with sync_playwright() as pw:
    - L34 assign browser = pw.chromium.launch()
    - L35 assign ctx = browser.new_context()
    - L36 assign page = ctx.new_page()
    - L37 expr page.set_content(html, wait_until='networkidle')
    - L38 expr page.emulate_media(media='print')
    - L39 expr page.add_style_tag(content=_a4_enforcing_css())
    - L40 expr _wait_for_fonts(page)
    - L41 assign pdf_bytes = page.pdf(format='A4', print_background=True, margin={'top': '0', 'right': '0', 'bottom': '0', 'left': '0'})
    - L46 expr ctx.close()
    - L47 expr browser.close()
    - L48 return pdf_bytes
- L51 def _rasterize_pdf_first_page_to_pngpdf_bytes: bytes, dpi: int:
  - L52 assign doc = fitz.open(stream=pdf_bytes, filetype='pdf')
  - L53 assign page = doc.load_page(0)
  - L54 assign scale = dpi / 72.0
  - L55 assign pix = page.get_pixmap(matrix=fitz.Matrix(scale, scale), colorspace=fitz.csRGB, alpha=False)
  - L56 return pix.tobytes('png')
- L59 def _screenshot_element_to_pnghtml: str, selector: str, dpi: int:
  - L61 assign css_w = round(A4_MM_W / MM_PER_INCH * 96)
  - L62 assign css_h = round(A4_MM_H / MM_PER_INCH * 96)
  - L63 assign dsf = dpi / 96.0
  - L64 with sync_playwright() as pw:
    - L65 assign browser = pw.chromium.launch()
    - L66 assign ctx = browser.new_context(device_scale_factor=dsf, viewport={'width': css_w, 'height': css_h})
    - L70 assign page = ctx.new_page()
    - L71 expr page.set_content(html, wait_until='networkidle')
    - L72 expr page.emulate_media(media='screen')
    - L73 expr page.add_style_tag(content=_a4_enforcing_css())
    - L74 expr _wait_for_fonts(page)
    - L75 assign locator = page.locator(selector)
    - L76 if locator.count() == 0:
      - L77 assign locator = page.locator('body')
    - L78 assign png = locator.screenshot(type='png')
    - L79 expr ctx.close()
    - L80 expr browser.close()
    - L81 return png
- L84 def rasterize_html_to_pnghtml: str, dpi: int=400, method: Literal['pdf', 'screenshot']='pdf', selector: str='.page':
  - L90 docstring: "\n    Returns a tightly-cropped A4 PNG at the requested DPI.\n    Preferred: met..."
  - L95 if method == 'pdf':
    - L96 assign pdf = _html_to_pdf_bytes_with_playwright(html)
    - L97 return _rasterize_pdf_first_page_to_png(pdf, dpi=dpi)
  - L98 return _screenshot_element_to_png(html, selector=selector, dpi=dpi)
- L101 def save_pngpng_bytes: bytes, out_path: str:
  - L102 expr os.makedirs(os.path.dirname(out_path), exist_ok=True)
  - L103 with open(out_path, 'wb') as fh:
    - L104 expr fh.write(png_bytes)
  - L105 return out_path

## backend\app\services\reports\__init__.py
- L1 docstring: "Report engine placeholder."

## backend\app\services\reports\common_helpers.py
- L2 docstring: "\nShared helpers for report generation (PDF + Excel).\n\nThe functions here are ..."
- L9 from __future__ import annotations
- L11 import re
- L12 from datetime import datetime
- L13 from typing import Callable, Iterable
- L15 annotated assign _TOKEN_REGEX_CACHE: dict[str, re.Pattern[str]] = {}
- L16 assign _TR_BLOCK_RE = re.compile('(?is)<tr\\b[^>]*>.*?</tr>')
- L17 assign _BATCH_BLOCK_ANY_TAG = re.compile('(?is)<(?P<tag>section|div|article|main|tbody|tr)\\b[^>]*\\bclass\\s*=\\s*["\\\'][^"\\\']*\\bbatch-block\\b[^"\\\']*["\\\'][^>]*>(?P<inner>.*?)</(?P=tag)>')
- L27 def _token_regextoken: str:
  - L28 assign cleaned = (token or '').strip()
  - L29 if not cleaned:
    - L30 raise ValueError('Token must be a non-empty string')
  - L31 assign cached = _TOKEN_REGEX_CACHE.get(cleaned)
  - L32 if cached is None:
    - L33 assign cached = re.compile(f'\\{{\\{{?\\s*{re.escape(cleaned)}\\s*\\}}\\}}?')
    - L34 assign _TOKEN_REGEX_CACHE[cleaned] = cached
  - L35 return cached
- L38 def _segment_has_any_tokensegment: str, tokens: Iterable[str]:
  - L39 for token in tokens:
    - L40 if not token:
      - L41 continue
    - L42 if _token_regex(token).search(segment):
      - L43 return True
  - L44 return False
- L47 def _find_rowish_blockhtml_text: str, row_tokens: Iterable[str]:
  - L48 assign candidate_tokens = [tok for tok in row_tokens if isinstance(tok, str) and tok.strip()]
  - L49 if not candidate_tokens:
    - L50 return None
  - L52 assign matches = [m for m in _TR_BLOCK_RE.finditer(html_text) if _segment_has_any_token(m.group(0), candidate_tokens)]
  - L53 if not matches:
    - L54 return None
  - L56 assign prototype = matches[0].group(0).strip()
  - L57 assign start_index = matches[0].start()
  - L58 assign end_index = matches[-1].end()
  - L59 return (prototype, start_index, end_index)
- L62 def _find_or_infer_batch_blockhtml_text: str:
  - L63 docstring: "\n    Return (full_match, tag_name, inner_html) of the repeating unit.\n    Pref..."
  - L71 assign m = _BATCH_BLOCK_ANY_TAG.search(html_text)
  - L72 if m:
    - L73 return (m.group(0), m.group('tag').lower(), m.group('inner'))
  - L75 assign m_tbody = re.search('(?is)<tbody\\b[^>]*>(?P<body>.*?)</tbody>', html_text)
  - L76 if m_tbody:
    - L77 assign tbody = m_tbody.group('body')
    - L78 assign m_tr = re.search('(?is)<tr\\b[^>]*>(?P<tr>.*?)</tr>', tbody)
    - L79 if m_tr:
      - L80 return (m_tr.group(0), 'tr', m_tr.group('tr'))
  - L82 assign m_div = re.search('(?is)<div\\b[^>]*\\b(row|item|card)\\b[^>]*>(?P<inner>.*?)</div>', html_text)
  - L83 if m_div:
    - L84 return (m_div.group(0), 'div', m_div.group('inner'))
  - L86 assign m_body = re.search('(?is)<body\\b[^>]*>(?P<body>.*?)</body>', html_text)
  - L87 if m_body:
    - L88 assign body = m_body.group('body')
    - L89 assign m_cont = re.search('(?is)<(section|main|div|article)\\b[^>]*>(?P<inner>.*?)</\\1>', body)
    - L90 if m_cont:
      - L91 return (m_cont.group(0), m_cont.group(1).lower(), m_cont.group('inner'))
  - L93 raise RuntimeError('No explicit batch-block and no suitable repeating unit could be inferred.')
- L96 def _select_prototype_blockhtml_text: str, row_tokens: Iterable[str]:
  - L97 assign explicit_blocks = list(_BATCH_BLOCK_ANY_TAG.finditer(html_text))
  - L98 if explicit_blocks:
    - L99 assign chosen_match = explicit_blocks[0]
    - L100 if row_tokens:
      - L101 for match in explicit_blocks:
        - L102 if _segment_has_any_token(match.group(0), row_tokens):
          - L103 assign chosen_match = match
          - L104 break
    - L105 assign prototype = chosen_match.group(0).strip()
    - L106 assign start0 = explicit_blocks[0].start()
    - L107 assign end_last = explicit_blocks[-1].end()
    - L108 return (prototype, start0, end_last)
  - L110 assign rowish = _find_rowish_block(html_text, row_tokens)
  - L111 if rowish:
    - L112 return rowish
  - L114 assign (block_full, _, _) = _find_or_infer_batch_block(html_text)
  - L115 assign start0 = html_text.find(block_full)
  - L116 if start0 < 0:
    - L117 raise RuntimeError('Inferred batch block could not be located in HTML via .find()')
  - L118 assign end_last = start0 + len(block_full)
  - L119 return (block_full.strip(), start0, end_last)
- L122 def _strip_found_blockhtml_text: str, block_full: str, block_tag: str:
  - L123 docstring: "Remove the found/inferred block once (used to build shell)."
  - L124 return html_text.replace(block_full, '', 1)
- L127 def html_without_batch_blockshtml_text: str:
  - L128 docstring: "Legacy stripper kept for compatibility."
  - L129 assign pat = re.compile('(?is)\\s*<section\\s+class=["\\\']batch-block["\\\']\\s*>.*?</section>\\s*')
  - L130 return pat.sub('', html_text)
- L133 def _raise_no_blockhtml: str, cause: Exception | None=None:
  - L134 docstring: "Build a short <section ...> preview and raise ValueError from here."
  - L135 assign sec_tags = re.findall('(?is)<section\\b[^>]*>', html)
  - L136 assign preview_lines = []
  - L137 for (i, t) in enumerate(sec_tags[:12]):
    - L138 assign snip = t[:140].replace('\n', ' ')
    - L139 expr preview_lines.append(f"{i + 1:02d}: {snip}{(' ...' if len(t) > 140 else '')}")
  - L140 assign preview = '\n'.join(preview_lines)
  - L141 assign msg = "Could not find any <section class='batch-block'> blocks and no suitable fallback could be inferred.\nFirst few <section> tags present:\n" + preview
  - L145 raise ValueError(msg)
- L148 def _parse_date_likevalue:
  - L149 if value is None:
    - L150 return None
  - L151 assign val = str(value).strip()
  - L152 if not val:
    - L153 return None
  - L155 assign iso_try = val.replace('Z', '+00:00')
  - L156 if ' ' in iso_try and 'T' not in iso_try:
    - L157 assign iso_try = iso_try.replace(' ', 'T', 1)
  - L158 try:
    - L159 return datetime.fromisoformat(iso_try)
    - L160 except ValueError:
      - L161 pass
  - L163 if re.fullmatch('\\d{10,}', val):
    - L164 try:
      - L165 assign seconds = int(val)
      - L166 if len(val) > 10:
        - L167 assign scale = 10 ** (len(val) - 10)
        - L168 return datetime.fromtimestamp(seconds / scale)
      - L169 return datetime.fromtimestamp(seconds)
      - L170 except ValueError:
        - L171 pass
  - L173 try:
    - L174 from email.utils import parsedate_to_datetime
    - L175 except ImportError:
      - L176 assign parsedate_to_datetime = None
  - L178 if parsedate_to_datetime is not None:
    - L179 try:
      - L180 assign dt = parsedate_to_datetime(val)
      - L181 if dt:
        - L182 return dt if dt.tzinfo is None else dt.astimezone()
      - L183 except (TypeError, ValueError):
        - L184 pass
  - L186 assign candidates = ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y', '%m-%d-%Y', '%d.%m.%Y', '%d %b %Y', '%d %B %Y', '%b %d %Y', '%B %d %Y', '%Y-%m-%d %H:%M', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M', '%Y/%m/%d %H:%M:%S', '%d/%m/%Y %H:%M', '%d/%m/%Y %H:%M:%S', '%m/%d/%Y %H:%M', '%m/%d/%Y %H:%M:%S', '%d-%m-%Y %H:%M', '%d-%m-%Y %H:%M:%S', '%d.%m.%Y %H:%M', '%d.%m.%Y %H:%M:%S', '%d %b %Y %H:%M', '%d %b %Y %H:%M:%S', '%d %B %Y %H:%M', '%d %B %Y %H:%M:%S', '%b %d %Y %H:%M', '%b %d %Y %H:%M:%S']
  - L217 for fmt in candidates:
    - L218 try:
      - L219 return datetime.strptime(val, fmt)
      - L220 except ValueError:
        - L221 continue
  - L222 return None
- L225 def _has_time_componentraw_value, dt_obj: datetime | None:
  - L226 if dt_obj and (dt_obj.hour or dt_obj.minute or dt_obj.second or dt_obj.microsecond):
    - L227 return True
  - L228 if raw_value is None:
    - L229 return False
  - L230 assign text = str(raw_value)
  - L231 if re.search('\\d{1,2}:\\d{2}', text):
    - L232 return True
  - L233 if re.search('\\b(am|pm)\\b', text, flags=re.IGNORECASE):
    - L234 return True
  - L235 if 'T' in text or 't' in text:
    - L236 return True
  - L237 return False
- L240 def _format_for_tokentoken: str, dt_obj: datetime | None, include_time_default: bool=False:
  - L241 if not dt_obj:
    - L242 return ''
  - L244 assign token_lower = token.lower()
  - L245 assign token_clean = re.sub('[^a-z0-9]', '', token_lower)
  - L247 def _has*needles: str:
    - L248 return any((needle in token_clean for needle in needles))
  - L250 assign include_time = include_time_default or _has('time', 'clock', 'datetime', 'timestamp')
  - L251 assign include_seconds = _has('second', 'seconds', 'sec', 'timestamp', 'precise', 'fulltime')
  - L252 assign use_ampm = _has('ampm', '12h', 'twelvehour')
  - L253 if include_seconds and (not include_time):
    - L254 assign include_time = True
  - L255 if use_ampm and (not include_time):
    - L256 assign include_time = True
  - L258 assign include_timezone = _has('timezone', 'tz', 'utc', 'offset', 'gmtoffset', 'withtz', 'withzone', 'zulu')
  - L259 assign iso_like = _has('iso', 'iso8601', 'ymd', 'rfc3339')
  - L260 assign rfc822_like = _has('rfc2822', 'rfc822')
  - L261 assign http_like = _has('httpdate', 'rfc7231')
  - L262 assign compact_like = _has('compact', 'slug', 'filename', 'filestamp', 'yyyymmdd', 'numeric', 'digits')
  - L263 assign us_like = _has('us', 'usa', 'mdy', 'mmdd')
  - L264 assign dashed_like = _has('dash', 'hyphen')
  - L265 assign long_like = _has('long', 'verbose', 'friendly', 'pretty', 'human')
  - L266 assign short_like = _has('short', 'abbr', 'mini', 'brief')
  - L267 assign month_long_like = _has('monthname', 'monthlong')
  - L268 assign month_short_like = _has('monthabbr', 'monthshort')
  - L269 assign weekday_like = _has('weekday', 'dayname')
  - L270 assign weekday_short = _has('weekdayshort', 'weekdayabbr', 'daynameshort')
  - L271 assign epoch_ms_like = _has('epochms', 'millis', 'milliseconds', 'unixms')
  - L272 assign epoch_like = _has('epoch', 'unixtime', 'unix')
  - L274 assign dt_for_format = dt_obj
  - L275 if include_timezone and dt_for_format.tzinfo is None:
    - L276 try:
      - L277 assign dt_for_format = dt_for_format.astimezone()
      - L278 except ValueError:
        - L279 pass
  - L281 if epoch_ms_like:
    - L282 try:
      - L283 return str(int(dt_for_format.timestamp() * 1000))
      - L284 except (OSError, OverflowError, ValueError):
        - L285 pass
  - L286 if epoch_like:
    - L287 try:
      - L288 return str(int(dt_for_format.timestamp()))
      - L289 except (OSError, OverflowError, ValueError):
        - L290 pass
  - L292 if rfc822_like or http_like:
    - L293 try:
      - L294 from email.utils import format_datetime as _email_format_datetime
      - L296 assign base_dt = dt_for_format
      - L297 if base_dt.tzinfo is None:
        - L298 assign base_dt = base_dt.astimezone()
      - L299 return _email_format_datetime(base_dt)
      - L300 except Exception:
        - L301 pass
  - L303 if iso_like:
    - L304 assign dt_use = dt_for_format
    - L305 if include_time:
      - L306 assign timespec = 'seconds' if include_seconds else 'minutes'
      - L307 try:
        - L308 return dt_use.isoformat(timespec=timespec)
        - L309 except TypeError:
          - L310 return dt_use.isoformat()
    - L311 return dt_use.date().isoformat()
  - L313 if compact_like:
    - L314 assign date_part = dt_for_format.strftime('%Y%m%d')
    - L315 if include_time:
      - L316 if use_ampm:
        - L317 assign time_fmt = '%I%M%S%p' if include_seconds else '%I%M%p'
        - L319 else:
          - L319 assign time_fmt = '%H%M%S' if include_seconds else '%H%M'
      - L320 assign date_part = f'{date_part}_{dt_for_format.strftime(time_fmt)}'
    - L321 if include_timezone:
      - L322 assign tz = dt_for_format.strftime('%z')
      - L323 if tz:
        - L324 assign date_part = f'{date_part}{tz}'
    - L325 return date_part
  - L327 assign date_part = '%d/%m/%Y'
  - L328 if us_like:
    - L329 assign date_part = '%m/%d/%Y'
    - L330 else:
      - L330 if dashed_like:
        - L331 assign date_part = '%d-%m-%Y'
        - L332 else:
          - L332 if long_like:
            - L333 assign date_part = '%B %d, %Y'
            - L334 else:
              - L334 if short_like or month_short_like:
                - L335 assign date_part = '%d %b %Y'
                - L336 else:
                  - L336 if month_long_like:
                    - L337 assign date_part = '%d %B %Y'
  - L339 if weekday_like:
    - L340 assign prefix = '%a, ' if weekday_short else '%A, '
    - L341 assign date_part = prefix + date_part
  - L343 assign fmt = date_part
  - L344 if include_time:
    - L345 if use_ampm:
      - L346 assign time_fmt = '%I:%M:%S %p' if include_seconds else '%I:%M %p'
      - L348 else:
        - L348 assign time_fmt = '%H:%M:%S' if include_seconds else '%H:%M'
    - L349 assign fmt = f'{fmt} {time_fmt}'
  - L350 if include_timezone:
    - L351 assign fmt = f'{fmt} %Z'.strip()
  - L353 try:
    - L354 assign rendered = dt_for_format.strftime(fmt).strip()
    - L355 if not rendered and '%Z' in fmt:
      - L356 assign rendered = dt_for_format.strftime(fmt.replace('%Z', '%z')).strip()
    - L357 return rendered
    - L358 except Exception:
      - L359 if include_time:
        - L360 try:
          - L361 return dt_for_format.isoformat(timespec='seconds')
          - L362 except TypeError:
            - L363 return dt_for_format.isoformat()
      - L364 return dt_for_format.date().isoformat()
- L367 assign STYLE_OR_SCRIPT_RE = re.compile('(?is)(<style\\b[^>]*>.*?</style>|<script\\b[^>]*>.*?</script>)')
- L370 def _apply_outside_styles_scriptshtml_in: str, transform_fn: Callable[[str], str]:
  - L371 assign parts = STYLE_OR_SCRIPT_RE.split(html_in)
  - L372 for i in range(len(parts)):
    - L373 if i % 2 == 0:
      - L374 assign parts[i] = transform_fn(parts[i])
  - L375 return ''.join(parts)
- L378 def _sub_token_texttext: str, token: str, val: str:
  - L379 assign pat = re.compile('(\\{\\{\\s*' + re.escape(token) + '\\s*\\}\\}|\\{\\s*' + re.escape(token) + '\\s*\\})')
  - L380 return pat.sub(val, text)
- L383 def sub_tokenhtml_in: str, token: str, val: str:
  - L384 return _apply_outside_styles_scripts(html_in, lambda txt: _sub_token_text(txt, token, val))
- L387 def _blank_known_tokens_texttext: str, tokens:
  - L388 for t in tokens:
    - L389 assign text = re.sub('\\{\\{\\s*' + re.escape(t) + '\\s*\\}\\}', '', text)
    - L390 assign text = re.sub('\\{\\s*' + re.escape(t) + '\\s*\\}', '', text)
  - L391 return text
- L394 def blank_known_tokenshtml_in: str, tokens:
  - L395 return _apply_outside_styles_scripts(html_in, lambda txt: _blank_known_tokens_text(txt, tokens))
- L398 def _convert_css_length_to_mmraw: str:
  - L399 if not raw:
    - L400 return None
  - L401 assign text = raw.strip().lower()
  - L402 if not text or text == 'auto':
    - L403 return None
  - L404 assign m = re.match('([-+]?\\d*\\.?\\d+)\\s*(mm|cm|in|pt|pc|px)?', text)
  - L405 if not m:
    - L406 return None
  - L407 assign value = float(m.group(1))
  - L408 assign unit = m.group(2) or 'px'
  - L409 if unit == 'mm':
    - L410 return value
  - L411 if unit == 'cm':
    - L412 return value * 10.0
  - L413 if unit in {'in', 'inch', 'inches'}:
    - L414 return value * 25.4
  - L415 if unit == 'pt':
    - L416 return value * (25.4 / 72.0)
  - L417 if unit == 'pc':
    - L418 return value * (25.4 / 6.0)
  - L419 if unit == 'px':
    - L420 return value * (25.4 / 96.0)
  - L421 return None
- L424 def _parse_page_size_valuevalue: str:
  - L425 if not value:
    - L426 return None
  - L427 assign text = value.strip().lower()
  - L428 if not text:
    - L429 return None
  - L430 assign size_map = {'a0': (841.0, 1189.0), 'a1': (594.0, 841.0), 'a2': (420.0, 594.0), 'a3': (297.0, 420.0), 'a4': (210.0, 297.0), 'a5': (148.0, 210.0), 'letter': (215.9, 279.4), 'legal': (215.9, 355.6), 'tabloid': (279.4, 431.8)}
  - L441 assign orientation = None
  - L442 assign tokens = [t for t in re.split('\\s+', text) if t]
  - L443 assign size_tokens = tokens
  - L444 if tokens and tokens[-1] in {'portrait', 'landscape'}:
    - L445 assign orientation = tokens[-1]
    - L446 assign size_tokens = tokens[:-1]
  - L447 if len(size_tokens) == 1 and size_tokens[0] in size_map:
    - L448 assign (width_mm, height_mm) = size_map[size_tokens[0]]
    - L449 else:
      - L449 if len(size_tokens) >= 2:
        - L450 assign first = _convert_css_length_to_mm(size_tokens[0])
        - L451 assign second = _convert_css_length_to_mm(size_tokens[1])
        - L452 if first is None or second is None:
          - L453 return None
        - L454 assign (width_mm, height_mm) = (first, second)
        - L456 else:
          - L456 return None
  - L457 if orientation == 'landscape':
    - L458 assign (width_mm, height_mm) = (height_mm, width_mm)
  - L459 return (width_mm, height_mm)
- L462 def _parse_margin_shorthandvalue: str:
  - L463 assign parts = [p for p in re.split('\\s+', value.strip()) if p]
  - L464 assign values = [_convert_css_length_to_mm(p) for p in parts]
  - L465 if not values:
    - L466 return (None, None, None, None)
  - L467 if len(values) == 1:
    - L468 assign top, right, bottom, left = values[0]
    - L469 else:
      - L469 if len(values) == 2:
        - L470 assign top, bottom = values[0]
        - L471 assign right, left = values[1]
        - L472 else:
          - L472 if len(values) == 3:
            - L473 assign top = values[0]
            - L474 assign right, left = values[1]
            - L475 assign bottom = values[2]
            - L477 else:
              - L477 assign (top, right, bottom, left) = values[:4]
  - L478 return (top, right, bottom, left)
- L481 def _extract_page_metricshtml_in: str:
  - L482 assign (default_width_mm, default_height_mm) = (210.0, 297.0)
  - L483 assign margin_top_mm = 0.0
  - L484 assign margin_bottom_mm = 0.0
  - L485 assign page_match = re.search('@page\\b[^{}]*\\{(?P<body>.*?)\\}', html_in, re.IGNORECASE | re.DOTALL)
  - L486 if page_match:
    - L487 assign block = page_match.group('body')
    - L488 assign size_match = re.search('size\\s*:\\s*([^;]+);?', block, re.IGNORECASE)
    - L489 if size_match:
      - L490 assign parsed_size = _parse_page_size_value(size_match.group(1))
      - L491 if parsed_size:
        - L492 assign (default_width_mm, default_height_mm) = parsed_size
    - L493 assign margin_match = re.search('margin\\s*:\\s*([^;]+);?', block, re.IGNORECASE)
    - L494 if margin_match:
      - L495 assign (mt, _, mb, _) = _parse_margin_shorthand(margin_match.group(1))
      - L496 if mt is not None:
        - L497 assign margin_top_mm = mt
      - L498 if mb is not None:
        - L499 assign margin_bottom_mm = mb
    - L500 for (name, setter) in (('margin-top', 'top'), ('margin-bottom', 'bottom')):
      - L501 assign specific = re.search(f'{name}\\s*:\\s*([^;]+);?', block, re.IGNORECASE)
      - L502 if specific:
        - L503 assign as_mm = _convert_css_length_to_mm(specific.group(1))
        - L504 if as_mm is None:
          - L505 continue
        - L506 if setter == 'top':
          - L507 assign margin_top_mm = as_mm
          - L509 else:
            - L509 assign margin_bottom_mm = as_mm
  - L510 return {'page_width_mm': default_width_mm, 'page_height_mm': default_height_mm, 'margin_top_mm': max(margin_top_mm, 0.0), 'margin_bottom_mm': max(margin_bottom_mm, 0.0)}

## backend\app\services\reports\contract_adapter.py
- L1 from __future__ import annotations
- L3 import re
- L4 from dataclasses import dataclass
- L5 from decimal import ROUND_HALF_UP, Decimal, InvalidOperation
- L6 from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple
- L8 assign _PARAM_RE = re.compile('PARAM:([A-Za-z0-9_]+)')
- L9 assign _AGG_FN_RE = re.compile('\\b(SUM|COUNT|AVG|MIN|MAX)\\s*\\(', re.IGNORECASE)
- L10 assign _DIRECT_COLUMN_RE = re.compile('^\\s*(?P<table>[A-Za-z_][\\w]*)\\s*\\.\\s*(?P<column>[A-Za-z_][\\w]*)\\s*$')
- L13 def _ensure_mappingvalue: Any:
  - L14 if not isinstance(value, Mapping):
    - L15 return {}
  - L16 annotated assign result: Dict[str, str] = {}
  - L17 for (key, expr) in value.items():
    - L18 if key is None:
      - L19 continue
    - L20 assign key_text = str(key).strip()
    - L21 if not key_text:
      - L22 continue
    - L23 assign expr_text = '' if expr is None else str(expr).strip()
    - L24 if not expr_text:
      - L25 continue
    - L26 assign result[key_text] = expr_text
  - L27 return result
- L30 def _ensure_sequencevalue: Any:
  - L31 if value is None:
    - L32 return []
  - L33 if isinstance(value, str):
    - L34 return [value]
  - L35 if isinstance(value, Iterable):
    - L36 return [str(item) for item in value]
  - L37 return [str(value)]
- L40 def format_decimal_strvalue: Any, max_decimals: int=3:
  - L41 docstring: "\n    Format numeric values with rounding up to max_decimals and trim trailing z..."
  - L45 if value is None:
    - L46 return ''
  - L48 annotated assign decimal_value: Optional[Decimal] = None
  - L49 if isinstance(value, Decimal):
    - L50 assign decimal_value = value
    - L52 else:
      - L52 assign text = str(value).strip()
      - L53 if not text:
        - L54 return ''
      - L55 try:
        - L56 assign decimal_value = Decimal(text)
        - L57 except (InvalidOperation, ValueError):
          - L58 return str(value)
  - L60 if not decimal_value.is_finite():
    - L61 return '0'
  - L63 if max_decimals <= 0:
    - L64 assign rounded = decimal_value.quantize(Decimal('1'), rounding=ROUND_HALF_UP)
    - L66 else:
      - L66 assign quantizer = Decimal('1').scaleb(-max_decimals)
      - L67 assign rounded = decimal_value.quantize(quantizer, rounding=ROUND_HALF_UP)
  - L69 assign formatted = format(rounded, 'f')
  - L70 if '.' in formatted:
    - L71 assign formatted = formatted.rstrip('0').rstrip('.')
  - L72 if formatted == '-0':
    - L73 assign formatted = '0'
  - L74 return formatted
- L77 def format_fixed_decimalsvalue: Any, decimals: int, max_decimals: int=3:
  - L78 assign decimals = max(0, min(decimals, max_decimals))
  - L79 try:
    - L80 assign number = Decimal(str(value))
    - L81 except (InvalidOperation, ValueError, TypeError):
      - L82 return str(value)
  - L83 if not number.is_finite():
    - L84 assign number = Decimal(0)
  - L85 assign quantizer = Decimal('1').scaleb(-decimals) if decimals else Decimal('1')
  - L86 assign rounded = number.quantize(quantizer, rounding=ROUND_HALF_UP)
  - L87 if rounded == 0:
    - L88 assign rounded = Decimal(0).quantize(quantizer, rounding=ROUND_HALF_UP) if decimals else Decimal(0)
  - L89 assign formatted = format(rounded, 'f')
  - L90 if formatted.startswith('-0'):
    - L91 assign formatted = format(Decimal(0).quantize(quantizer, rounding=ROUND_HALF_UP) if decimals else Decimal(0), 'f')
  - L92 return formatted
- L96 class FormatterSpec:
  - L97 annotated assign kind: str
  - L98 annotated assign arg: Optional[str] = None
  - L101 def parseraw: str | None:
    - L102 if not raw:
      - L103 return None
    - L104 assign text = raw.strip()
    - L105 if not text:
      - L106 return None
    - L107 assign m = re.match('([a-zA-Z0-9_]+)(?:\\((.*)\\))?$', text)
    - L108 if not m:
      - L109 return None
    - L110 assign kind = m.group(1).lower()
    - L111 assign arg = m.group(2)
    - L112 return FormatterSpec(kind=kind, arg=arg)
- L115 class ContractAdapter:
  - L116 docstring: "\n    Convenience wrapper around a Step-5 contract payload.\n\n    Exposes norma..."
  - L123 def __init__self, contract: Mapping[str, Any] | None:
    - L124 assign self._raw = contract or {}
    - L126 assign tokens = self._raw.get('tokens') or {}
    - L127 assign self._scalar_tokens = _ensure_sequence(tokens.get('scalars'))
    - L128 assign self._row_tokens = _ensure_sequence(tokens.get('row_tokens'))
    - L129 assign self._total_tokens = _ensure_sequence(tokens.get('totals'))
    - L131 assign join_block = self._raw.get('join') or {}
    - L132 assign self._parent_table = str(join_block.get('parent_table') or '').strip()
    - L133 assign self._child_table = str(join_block.get('child_table') or '').strip()
    - L134 assign self._parent_key = join_block.get('parent_key')
    - L135 assign self._child_key = join_block.get('child_key')
    - L137 assign self._date_columns = _ensure_mapping(self._raw.get('date_columns'))
    - L139 assign filters = self._raw.get('filters') or {}
    - L140 assign self._required_filters = _ensure_mapping(filters.get('required'))
    - L141 assign self._optional_filters = _ensure_mapping(filters.get('optional'))
    - L143 assign self._reshape_rules = self._raw.get('reshape_rules') or []
    - L144 assign self._row_computed = _ensure_mapping(self._raw.get('row_computed'))
    - L145 assign self._totals_math = _ensure_mapping(self._raw.get('totals_math'))
    - L146 assign self._formatters_raw = _ensure_mapping(self._raw.get('formatters'))
    - L148 assign order_by_block = self._raw.get('order_by') or {}
    - L149 assign self._order_by_rows = _ensure_sequence(order_by_block.get('rows'))
    - L150 assign self._row_order = _ensure_sequence(self._raw.get('row_order'))
    - L152 assign self._totals_mapping = _ensure_mapping(self._raw.get('totals'))
    - L153 assign self._mapping = _ensure_mapping(self._raw.get('mapping'))
    - L154 if not self._parent_table:
      - L155 assign inferred_parent = self._infer_parent_table(self._mapping)
      - L156 if inferred_parent:
        - L157 assign self._parent_table = inferred_parent
    - L159 assign self._param_tokens = self._discover_param_tokens()
    - L160 annotated assign self._formatter_cache: Dict[str, FormatterSpec | None] = {}
  - L166 def mappingself:
    - L167 return dict(self._mapping)
  - L170 def scalar_tokensself:
    - L171 return list(self._scalar_tokens)
  - L174 def row_tokensself:
    - L175 return list(self._row_tokens)
  - L178 def total_tokensself:
    - L179 return list(self._total_tokens)
  - L182 def parent_tableself:
    - L183 return self._parent_table
  - L186 def child_tableself:
    - L187 return self._child_table
  - L190 def parent_keyself:
    - L191 return self._parent_key
  - L194 def child_keyself:
    - L195 return self._child_key
  - L198 def date_columnsself:
    - L199 return dict(self._date_columns)
  - L202 def required_filtersself:
    - L203 return dict(self._required_filters)
  - L206 def optional_filtersself:
    - L207 return dict(self._optional_filters)
  - L210 def reshape_rulesself:
    - L211 return list(self._reshape_rules)
  - L214 def row_computedself:
    - L215 return dict(self._row_computed)
  - L218 def totals_mathself:
    - L219 return dict(self._totals_math)
  - L222 def formattersself:
    - L223 return dict(self._formatters_raw)
  - L226 def order_by_rowsself:
    - L227 return list(self._order_by_rows)
  - L230 def row_orderself:
    - L231 return list(self._row_order)
  - L234 def totals_mappingself:
    - L235 return dict(self._totals_mapping)
  - L238 def param_tokensself:
    - L239 return sorted(self._param_tokens)
  - L244 def get_formatter_specself, token: str:
    - L245 if token in self._formatter_cache:
      - L246 return self._formatter_cache[token]
    - L247 assign raw = self._formatters_raw.get(token)
    - L248 assign spec = FormatterSpec.parse(raw)
    - L249 assign self._formatter_cache[token] = spec
    - L250 return spec
  - L252 def format_valueself, token: str, value: Any:
    - L253 assign spec = self.get_formatter_spec(token)
    - L254 if spec is None:
      - L255 if value is None:
        - L256 return ''
      - L257 if isinstance(value, (int, float, Decimal)):
        - L258 return format_decimal_str(value)
      - L259 if isinstance(value, str):
        - L260 assign candidate = value.strip()
        - L261 if not candidate:
          - L262 return ''
        - L263 assign lowered = candidate.lower()
        - L264 if '.' in candidate or 'e' in lowered:
          - L265 return format_decimal_str(candidate)
        - L266 return value
      - L267 return str(value)
    - L268 assign kind = spec.kind
    - L269 if value is None:
      - L270 return ''
    - L272 if kind == 'number':
      - L273 assign decimals = 0
      - L274 if spec.arg:
        - L275 try:
          - L276 assign decimals = int(spec.arg.strip())
          - L277 except ValueError:
            - L278 assign decimals = 0
      - L279 return format_fixed_decimals(value, decimals, max_decimals=3)
    - L281 if kind == 'percent':
      - L282 assign decimals = 0
      - L283 if spec.arg:
        - L284 try:
          - L285 assign decimals = int(spec.arg.strip())
          - L286 except ValueError:
            - L287 assign decimals = 0
      - L288 return format_fixed_decimals(value, decimals, max_decimals=3)
    - L290 if kind == 'date':
      - L291 assign fmt = (spec.arg or 'YYYY-MM-DD').strip()
      - L292 return self._format_date_like(value, fmt)
    - L294 return str(value)
  - L297 def _format_date_likevalue: Any, fmt: str:
    - L298 from datetime import datetime
    - L300 if value is None:
      - L301 return ''
    - L302 assign text = str(value).strip()
    - L303 if not text:
      - L304 return ''
    - L305 annotated assign dt: Optional[datetime] = None
    - L306 for pattern in ('%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%d/%m/%Y', '%m/%d/%Y'):
      - L313 try:
        - L314 assign dt = datetime.strptime(text, pattern)
        - L315 break
        - L316 except ValueError:
          - L317 continue
    - L318 if dt is None:
      - L319 try:
        - L320 assign dt = datetime.fromisoformat(text.replace('Z', '+00:00'))
        - L321 except ValueError:
          - L322 return text
    - L324 assign fmt_map = {'DD/MM/YYYY': '%d/%m/%Y', 'YYYY-MM-DD': '%Y-%m-%d', 'DD-MM-YYYY': '%d-%m-%Y', 'MM/DD/YYYY': '%m/%d/%Y'}
    - L330 assign pattern = fmt_map.get(fmt.upper(), '%Y-%m-%d')
    - L331 return dt.strftime(pattern)
  - L334 def _infer_parent_tablemapping: Mapping[str, str]:
    - L335 for expr in mapping.values():
      - L336 if not isinstance(expr, str):
        - L337 continue
      - L338 assign match = _DIRECT_COLUMN_RE.match(expr.strip())
      - L339 if not match:
        - L340 continue
      - L341 assign table_name = match.group('table').strip(' "`[]')
      - L342 if table_name and (not table_name.lower().startswith('params')):
        - L343 return table_name
    - L344 return None
  - L349 def _translate_expressionself, expr: str:
    - L350 annotated assign tokens: List[str] = []
    - L352 def replacermatch: re.Match[str]:
      - L353 assign name = match.group(1)
      - L354 expr tokens.append(name)
      - L355 return f':{name}'
    - L357 assign translated = _PARAM_RE.sub(replacer, expr)
    - L358 return (translated, tokens)
  - L360 def build_base_where_clausesself, include_date_range: bool=True:
    - L361 docstring: "\n        Returns `(predicates, params)` for the parent table using required + o..."
    - L367 annotated assign predicates: List[str] = []
    - L368 annotated assign params: List[str] = []
    - L369 annotated assign used_tokens: set[str] = set()
    - L371 if include_date_range and self._parent_table:
      - L372 assign date_col = self._date_columns.get(self._parent_table)
      - L373 if date_col:
        - L374 expr predicates.append(f'datetime({date_col}) BETWEEN datetime(:from_date) AND datetime(:to_date)')
        - L375 expr params.extend(['from_date', 'to_date'])
        - L376 expr used_tokens.update({'from_date', 'to_date'})
    - L378 def _wrap_date_paramsql: str, param: str:
      - L379 assign pattern = re.compile(f':{param}\\b')
      - L381 def _replacematch: re.Match[str]:
        - L382 assign start = match.start()
        - L383 assign prefix = sql[:start].rstrip()
        - L384 assign prefix_lower = prefix.lower()
        - L385 if prefix_lower.endswith('date(') or prefix_lower.endswith('datetime('):
          - L386 return match.group(0)
        - L387 return f'DATE({match.group(0)})'
      - L389 return pattern.sub(_replace, sql)
    - L391 for (token, expr) in self._required_filters.items():
      - L392 if include_date_range and token in used_tokens:
        - L393 continue
      - L394 assign (translated, tokens) = self._translate_expression(expr)
      - L395 if not tokens:
        - L396 assign param_name = str(token or '').strip()
        - L397 if param_name:
          - L398 assign placeholder = f':{param_name}'
          - L399 assign translated = f'{translated} = {placeholder}'
          - L400 assign tokens = [param_name]
      - L401 if 'DATE(' in expr.upper():
        - L402 assign translated = _wrap_date_param(translated, 'from_date')
        - L403 assign translated = _wrap_date_param(translated, 'to_date')
      - L404 expr predicates.append(translated)
      - L405 for name in tokens:
        - L406 if name not in params:
          - L407 expr params.append(name)
      - L408 expr used_tokens.update(tokens)
    - L409 for (token, expr) in self._optional_filters.items():
      - L410 if include_date_range and token in used_tokens:
        - L411 continue
      - L412 assign (translated, tokens) = self._translate_expression(expr)
      - L413 if not tokens:
        - L414 continue
      - L415 if 'DATE(' in expr.upper():
        - L416 assign translated = _wrap_date_param(translated, 'from_date')
        - L417 assign translated = _wrap_date_param(translated, 'to_date')
      - L418 assign clauses = [f":{name} IS NULL OR TRIM(:{name}) = ''" for name in tokens]
      - L419 expr clauses.append(translated)
      - L420 expr predicates.append('(' + ' OR '.join(clauses) + ')')
      - L421 for name in tokens:
        - L422 if name not in params:
          - L423 expr params.append(name)
      - L424 expr used_tokens.update(tokens)
    - L425 return (predicates, params)
  - L427 def build_union_cteself, base_alias: str='base', cte_name: str='long_bins':
    - L428 docstring: "\n        Construct SQL for the reshape_rules UNION_ALL description.\n        Re..."
    - L432 if not self._reshape_rules:
      - L433 raise ValueError('contract.reshape_rules is required to synthesise SQL')
    - L434 assign rule = next((item for item in self._reshape_rules if isinstance(item, Mapping) and item.get('columns')), None)
    - L438 if not rule:
      - L439 raise ValueError('contract.reshape_rules must include at least one rule with column definitions')
    - L440 assign columns = rule.get('columns') or []
    - L441 if not columns:
      - L442 raise ValueError('contract.reshape_rules columns must define at least one column')
    - L444 annotated assign per_column_sources: List[Tuple[str, List[str]]] = []
    - L445 annotated assign select_aliases: List[str] = []
    - L447 for col in columns:
      - L448 assign alias = str(col.get('as') or '').strip()
      - L449 if not alias:
        - L450 raise ValueError("reshape_rules columns must specify 'as'")
      - L451 assign sources = col.get('from')
      - L452 if not isinstance(sources, Sequence) or not sources:
        - L453 raise ValueError(f"reshape_rules column {alias} must provide a non-empty 'from' list")
      - L454 expr select_aliases.append(alias)
      - L455 assign normalized_sources = []
      - L456 for item in sources:
        - L457 assign source_expr = str(item).strip()
        - L458 if self._parent_table and f'{self._parent_table}.' in source_expr:
          - L459 assign source_expr = source_expr.replace(f'{self._parent_table}.', f'{base_alias}.')
        - L460 expr normalized_sources.append(source_expr)
      - L461 expr per_column_sources.append((alias, normalized_sources))
    - L463 assign column_count = len(per_column_sources[0][1])
    - L464 for (_, sources) in per_column_sources:
      - L465 if len(sources) != column_count:
        - L466 raise ValueError("All reshape_rules column 'from' lists must be the same length")
    - L468 annotated assign union_selects: List[str] = []
    - L469 for idx in range(column_count):
      - L470 assign select_parts = []
      - L471 for (alias, sources) in per_column_sources:
        - L472 assign source_expr = sources[idx]
        - L473 expr select_parts.append(f'{source_expr} AS {alias}')
      - L474 expr union_selects.append(f"SELECT {', '.join(select_parts)} FROM {base_alias}")
    - L476 assign union_sql = '\n  UNION ALL\n  '.join(union_selects)
    - L477 assign cte_sql = f'{cte_name} AS (\n  {union_sql}\n)'
    - L478 return (cte_sql, select_aliases)
  - L480 def build_row_aggregate_sqlself, long_cte_name: str, union_columns: Sequence[str], rows_alias: str='rows':
    - L486 docstring: "\n        Build row-level CTEs (aggregated dataset and ordered dataset) from the..."
    - L490 assign union_set = {col.strip() for col in union_columns}
    - L492 assign aggregated_alias = f'{rows_alias}_agg'
    - L493 annotated assign select_clauses: List[str] = []
    - L494 annotated assign group_by_exprs: List[str] = []
    - L495 annotated assign handled_tokens: set[str] = set()
    - L497 def _sanitize_order_clauseraw_order: Sequence[str]:
      - L498 annotated assign cleaned: List[str] = []
      - L499 for clause in raw_order:
        - L500 assign text = str(clause or '').strip()
        - L501 if not text:
          - L502 continue
        - L503 if 'rowid' in text.replace(' ', '').lower():
          - L504 continue
        - L505 expr cleaned.append(text)
      - L506 if not cleaned:
        - L507 return 'material_name ASC'
      - L508 return ', '.join(cleaned)
    - L510 def _normalise_exprexpr: str:
      - L511 assign (translated, _) = self._translate_expression(expr)
      - L512 for prefix in (f'{long_cte_name}.', 'long_bins.', f'{rows_alias}.', f'{aggregated_alias}.'):
        - L518 assign translated = translated.replace(prefix, '')
      - L519 return translated.strip()
    - L521 def _add_clause_from_exprtoken: str, expr: str:
      - L522 Nonlocal
      - L523 if not expr:
        - L524 return None
      - L525 assign translated = _normalise_expr(expr)
      - L526 if not translated:
        - L527 return None
      - L528 assign aggregate = bool(_AGG_FN_RE.search(expr))
      - L530 assign existing_aliases = {alias for _, alias in (clause.split(' AS ') for clause in select_clauses)}
      - L531 if token in existing_aliases:
        - L532 assign select_clauses[:] = [clause for clause in select_clauses if not clause.endswith(f' AS {token}')]
      - L533 expr select_clauses.append(f'{translated} AS {token}')
      - L534 if not aggregate:
        - L535 expr group_by_exprs.append(translated)
      - L536 expr handled_tokens.add(token)
    - L539 for token in self._row_tokens:
      - L540 if token.lower() == 'sl_no':
        - L541 continue
      - L542 assign mapping_expr = self._mapping.get(token)
      - L543 if mapping_expr and 'ROW_NUMBER' not in mapping_expr.upper():
        - L544 expr _add_clause_from_expr(token, mapping_expr)
    - L547 for token in self._row_tokens:
      - L548 if token in handled_tokens or token.lower() == 'sl_no':
        - L549 continue
      - L550 assign computed_expr = self._row_computed.get(token)
      - L551 if computed_expr:
        - L552 expr _add_clause_from_expr(token, computed_expr)
    - L555 for token in self._row_tokens:
      - L556 if token in handled_tokens or token.lower() == 'sl_no':
        - L557 continue
      - L558 if token in union_set:
        - L559 assign fallback_expr = token
        - L560 if token == 'material_name':
          - L561 assign fallback_expr = 'TRIM(material_name)'
        - L562 expr _add_clause_from_expr(token, fallback_expr)
    - L564 if not select_clauses:
      - L565 raise ValueError('Unable to build row dataset without select expressions')
    - L568 assign has_material = any((clause.endswith(' AS material_name') for clause in select_clauses))
    - L569 assign material_expr = None
    - L570 if has_material:
      - L571 assign material_expr = next((clause.split(' AS ')[0] for clause in select_clauses if clause.endswith(' AS material_name')))
      - L574 else:
        - L574 if 'material_name' in union_set:
          - L575 assign material_expr = 'TRIM(material_name)'
          - L576 expr _add_clause_from_expr('material_name', material_expr)
          - L577 assign has_material = True
    - L579 annotated assign unique_group_by: List[str] = []
    - L580 for expr in group_by_exprs:
      - L581 assign expr_clean = expr.strip()
      - L582 if expr_clean and expr_clean not in unique_group_by:
        - L583 expr unique_group_by.append(expr_clean)
    - L585 assign group_by_clause = ''
    - L586 if unique_group_by:
      - L587 assign group_by_clause = 'GROUP BY ' + ', '.join(unique_group_by)
    - L589 assign where_clause = ''
    - L590 if has_material and material_expr:
      - L591 assign where_clause = f"WHERE TRIM(COALESCE({material_expr}, '')) <> ''"
    - L593 assign aggregated_cte = f'{aggregated_alias} AS (\n  SELECT\n    ' + ',\n    '.join(select_clauses) + f'\n  FROM {long_cte_name}\n  {where_clause}\n  {group_by_clause}\n)'
    - L600 assign order_clause = _sanitize_order_clause(self._row_order or ['material_name ASC'])
    - L601 assign ordered_cte = f'{rows_alias} AS (\n  SELECT\n    ROW_NUMBER() OVER (ORDER BY {order_clause}) AS sl_no,\n    {aggregated_alias}.*\n  FROM {aggregated_alias}\n)'
    - L609 assign available_tokens = ['sl_no'] + [clause.split(' AS ')[1] for clause in select_clauses]
    - L610 return ([aggregated_cte, ordered_cte], available_tokens, order_clause)
  - L612 def build_totals_sqlself, rows_alias: str='rows', totals_alias: str='totals':
    - L613 docstring: "\n        Build SQL SELECT for totals tokens based on totals_math mapping.\n    ..."
    - L616 if not self._totals_math:
      - L617 raise ValueError('contract.totals_math is required to compute totals')
    - L619 annotated assign select_clauses: List[str] = []
    - L620 annotated assign tokens_in_order: List[str] = []
    - L621 for token in self._total_tokens or self._totals_math.keys():
      - L622 assign expr = self._totals_math.get(token)
      - L623 if not expr:
        - L624 continue
      - L625 assign translated = expr.replace(f'{rows_alias}.', '')
      - L626 expr select_clauses.append(f'{translated} AS {token}')
      - L627 expr tokens_in_order.append(token)
    - L629 if not select_clauses:
      - L630 raise ValueError('Unable to build totals dataset without expressions')
    - L632 assign totals_sql = f'{totals_alias} AS (\n  SELECT\n    ' + ',\n    '.join(select_clauses) + f'\n  FROM {rows_alias}\n)'
    - L635 return (totals_sql, tokens_in_order)
  - L637 def build_default_sql_packself:
    - L638 docstring: "\n        Synthesise a minimal sql_pack (dialect + entrypoints + script + params..."
    - L643 if not self._parent_table:
      - L644 raise ValueError('contract.join.parent_table is required to synthesise SQL')
    - L646 assign (predicates, _) = self.build_base_where_clauses()
    - L647 assign where_clause = ''
    - L648 if predicates:
      - L649 assign where_clause = 'WHERE ' + '\n    AND '.join(predicates)
    - L651 assign base_cte = f'base AS (\n  SELECT *\n  FROM {self._parent_table}\n  {where_clause}\n)'
    - L652 assign (long_cte, long_columns) = self.build_union_cte(base_alias='base')
    - L653 assign (row_ctes, available_row_tokens, row_order_clause) = self.build_row_aggregate_sql(long_cte_name='long_bins', union_columns=long_columns)
    - L656 assign (totals_cte, _) = self.build_totals_sql(rows_alias='rows')
    - L658 assign cte_parts = [base_cte, long_cte] + row_ctes + [totals_cte]
    - L659 assign cte_chain = ',\n'.join(cte_parts)
    - L661 annotated assign header_select_parts: List[str] = []
    - L662 for token in self._scalar_tokens:
      - L663 assign target = self._mapping.get(token)
      - L664 if not target:
        - L665 expr header_select_parts.append(f"'' AS {token}")
        - L666 continue
      - L667 if target.startswith('PARAM:'):
        - L668 assign param = target.split(':', 1)[1]
        - L669 expr header_select_parts.append(f':{param} AS {token}')
        - L671 else:
          - L671 expr header_select_parts.append(f'{target} AS {token}')
    - L672 assign header_sql = 'SELECT ' + ', '.join(header_select_parts)
    - L673 if self._parent_table:
      - L674 assign header_from = f'\nFROM {self._parent_table}'
      - L675 if where_clause:
        - L676 aug assign header_from Add f'\n  {where_clause}'
      - L677 aug assign header_sql Add f'{header_from}\nLIMIT 1'
    - L679 assign row_tokens_to_use = [token for token in self._row_tokens if token in available_row_tokens]
    - L680 if not row_tokens_to_use:
      - L681 assign row_tokens_to_use = available_row_tokens
    - L683 assign rows_select = 'SELECT ' + ', '.join([f'{token}' for token in row_tokens_to_use])
    - L684 aug assign rows_select Add ' FROM rows'
    - L685 if row_order_clause:
      - L686 aug assign rows_select Add f'\nORDER BY {row_order_clause}'
    - L688 assign totals_select = 'SELECT ' + ', '.join([f'{token}' for token in self._total_tokens]) + ' FROM totals'
    - L690 assign script = f'-- HEADER SELECT --\n{header_sql};\n\n-- ROWS SELECT --\nWITH {cte_chain}\n{rows_select};\n\n-- TOTALS SELECT --\nWITH {cte_chain}\n{totals_select};'
    - L699 assign entrypoints = {'header': header_sql, 'rows': f'WITH {cte_chain}\n{rows_select}', 'totals': f'WITH {cte_chain}\n{totals_select}'}
    - L705 assign params_required = sorted(self._required_filters.keys())
    - L706 assign params_optional = sorted(set(self._optional_filters.keys()) | set(self._param_tokens))
    - L708 return {'dialect': 'duckdb', 'script': script, 'entrypoints': entrypoints, 'params': {'required': params_required, 'optional': params_optional}}
  - L721 def _discover_param_tokensself:
    - L722 annotated assign tokens: set[str] = set()
    - L723 for expr in self._mapping.values():
      - L724 for match in _PARAM_RE.findall(expr):
        - L725 expr tokens.add(match)
    - L726 for expr in self._required_filters.values():
      - L727 expr tokens.update(_PARAM_RE.findall(expr))
    - L728 for expr in self._optional_filters.values():
      - L729 expr tokens.update(_PARAM_RE.findall(expr))
    - L730 return list(tokens)

## backend\app\services\reports\date_utils.py
- L1 from __future__ import annotations
- L3 from pathlib import Path
- L4 from typing import Callable, Tuple
- L6 from ..dataframes.sqlite_loader import get_loader
- L9 def get_col_typedb_path: Path, table: str, col: str:
  - L10 docstring: "\n    Return the inferred column type (uppercased) for table.col or '' when unav..."
  - L14 if not col or not table:
    - L15 return ''
  - L16 try:
    - L17 assign loader = get_loader(db_path)
    - L18 return (loader.column_type(table, col) or '').upper()
    - L19 except Exception:
      - L20 return ''
- L23 def mk_between_pred_for_datecol: str, col_type: str:
  - L24 docstring: "\n    Returns (predicate_sql, adapter) used to build BETWEEN date filters.\n    ..."
  - L30 if not col or not col_type:
    - L31 return ('1=1', lambda _s, _e: tuple())
  - L33 assign t = col_type.upper()
  - L34 if 'INT' in t:
    - L35 assign predicate = f"(CASE WHEN ABS({col}) > 32503680000 THEN {col}/1000 ELSE {col} END) BETWEEN strftime('%s', ?) AND strftime('%s', ?)"
    - L39 return (predicate, lambda start, end: (start, end))
  - L41 assign predicate = f'datetime({col}) BETWEEN datetime(?) AND datetime(?)'
  - L42 return (predicate, lambda start, end: (start, end))
- L45 assign __all__ = ['get_col_type', 'mk_between_pred_for_date']

## backend\app\services\reports\discovery.py
- L1 docstring: "Legacy PDF discovery module that now delegates to the shared DataFrame flow."
- L3 from __future__ import annotations
- L5 from pathlib import Path
- L6 from typing import Any, Mapping
- L8 from .discovery_excel import discover_batches_and_counts as _discover_batches_df
- L10 assign __all__ = ['discover_batches_and_counts']
- L13 def discover_batches_and_counts*, db_path: Path, contract: dict, start_date: str, end_date: str, key_values: Mapping[str, Any] | None=None:
  - L21 docstring: "\n    Wrapper retained for backwards compatibility with the PDF pipeline.\n    R..."
  - L26 return _discover_batches_df(db_path=db_path, contract=contract, start_date=start_date, end_date=end_date, key_values=key_values)

## backend\app\services\reports\discovery_excel.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\reports\discovery_metrics.py
- L1 docstring: "Shared helpers used by discovery flows to expose per-batch metrics."
- L3 from __future__ import annotations
- L5 from typing import Any, Iterable, Mapping, Sequence
- L7 assign __all__ = ['build_batch_field_catalog_and_stats', 'build_batch_metrics', 'build_discovery_schema', 'bin_numeric_metric', 'group_metrics_by_field', 'build_resample_support']
- L17 def _coerce_numbervalue: Any:
  - L18 if isinstance(value, (int, float)):
    - L19 return float(value)
  - L20 try:
    - L21 return float(value)
    - L22 except Exception:
      - L23 return 0.0
- L26 def _normalize_typeraw: Any:
  - L27 docstring: "\n    Collapse loose field types coming from discovery into a stable set we can ..."
  - L32 assign text = str(raw or '').strip().lower()
  - L33 if text in {'number', 'numeric', 'float', 'double', 'integer', 'int'}:
    - L34 return 'number'
  - L35 if text in {'datetime', 'timestamp', 'date', 'time'}:
    - L36 return 'datetime'
  - L37 if text in {'category', 'categorical'}:
    - L38 return 'string'
  - L39 return 'string'
- L42 def build_batch_field_catalog_and_statsbatches: Sequence[Mapping[str, Any]], *, time_source: str | None=None, categorical_fields: Sequence[str] | None=None, numeric_fields: Sequence[str] | None=None, field_sources: Mapping[str, str] | None=None:
  - L50 annotated assign rows_values: list[float] = []
  - L51 annotated assign parent_values: list[float] = []
  - L52 for raw in batches:
    - L53 if not isinstance(raw, Mapping):
      - L54 continue
    - L55 assign rows_val = raw.get('rows')
    - L56 assign parent_val = raw.get('parent')
    - L57 try:
      - L58 assign rows_num = float(rows_val)
      - L59 except Exception:
        - L60 assign rows_num = 0.0
    - L61 try:
      - L62 assign parent_num = float(parent_val)
      - L63 except Exception:
        - L64 assign parent_num = 0.0
    - L65 expr rows_values.append(rows_num)
    - L66 expr parent_values.append(parent_num)
  - L68 def _basic_statsvalues: list[float]:
    - L69 if not values:
      - L70 return {'min': 0.0, 'max': 0.0, 'avg': 0.0}
    - L71 assign total = sum(values)
    - L72 return {'min': float(min(values)), 'max': float(max(values)), 'avg': float(total / len(values))}
  - L78 annotated assign stats: dict[str, Any] = {'batch_count': len(batches), 'rows_total': int(sum(rows_values)), 'rows_stats': _basic_stats(rows_values), 'parent_stats': _basic_stats(parent_values)}
  - L85 annotated assign sources: dict[str, str] = {str(k): str(v) for k, v in (field_sources or {}).items() if str(k).strip()}
  - L86 annotated assign field_catalog: list[dict[str, Any]] = []
  - L87 annotated assign seen: set[str] = set()
  - L89 def _add_fieldname: str, ftype: str, description: str, *, source: str | None=None:
    - L90 assign key = (name or '').strip()
    - L91 if not key or key in seen:
      - L92 return None
    - L93 expr field_catalog.append({'name': key, 'type': (ftype or 'unknown').strip(), 'description': description, 'source': source or sources.get(key) or 'computed'})
    - L101 expr seen.add(key)
  - L103 expr _add_field('batch_index', 'numeric', '1-based index of the batch in discovery order.', source='computed')
  - L109 expr _add_field('batch_id', 'categorical', 'Batch identifier (composite key from join keys).', source='computed')
  - L115 expr _add_field('rows', 'numeric', 'Number of child rows in this batch.', source=sources.get('rows') or 'child_rows')
  - L121 expr _add_field('parent', 'numeric', 'Number of parent rows associated with this batch.', source=sources.get('parent') or 'parent_rows')
  - L127 expr _add_field('rows_per_parent', 'numeric', 'Child rows divided by parent rows (if parent is zero, treat as rows).', source='computed')
  - L134 if time_source:
    - L135 assign time_desc = f'Earliest timestamp per batch sourced from {time_source}.'
    - L137 else:
      - L137 assign time_desc = 'Earliest timestamp associated with the batch (if available).'
  - L138 expr _add_field('time', 'time', time_desc, source=time_source or sources.get('time') or 'computed')
  - L145 annotated assign cat_fields: list[str] = []
  - L146 if categorical_fields:
    - L147 for field in categorical_fields:
      - L148 assign text = str(field or '').strip()
      - L149 if text and text not in cat_fields:
        - L150 expr cat_fields.append(text)
  - L151 if cat_fields:
    - L152 assign primary_cat = cat_fields[0]
    - L153 expr _add_field('category', 'categorical', f"Categorical label derived from key column '{primary_cat}'.", source=sources.get(primary_cat) or 'computed')
    - L159 for field in cat_fields:
      - L160 expr _add_field(field, 'categorical', f"Key column '{field}' used to build the batch identifier.", source=sources.get(field) or 'computed')
    - L167 else:
      - L167 expr _add_field('category', 'categorical', 'Categorical label derived from key columns (if available).', source='computed')
  - L174 for field in numeric_fields or []:
    - L175 assign fname = (field or '').strip()
    - L176 if not fname or fname in seen:
      - L177 continue
    - L178 expr _add_field(fname, 'numeric', f"Numeric measure '{fname}' derived from discovery results.", source=sources.get(fname) or 'computed')
  - L185 return (field_catalog, stats)
- L188 def build_batch_metricsbatches: Sequence[Mapping[str, Any]], batch_metadata: Mapping[str, Any] | None, *, limit: int | None=None, extra_fields: Sequence[str] | None=None:
  - L195 annotated assign metrics: list[dict[str, Any]] = []
  - L196 annotated assign metadata_lookup: Mapping[str, Any] = batch_metadata if isinstance(batch_metadata, Mapping) else {}
  - L197 annotated assign iterable: Sequence[Mapping[str, Any]] = batches
  - L198 if limit is not None:
    - L199 assign iterable = list(iterable)[:limit]
  - L201 annotated assign extras: list[str] = []
  - L202 if extra_fields is None:
    - L203 annotated assign seen_extras: set[str] = set()
    - L204 for meta in metadata_lookup.values():
      - L205 if not isinstance(meta, Mapping):
        - L206 continue
      - L207 for key in meta.keys():
        - L208 assign key_text = str(key or '').strip()
        - L209 if not key_text or key_text in ('time', 'category') or key_text in seen_extras:
          - L210 continue
        - L211 expr seen_extras.add(key_text)
    - L212 assign extras = sorted(seen_extras)
    - L214 else:
      - L214 assign extras = []
      - L215 for field in extra_fields:
        - L216 assign name = str(field or '').strip()
        - L217 if not name or name in ('time', 'category') or name in extras:
          - L218 continue
        - L219 expr extras.append(name)
  - L221 for (idx, raw) in enumerate(iterable, start=1):
    - L222 if not isinstance(raw, Mapping):
      - L223 continue
    - L224 assign batch_id = raw.get('id')
    - L225 assign rows_val = _coerce_number(raw.get('rows'))
    - L226 assign parent_val = _coerce_number(raw.get('parent'))
    - L227 assign safe_parent = parent_val if parent_val not in (None, 0) else 1.0
    - L228 annotated assign entry: dict[str, Any] = {'batch_index': idx, 'batch_id': str(batch_id) if batch_id is not None else str(idx), 'rows': rows_val, 'parent': parent_val, 'rows_per_parent': rows_val / safe_parent if safe_parent else rows_val}
    - L235 assign meta = metadata_lookup.get(str(batch_id)) if batch_id is not None else None
    - L236 if isinstance(meta, Mapping):
      - L237 for key in ('time', 'category'):
        - L238 if key in meta:
          - L239 assign entry[key] = meta[key]
      - L240 for extra in extras:
        - L241 if extra in meta:
          - L242 assign entry[extra] = meta[extra]
    - L243 expr metrics.append(entry)
  - L244 return metrics
- L261 def build_discovery_schemafield_catalog: Iterable[Mapping[str, Any]]:
  - L262 docstring: "\n    Build a concise list of allowed metrics/dimensions from a field catalog.\n..."
  - L268 annotated assign metrics: list[dict[str, Any]] = []
  - L269 annotated assign dimensions: list[dict[str, Any]] = []
  - L270 annotated assign seen_metric_names: set[str] = set()
  - L271 annotated assign seen_dimension_names: set[str] = set()
  - L273 for field in field_catalog or []:
    - L274 assign name = str(field.get('name') or '').strip()
    - L275 if not name:
      - L276 continue
    - L277 assign normalized_type = _normalize_type(field.get('type'))
    - L278 assign description = str(field.get('description') or '').strip()
    - L279 assign base = {'name': name, 'type': normalized_type}
    - L280 if description:
      - L281 assign base['description'] = description
    - L284 if normalized_type == 'number' and name not in seen_metric_names:
      - L285 expr metrics.append({**base, 'bucketable': True})
      - L286 expr seen_metric_names.add(name)
    - L288 if name not in seen_dimension_names:
      - L289 assign kind = 'categorical'
      - L290 assign bucketable = False
      - L291 if normalized_type == 'datetime':
        - L292 assign kind = 'temporal'
        - L293 assign bucketable = True
        - L294 else:
          - L294 if normalized_type == 'number':
            - L295 assign kind = 'numeric'
            - L296 assign bucketable = True
      - L297 expr dimensions.append({**base, 'kind': kind, 'bucketable': bucketable})
      - L298 expr seen_dimension_names.add(name)
  - L300 def _pick_default_dimension:
    - L301 for preferred in ('time', 'timestamp', 'date'):
      - L302 if any((dim['name'] == preferred for dim in dimensions)):
        - L303 return preferred
    - L304 for fallback in ('category', 'batch_index'):
      - L305 if any((dim['name'] == fallback for dim in dimensions)):
        - L306 return fallback
    - L307 return dimensions[0]['name'] if dimensions else 'batch_index'
  - L309 def _pick_default_metric:
    - L310 for preferred in ('rows', 'rows_per_parent', 'parent'):
      - L311 if any((metric['name'] == preferred for metric in metrics)):
        - L312 return preferred
    - L313 return metrics[0]['name'] if metrics else 'rows'
  - L315 return {'metrics': metrics, 'dimensions': dimensions, 'defaults': {'dimension': _pick_default_dimension(), 'metric': _pick_default_metric()}}
- L325 def bin_numeric_metricmetrics: Sequence[Mapping[str, Any]], metric_name: str, *, bucket_count: int=10, bucket_edges: Sequence[float] | None=None, value_range: tuple[float, float] | None=None:
  - L333 docstring: "\n    Bucket a numeric metric into ranges and compute aggregates per bucket.\n\n..."
  - L351 if bucket_edges is not None:
    - L352 assign edges = sorted({float(v) for v in bucket_edges if v is not None})
    - L353 if len(edges) < 2:
      - L354 return []
    - L356 else:
      - L356 assign values = [_coerce_number(entry.get(metric_name)) for entry in metrics]
      - L357 if not values:
        - L358 return []
      - L359 assign min_value = float(values[0])
      - L360 assign max_value = float(values[0])
      - L361 for val in values[1:]:
        - L362 assign min_value = min(min_value, val)
        - L363 assign max_value = max(max_value, val)
      - L364 if value_range is not None:
        - L365 assign min_value = float(value_range[0])
        - L366 assign max_value = float(value_range[1])
      - L367 if max_value < min_value:
        - L368 assign (min_value, max_value) = (max_value, min_value)
      - L369 if bucket_count < 1:
        - L370 assign bucket_count = 1
      - L371 if max_value == min_value:
        - L372 assign edges = [min_value, max_value]
        - L374 else:
          - L374 assign step = (max_value - min_value) / bucket_count
          - L375 assign edges = [min_value + step * i for i in range(bucket_count)]
          - L376 expr edges.append(max_value)
  - L378 annotated assign buckets: list[dict[str, Any]] = []
  - L379 for idx in range(len(edges) - 1):
    - L380 assign start = edges[idx]
    - L381 assign end = edges[idx + 1]
    - L382 expr buckets.append({'bucket_index': idx, 'start': start, 'end': end, 'count': 0, 'sum': 0.0, 'min': float('inf'), 'max': float('-inf'), 'batch_ids': []})
  - L395 for entry in metrics:
    - L396 assign value = _coerce_number(entry.get(metric_name))
    - L397 assign batch_id = entry.get('batch_id') or entry.get('id')
    - L398 assign target_idx = None
    - L399 for idx in range(len(edges) - 1):
      - L400 assign start = edges[idx]
      - L401 assign end = edges[idx + 1]
      - L402 assign is_last = idx == len(edges) - 2
      - L403 if value >= start and value < end or (is_last and value == end):
        - L404 assign target_idx = idx
        - L405 break
    - L406 if target_idx is None:
      - L407 continue
    - L408 assign bucket = buckets[target_idx]
    - L409 aug assign bucket['count'] Add 1
    - L410 aug assign bucket['sum'] Add value
    - L411 assign bucket['min'] = min(bucket['min'], value)
    - L412 assign bucket['max'] = max(bucket['max'], value)
    - L413 if batch_id is not None:
      - L414 assign batch_id_text = str(batch_id)
      - L415 if batch_id_text not in bucket['batch_ids']:
        - L416 expr bucket['batch_ids'].append(batch_id_text)
  - L418 for bucket in buckets:
    - L419 if bucket['min'] == float('inf'):
      - L420 assign bucket['min'] = 0.0
    - L421 if bucket['max'] == float('-inf'):
      - L422 assign bucket['max'] = 0.0
  - L423 return buckets
- L426 def group_metrics_by_fieldmetrics: Sequence[Mapping[str, Any]], dimension_field: str, *, metric_field: str='rows', aggregation: str='sum':
  - L433 docstring: "\n    Group metrics by a categorical field and aggregate a numeric metric.\n\n  ..."
  - L438 annotated assign groups: dict[str, dict[str, Any]] = {}
  - L439 assign agg = aggregation.lower().strip()
  - L440 for entry in metrics:
    - L441 assign key_raw = entry.get(dimension_field)
    - L442 assign key = str(key_raw) if key_raw is not None else ''
    - L443 assign metric_value = _coerce_number(entry.get(metric_field))
    - L444 assign bucket = groups.setdefault(key, {'key': key, 'label': key or '(empty)', 'sum': 0.0, 'count': 0, 'min': float('inf'), 'max': float('-inf'), 'batch_ids': []})
    - L456 aug assign bucket['sum'] Add metric_value
    - L457 aug assign bucket['count'] Add 1
    - L458 assign bucket['min'] = min(bucket['min'], metric_value)
    - L459 assign bucket['max'] = max(bucket['max'], metric_value)
    - L460 assign batch_id = entry.get('batch_id') or entry.get('id')
    - L461 if batch_id is not None:
      - L462 assign batch_id_text = str(batch_id)
      - L463 if batch_id_text not in bucket['batch_ids']:
        - L464 expr bucket['batch_ids'].append(batch_id_text)
  - L466 annotated assign results: list[dict[str, Any]] = []
  - L467 for bucket in groups.values():
    - L468 annotated assign value: float
    - L469 if agg == 'avg':
      - L470 assign value = bucket['sum'] / bucket['count'] if bucket['count'] else 0.0
      - L471 else:
        - L471 if agg == 'min':
          - L472 assign value = 0.0 if bucket['min'] == float('inf') else bucket['min']
          - L473 else:
            - L473 if agg == 'max':
              - L474 assign value = 0.0 if bucket['max'] == float('-inf') else bucket['max']
              - L475 else:
                - L475 if agg == 'count':
                  - L476 assign value = float(bucket['count'])
                  - L478 else:
                    - L478 assign value = bucket['sum']
    - L479 expr results.append({**bucket, 'value': value})
  - L486 return sorted(results, key=lambda item: item['label'])
- L489 def build_resample_supportfield_catalog: Iterable[Mapping[str, Any]], batch_metrics: Sequence[Mapping[str, Any]], *, schema: Mapping[str, Any] | None=None, default_metric: str | None=None, bucket_count: int=10:
  - L497 docstring: "\n    Pre-compute numeric bins and categorical groups for the discovery payload...."
  - L506 assign schema_data = schema if isinstance(schema, Mapping) else build_discovery_schema(field_catalog)
  - L507 annotated assign numeric_bins: dict[str, list[dict[str, Any]]] = {}
  - L508 annotated assign category_groups: dict[str, list[dict[str, Any]]] = {}
  - L509 assign defaults = schema_data.get('defaults') or {}
  - L510 assign metric_default = default_metric or defaults.get('metric') or 'rows'
  - L512 for metric in schema_data.get('metrics', []):
    - L513 if not isinstance(metric, Mapping):
      - L514 continue
    - L515 assign name = str(metric.get('name') or '').strip()
    - L516 if not name:
      - L517 continue
    - L518 if not metric.get('bucketable'):
      - L519 continue
    - L520 if _normalize_type(metric.get('type')) != 'number':
      - L521 continue
    - L522 assign numeric_bins[name] = bin_numeric_metric(batch_metrics, name, bucket_count=bucket_count)
  - L524 for dim in schema_data.get('dimensions', []):
    - L525 if not isinstance(dim, Mapping):
      - L526 continue
    - L527 assign name = str(dim.get('name') or '').strip()
    - L528 if not name:
      - L529 continue
    - L530 assign dim_kind = str(dim.get('kind') or dim.get('type') or '').lower()
    - L531 if dim_kind in {'categorical', 'string'}:
      - L532 assign category_groups[name] = group_metrics_by_field(batch_metrics, name, metric_field=metric_default, aggregation='sum')
      - L538 else:
        - L538 if dim_kind in {'numeric', 'number'} and dim.get('bucketable') and (name not in numeric_bins):
          - L539 assign numeric_bins[name] = bin_numeric_metric(batch_metrics, name, bucket_count=bucket_count)
  - L541 return {'numeric_bins': numeric_bins, 'category_groups': category_groups}

## backend\app\services\reports\docx_export.py
- L1 from __future__ import annotations
- L3 import contextlib
- L4 import logging
- L5 import re
- L6 from io import BytesIO
- L7 from pathlib import Path
- L8 from typing import Iterable, Optional
- L10 try:
  - L11 from html2docx import html2docx
  - L12 except ImportError:
    - L13 assign html2docx = None
- L15 try:
  - L16 from docx import Document
  - L17 from docx.enum.section import WD_ORIENT
  - L18 from docx.enum.text import WD_ALIGN_PARAGRAPH
  - L19 from docx.shared import Mm, Pt
  - L20 except ImportError:
    - L21 assign Document = None
    - L22 assign WD_ORIENT = None
    - L23 assign WD_ALIGN_PARAGRAPH = None
    - L24 assign Mm = None
    - L25 assign Pt = None
- L27 try:
  - L28 from pdf2docx import Converter
  - L29 except ImportError:
    - L30 assign Converter = None
- L32 from lxml import etree
- L33 from lxml import html as lxml_html
- L35 from .html_table_parser import extract_tables
- L37 assign logger = logging.getLogger('neura.reports.docx')
- L39 assign _BODY_TAG_RE = re.compile('(?is)<body\\b(?P<attrs>[^>]*)>', re.MULTILINE)
- L40 assign _STYLE_ATTR_RE = re.compile('(?is)(style\\s*=\\s*)(["\\\'])(?P<value>.*?)\\2')
- L41 assign _STYLE_BLOCK_RE = re.compile('(?is)<style\\b[^>]*>.*?</style>')
- L42 assign _SCRIPT_BLOCK_RE = re.compile('(?is)<script\\b[^>]*>.*?</script>')
- L43 assign _TITLE_RE = re.compile('(?is)<title\\b[^>]*>(?P<value>.*?)</title>')
- L44 assign _TAG_RE = re.compile('(?is)<[^>]+>')
- L45 assign _WHITESPACE_RE = re.compile('\\s+')
- L47 assign _A4_LANDSCAPE_WIDTH_MM = 297
- L48 assign _A4_LANDSCAPE_HEIGHT_MM = 210
- L49 assign _MAX_FALLBACK_ROWS = 500
- L50 assign _MAX_FALLBACK_TABLES = 8
- L53 def _inject_body_stylehtml_text: str, style_rule: str:
  - L54 docstring: "Attach/extend a style attribute on the <body> tag without disturbing the markup."
  - L55 assign match = _BODY_TAG_RE.search(html_text)
  - L56 if not match:
    - L57 return f'<body style="{style_rule}">{html_text}</body>'
  - L59 assign attrs = match.group('attrs') or ''
  - L60 assign new_attrs = attrs
  - L62 def _style_replstyle_match: re.Match[str]:
    - L63 assign existing = (style_match.group('value') or '').strip().rstrip(';')
    - L64 assign merged = '; '.join(filter(None, [existing, style_rule]))
    - L65 return f'{style_match.group(1)}{style_match.group(2)}{merged}{style_match.group(2)}'
  - L67 if _STYLE_ATTR_RE.search(attrs):
    - L68 assign new_attrs = _STYLE_ATTR_RE.sub(_style_repl, attrs, count=1)
    - L70 else:
      - L70 assign spacer = '' if attrs.endswith(' ') or not attrs else ' '
      - L71 assign new_attrs = f'{attrs}{spacer}style="{style_rule}"'
  - L73 assign new_tag = f'<body{new_attrs}>'
  - L74 return f'{html_text[:match.start()]}{new_tag}{html_text[match.end():]}'
- L77 def _apply_body_font_scalehtml_text: str, scale: float | None:
  - L78 if not scale or scale <= 0:
    - L79 return html_text
  - L81 assign clamped = max(0.5, min(scale, 1.0))
  - L82 assign percent = round(clamped * 100, 1)
  - L83 assign style_rule = f'font-size: {percent}%; line-height: 1.15;'
  - L84 return _inject_body_style(html_text, style_rule)
- L87 def _append_inline_stylenode, style: str:
  - L88 if node is None or not style:
    - L89 return None
  - L90 assign existing = (node.get('style') or '').strip()
  - L91 if existing and (not existing.endswith(';')):
    - L92 assign existing = f'{existing};'
  - L93 assign parts = [part.strip() for part in (existing.rstrip(';'), style.strip()) if part and part.strip()]
  - L94 expr node.set('style', '; '.join(parts))
- L97 def _inline_report_styleshtml_text: str:
  - L98 if not html_text:
    - L99 return html_text
  - L100 try:
    - L101 assign document = lxml_html.fromstring(html_text)
    - L102 except Exception:
      - L103 return html_text
  - L105 def _set_stylexpath: str, style: str:
    - L106 if not style:
      - L107 return None
    - L108 for node in document.xpath(xpath):
      - L109 expr _append_inline_style(node, style)
  - L111 expr _set_style('//body', "font-family: 'Times New Roman', serif; font-size: 12px; color: #000; line-height: 1.2; margin: 0;")
  - L114 expr _set_style("//div[@id='report-header']", 'margin-top: 0; page-break-inside: avoid;')
  - L115 expr _set_style("//div[@id='report-header']//table", 'width: 100%; border: 1px solid #000; border-collapse: collapse; table-layout: fixed;')
  - L119 expr _set_style("//div[@id='report-header']//td", 'vertical-align: top; padding: 1.6mm 2.4mm; border: 1px solid #000;')
  - L120 expr _set_style("//div[contains(concat(' ', normalize-space(@class), ' '), ' title-wrap ')]", 'margin: 3mm 0 2.5mm 0; text-align: center; font-weight: bold; font-size: 18px; border-top: 1px solid #000; border-bottom: 1px solid #000; padding: 2mm 0;')
  - L124 expr _set_style("//table[@id='data-table']", 'width: 100%; border-collapse: collapse; table-layout: fixed;')
  - L125 expr _set_style("//table[@id='data-table']//th | //table[@id='data-table']//td", 'border: 1px solid #000; padding: 1mm 2.2mm;')
  - L128 expr _set_style("//table[@id='data-table']//thead//th", 'text-align: center; font-weight: bold; white-space: nowrap; padding: 1.6mm 2.4mm;')
  - L132 expr _set_style("//table[@id='data-table']//td[contains(concat(' ', normalize-space(@class), ' '), ' num ')]", 'text-align: right;')
  - L136 expr _set_style("//tfoot[@id='report-totals']//td", 'font-weight: bold; border-top: 1.2px solid #000;')
  - L137 expr _set_style("//tfoot[@id='report-totals']//td[contains(concat(' ', normalize-space(@class), ' '), ' label ')]", 'text-align: left;')
  - L141 expr _set_style("//footer[@id='report-footer']", 'font-size: 11px; color: #000; display: flex; justify-content: space-between; align-items: center;')
  - L145 expr _set_style("//footer[@id='report-footer']//div[contains(concat(' ', normalize-space(@class), ' '), ' page ')]", 'text-align: center;')
  - L150 return etree.tostring(document, encoding='unicode', method='html')
- L153 def _extract_report_titlehtml_text: str:
  - L154 if not html_text:
    - L155 return ''
  - L156 try:
    - L157 assign document = lxml_html.fromstring(html_text)
    - L158 except Exception:
      - L159 return _extract_html_title(html_text)
  - L160 assign title_nodes = document.xpath("//*[contains(concat(' ', normalize-space(@class), ' '), ' title-wrap ')]")
  - L161 if title_nodes:
    - L162 return _normalize_whitespace(title_nodes[0].text_content())
  - L163 return _extract_html_title(html_text)
- L166 def _extract_footer_brandhtml_text: str:
  - L167 if not html_text:
    - L168 return ''
  - L169 try:
    - L170 assign document = lxml_html.fromstring(html_text)
    - L171 except Exception:
      - L172 return ''
  - L173 assign brand_nodes = document.xpath("//footer[@id='report-footer']//div[contains(concat(' ', normalize-space(@class), ' '), ' brand ')]")
  - L176 if brand_nodes:
    - L177 return _normalize_whitespace(brand_nodes[0].text_content())
  - L178 return ''
- L181 def _configure_document_layoutdocument, *, body_font_scale: float | None=None:
  - L182 if document is None:
    - L183 return None
  - L184 if Mm is not None:
    - L185 try:
      - L186 assign section = document.sections[0]
      - L187 except Exception:
        - L188 assign section = None
    - L189 if section is not None:
      - L190 assign section.left_margin = Mm(16)
      - L191 assign section.right_margin = Mm(16)
      - L192 assign section.top_margin = Mm(14)
      - L193 assign section.bottom_margin = Mm(14)
- L196 def _infer_numeric_columnsheader_row: list[str]:
  - L197 annotated assign numeric_columns: set[int] = set()
  - L198 assign tokens = ('wt', 'weight', 'error', '%', 'kg', 'total', 'qty')
  - L199 for (idx, cell) in enumerate(header_row or []):
    - L200 assign text = (cell or '').lower()
    - L201 if idx == 0:
      - L202 continue
    - L203 if any((token in text for token in tokens)):
      - L204 expr numeric_columns.add(idx)
  - L205 return numeric_columns
- L208 def _column_widthsmax_columns: int, *, ratios: Optional[Iterable[float]], document:
  - L209 if max_columns <= 0 or document is None or Mm is None:
    - L210 return None
  - L211 try:
    - L212 assign section = document.sections[0]
    - L213 assign available = section.page_width - section.left_margin - section.right_margin
    - L214 except Exception:
      - L215 return None
  - L216 if available <= 0:
    - L217 return None
  - L218 assign ratio_list = list(ratios or [])
  - L219 if ratio_list and len(ratio_list) < max_columns:
    - L220 assign last = ratio_list[-1]
    - L221 expr ratio_list.extend([last] * (max_columns - len(ratio_list)))
  - L222 if not ratio_list:
    - L223 assign ratio_list = [1.0] * max_columns
  - L224 assign total = sum(ratio_list) or 1.0
  - L225 return [available * (value / total) for value in ratio_list[:max_columns]]
- L228 def _write_docx_tabledocument, rows: list[list[str]], *, header_rows: int=0, column_widths: Optional[Iterable[float]]=None, numeric_columns: Optional[set[int]]=None:
  - L236 if not rows:
    - L237 return None
  - L238 assign max_cols = max((len(r) for r in rows))
  - L239 assign table = document.add_table(rows=len(rows), cols=max_cols)
  - L240 try:
    - L241 assign table.style = 'Table Grid'
    - L242 except Exception:
      - L243 pass
  - L244 assign table.autofit = False
  - L245 assign widths = _column_widths(max_cols, ratios=column_widths, document=document)
  - L247 for (r_idx, row) in enumerate(rows):
    - L248 for c_idx in range(max_cols):
      - L249 assign value = row[c_idx] if c_idx < len(row) else ''
      - L250 assign cell = table.rows[r_idx].cells[c_idx]
      - L251 assign cell.text = ''
      - L252 assign paragraph = cell.paragraphs[0]
      - L253 assign paragraph.text = _normalize_whitespace(value)
      - L254 if WD_ALIGN_PARAGRAPH is not None:
        - L255 if numeric_columns and c_idx in (numeric_columns or set()) and (r_idx >= header_rows):
          - L256 assign paragraph.alignment = WD_ALIGN_PARAGRAPH.RIGHT
          - L257 else:
            - L257 if r_idx < header_rows:
              - L258 assign paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
      - L259 for run in paragraph.runs:
        - L260 assign run.font.bold = r_idx < header_rows
  - L262 if widths:
    - L263 for (c_idx, width) in enumerate(widths):
      - L264 for cell in table.columns[c_idx].cells:
        - L265 assign cell.width = width
- L268 def _strip_style_blockshtml_text: str:
  - L269 assign cleaned = _STYLE_BLOCK_RE.sub('', html_text or '')
  - L270 assign cleaned = _SCRIPT_BLOCK_RE.sub('', cleaned)
  - L271 return cleaned
- L274 def _normalize_whitespacetext: str:
  - L275 return _WHITESPACE_RE.sub(' ', text or '').strip()
- L278 def _strip_html_to_texthtml_text: str:
  - L279 return _normalize_whitespace(_TAG_RE.sub(' ', html_text or ''))
- L282 def _extract_html_titlehtml_text: str:
  - L283 assign match = _TITLE_RE.search(html_text or '')
  - L284 if not match:
    - L285 return ''
  - L286 return _strip_html_to_text(match.group('value'))
- L289 def _extract_section_nodeshtml_text: str:
  - L290 try:
    - L291 assign document = lxml_html.fromstring(html_text or '')
    - L292 except Exception:
      - L293 return []
  - L294 assign sections = document.xpath("//div[contains(concat(' ', normalize-space(@class), ' '), ' nr-key-section ')]")
  - L295 if sections:
    - L296 return sections
  - L297 assign body = document.xpath('//body')
  - L298 return body or [document]
- L301 def _fallback_docx_from_tableshtml_text: str, output_path: Path, *, body_font_scale: float | None:
  - L302 if Document is None:
    - L303 return None
  - L305 assign sections = _extract_section_nodes(html_text)
  - L306 if not sections:
    - L307 try:
      - L308 assign sections = [lxml_html.fromstring(html_text or '<div></div>')]
      - L309 except Exception:
        - L310 assign sections = []
  - L311 if not sections:
    - L312 return None
  - L314 try:
    - L315 assign document = Document()
    - L316 except Exception as exc:
      - L317 expr logger.warning('docx_fallback_init_failed', extra={'event': 'docx_fallback_init_failed', 'error': str(exc)})
      - L324 return None
  - L326 expr _configure_document_layout(document, body_font_scale=body_font_scale)
  - L328 for (index, section_node) in enumerate(sections):
    - L329 if index > 0:
      - L330 expr document.add_page_break()
    - L332 assign section_html = etree.tostring(section_node, encoding='unicode', method='html')
    - L333 assign title_text = _extract_report_title(section_html) or _extract_report_title(html_text)
    - L334 if title_text:
      - L335 assign paragraph = document.add_paragraph(title_text)
      - L336 if WD_ALIGN_PARAGRAPH is not None:
        - L337 assign paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
      - L338 for run in paragraph.runs:
        - L339 assign run.font.bold = True
    - L341 assign tables = extract_tables(section_html, max_tables=_MAX_FALLBACK_TABLES)
    - L342 if not tables:
      - L343 assign plain_text = _strip_html_to_text(section_html)
      - L344 expr document.add_paragraph(plain_text or 'Report data unavailable.')
      - L345 continue
    - L347 assign header_rows = tables[0] if len(tables) >= 1 else []
    - L348 assign data_rows = tables[1] if len(tables) >= 2 else []
    - L349 assign extra_tables = tables[2:] if len(tables) > 2 else []
    - L351 if header_rows:
      - L352 assign capped_header = header_rows[:_MAX_FALLBACK_ROWS]
      - L353 expr _write_docx_table(document, capped_header, header_rows=0, column_widths=[2.5, 1.5])
      - L359 expr document.add_paragraph('')
    - L361 if data_rows:
      - L362 assign capped_rows = data_rows[:_MAX_FALLBACK_ROWS]
      - L363 assign numeric_columns = _infer_numeric_columns(capped_rows[0] if capped_rows else [])
      - L364 expr _write_docx_table(document, capped_rows, header_rows=1, numeric_columns=numeric_columns)
      - L370 expr document.add_paragraph('')
    - L372 for rows in extra_tables:
      - L373 if not rows:
        - L374 continue
      - L375 assign capped_rows = rows[:_MAX_FALLBACK_ROWS]
      - L376 expr _write_docx_table(document, capped_rows, header_rows=1)
      - L377 expr document.add_paragraph('')
    - L379 assign brand_text = _extract_footer_brand(section_html) or _extract_footer_brand(html_text)
    - L380 if brand_text:
      - L381 assign footer_paragraph = document.add_paragraph(brand_text)
      - L382 if WD_ALIGN_PARAGRAPH is not None:
        - L383 assign footer_paragraph.alignment = WD_ALIGN_PARAGRAPH.RIGHT
  - L385 try:
    - L386 expr document.save(output_path)
    - L387 except Exception as exc:
      - L388 expr logger.warning('docx_fallback_save_failed', extra={'event': 'docx_fallback_save_failed', 'docx_path': str(output_path), 'error': str(exc)})
      - L396 return None
  - L397 return output_path
- L400 def _clamp_body_scalescale: float | None:
  - L401 if not scale or not isinstance(scale, (int, float)):
    - L402 return 1.0
  - L403 return max(0.3, min(float(scale), 1.0))
- L406 def _enforce_landscape_layoutdocx_path: Path, *, margin_mm: float=10.0:
  - L407 if Document is None or WD_ORIENT is None or Mm is None:
    - L408 expr logger.debug('docx_landscape_skipped', extra={'event': 'docx_landscape_skipped', 'reason': 'python-docx unavailable', 'docx_path': str(docx_path)})
    - L416 return None
  - L418 try:
    - L419 assign document = Document(docx_path)
    - L420 except Exception as exc:
      - L421 expr logger.warning('docx_landscape_open_failed', extra={'event': 'docx_landscape_open_failed', 'docx_path': str(docx_path), 'error': str(exc)})
      - L429 return None
  - L431 assign width = Mm(_A4_LANDSCAPE_WIDTH_MM)
  - L432 assign height = Mm(_A4_LANDSCAPE_HEIGHT_MM)
  - L433 assign margin = Mm(margin_mm)
  - L435 for section in document.sections:
    - L436 assign section.orientation = WD_ORIENT.LANDSCAPE
    - L437 assign section.page_width = width
    - L438 assign section.page_height = height
    - L439 assign section.left_margin = margin
    - L440 assign section.right_margin = margin
    - L441 assign section.top_margin = margin
    - L442 assign section.bottom_margin = margin
  - L444 try:
    - L445 expr document.save(docx_path)
    - L446 except Exception as exc:
      - L447 expr logger.warning('docx_landscape_save_failed', extra={'event': 'docx_landscape_save_failed', 'docx_path': str(docx_path), 'error': str(exc)})
- L457 def html_file_to_docxhtml_path: Path, output_path: Path, *, landscape: bool=False, body_font_scale: float | None=None:
  - L464 docstring: "\n    Convert an HTML file into a DOCX document using html2docx.\n\n    Paramete..."
  - L480 assign html_text = html_path.read_text(encoding='utf-8', errors='ignore')
  - L481 assign output_path = Path(output_path)
  - L482 expr output_path.parent.mkdir(parents=True, exist_ok=True)
  - L484 assign html_with_inline = _inline_report_styles(html_text)
  - L485 assign html_for_docx = _apply_body_font_scale(_strip_style_blocks(html_with_inline), body_font_scale)
  - L487 if html2docx is not None:
    - L488 try:
      - L489 assign title_text = _extract_html_title(html_for_docx) or 'Report'
      - L490 annotated assign buffer: BytesIO = html2docx(html_for_docx, title_text)
      - L491 except Exception as exc:
        - L492 expr logger.exception('docx_export_html2docx_failed', extra={'event': 'docx_export_html2docx_failed', 'html_path': str(html_path), 'docx_path': str(output_path), 'error': str(exc)})
      - L502 else:
        - L502 with output_path.open('wb') as handle:
          - L503 expr handle.write(buffer.getvalue())
        - L504 if landscape:
          - L505 expr _enforce_landscape_layout(output_path)
        - L506 expr logger.info('docx_export_success', extra={'event': 'docx_export_success', 'html_path': str(html_path), 'docx_path': str(output_path), 'landscape': landscape, 'font_scale': body_font_scale, 'strategy': 'html2docx'})
        - L517 return output_path
  - L519 assign structured = _fallback_docx_from_tables(html_text, output_path, body_font_scale=body_font_scale)
  - L520 if structured:
    - L521 if landscape:
      - L522 expr _enforce_landscape_layout(output_path)
    - L523 expr logger.info('docx_export_success', extra={'event': 'docx_export_success', 'html_path': str(html_path), 'docx_path': str(output_path), 'landscape': landscape, 'font_scale': body_font_scale, 'strategy': 'structured'})
    - L534 return output_path
  - L536 expr logger.warning('docx_export_unavailable', extra={'event': 'docx_export_unavailable', 'reason': 'python-docx unavailable' if Document is None else 'html2docx not installed', 'html_path': str(html_path)})
  - L544 return None
- L547 def pdf_file_to_docxpdf_path: Path, output_path: Path, *, start_page: int=0, end_page: int | None=None:
  - L554 docstring: "\n    Convert an already-rendered PDF into DOCX using pdf2docx for near-carbon-c..."
  - L558 if Converter is None:
    - L559 expr logger.debug('docx_pdf_convert_skipped', extra={'event': 'docx_pdf_convert_skipped', 'reason': 'pdf2docx unavailable', 'pdf_path': str(pdf_path)})
    - L567 return None
  - L569 assign pdf_path = Path(pdf_path)
  - L570 if not pdf_path.exists():
    - L571 expr logger.warning('docx_pdf_convert_missing_pdf', extra={'event': 'docx_pdf_convert_missing_pdf', 'pdf_path': str(pdf_path)})
    - L578 return None
  - L580 assign output_path = Path(output_path)
  - L581 expr output_path.parent.mkdir(parents=True, exist_ok=True)
  - L583 try:
    - L584 assign converter = Converter(str(pdf_path))
    - L585 except Exception as exc:
      - L586 expr logger.warning('docx_pdf_convert_open_failed', extra={'event': 'docx_pdf_convert_open_failed', 'pdf_path': str(pdf_path), 'error': str(exc)})
      - L594 return None
  - L596 try:
    - L597 expr converter.convert(str(output_path), start=start_page, end=end_page)
    - L598 except Exception as exc:
      - L599 expr logger.warning('docx_pdf_convert_failed', extra={'event': 'docx_pdf_convert_failed', 'pdf_path': str(pdf_path), 'docx_path': str(output_path), 'error': str(exc)})
      - L608 return None
    - L610 finally:
      - L610 with contextlib.suppress(Exception):
        - L611 expr converter.close()
  - L613 expr logger.info('docx_pdf_convert_success', extra={'event': 'docx_pdf_convert_success', 'pdf_path': str(pdf_path), 'docx_path': str(output_path), 'start_page': start_page, 'end_page': end_page})
  - L623 return output_path

## backend\app\services\reports\html_table_parser.py
- L2 from __future__ import annotations
- L4 from html.parser import HTMLParser
- L7 class _SimpleTableParser(HTMLParser):
  - L8 def __init__self:
    - L9 expr super().__init__()
    - L10 annotated assign self.tables: list[list[list[str]]] = []
    - L11 assign self._table_depth = 0
    - L12 assign self._collecting = False
    - L13 annotated assign self._current_table: list[list[str]] | None = None
    - L14 annotated assign self._current_row: list[str] | None = None
    - L15 annotated assign self._current_cell: list[str] | None = None
  - L17 def handle_starttagself, tag: str, attrs:
    - L18 assign tag = tag.lower()
    - L19 if tag == 'table':
      - L20 aug assign self._table_depth Add 1
      - L21 if self._table_depth == 1:
        - L22 assign self._collecting = True
        - L23 assign self._current_table = []
      - L24 else:
        - L24 if self._collecting and tag == 'tr':
          - L25 assign self._current_row = []
          - L26 else:
            - L26 if self._collecting and tag in ('td', 'th'):
              - L27 assign self._current_cell = []
  - L29 def handle_endtagself, tag: str:
    - L30 assign tag = tag.lower()
    - L31 if tag in ('td', 'th') and self._collecting and (self._current_row is not None):
      - L32 assign text = ''.join(self._current_cell or []).strip()
      - L33 expr self._current_row.append(text)
      - L34 assign self._current_cell = None
      - L35 else:
        - L35 if tag == 'tr' and self._collecting:
          - L36 if self._current_row is not None:
            - L37 if any((cell.strip() for cell in self._current_row)):
              - L38 expr self._current_table.append(self._current_row[:])
            - L39 assign self._current_row = None
          - L40 else:
            - L40 if tag == 'table':
              - L41 if self._table_depth == 1 and self._collecting and (self._current_table is not None):
                - L42 expr self.tables.append(self._current_table[:])
                - L43 assign self._collecting = False
                - L44 assign self._current_table = None
              - L45 assign self._table_depth = max(0, self._table_depth - 1)
  - L47 def handle_dataself, data: str:
    - L48 if self._collecting and self._current_cell is not None:
      - L49 expr self._current_cell.append(data)
  - L51 def first_tableself:
    - L52 return self.tables[0] if self.tables else []
- L55 def _table_scoretable: list[list[str]]:
  - L56 if not table:
    - L57 return 0
  - L58 assign row_count = len(table)
  - L59 assign max_cols = max((len(row) for row in table), default=0)
  - L60 assign multi_col_rows = sum((1 for row in table if sum((1 for cell in row if cell)) >= 2))
  - L61 return (multi_col_rows or row_count) * max(1, max_cols)
- L64 def extract_first_tablehtml_text: str:
  - L65 assign tables = extract_tables(html_text, max_tables=None)
  - L66 if not tables:
    - L67 return []
  - L68 assign best_table = tables[0]
  - L69 assign best_score = _table_score(best_table)
  - L70 for table in tables[1:]:
    - L71 assign score = _table_score(table)
    - L72 if score > best_score:
      - L73 assign best_table = table
      - L74 assign best_score = score
  - L75 return best_table
- L78 def extract_tableshtml_text: str, *, max_tables: int | None=None:
  - L79 assign parser = _SimpleTableParser()
  - L80 expr parser.feed(html_text or '')
  - L81 annotated assign normalized_tables: list[list[list[str]]] = []
  - L82 for table in parser.tables:
    - L83 if max_tables is not None and len(normalized_tables) >= max_tables:
      - L84 break
    - L85 annotated assign normalized: list[list[str]] = []
    - L86 for row in table:
      - L87 assign cleaned = [(cell or '').strip() for cell in row]
      - L88 if any((cell for cell in cleaned)):
        - L89 expr normalized.append(cleaned)
    - L90 if normalized:
      - L91 expr normalized_tables.append(normalized)
  - L92 return normalized_tables
- L95 assign __all__ = ['extract_first_table', 'extract_tables']

## backend\app\services\reports\ReportGenerate.py
- L2 import asyncio
- L3 import contextlib
- L4 import json
- L5 import re
- L6 from ..dataframes import DuckDBDataFrameQuery, SQLiteDataFrameLoader, sqlite_shim as sqlite3
- L7 from collections import defaultdict
- L8 from datetime import datetime
- L9 from decimal import Decimal, InvalidOperation
- L10 from itertools import product
- L11 from pathlib import Path
- L12 from typing import Any, Iterable, Mapping, Sequence
- L14 try:
  - L15 from PIL import Image
  - L16 except ImportError:
    - L17 assign Image = None
- L19 try:
  - L20 import numpy as np
  - L21 except ImportError:
    - L22 assign np = None
- L24 try:
  - L25 import cv2
  - L26 except ImportError:
    - L27 assign cv2 = None
- L29 try:
  - L30 import fitz
  - L31 except ImportError:
    - L32 assign fitz = None
- L34 try:
  - L35 from skimage.metrics import structural_similarity as ssim
  - L36 except ImportError:
    - L37 assign ssim = None
- L39 try:
  - L40 from playwright.async_api import async_playwright
  - L41 except ImportError:
    - L42 assign async_playwright = None
- L44 from .contract_adapter import ContractAdapter, format_decimal_str
- L45 from .date_utils import get_col_type, mk_between_pred_for_date
- L46 from .discovery import discover_batches_and_counts
- L47 from .common_helpers import _format_for_token, _has_time_component, _parse_date_like, _raise_no_block, _segment_has_any_token, _select_prototype_block, _strip_found_block, _token_regex, _find_or_infer_batch_block, _find_rowish_block, _extract_page_metrics, sub_token, blank_known_tokens, html_without_batch_blocks
- L64 assign _DATE_PARAM_START_ALIASES = {'start_ts_utc', 'start_ts', 'start_timestamp', 'start_datetime', 'start_date', 'start_dt', 'start_iso', 'start_date_utc', 'from_ts_utc', 'from_ts', 'from_timestamp', 'from_datetime', 'from_date', 'from_dt', 'from_iso', 'from_date_utc', 'range_start', 'period_start'}
- L85 assign _DATE_PARAM_END_ALIASES = {'end_ts_utc', 'end_ts', 'end_timestamp', 'end_datetime', 'end_date', 'end_dt', 'end_iso', 'end_date_utc', 'to_ts_utc', 'to_ts', 'to_timestamp', 'to_datetime', 'to_date', 'to_dt', 'to_iso', 'to_date_utc', 'range_end', 'period_end'}
- L110 def fill_and_printOBJ: dict, TEMPLATE_PATH: Path, DB_PATH: Path, OUT_HTML: Path, OUT_PDF: Path, START_DATE: str, END_DATE: str, batch_ids: list[str] | None=None, KEY_VALUES: dict | None=None, GENERATOR_BUNDLE: dict | None=None, __force_single: bool=False:
  - L123 docstring: "\n    DB-driven renderer:\n      - Assumes TEMPLATE_PATH is already the *final s..."
  - L134 for name in ('OBJ', 'TEMPLATE_PATH', 'DB_PATH', 'START_DATE', 'END_DATE'):
    - L135 if locals().get(name) is None:
      - L136 raise NameError(f'Missing required variable: `{name}`')
  - L139 assign OUT_DIR = OUT_HTML.parent
  - L140 expr OUT_DIR.mkdir(parents=True, exist_ok=True)
  - L142 assign log_file_path = Path(__file__).with_name('fill_and_print.log')
  - L144 def _log_debug*parts: object:
    - L145 assign message = ' '.join((str(part) for part in parts))
    - L146 expr print(message)
    - L147 try:
      - L148 with log_file_path.open('a', encoding='utf-8') as fh:
        - L149 expr fh.write(f'[{datetime.now().isoformat()}] {message}\n')
      - L150 except Exception:
        - L151 pass
  - L153 expr _log_debug('=== fill_and_print call ===', 'force_single' if __force_single else 'fanout_root', 'KEY_VALUES raw=', KEY_VALUES or {})
  - L161 assign html = TEMPLATE_PATH.read_text(encoding='utf-8')
  - L163 assign dataframe_loader = SQLiteDataFrameLoader(DB_PATH)
  - L165 assign TOKEN_RE = re.compile('\\{\\{?\\s*([A-Za-z0-9_\\-\\.]+)\\s*\\}\\}?')
  - L166 assign TEMPLATE_TOKENS = {m.group(1) for m in TOKEN_RE.finditer(html)}
  - L169 assign OBJ = OBJ or {}
  - L170 assign contract_adapter = ContractAdapter(OBJ)
  - L171 assign param_token_set = {token for token in contract_adapter.param_tokens or [] if token}
  - L173 assign PLACEHOLDER_TO_COL = contract_adapter.mapping
  - L175 assign join_raw = OBJ.get('join', {}) or {}
  - L176 assign JOIN = {'parent_table': contract_adapter.parent_table or join_raw.get('parent_table', ''), 'child_table': contract_adapter.child_table or join_raw.get('child_table', ''), 'parent_key': contract_adapter.parent_key or join_raw.get('parent_key', ''), 'child_key': contract_adapter.child_key or join_raw.get('child_key', '')}
  - L183 assign DATE_COLUMNS = contract_adapter.date_columns or (OBJ.get('date_columns', {}) or {})
  - L185 assign HEADER_TOKENS = contract_adapter.scalar_tokens or OBJ.get('header_tokens', [])
  - L186 assign ROW_TOKENS = contract_adapter.row_tokens or OBJ.get('row_tokens', [])
  - L187 assign TOTALS = contract_adapter.totals_mapping or OBJ.get('totals', {})
  - L188 assign ROW_ORDER = contract_adapter.row_order or OBJ.get('row_order', ['ROWID'])
  - L189 assign LITERALS = {str(token): '' if value is None else str(value) for token, value in (OBJ.get('literals', {}) or {}).items()}
  - L192 assign FORMATTERS = contract_adapter.formatters
  - L193 annotated assign key_values_map: dict[str, list[str]] = {}
  - L194 if KEY_VALUES:
    - L195 for (token, raw_value) in KEY_VALUES.items():
      - L196 assign name = str(token or '').strip()
      - L197 if not name:
        - L198 continue
      - L199 annotated assign values: list[str] = []
      - L200 if isinstance(raw_value, (list, tuple, set)):
        - L201 assign seen = set()
        - L202 for item in raw_value:
          - L203 assign text = str(item or '').strip()
          - L204 if text and text not in seen:
            - L205 expr seen.add(text)
            - L206 expr values.append(text)
        - L208 else:
          - L208 assign text = str(raw_value or '').strip()
          - L209 if text:
            - L210 assign values = [text]
      - L211 if values:
        - L212 assign key_values_map[name] = values
  - L214 assign _DIRECT_COLUMN_RE = re.compile('^(?P<table>[A-Za-z_][\\w]*)\\.(?P<column>[A-Za-z_][\\w]*)$')
  - L215 assign _SQL_IDENT_RE = re.compile('^[A-Za-z_][A-Za-z0-9_]*$')
  - L217 def _safe_identname: str:
    - L218 if _SQL_IDENT_RE.match(name):
      - L219 return name
    - L220 assign safe = str(name).replace('"', '""')
    - L221 return f'"{safe}"'
  - L223 def _resolve_token_columntoken: str:
    - L224 assign mapping_expr = PLACEHOLDER_TO_COL.get(token)
    - L225 if isinstance(mapping_expr, str):
      - L226 assign match = _DIRECT_COLUMN_RE.match(mapping_expr.strip())
      - L227 if match:
        - L228 return (match.group('table'), match.group('column'))
    - L229 assign required_filters = contract_adapter.required_filters
    - L230 assign optional_filters = contract_adapter.optional_filters
    - L231 assign filter_expr = (required_filters.get(token) or optional_filters.get(token) or '').strip()
    - L232 assign match = _DIRECT_COLUMN_RE.match(filter_expr)
    - L233 if match:
      - L234 return (match.group('table'), match.group('column'))
    - L235 return None
  - L237 def _canonicalize_casetable: str, column: str, raw_value: str:
    - L238 assign normalized_table = str(table or '').strip().lower()
    - L239 assign normalized_column = str(column or '').strip().lower()
    - L240 assign normalized_value = str(raw_value or '').strip()
    - L241 assign cache_key = (normalized_table, normalized_column, normalized_value.lower())
    - L242 if cache_key in _canonicalize_cache:
      - L243 return _canonicalize_cache[cache_key]
    - L244 assign canonical = normalized_value
    - L245 if not normalized_table or not normalized_column or (not normalized_value):
      - L246 assign _canonicalize_cache[cache_key] = canonical
      - L247 return canonical
    - L248 try:
      - L249 assign frame = dataframe_loader.frame(table)
      - L250 except Exception:
        - L251 assign _canonicalize_cache[cache_key] = canonical
        - L252 return canonical
    - L253 if column not in frame.columns:
      - L254 assign _canonicalize_cache[cache_key] = canonical
      - L255 return canonical
    - L256 assign series = frame[column]
    - L257 try:
      - L258 assign matches = series.dropna().astype(str)
      - L259 except Exception:
        - L260 assign matches = series.dropna().apply(lambda v: str(v))
    - L261 assign lower_target = normalized_value.lower()
    - L262 assign mask = matches.str.lower() == lower_target
    - L263 assign filtered = matches[mask]
    - L264 if not filtered.empty:
      - L265 assign canonical = str(filtered.iloc[0])
    - L266 assign _canonicalize_cache[cache_key] = canonical
    - L267 return canonical
  - L269 annotated assign _canonicalize_cache: dict[tuple[str, str, str], str] = {}
  - L271 for (token, values) in list(key_values_map.items()):
    - L272 assign resolved = _resolve_token_column(token)
    - L273 if not resolved:
      - L274 continue
    - L275 assign (table_name, column_name) = resolved
    - L276 if not table_name or not column_name:
      - L277 continue
    - L278 annotated assign updated_values: list[str] = []
    - L279 assign changed = False
    - L280 for value in values:
      - L281 if not isinstance(value, str) or not value.strip():
        - L282 expr updated_values.append(value)
        - L283 continue
      - L284 assign canon = _canonicalize_case(table_name, column_name, value.strip())
      - L285 if canon != value:
        - L286 assign changed = True
      - L287 expr updated_values.append(canon)
    - L288 if changed:
      - L289 assign key_values_map[token] = updated_values
  - L291 for (token, values) in key_values_map.items():
    - L292 assign LITERALS[token] = ', '.join(values)
  - L294 annotated assign alias_link_map: dict[str, str] = {}
  - L295 assign recipe_key_values = key_values_map.get('row_recipe_code')
  - L296 if recipe_key_values:
    - L297 assign alias_link_map = {'recipe_code': 'row_recipe_code', 'filter_recipe_code': 'row_recipe_code'}
    - L301 assign literal_value = ', '.join(recipe_key_values)
    - L302 for alias in alias_link_map.keys():
      - L303 assign LITERALS[alias] = literal_value
  - L305 assign multi_key_selected = any((len(values) > 1 for values in key_values_map.values()))
  - L307 def _first_alias_valuetoken: str:
    - L308 assign source = alias_link_map.get(token)
    - L309 if not source:
      - L310 return None
    - L311 return _first_key_value(key_values_map.get(source, []))
  - L313 def _apply_alias_paramstarget: dict[str, Any]:
    - L314 for alias in alias_link_map:
      - L315 if alias in target and str(target[alias] or '').strip():
        - L316 continue
      - L317 assign alias_value = _first_alias_value(alias)
      - L318 if alias_value is not None:
        - L319 assign target[alias] = alias_value
  - L321 expr _log_debug('Normalized key_values_map', key_values_map, 'multi_key_selected', multi_key_selected)
  - L323 def _first_key_valuevalues: list[str]:
    - L324 for val in values:
      - L325 assign text = str(val or '').strip()
      - L326 if text:
        - L327 return text
    - L328 return None
  - L330 def _iter_key_combinationsvalues_map: dict[str, list[str]]:
    - L331 if not values_map:
      - L332 expr (yield {})
      - L333 return None
    - L334 annotated assign tokens: list[str] = []
    - L335 annotated assign value_lists: list[list[str]] = []
    - L336 for (token, raw_values) in values_map.items():
      - L337 annotated assign unique: list[str] = []
      - L338 annotated assign seen_local: set[str] = set()
      - L339 for val in raw_values:
        - L340 assign text = str(val or '').strip()
        - L341 if not text or text in seen_local:
          - L342 continue
        - L343 expr seen_local.add(text)
        - L344 expr unique.append(text)
      - L345 if unique:
        - L346 expr tokens.append(token)
        - L347 expr value_lists.append(unique)
    - L348 if not tokens:
      - L349 expr (yield {})
      - L350 return None
    - L351 for combo in product(*value_lists):
      - L352 expr (yield {token: value for token, value in zip(tokens, combo)})
  - L354 assign _PLAYWRIGHT_ROW_FRIENDLY_LIMIT = 6000
  - L356 async def html_to_pdf_asynchtml_path: Path, pdf_path: Path, base_dir: Path, pdf_scale: float | None=None:
    - L357 if async_playwright is None:
      - L358 expr print('Playwright not available; skipping PDF generation.')
      - L359 return None
    - L361 assign html_path_resolved = html_path.resolve()
    - L362 assign html_source = html_path_resolved.read_text(encoding='utf-8', errors='ignore')
    - L363 assign approx_row_count = html_source.lower().count('<tr')
    - L364 assign base_dir_resolved = (base_dir or html_path.parent).resolve()
    - L365 assign pdf_path_resolved = pdf_path.resolve()
    - L366 assign base_url = base_dir_resolved.as_uri()
    - L368 async with async_playwright() as p:
      - L369 assign browser = await p.chromium.launch()
      - L370 assign context = None
      - L371 try:
        - L372 assign context = await browser.new_context(base_url=base_url)
        - L373 assign page = await context.new_page()
        - L374 expr await page.set_content(html_source, wait_until='networkidle')
        - L375 expr await page.emulate_media(media='print')
        - L376 assign scale_value = pdf_scale or 1.0
        - L377 if not isinstance(scale_value, (int, float)):
          - L378 assign scale_value = 1.0
        - L379 assign scale_value = max(0.1, min(float(scale_value), 2.0))
        - L380 try:
          - L381 expr await page.pdf(path=str(pdf_path_resolved), format='A4', print_background=True, margin={'top': '10mm', 'right': '10mm', 'bottom': '10mm', 'left': '10mm'}, prefer_css_page_size=True, scale=scale_value)
          - L389 except Exception as exc:
            - L390 if approx_row_count >= _PLAYWRIGHT_ROW_FRIENDLY_LIMIT:
              - L391 raise RuntimeError(f'PDF rendering failed because the report contains approximately {approx_row_count:,} table rows, which exceeds the printable limit. Please filter the data further or split the report into smaller chunks and try again.')
            - L398 raise
        - L400 finally:
          - L400 if context is not None:
            - L401 expr await context.close()
          - L402 expr await browser.close()
  - L404 def _combine_html_documentshtml_sections: list[str]:
    - L405 if not html_sections:
      - L406 return ''
    - L407 annotated assign combined_body: list[str] = []
    - L408 assign doc_type = ''
    - L409 assign head_html = ''
    - L411 assign head_pattern = re.compile('(?is)<head\\b[^>]*>(?P<head>.*)</head>')
    - L412 assign body_pattern = re.compile('(?is)<body\\b[^>]*>(?P<body>.*)</body>')
    - L413 assign doctype_pattern = re.compile('(?is)^\\s*<!DOCTYPE[^>]*>', re.MULTILINE)
    - L415 for (idx, raw_html) in enumerate(html_sections):
      - L416 assign text = raw_html or ''
      - L417 if idx == 0:
        - L418 assign doctype_match = doctype_pattern.search(text)
        - L419 if doctype_match:
          - L420 assign doc_type = doctype_match.group(0).strip()
          - L421 assign text = text[doctype_match.end():]
        - L422 assign head_match = head_pattern.search(text)
        - L423 if head_match:
          - L424 assign head_html = head_match.group(0).strip()
        - L425 assign body_match = body_pattern.search(text)
        - L426 if body_match:
          - L427 assign section_body = body_match.group('body').strip()
          - L429 else:
            - L429 assign section_body = text.strip()
        - L430 expr combined_body.append(f'<div class="nr-key-section" data-nr-section="1">\n{section_body}\n</div>')
        - L432 else:
          - L432 assign body_match = body_pattern.search(text)
          - L433 assign section = body_match.group('body').strip() if body_match else text.strip()
          - L434 expr combined_body.append(f'<div class="nr-key-section" data-nr-section="{idx + 1}" style="page-break-before: always;">\n{section}\n</div>')
    - L438 assign doc_lines = []
    - L439 if doc_type:
      - L440 expr doc_lines.append(doc_type)
    - L441 expr doc_lines.append('<html>')
    - L442 if head_html:
      - L443 expr doc_lines.append(head_html)
    - L444 expr doc_lines.append('<body>')
    - L445 expr doc_lines.append('\n\n'.join(combined_body))
    - L446 expr doc_lines.append('</body>')
    - L447 expr doc_lines.append('</html>')
    - L448 return '\n'.join(doc_lines)
  - L450 def _value_has_contentvalue: Any:
    - L451 if value is None:
      - L452 return False
    - L453 if isinstance(value, (int, float, Decimal)):
      - L454 return value != 0
    - L455 assign text = str(value).strip()
    - L456 if not text:
      - L457 return False
    - L458 try:
      - L459 assign num = Decimal(text)
      - L460 except Exception:
        - L461 return True
      - L463 else:
        - L463 return num != 0
  - L465 def _row_has_significant_datarow: Mapping[str, Any], columns: list[str]:
    - L466 return _row_has_any_data(row, (), columns)
  - L468 def _token_values_have_datarow: Mapping[str, Any], tokens: list[str]:
    - L469 return _row_has_any_data(row, tokens, ())
  - L471 def _row_has_any_datarow: Mapping[str, Any], tokens: Sequence[str], columns: Sequence[str]:
    - L472 for token in tokens:
      - L473 if not token:
        - L474 continue
      - L475 if _value_has_content(_value_for_token(row, token)):
        - L476 return True
    - L477 for col in columns:
      - L478 if not col:
        - L479 continue
      - L480 if _value_has_content(row.get(col)):
        - L481 return True
    - L482 for (key, value) in row.items():
      - L483 if not isinstance(key, str):
        - L484 continue
      - L485 if _is_counter_field(key):
        - L486 continue
      - L487 if _value_has_content(value):
        - L488 return True
    - L489 return False
  - L491 def _is_counter_fieldname: str | None:
    - L492 if not name:
      - L493 return False
    - L494 if not isinstance(name, str):
      - L495 assign name = str(name)
    - L496 assign normalized = re.sub('[^a-z0-9]', '', name.lower())
    - L497 if not normalized:
      - L498 return False
    - L499 if normalized in {'row', 'rowid', 'rowno', 'rownum', 'rownumber', 'rowindex', 'rowcounter', 'srno', 'sno'}:
      - L510 return True
    - L511 assign counter_markers = ('serial', 'sequence', 'seq', 'counter')
    - L512 if any((marker in normalized for marker in counter_markers)):
      - L513 return True
    - L514 assign counter_suffixes = ('slno', 'srno', 'sno', 'snum', 'snumber', 'sl', 'no', 'num', 'number', 'idx', 'index')
    - L527 return any((normalized.endswith(suffix) and normalized.startswith('row') for suffix in counter_suffixes))
  - L529 def _reindex_serial_fieldsrows: list[dict], tokens: Sequence[str], columns: Sequence[str]:
    - L530 assign serial_tokens = [tok for tok in tokens if _is_counter_field(tok)]
    - L531 assign serial_columns = [col for col in columns if _is_counter_field(col)]
    - L532 if not serial_tokens and (not serial_columns):
      - L533 return None
    - L534 for (idx, row) in enumerate(rows, start=1):
      - L535 for tok in serial_tokens:
        - L536 assign row[tok] = idx
      - L537 for col in serial_columns:
        - L538 assign row[col] = idx
  - L540 def _value_for_tokenrow: Mapping[str, Any], token: str:
    - L541 if not token:
      - L542 return None
    - L543 if token in row:
      - L544 return row[token]
    - L545 assign normalized = str(token).lower()
    - L546 for key in row.keys():
      - L547 if isinstance(key, str) and key.lower() == normalized:
        - L548 return row[key]
    - L549 assign mapped = PLACEHOLDER_TO_COL.get(token)
    - L550 if mapped:
      - L551 assign col = _extract_col_name(mapped)
      - L552 if col:
        - L553 if col in row:
          - L554 return row[col]
        - L555 for key in row.keys():
          - L556 if isinstance(key, str) and key.lower() == col.lower():
            - L557 return row[key]
    - L558 return None
  - L560 def _prune_placeholder_rowsrows: Sequence[Mapping[str, Any]], tokens: Sequence[str]:
    - L561 assign material_tokens = [tok for tok in tokens if tok and 'material' in tok.lower()]
    - L562 annotated assign pruned: list[dict[str, Any]] = []
    - L563 for row in rows:
      - L564 assign keep = True
      - L565 for tok in material_tokens:
        - L566 if not _value_has_content(_value_for_token(row, tok)):
          - L567 assign keep = False
          - L568 break
      - L569 if keep:
        - L570 expr pruned.append(dict(row))
    - L571 return pruned if pruned else [dict(row) for row in rows]
  - L573 def _filter_rows_for_renderrows: Sequence[Mapping[str, Any]], row_tokens_template: Sequence[str], row_columns: Sequence[str], *, treat_all_as_data: bool:
    - L580 if not rows:
      - L581 return []
    - L583 if treat_all_as_data:
      - L584 assign prepared = [dict(row) for row in rows]
      - L586 else:
        - L586 assign significant_tokens = [tok for tok in row_tokens_template if tok and (not _is_counter_field(tok))]
        - L587 assign significant_columns = [col for col in row_columns if col and (not _is_counter_field(col))]
        - L588 assign guard_rows = bool(significant_tokens or significant_columns)
        - L589 annotated assign prepared: list[dict[str, Any]] = []
        - L590 for row in rows:
          - L591 if guard_rows and (not _row_has_any_data(row, significant_tokens, significant_columns)):
            - L592 continue
          - L593 expr prepared.append(dict(row))
    - L595 if prepared:
      - L596 expr _reindex_serial_fields(prepared, row_tokens_template, row_columns)
    - L597 return prepared
  - L599 if multi_key_selected and (not __force_single):
    - L600 annotated assign html_sections: list[str] = []
    - L601 annotated assign tmp_outputs: list[tuple[Path, Path]] = []
    - L602 try:
      - L603 for (idx, combo) in enumerate(_iter_key_combinations(key_values_map), start=1):
        - L604 annotated assign selection: dict[str, str] = {token: value for token, value in combo.items()}
        - L605 for (alias, source) in alias_link_map.items():
          - L606 if alias not in selection and source in selection:
            - L607 assign selection[alias] = selection[source]
        - L608 expr _log_debug('Fanout iteration', idx, 'selection', selection)
        - L609 assign tmp_html = OUT_HTML.with_name(f'{OUT_HTML.stem}__key{idx}.html')
        - L610 assign tmp_pdf = OUT_PDF.with_name(f'{OUT_PDF.stem}__key{idx}.pdf')
        - L611 assign result = fill_and_print(OBJ=OBJ, TEMPLATE_PATH=TEMPLATE_PATH, DB_PATH=DB_PATH, OUT_HTML=tmp_html, OUT_PDF=tmp_pdf, START_DATE=START_DATE, END_DATE=END_DATE, batch_ids=None, KEY_VALUES=selection or None, GENERATOR_BUNDLE=GENERATOR_BUNDLE, __force_single=True)
        - L624 expr html_sections.append(Path(result['html_path']).read_text(encoding='utf-8', errors='ignore'))
        - L625 expr tmp_outputs.append((Path(result['html_path']), Path(result['pdf_path'])))
      - L627 if not html_sections:
        - L628 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': False}
      - L630 assign combined_html = _combine_html_documents(html_sections)
      - L631 expr OUT_HTML.write_text(combined_html, encoding='utf-8')
      - L632 expr asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent))
      - L633 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': True}
      - L635 finally:
        - L635 for (tmp_html_path, tmp_pdf_path) in tmp_outputs:
          - L636 for path_sel in (tmp_html_path, tmp_pdf_path):
            - L637 with contextlib.suppress(FileNotFoundError):
              - L638 expr path_sel.unlink()
  - L640 def _get_literal_rawtoken: str:
    - L641 if token not in LITERALS:
      - L642 return ''
    - L643 assign raw = LITERALS[token]
    - L644 return '' if raw is None else str(raw)
  - L646 def _literal_has_contenttoken: str:
    - L647 return bool(_get_literal_raw(token).strip())
  - L649 def _first_nonempty_literaltokens: Iterable[str]:
    - L650 for tok in tokens:
      - L651 assign raw = _get_literal_raw(tok)
      - L652 if raw.strip():
        - L653 return (tok, raw)
    - L654 return (None, None)
  - L656 def _record_special_valuetarget: dict[str, str], token: str, value: str:
    - L657 assign existing_raw = _get_literal_raw(token)
    - L658 if existing_raw.strip():
      - L659 assign target[token] = existing_raw
      - L661 else:
        - L661 assign target[token] = value
        - L662 if token in LITERALS:
          - L663 assign LITERALS[token] = value
  - L665 def _filter_tokens_without_literaltokens: set[str]:
    - L666 return {tok for tok in tokens if not _literal_has_content(tok)}
  - L668 assign BEGIN_TAG = '<!-- BEGIN:BATCH (auto) -->'
  - L669 assign END_TAG = '<!-- END:BATCH (auto) -->'
  - L670 try:
    - L671 assign (prototype_block, start0, end_last) = _select_prototype_block(html, ROW_TOKENS)
    - L672 except Exception as exc:
      - L673 expr _raise_no_block(html, exc)
  - L674 assign shell_prefix = html[:start0] + BEGIN_TAG
  - L675 assign shell_suffix = END_TAG + html[end_last:]
  - L677 assign parent_table = JOIN.get('parent_table', '')
  - L678 assign parent_key = JOIN.get('parent_key', '')
  - L679 assign child_table = JOIN.get('child_table', '')
  - L680 assign child_key = JOIN.get('child_key', '')
  - L681 assign parent_date = DATE_COLUMNS.get(parent_table, '')
  - L682 assign child_date = DATE_COLUMNS.get(child_table, '')
  - L683 assign order_col = ROW_ORDER[0] if ROW_ORDER else 'ROWID'
  - L685 def _normalize_token_namename: str:
    - L686 return re.sub('[^a-z0-9]', '', name.lower())
  - L688 annotated assign token_index: dict[str, set[str]] = defaultdict(set)
  - L689 assign all_candidate_tokens = set(TEMPLATE_TOKENS) | set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())
  - L693 def _token_synonym_keysnorm: str:
    - L694 docstring: "\n        Generate lightweight normalization aliases so that abbreviated tokens ..."
    - L699 if not norm:
      - L700 return set()
    - L701 assign aliases = {norm}
    - L702 annotated assign replacements: tuple[tuple[str, str], ...] = (('pg', 'page'), ('num', 'number'), ('no', 'number'), ('cnt', 'count'), ('ttl', 'total'))
    - L709 for (src, dest) in replacements:
      - L710 if src in norm and dest not in norm:
        - L711 expr aliases.add(norm.replace(src, dest))
    - L715 if norm == 'pg':
      - L716 expr aliases.add('page')
    - L717 return {alias for alias in aliases if alias}
  - L719 for tok in all_candidate_tokens:
    - L720 assign norm = _normalize_token_name(tok)
    - L721 for key in _token_synonym_keys(norm):
      - L722 expr token_index[key].add(tok)
  - L724 def _tokens_for_keyskeys: set[str]:
    - L725 annotated assign found: set[str] = set()
    - L726 for key in keys:
      - L727 expr found.update(token_index.get(key, set()))
    - L728 return found
  - L730 def _format_for_dbdt_obj: datetime | None, raw_value, include_time_default: bool:
    - L731 docstring: "\n        Normalize input dates for SQLite bindings:\n          - prefer ISO 860..."
    - L736 if dt_obj:
      - L737 assign include_time = include_time_default or bool(dt_obj.hour or dt_obj.minute or dt_obj.second or dt_obj.microsecond)
      - L740 if include_time:
        - L741 return dt_obj.strftime('%Y-%m-%d %H:%M:%S')
      - L742 return dt_obj.strftime('%Y-%m-%d')
    - L743 return '' if raw_value is None else str(raw_value).strip()
  - L745 def _run_generator_entrypointsentrypoints: dict, sql_params: dict[str, object], df_loader: SQLiteDataFrameLoader:
    - L750 if not entrypoints:
      - L751 return {}
    - L752 assign alias_fix_patterns = [re.compile('\\brows\\.', re.IGNORECASE), re.compile('\\brows_agg\\.', re.IGNORECASE)]
    - L757 def _wrap_date_paramsql_text: str, param: str:
      - L758 assign pattern = re.compile(f':{param}\\b')
      - L760 def _replacematch: re.Match[str]:
        - L761 assign start = match.start()
        - L762 assign prefix = sql_text[:start].rstrip()
        - L763 assign prefix_lower = prefix.lower()
        - L764 if prefix_lower.endswith('date(') or prefix_lower.endswith('datetime('):
          - L765 return match.group(0)
        - L766 return f'DATE({match.group(0)})'
      - L768 return pattern.sub(_replace, sql_text)
    - L770 def _prepare_sqlsql_text: str:
      - L771 assign updated = re.sub('PARAM:([A-Za-z0-9_]+)', ':\\1', sql_text)
      - L772 if 'DATE(' not in updated.upper():
        - L773 return updated
      - L774 for param in ('from_date', 'to_date'):
        - L775 assign updated = _wrap_date_param(updated, param)
      - L776 return updated
    - L778 def _attempt_sql_fixsql_text: str, exc: Exception | None:
      - L779 if exc is None:
        - L780 return None
      - L781 assign message = str(exc).lower()
      - L782 if 'no such column' not in message:
        - L783 return None
      - L784 assign fixed_sql = sql_text
      - L785 for pattern in alias_fix_patterns:
        - L786 assign fixed_sql = pattern.sub('', fixed_sql)
      - L787 return fixed_sql if fixed_sql != sql_text else None
    - L789 assign frames = df_loader.frames()
    - L790 assign query_engine = DuckDBDataFrameQuery(frames)
    - L791 try:
      - L792 annotated assign results: dict[str, list[dict[str, object]]] = {}
      - L793 for name in ('header', 'rows', 'totals'):
        - L794 assign sql = entrypoints.get(name)
        - L795 if not sql:
          - L796 assign results[name] = []
          - L797 continue
        - L798 assign current_sql = _prepare_sql(sql)
        - L799 annotated assign last_error: Exception | None = None
        - L800 for attempt in range(2):
          - L801 try:
            - L802 assign df_rows = query_engine.execute(current_sql, sql_params)
            - L803 except sqlite3.OperationalError as exc:
              - L804 assign last_error = exc
              - L805 assign fixed_sql = _attempt_sql_fix(current_sql, exc)
              - L806 if fixed_sql is not None and fixed_sql != current_sql:
                - L807 assign current_sql = fixed_sql
                - L808 continue
              - L809 raise
            - L811 else:
              - L811 assign last_error = None
              - L812 assign results[name] = df_rows.to_dict('records')
              - L813 break
          - L815 else:
            - L815 assert last_error is not None
            - L816 raise last_error
      - L817 return results
      - L819 finally:
        - L819 expr query_engine.close()
  - L821 assign start_dt = _parse_date_like(START_DATE)
  - L822 assign end_dt = _parse_date_like(END_DATE)
  - L823 assign print_dt = datetime.now()
  - L825 assign start_has_time = _has_time_component(START_DATE, start_dt)
  - L826 assign end_has_time = _has_time_component(END_DATE, end_dt)
  - L828 assign START_DATE_KEYS = {'fromdate', 'datefrom', 'startdate', 'periodstart', 'rangefrom', 'fromdt', 'startdt'}
  - L829 assign END_DATE_KEYS = {'todate', 'dateto', 'enddate', 'periodend', 'rangeto', 'todt', 'enddt'}
  - L830 assign PRINT_DATE_KEYS = {'printdate', 'printedon', 'printeddate', 'generatedon', 'generateddate', 'rundate', 'runon', 'generatedat'}
  - L840 assign PAGE_NO_KEYS = {'page', 'pageno', 'pagenum', 'pagenumber', 'pageindex', 'pageidx', 'pagecurrent', 'currentpage', 'currpage', 'pgno', 'pgnum', 'pgnumber', 'pgindex', 'pgcurrent'}
  - L856 assign PAGE_COUNT_KEYS = {'pagecount', 'pagecounts', 'totalpages', 'pagestotal', 'pages', 'pagetotal', 'totalpage', 'pagecounttotal', 'totalpagecount', 'pagescount', 'countpages', 'lastpage', 'finalpage', 'maxpage', 'pgtotal', 'totalpg', 'pgcount', 'countpg', 'pgs', 'pgscount', 'pgstotal', 'totalpgs'}
  - L880 assign PAGE_LABEL_KEYS = {'pagelabel', 'pageinfo', 'pagesummary', 'pagefooter', 'pagefootertext', 'pageindicator', 'pagecaption', 'pagefooterlabel', 'pagetext', 'pagefooterinfo', 'pagehint'}
  - L894 annotated assign special_values: dict[str, str] = {}
  - L896 assign start_tokens = _tokens_for_keys(START_DATE_KEYS)
  - L897 assign end_tokens = _tokens_for_keys(END_DATE_KEYS)
  - L898 assign print_tokens = _tokens_for_keys(PRINT_DATE_KEYS)
  - L899 assign page_number_tokens = _tokens_for_keys(PAGE_NO_KEYS)
  - L900 assign page_count_tokens = _tokens_for_keys(PAGE_COUNT_KEYS)
  - L901 assign page_label_tokens = _tokens_for_keys(PAGE_LABEL_KEYS)
  - L903 for tok in start_tokens:
    - L904 expr _record_special_value(special_values, tok, _format_for_token(tok, start_dt, include_time_default=start_has_time))
  - L909 for tok in end_tokens:
    - L910 expr _record_special_value(special_values, tok, _format_for_token(tok, end_dt, include_time_default=end_has_time))
  - L916 assign (_, print_literal_value) = _first_nonempty_literal(print_tokens)
  - L917 assign parsed_print_dt = _parse_date_like(print_literal_value) if print_literal_value else None
  - L918 assign print_dt_source = parsed_print_dt or print_dt
  - L919 assign print_has_time = _has_time_component(print_literal_value, parsed_print_dt)
  - L921 for tok in print_tokens:
    - L922 if print_literal_value and (not parsed_print_dt):
      - L923 assign value = print_literal_value
      - L925 else:
        - L925 assign value = _format_for_token(tok, print_dt_source, include_time_default=print_has_time)
    - L926 expr _record_special_value(special_values, tok, value)
  - L928 assign page_number_tokens = _filter_tokens_without_literal(page_number_tokens)
  - L929 assign page_count_tokens = _filter_tokens_without_literal(page_count_tokens)
  - L930 assign page_label_tokens = _filter_tokens_without_literal(page_label_tokens)
  - L932 assign post_literal_specials = {tok: val for tok, val in special_values.items() if tok not in LITERALS}
  - L934 assign _ident_re = re.compile('^[A-Za-z_][A-Za-z0-9_]*$')
  - L936 def qidentname: str:
    - L937 if _ident_re.match(name):
      - L938 return name
    - L939 assign safe = name.replace('"', '""')
    - L940 return f'"{safe}"'
  - L943 def _parse_key_colskey_spec: str:
    - L944 return [c.strip() for c in str(key_spec).split(',') if c and c.strip()]
  - L946 def _key_exprcols: list[str]:
    - L947 assign parts = [f"COALESCE(CAST({qident(c)} AS TEXT),'')" for c in cols]
    - L948 if not parts:
      - L949 return "''"
    - L950 assign expr = parts[0]
    - L951 for p in parts[1:]:
      - L952 assign expr = f"{expr} || '|' || {p}"
    - L953 return expr
  - L955 def _split_bidbid: str, n: int:
    - L956 assign parts = str(bid).split('|')
    - L957 if len(parts) != n:
      - L958 raise ValueError(f'Composite key mismatch: expected {n} parts, got {len(parts)} in {bid!r}')
    - L959 return parts
  - L961 def _looks_like_composite_idx: str, n: int:
    - L962 return isinstance(x, str) and x.count('|') == n - 1
  - L964 assign pcols = _parse_key_cols(parent_key)
  - L965 assign ccols = _parse_key_cols(child_key)
  - L967 assign has_child = bool(child_table and ccols)
  - L968 assign parent_table_lc = parent_table.lower()
  - L969 assign child_table_lc = child_table.lower()
  - L970 annotated assign parent_filter_map: dict[str, list[str]] = {}
  - L971 annotated assign child_filter_map: dict[str, list[str]] = {}
  - L972 if key_values_map:
    - L973 for (token, values) in key_values_map.items():
      - L974 assign mapping_value = PLACEHOLDER_TO_COL.get(token)
      - L975 if not isinstance(mapping_value, str):
        - L976 continue
      - L977 assign target = mapping_value.strip()
      - L978 if not target or target.upper().startswith('PARAM:') or '.' not in target:
        - L979 continue
      - L980 assign (table_name, column_name) = target.split('.', 1)
      - L981 assign table_name = table_name.strip(' "`[]')
      - L982 assign column_name = column_name.strip(' "`[]')
      - L983 if not column_name:
        - L984 continue
      - L985 assign table_key = table_name.lower()
      - L986 if table_key in (parent_table_lc, 'header'):
        - L987 assign bucket = list(parent_filter_map.get(column_name, []))
        - L988 for val in values:
          - L989 if val not in bucket:
            - L990 expr bucket.append(val)
        - L991 if bucket:
          - L992 assign parent_filter_map[column_name] = bucket
      - L993 if has_child and table_key in (child_table_lc, 'rows'):
        - L994 assign bucket = list(child_filter_map.get(column_name, []))
        - L995 for val in values:
          - L996 if val not in bucket:
            - L997 expr bucket.append(val)
        - L998 if bucket:
          - L999 assign child_filter_map[column_name] = bucket
  - L1000 assign parent_filter_items = list(parent_filter_map.items())
  - L1001 assign child_filter_items = list(child_filter_map.items())
  - L1002 annotated assign parent_filter_sqls: list[str] = []
  - L1003 annotated assign parent_filter_values: list[str] = []
  - L1004 for (col, values) in parent_filter_items:
    - L1005 annotated assign normalized: list[str] = []
    - L1006 for val in values:
      - L1007 if not isinstance(val, str):
        - L1008 continue
      - L1009 assign text = val.strip()
      - L1010 if text and text not in normalized:
        - L1011 expr normalized.append(text)
    - L1012 if not normalized:
      - L1013 continue
    - L1014 if len(normalized) == 1:
      - L1015 expr parent_filter_sqls.append(f'{qident(col)} = ?')
      - L1017 else:
        - L1017 assign placeholders = ', '.join(('?' for _ in normalized))
        - L1018 expr parent_filter_sqls.append(f'{qident(col)} IN ({placeholders})')
    - L1019 expr parent_filter_values.extend(normalized)
  - L1020 assign parent_filter_values_tuple = tuple(parent_filter_values)
  - L1022 annotated assign child_filter_sqls: list[str] = []
  - L1023 annotated assign child_filter_values: list[str] = []
  - L1024 for (col, values) in child_filter_items:
    - L1025 annotated assign normalized: list[str] = []
    - L1026 for val in values:
      - L1027 if not isinstance(val, str):
        - L1028 continue
      - L1029 assign text = val.strip()
      - L1030 if text and text not in normalized:
        - L1031 expr normalized.append(text)
    - L1032 if not normalized:
      - L1033 continue
    - L1034 if len(normalized) == 1:
      - L1035 expr child_filter_sqls.append(f'{qident(col)} = ?')
      - L1037 else:
        - L1037 assign placeholders = ', '.join(('?' for _ in normalized))
        - L1038 expr child_filter_sqls.append(f'{qident(col)} IN ({placeholders})')
    - L1039 expr child_filter_values.extend(normalized)
  - L1040 assign child_filter_values_tuple = tuple(child_filter_values)
  - L1042 def _merge_predicatebase_sql: str, extras: list[str]:
    - L1043 if not extras:
      - L1044 return base_sql
    - L1045 assign extras_joined = ' AND '.join(extras)
    - L1046 assign base_sql = (base_sql or '1=1').strip()
    - L1047 return f'({base_sql}) AND {extras_joined}'
  - L1050 assign parent_type = get_col_type(DB_PATH, parent_table, parent_date)
  - L1051 assign child_type = get_col_type(DB_PATH, child_table, child_date)
  - L1052 assign (parent_pred, adapt_parent) = mk_between_pred_for_date(parent_date, parent_type)
  - L1053 assign (child_pred, adapt_child) = mk_between_pred_for_date(child_date, child_type)
  - L1054 assign parent_where_clause = _merge_predicate(parent_pred, parent_filter_sqls)
  - L1055 assign child_where_clause = _merge_predicate(child_pred, child_filter_sqls) if has_child else child_pred
  - L1056 assign db_start = _format_for_db(start_dt, START_DATE, start_has_time)
  - L1057 assign db_end = _format_for_db(end_dt, END_DATE, end_has_time)
  - L1058 assign PDATE = tuple(adapt_parent(db_start, db_end))
  - L1059 assign CDATE = tuple(adapt_child(db_start, db_end)) if has_child else tuple()
  - L1060 assign parent_params_all = tuple(PDATE) + parent_filter_values_tuple
  - L1061 assign child_params_all = tuple(CDATE) + child_filter_values_tuple if has_child else tuple()
  - L1063 annotated assign sql_params: dict[str, object] = {'from_date': db_start, 'to_date': db_end}
  - L1068 for token in contract_adapter.param_tokens:
    - L1069 if token in ('from_date', 'to_date'):
      - L1070 continue
    - L1071 if token in key_values_map:
      - L1072 assign first_value = _first_key_value(key_values_map[token])
      - L1073 if first_value is not None:
        - L1074 assign sql_params[token] = first_value
      - L1075 else:
        - L1075 if alias_link_map.get(token):
          - L1076 assign alias_value = _first_alias_value(token)
          - L1077 if alias_value is not None:
            - L1078 assign sql_params[token] = alias_value
          - L1079 else:
            - L1079 if token in LITERALS:
              - L1080 assign sql_params[token] = LITERALS[token]
              - L1081 else:
                - L1081 if token in special_values:
                  - L1082 assign sql_params[token] = special_values[token]
                  - L1084 else:
                    - L1084 expr sql_params.setdefault(token, '')
  - L1086 expr _apply_alias_params(sql_params)
  - L1088 def _apply_date_param_defaultstarget: dict[str, object]:
    - L1089 if not isinstance(target, dict):
      - L1090 return None
    - L1092 def _injectnames: set[str], default_value: str:
      - L1093 if not default_value:
        - L1094 return None
      - L1095 for alias in names:
        - L1096 if alias not in param_token_set and alias not in target:
          - L1097 continue
        - L1098 assign current = target.get(alias)
        - L1099 if isinstance(current, str):
          - L1100 if current.strip():
            - L1101 continue
          - L1102 else:
            - L1102 if current not in (None, ''):
              - L1103 continue
        - L1104 assign target[alias] = default_value
    - L1106 expr _inject(_DATE_PARAM_START_ALIASES, db_start)
    - L1107 expr _inject(_DATE_PARAM_END_ALIASES, db_end)
  - L1109 expr _apply_date_param_defaults(sql_params)
  - L1111 if 'recipe_code' in sql_params:
    - L1112 expr _log_debug('[multi-debug] generator param binding', 'force_single' if __force_single else 'fanout_root', 'key_values=', key_values_map, 'recipe_code=', sql_params.get('recipe_code'))
  - L1121 if GENERATOR_BUNDLE is None:
    - L1122 assign generator_dir = TEMPLATE_PATH.parent / 'generator'
    - L1123 assign meta_path = generator_dir / 'generator_assets.json'
    - L1124 if meta_path.exists():
      - L1125 try:
        - L1126 assign meta_payload = json.loads(meta_path.read_text(encoding='utf-8'))
        - L1127 except Exception:
          - L1128 assign meta_payload = None
        - L1130 else:
          - L1130 annotated assign bundle: dict[str, object] = {'meta': meta_payload}
          - L1131 assign output_schemas_path = generator_dir / 'output_schemas.json'
          - L1132 if output_schemas_path.exists():
            - L1133 try:
              - L1134 assign bundle['output_schemas'] = json.loads(output_schemas_path.read_text(encoding='utf-8'))
              - L1135 except Exception:
                - L1136 pass
          - L1137 assign GENERATOR_BUNDLE = bundle
  - L1139 annotated assign generator_results: dict[str, list[dict[str, object]]] | None = None
  - L1140 if GENERATOR_BUNDLE and (not multi_key_selected):
    - L1141 assign meta_payload = GENERATOR_BUNDLE.get('meta') or {}
    - L1142 assign entrypoints = meta_payload.get('entrypoints')
    - L1143 if not isinstance(entrypoints, dict):
      - L1144 assign entrypoints = {}
    - L1146 if any(entrypoints.values()):
      - L1147 assign params_spec = meta_payload.get('params') or {}
      - L1148 assign required_params = list(params_spec.get('required') or [])
      - L1149 assign optional_params = list(params_spec.get('optional') or [])
      - L1150 for name in required_params + optional_params:
        - L1151 expr sql_params.setdefault(name, None)
      - L1153 if 'plant_name' in sql_params:
        - L1154 assign sql_params['plant_name'] = LITERALS.get('plant_name') or special_values.get('plant_name', '')
      - L1155 if 'location' in sql_params:
        - L1156 assign sql_params['location'] = LITERALS.get('location') or special_values.get('location', '')
      - L1157 if 'recipe_code' in sql_params:
        - L1158 if 'recipe_code' in key_values_map:
          - L1159 assign sql_params['recipe_code'] = _first_key_value(key_values_map['recipe_code'])
          - L1161 else:
            - L1161 assign fallback_val = LITERALS.get('recipe_code')
            - L1162 assign sql_params['recipe_code'] = fallback_val if fallback_val not in (None, '', []) else None
      - L1163 if 'page_info' in sql_params:
        - L1164 assign sql_params['page_info'] = LITERALS.get('page_info') or ''
      - L1165 if key_values_map:
        - L1166 for (name, values) in key_values_map.items():
          - L1167 assign sql_params[name] = _first_key_value(values)
      - L1168 expr _apply_alias_params(sql_params)
      - L1170 try:
        - L1171 assign generator_results = _run_generator_entrypoints(entrypoints, sql_params, dataframe_loader)
        - L1172 except Exception as exc:
          - L1173 expr print(f'Generator SQL execution failed; falling back to contract mapping: {exc}')
          - L1174 assign generator_results = None
  - L1176 if generator_results is None and (not multi_key_selected):
    - L1177 try:
      - L1178 assign default_sql_pack = contract_adapter.build_default_sql_pack()
      - L1179 except Exception as exc:
        - L1180 expr print(f'Contract-derived SQL synthesis failed: {exc}')
      - L1182 else:
        - L1182 assign default_params = default_sql_pack.get('params') or {}
        - L1183 assign required_default = list(default_params.get('required') or [])
        - L1184 assign optional_default = list(default_params.get('optional') or [])
        - L1185 for name in required_default + optional_default:
          - L1186 expr sql_params.setdefault(name, None)
        - L1187 expr _apply_alias_params(sql_params)
        - L1188 try:
          - L1189 assign fallback_results = _run_generator_entrypoints(default_sql_pack['entrypoints'], sql_params, dataframe_loader)
          - L1194 except Exception as exc:
            - L1195 expr print(f'Contract SQL execution failed; continuing with discovery fallback: {exc}')
          - L1197 else:
            - L1197 if any((fallback_results.get(section) for section in ('rows', 'header', 'totals'))):
              - L1198 assign generator_results = fallback_results
  - L1201 if generator_results is None:
    - L1202 assign need_discover = False
    - L1203 assign existing = batch_ids
    - L1205 if isinstance(existing, str):
      - L1206 assign existing = [existing]
    - L1208 if not existing:
      - L1209 assign need_discover = True
      - L1211 else:
        - L1211 if not isinstance(existing, (list, tuple)):
          - L1212 assign need_discover = True
          - L1214 else:
            - L1214 assign existing = list(existing)
            - L1215 if len(pcols) > 1:
              - L1216 if any((not _looks_like_composite_id(i, len(pcols)) for i in existing)):
                - L1217 expr print('Provided BATCH_IDS do not match composite key format; falling back to auto-discovery.')
                - L1218 assign need_discover = True
    - L1220 if need_discover:
      - L1221 assign discovery_summary = discover_batches_and_counts(db_path=DB_PATH, contract=OBJ, start_date=START_DATE, end_date=END_DATE, key_values=key_values_map)
      - L1228 assign BATCH_IDS = [str(batch['id']) for batch in discovery_summary.get('batches', [])]
      - L1230 else:
        - L1230 assign BATCH_IDS = existing
    - L1232 else:
      - L1232 assign BATCH_IDS = ['__GENERATOR_SINGLE__']
  - L1234 expr print('BATCH_IDS:', len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])
  - L1236 def format_token_valuetoken: str, raw_value: Any:
    - L1237 return contract_adapter.format_value(token, raw_value)
  - L1239 def _inject_page_counter_spanshtml_in: str, page_tokens: set[str], count_tokens: set[str], label_tokens: set[str] | None=None:
    - L1245 assign label_tokens = label_tokens or set()
    - L1246 assign updated = html_in
    - L1247 assign page_markup = '<span class="nr-page-number" data-nr-counter="page" aria-label="Current page number"></span>'
    - L1248 assign count_markup = '<span class="nr-page-count" data-nr-counter="pages" aria-label="Total page count"></span>'
    - L1250 for tok in page_tokens:
      - L1251 assign updated = sub_token(updated, tok, page_markup)
    - L1252 for tok in count_tokens:
      - L1253 assign updated = sub_token(updated, tok, count_markup)
    - L1254 for tok in label_tokens:
      - L1255 if count_tokens:
        - L1256 assign label_markup = f'<span class="nr-page-label" data-nr-counter-label="1">Page {page_markup} of {count_markup}</span>'
        - L1260 else:
          - L1260 assign label_markup = f'<span class="nr-page-label" data-nr-counter-label="1">Page {page_markup}</span>'
      - L1261 assign updated = sub_token(updated, tok, label_markup)
    - L1263 if (page_tokens or count_tokens or label_tokens) and 'nr-page-counter-style' not in updated:
      - L1264 assign style_block = '\n<style id="nr-page-counter-style">\n  .nr-page-number,\n  .nr-page-count { white-space: nowrap; font-variant-numeric: tabular-nums; }\n  .nr-page-label { white-space: nowrap; }\n  @media screen {\n    .nr-page-number::after { content: attr(data-nr-screen); }\n    .nr-page-count::after { content: attr(data-nr-total-pages); }\n  }\n  @media print {\n    body { counter-reset: page; }\n    .nr-page-number::after { content: counter(page); }\n    .nr-page-count::after { content: counter(pages); }\n    .nr-page-count[data-nr-total-pages]::after { content: attr(data-nr-total-pages); }\n  }\n</style>\n'
      - L1281 if '</head>' in updated:
        - L1282 assign updated = updated.replace('</head>', style_block + '</head>', 1)
        - L1284 else:
          - L1284 assign updated = style_block + updated
    - L1286 if (page_tokens or count_tokens or label_tokens) and 'nr-page-counter-script' not in updated:
      - L1287 assign metrics = _extract_page_metrics(updated)
      - L1288 assign metrics_json = json.dumps(metrics)
      - L1289 assign script_template = '\n<script id="nr-page-counter-script">\n(function() {\n  const METRICS = __NR_METRICS__;\n  const PX_PER_MM = 96 / 25.4;\n  const BREAK_VALUES = [\'page\', \'always\', \'left\', \'right\'];\n  const TRAILING_BREAK_SENTINEL = \'__nr_trailing_break__\';\n  let lastPageNodes = [];\n  let lastCountNodes = [];\n\n  function isForcedBreak(value) {\n    if (!value) return false;\n    const normalized = String(value).toLowerCase().trim();\n    if (!normalized) return false;\n    return BREAK_VALUES.indexOf(normalized) !== -1;\n  }\n\n  function readBreakValue(style, which) {\n    if (!style) return \'\';\n    if (which === \'before\') {\n      return (\n        style.getPropertyValue(\'break-before\') ||\n        style.getPropertyValue(\'page-break-before\') ||\n        style.breakBefore ||\n        style.pageBreakBefore ||\n        \'\'\n      );\n    }\n    return (\n      style.getPropertyValue(\'break-after\') ||\n      style.getPropertyValue(\'page-break-after\') ||\n      style.breakAfter ||\n      style.pageBreakAfter ||\n      \'\'\n    );\n  }\n\n  function findNextElement(node) {\n    if (!node) return null;\n    let current = node;\n    while (current) {\n      if (current.nextElementSibling) return current.nextElementSibling;\n      current = current.parentElement;\n    }\n    return null;\n  }\n\n  function resolveNodeOffset(node) {\n    if (!node || typeof node.getBoundingClientRect !== \'function\') return 0;\n    const rect = node.getBoundingClientRect();\n    const scrollY = typeof window !== \'undefined\' ? window.scrollY || window.pageYOffset || 0 : 0;\n    return Math.max(0, rect.top + scrollY);\n  }\n\n  function collectManualBreakAnchors(root) {\n    if (!root || !root.ownerDocument) return [];\n    const anchors = [];\n    const seen = new Set();\n    const showElement = typeof NodeFilter !== \'undefined\' && NodeFilter.SHOW_ELEMENT ? NodeFilter.SHOW_ELEMENT : 1;\n    const walker = root.ownerDocument.createTreeWalker(root, showElement);\n\n    function pushAnchor(target) {\n      if (!target) return;\n      if (seen.has(target)) return;\n      seen.add(target);\n      anchors.push(target);\n    }\n\n    while (walker.nextNode()) {\n      const element = walker.currentNode;\n      const style = window.getComputedStyle ? window.getComputedStyle(element) : null;\n      if (!style) continue;\n      if (isForcedBreak(readBreakValue(style, \'before\'))) {\n        pushAnchor(element);\n      }\n      if (isForcedBreak(readBreakValue(style, \'after\'))) {\n        const next = findNextElement(element);\n        pushAnchor(next || TRAILING_BREAK_SENTINEL);\n      }\n    }\n    return anchors;\n  }\n\n  function buildPageStartOffsets(manualAnchors, usableHeightPx, contentHeight, totalPages) {\n    const offsets = [0];\n    const seenOffsets = new Set([0]);\n    manualAnchors.forEach((anchor) => {\n      let offset = null;\n      if (anchor === TRAILING_BREAK_SENTINEL) {\n        offset = contentHeight + usableHeightPx;\n      } else if (anchor && typeof anchor.getBoundingClientRect === \'function\') {\n        offset = resolveNodeOffset(anchor);\n      }\n      if (offset == null || !Number.isFinite(offset)) {\n        return;\n      }\n      const key = Math.round(offset * 1000) / 1000;\n      if (seenOffsets.has(key)) return;\n      seenOffsets.add(key);\n      offsets.push(offset);\n    });\n\n    offsets.sort((a, b) => a - b);\n\n    while (offsets.length < totalPages) {\n      const last = offsets[offsets.length - 1];\n      offsets.push(last + usableHeightPx);\n    }\n\n    return offsets;\n  }\n\n  function resolvePageIndexFromOffsets(offset, startOffsets) {\n    if (!startOffsets || !startOffsets.length) return 0;\n    let index = 0;\n    for (let i = 0; i < startOffsets.length; i += 1) {\n      if (offset >= startOffsets[i] - 0.5) {\n        index = i;\n      } else {\n        break;\n      }\n    }\n    return index;\n  }\n\n  function indexOfSection(node, sections) {\n    if (!node || !sections || !sections.length) return -1;\n    const target = typeof node.closest === \'function\' ? node.closest(\'.nr-key-section\') : null;\n    if (!target) return -1;\n    for (let i = 0; i < sections.length; i += 1) {\n      if (sections[i] === target) {\n        return i;\n      }\n    }\n    return -1;\n  }\n\n  function assignScreenText(node, text, key) {\n    if (!node) return;\n    const stringText = text == null ? \'\' : String(text);\n    if (node.getAttribute(\'aria-label\') === null) {\n      node.setAttribute(\'aria-label\', key === \'count\' ? \'Total page count\' : \'Current page number\');\n    }\n    node.setAttribute(\'data-nr-screen\', stringText);\n    if (key === \'count\') {\n      node.setAttribute(\'data-nr-total-pages\', stringText);\n    } else {\n      node.setAttribute(\'data-nr-page-estimate\', stringText);\n    }\n    node.setAttribute(\'data-nr-\' + key + \'-text\', stringText);\n    node.textContent = stringText;\n  }\n\n  function clearNodesForPrint() {\n    const nodes = lastPageNodes.concat(lastCountNodes);\n    nodes.forEach((node) => {\n      if (!node) return;\n      if (!node.hasAttribute(\'data-nr-print-cache\')) {\n        node.setAttribute(\'data-nr-print-cache\', node.textContent || \'\');\n      }\n      node.textContent = \'\';\n    });\n  }\n\n  function restoreNodesAfterPrint() {\n    const nodes = lastPageNodes.concat(lastCountNodes);\n    nodes.forEach((node) => {\n      if (!node) return;\n      const cached = node.getAttribute(\'data-nr-print-cache\');\n      if (cached != null) {\n        const key = node.getAttribute(\'data-nr-counter\') === \'pages\' ? \'count\' : \'page\';\n        const preferred = node.getAttribute(\'data-nr-\' + key + \'-text\');\n        node.textContent = preferred != null ? preferred : cached;\n        node.removeAttribute(\'data-nr-print-cache\');\n      }\n    });\n  }\n\n  function computeTotals() {\n    try {\n      const doc = document.documentElement;\n      const body = document.body;\n      if (!doc || !body) return;\n      const usableHeightMm = Math.max(METRICS.page_height_mm - (METRICS.margin_top_mm + METRICS.margin_bottom_mm), 0.1);\n      const usableHeightPx = usableHeightMm * PX_PER_MM;\n      const contentHeight = Math.max(\n        body.scrollHeight,\n        body.offsetHeight,\n        doc.scrollHeight,\n        doc.offsetHeight\n      );\n      const contentPages = Math.max(1, Math.ceil(contentHeight / usableHeightPx));\n      const manualAnchors = collectManualBreakAnchors(body);\n      const manualPages = manualAnchors.length > 0 ? manualAnchors.length + 1 : 1;\n      const totalPages = Math.max(contentPages, manualPages);\n      const startOffsets = buildPageStartOffsets(manualAnchors, usableHeightPx, contentHeight, totalPages);\n      const totalAsString = String(totalPages);\n      doc.setAttribute(\'data-nr-total-pages\', totalAsString);\n      const countNodes = Array.from(document.querySelectorAll(\'[data-nr-counter="pages"]\'));\n      countNodes.forEach((node) => assignScreenText(node, totalAsString, \'count\'));\n      const pageNodes = Array.from(document.querySelectorAll(\'[data-nr-counter="page"]\'));\n      const sections = Array.from(document.querySelectorAll(\'.nr-key-section\'));\n      pageNodes.forEach((node) => {\n        const sectionIndex = indexOfSection(node, sections);\n        let pageIndex;\n        if (sectionIndex >= 0) {\n          pageIndex = sectionIndex;\n        } else {\n          const offset = resolveNodeOffset(node);\n          pageIndex = resolvePageIndexFromOffsets(offset, startOffsets);\n        }\n        const pageNumber = Math.min(totalPages, Math.max(1, pageIndex + 1));\n        assignScreenText(node, String(pageNumber), \'page\');\n      });\n      lastPageNodes = pageNodes;\n      lastCountNodes = countNodes;\n    } catch (err) {\n      if (typeof console !== \'undefined\' && console.warn) {\n        console.warn(\'nr-page-counter: unable to compute preview counters\', err);\n      }\n    }\n  }\n\n  function scheduleCompute() {\n    computeTotals();\n    setTimeout(computeTotals, 180);\n  }\n\n  if (document.readyState === \'loading\') {\n    document.addEventListener(\'DOMContentLoaded\', () => {\n      scheduleCompute();\n    }, { once: true });\n  } else {\n    scheduleCompute();\n  }\n\n  if (typeof window !== \'undefined\') {\n    window.addEventListener(\'resize\', computeTotals, { passive: true });\n    window.addEventListener(\'beforeprint\', clearNodesForPrint);\n    window.addEventListener(\'afterprint\', () => {\n      restoreNodesAfterPrint();\n      scheduleCompute();\n    });\n    if (typeof window.matchMedia === \'function\') {\n      const mediaQuery = window.matchMedia(\'print\');\n      if (typeof mediaQuery.addEventListener === \'function\') {\n        mediaQuery.addEventListener(\'change\', (event) => {\n          if (event.matches) {\n            clearNodesForPrint();\n          } else {\n            restoreNodesAfterPrint();\n            scheduleCompute();\n          }\n        });\n      } else if (typeof mediaQuery.addListener === \'function\') {\n        mediaQuery.addListener((event) => {\n          if (event.matches) {\n            clearNodesForPrint();\n          } else {\n            restoreNodesAfterPrint();\n            scheduleCompute();\n          }\n        });\n      }\n    }\n  }\n})();\n</script>\n'
      - L1558 assign script_block = script_template.replace('__NR_METRICS__', metrics_json)
      - L1559 if '</body>' in updated:
        - L1560 assign updated = updated.replace('</body>', script_block + '</body>', 1)
        - L1562 else:
          - L1562 assign updated = updated + script_block
    - L1563 return updated
  - L1566 def best_rows_tbodyinner_html: str, allowed_tokens: set:
    - L1567 assign tbodys = list(re.finditer('(?is)<tbody\\b[^>]*>(.*?)</tbody>', inner_html))
    - L1568 assign best = (None, None, -1)
    - L1569 for m in tbodys:
      - L1570 assign tin = m.group(1)
      - L1571 assign hits = 0
      - L1572 for trm in re.finditer('(?is)<tr\\b[^>]*>.*?</tr>', tin):
        - L1573 assign tr_html = trm.group(0)
        - L1574 assign toks = re.findall('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', tr_html)
        - L1575 assign flat = [a.strip() if a else b.strip() for a, b in toks]
        - L1576 aug assign hits Add sum((1 for t in flat if t in allowed_tokens))
      - L1577 if hits > best[2]:
        - L1578 assign best = (m, tin, hits)
    - L1579 if best[0] is not None:
      - L1580 return (best[0], best[1])
    - L1581 return (tbodys[0], tbodys[0].group(1)) if tbodys else (None, None)
  - L1583 def find_row_templatetbody_inner: str, allowed_tokens: set:
    - L1584 for m in re.finditer('(?is)<tr\\b[^>]*>.*?</tr>', tbody_inner):
      - L1585 assign tr_html = m.group(0)
      - L1586 assign toks = re.findall('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', tr_html)
      - L1587 assign flat = []
      - L1588 for (a, b) in toks:
        - L1589 if a:
          - L1590 expr flat.append(a.strip())
        - L1591 if b:
          - L1592 expr flat.append(b.strip())
      - L1593 assign flat = [t for t in flat if t in allowed_tokens]
      - L1594 if flat:
        - L1595 return (tr_html, (m.start(0), m.end(0)), sorted(set(flat), key=len, reverse=True))
    - L1596 return (None, None, [])
  - L1598 def majority_table_for_tokenstokens, mapping:
    - L1599 from collections import Counter
    - L1601 assign tbls = []
    - L1602 for t in tokens:
      - L1603 assign tc = mapping.get(t, '')
      - L1604 if '.' in tc:
        - L1605 expr tbls.append(tc.split('.', 1)[0])
    - L1606 return Counter(tbls).most_common(1)[0][0] if tbls else None
  - L1609 def _extract_col_namemapping_value: str | None:
    - L1610 if not isinstance(mapping_value, str):
      - L1611 return None
    - L1612 assign target = mapping_value.strip()
    - L1613 if '.' not in target:
      - L1614 return None
    - L1615 return target.split('.', 1)[1].strip() or None
  - L1617 assign header_cols = sorted({col for t in HEADER_TOKENS for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
  - L1618 assign row_cols = sorted({col for t in ROW_TOKENS for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
  - L1620 assign totals_by_table = defaultdict(lambda: defaultdict(list))
  - L1621 assign total_token_to_target = {}
  - L1623 for (token, raw_target) in TOTALS.items():
    - L1624 assign target = (raw_target or PLACEHOLDER_TO_COL.get(token, '')).strip()
    - L1625 if not target or '.' not in target:
      - L1626 continue
    - L1627 assign (table_name, col_name) = [part.strip() for part in target.split('.', 1)]
    - L1628 if not table_name or not col_name:
      - L1629 continue
    - L1630 expr totals_by_table[table_name][col_name].append(token)
    - L1631 assign total_token_to_target[token] = (table_name, col_name)
  - L1633 def _coerce_total_valueraw:
    - L1634 if raw is None:
      - L1635 return (None, '0')
    - L1636 try:
      - L1637 assign decimal_value = Decimal(str(raw).strip())
      - L1638 except (InvalidOperation, ValueError, TypeError, AttributeError):
        - L1639 return (None, '0')
    - L1640 if not decimal_value.is_finite():
      - L1641 return (None, '0')
    - L1642 assign formatted = format_decimal_str(decimal_value, max_decimals=3) or '0'
    - L1643 return (float(decimal_value), formatted)
  - L1645 assign totals_accum = defaultdict(float)
  - L1646 assign last_totals_per_token = {token: '0' for token in TOTALS}
  - L1648 assign child_totals_cols = {col: list(tokens) for col, tokens in totals_by_table.get(child_table, {}).items()}
  - L1650 assign rendered_blocks = []
  - L1651 if generator_results is not None:
    - L1652 assign block_html = prototype_block
    - L1654 assign header_rows = generator_results.get('header') or []
    - L1655 assign header_row = header_rows[0] if header_rows else {}
    - L1656 for t in HEADER_TOKENS:
      - L1657 if t in header_row:
        - L1658 assign value = header_row[t]
        - L1659 assign block_html = sub_token(block_html, t, format_token_value(t, value))
    - L1661 assign allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)
    - L1662 assign rows_data = generator_results.get('rows') or []
    - L1663 annotated assign filtered_rows: list[dict[str, Any]] = []
    - L1664 annotated assign row_tokens_in_template: list[str] = []
    - L1666 if rows_data:
      - L1667 assign (tbody_m, tbody_inner) = best_rows_tbody(block_html, allowed_row_tokens)
      - L1668 if tbody_m and tbody_inner:
        - L1669 assign (row_template, row_span, row_tokens_in_template) = find_row_template(tbody_inner, allowed_row_tokens)
        - L1670 if row_template and row_tokens_in_template:
          - L1671 assign row_columns_template = [_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or '' for tok in row_tokens_in_template]
          - L1674 assign filtered_rows = _filter_rows_for_render(rows_data, row_tokens_in_template, row_columns_template, treat_all_as_data=bool(__force_single))
          - L1680 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
          - L1681 if __force_single:
            - L1682 expr _log_debug(f'[multi-debug] generator rows: total={len(rows_data)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
          - L1685 if filtered_rows:
            - L1686 annotated assign parts: list[str] = []
            - L1687 for row in filtered_rows:
              - L1688 assign tr = row_template
              - L1689 for tok in row_tokens_in_template:
                - L1690 assign val = _value_for_token(row, tok)
                - L1691 assign tr = sub_token(tr, tok, format_token_value(tok, val))
              - L1692 expr parts.append(tr)
            - L1693 assign new_tbody_inner = tbody_inner[:row_span[0]] + '\n'.join(parts) + tbody_inner[row_span[1]:]
            - L1694 assign block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]
        - L1696 else:
          - L1696 assign tr_tokens = [m.group(1) or m.group(2) for m in re.finditer('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', block_html)]
          - L1700 assign row_tokens_in_template = [t.strip() for t in tr_tokens if t and t.strip() in allowed_row_tokens]
          - L1701 if row_tokens_in_template:
            - L1702 assign row_columns_template = [_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or '' for tok in row_tokens_in_template]
            - L1705 assign filtered_rows = _filter_rows_for_render(rows_data, row_tokens_in_template, row_columns_template, treat_all_as_data=bool(__force_single))
            - L1711 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
            - L1712 if __force_single:
              - L1713 expr _log_debug(f'[multi-debug] generator rows (no tbody): total={len(rows_data)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
            - L1716 if filtered_rows:
              - L1717 assign parts = []
              - L1718 for row in filtered_rows:
                - L1719 assign tr = prototype_block
                - L1720 for tok in row_tokens_in_template:
                  - L1721 assign val = _value_for_token(row, tok)
                  - L1722 assign tr = sub_token(tr, tok, format_token_value(tok, val))
                - L1723 expr parts.append(tr)
              - L1724 assign block_html = '\n'.join(parts)
    - L1726 if filtered_rows:
      - L1727 assign totals_row = (generator_results.get('totals') or [{}])[0]
      - L1728 for token in TOTALS:
        - L1729 assign value = totals_row.get(token)
        - L1730 assign formatted = format_token_value(token, value)
        - L1731 assign block_html = sub_token(block_html, token, formatted)
        - L1732 assign last_totals_per_token[token] = formatted
        - L1733 assign target = total_token_to_target.get(token)
        - L1734 if target:
          - L1735 assign (fv, _formatted) = _coerce_total_value(value)
          - L1736 if fv is not None:
            - L1737 assign totals_accum[target] = totals_accum.get(target, 0.0) + fv
      - L1739 expr rendered_blocks.append(block_html)
      - L1741 else:
        - L1741 expr print('Generator SQL produced no usable row data after filtering; skipping block.')
    - L1743 else:
      - L1743 for batch_id in BATCH_IDS or []:
        - L1744 assign block_html = prototype_block
        - L1747 if header_cols:
          - L1748 if len(pcols) == 1:
            - L1749 assign sql = f"SELECT {', '.join((qident(c) for c in header_cols))} FROM {qident(parent_table)} WHERE {qident(pcols[0])} = ? AND {parent_where_clause} LIMIT 1"
            - L1755 assign hdr_params = (batch_id,) + tuple(PDATE) + parent_filter_values_tuple
            - L1757 else:
              - L1757 assign where = ' AND '.join([f'{qident(c)} = ?' for c in pcols])
              - L1758 assign sql = f"SELECT {', '.join((qident(c) for c in header_cols))} FROM {qident(parent_table)} WHERE {where} AND {parent_where_clause} LIMIT 1"
              - L1764 assign hdr_parts = _split_bid(batch_id, len(pcols))
              - L1765 assign hdr_params = tuple(hdr_parts) + tuple(PDATE) + parent_filter_values_tuple
          - L1767 assign con = sqlite3.connect(str(DB_PATH))
          - L1768 assign con.row_factory = sqlite3.Row
          - L1769 assign cur = con.cursor()
          - L1770 expr cur.execute(sql, hdr_params)
          - L1771 assign row = cur.fetchone()
          - L1772 expr con.close()
          - L1773 if row:
            - L1774 assign r = dict(row)
            - L1775 for t in HEADER_TOKENS:
              - L1776 if t in PLACEHOLDER_TO_COL:
                - L1777 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                - L1778 if not col:
                  - L1779 continue
                - L1780 assign val = r.get(col, '')
                - L1781 assign block_html = sub_token(block_html, t, format_token_value(t, val))
        - L1784 assign allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)
        - L1787 assign (tbody_m, tbody_inner) = best_rows_tbody(block_html, allowed_row_tokens)
        - L1788 if tbody_m and tbody_inner:
          - L1789 assign (row_template, row_span, row_tokens_in_template) = find_row_template(tbody_inner, allowed_row_tokens)
          - L1790 if row_template and row_tokens_in_template:
            - L1791 assign row_cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
            - L1800 if order_col.upper() != 'ROWID' and order_col not in row_cols_needed:
              - L1801 expr row_cols_needed.append(order_col)
            - L1803 assign order_clause = 'ORDER BY ROWID' if order_col.upper() == 'ROWID' else f'ORDER BY {qident(order_col)}, ROWID'
            - L1807 if len(ccols) == 1:
              - L1808 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause} {order_clause}"
              - L1814 assign row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
              - L1816 else:
                - L1816 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                - L1817 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {where} AND {child_where_clause} {order_clause}"
                - L1823 assign row_parts = _split_bid(batch_id, len(ccols))
                - L1824 assign row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple
            - L1826 assign con = sqlite3.connect(str(DB_PATH))
            - L1827 assign con.row_factory = sqlite3.Row
            - L1828 assign cur = con.cursor()
            - L1829 expr cur.execute(sql, row_params)
            - L1830 assign rows = [dict(r) for r in cur.fetchall()]
            - L1831 expr con.close()
            - L1834 if not rows:
              - L1835 assign maj_table = majority_table_for_tokens(row_tokens_in_template, PLACEHOLDER_TO_COL)
              - L1836 if maj_table:
                - L1837 assign date_col = DATE_COLUMNS.get(maj_table, '')
                - L1838 if date_col:
                  - L1839 assign cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
                  - L1847 if date_col not in cols_needed:
                    - L1848 expr cols_needed.append(date_col)
                  - L1849 assign sql_fb = f"SELECT {', '.join((qident(c) for c in cols_needed))} FROM {qident(maj_table)} WHERE datetime({qident(date_col)}) BETWEEN datetime(?) AND datetime(?) ORDER BY {qident(date_col)} ASC, ROWID ASC"
                  - L1855 assign con = sqlite3.connect(str(DB_PATH))
                  - L1856 assign con.row_factory = sqlite3.Row
                  - L1857 assign cur = con.cursor()
                  - L1858 expr cur.execute(sql_fb, (START_DATE, END_DATE))
                  - L1859 assign rows = [dict(r) for r in cur.fetchall()]
                  - L1860 expr con.close()
                  - L1861 expr print(f'Row fallback used: table={maj_table}, rows={len(rows)}')
            - L1863 if not rows:
              - L1864 expr print(f'No child rows found for batch {batch_id}; skipping block.')
              - L1865 continue
            - L1867 assign significant_cols = [col for col in row_cols_needed if col and (not any((keyword in col.lower() for keyword in ('row', 'serial', 'sl'))))]
            - L1872 assign filtered_rows = []
            - L1873 for r in rows:
              - L1874 if significant_cols and (not _row_has_significant_data(r, significant_cols)):
                - L1875 continue
              - L1876 expr filtered_rows.append(dict(r))
            - L1878 if not filtered_rows and rows:
              - L1879 assign filtered_rows = [dict(r) for r in rows]
            - L1880 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
            - L1881 if __force_single:
              - L1882 expr _log_debug(f'[multi-debug] sql rows: total={len(rows)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
            - L1885 if not filtered_rows:
              - L1886 expr print(f'No significant child rows for batch {batch_id}; skipping block.')
              - L1887 continue
            - L1889 expr _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)
            - L1891 annotated assign parts: list[str] = []
            - L1892 for r in filtered_rows:
              - L1893 assign tr = row_template
              - L1894 for t in row_tokens_in_template:
                - L1895 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                - L1896 if not col:
                  - L1897 continue
                - L1898 assign tr = sub_token(tr, t, format_token_value(t, r.get(col)))
              - L1899 expr parts.append(tr)
            - L1901 assign new_tbody_inner = tbody_inner[:row_span[0]] + '\n'.join(parts) + tbody_inner[row_span[1]:]
            - L1902 assign block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]
          - L1906 else:
            - L1906 assign tr_tokens = [m.group(1) or m.group(2) for m in re.finditer('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', block_html)]
            - L1910 assign tr_tokens = sorted({t.strip() for t in tr_tokens if t}, key=len, reverse=True)
            - L1912 assign row_tokens_in_template = [t for t in tr_tokens if t in allowed_row_tokens]
            - L1913 if row_tokens_in_template:
              - L1914 assign row_cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
              - L1922 if order_col.upper() != 'ROWID' and order_col not in row_cols_needed:
                - L1923 expr row_cols_needed.append(order_col)
              - L1924 assign order_clause = 'ORDER BY ROWID' if order_col.upper() == 'ROWID' else f'ORDER BY {qident(order_col)}, ROWID'
              - L1928 if len(ccols) == 1:
                - L1929 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause} {order_clause}"
                - L1935 assign row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
                - L1937 else:
                  - L1937 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                  - L1938 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {where} AND {child_where_clause} {order_clause}"
                  - L1944 assign row_parts = _split_bid(batch_id, len(ccols))
                  - L1945 assign row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple
              - L1947 assign con = sqlite3.connect(str(DB_PATH))
              - L1948 assign con.row_factory = sqlite3.Row
              - L1949 assign cur = con.cursor()
              - L1950 expr cur.execute(sql, row_params)
              - L1951 assign rows = [dict(r) for r in cur.fetchall()]
              - L1952 expr con.close()
              - L1954 assign significant_cols = [col for col in row_cols_needed if col and (not any((keyword in col.lower() for keyword in ('row', 'serial', 'sl'))))]
              - L1959 assign filtered_rows = []
              - L1960 for r in rows:
                - L1961 if significant_cols and (not _row_has_significant_data(r, significant_cols)):
                  - L1962 continue
                - L1963 expr filtered_rows.append(dict(r))
              - L1965 if not filtered_rows and rows:
                - L1966 assign filtered_rows = [dict(r) for r in rows]
              - L1967 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
              - L1968 if __force_single:
                - L1969 expr _log_debug(f'[multi-debug] sql rows (no tbody): total={len(rows)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
              - L1972 if not filtered_rows:
                - L1973 expr print(f'No significant child rows (no tbody path) for batch {batch_id}; skipping block.')
                - L1974 continue
              - L1976 expr _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)
              - L1978 assign parts = []
              - L1979 for r in filtered_rows:
                - L1980 assign tr = prototype_block
                - L1981 for t in row_tokens_in_template:
                  - L1982 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                  - L1983 if not col:
                    - L1984 continue
                  - L1985 assign tr = sub_token(tr, t, format_token_value(t, r.get(col)))
                - L1986 expr parts.append(tr)
              - L1988 assign block_html = '\n'.join(parts)
        - L1991 assign batch_total_values = {token: '0' for token in TOTALS}
        - L1993 if child_totals_cols:
          - L1994 assign child_cols = sorted(child_totals_cols.keys())
          - L1995 if child_cols:
            - L1996 assign exprs = ', '.join([f'COALESCE(SUM({qident(c)}),0) AS {qident(c)}' for c in child_cols])
            - L1998 if len(ccols) == 1:
              - L1999 assign sql = f'SELECT {exprs} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause}'
              - L2004 assign tot_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
              - L2006 else:
                - L2006 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                - L2007 assign sql = f'SELECT {exprs} FROM {qident(child_table)} WHERE {where} AND {child_where_clause}'
                - L2010 assign tot_parts = _split_bid(batch_id, len(ccols))
                - L2011 assign tot_params = tuple(tot_parts) + tuple(CDATE) + child_filter_values_tuple
            - L2013 assign con = sqlite3.connect(str(DB_PATH))
            - L2014 assign con.row_factory = sqlite3.Row
            - L2015 assign cur = con.cursor()
            - L2016 expr cur.execute(sql, tot_params)
            - L2017 assign sums = dict(cur.fetchone() or {})
            - L2018 expr con.close()
            - L2020 for col in child_cols:
              - L2021 assign raw_val = sums.get(col, 0)
              - L2022 assign (fv, formatted) = _coerce_total_value(raw_val)
              - L2023 if fv is not None:
                - L2024 assign key = (child_table, col)
                - L2025 assign totals_accum[key] = totals_accum.get(key, 0.0) + fv
              - L2026 for token in child_totals_cols[col]:
                - L2027 assign batch_total_values[token] = formatted
        - L2029 for (token, value) in batch_total_values.items():
          - L2030 assign block_html = sub_token(block_html, token, value)
          - L2031 assign last_totals_per_token[token] = value
        - L2033 expr rendered_blocks.append(block_html)
  - L2036 assign rows_rendered = bool(rendered_blocks)
  - L2037 if not rows_rendered:
    - L2038 expr print('No rendered blocks generated for this selection.')
  - L2040 assign html_multi = shell_prefix + '\n'.join(rendered_blocks) + shell_suffix
  - L2042 for (tok, val) in post_literal_specials.items():
    - L2043 assign html_multi = sub_token(html_multi, tok, val if val is not None else '')
  - L2045 if page_number_tokens or page_count_tokens or page_label_tokens:
    - L2046 assign html_multi = _inject_page_counter_spans(html_multi, page_number_tokens, page_count_tokens, page_label_tokens)
  - L2048 if total_token_to_target:
    - L2049 assign overall_formatted = {}
    - L2050 for ((table_name, col_name), total) in totals_accum.items():
      - L2051 assign (_, formatted) = _coerce_total_value(total)
      - L2052 assign overall_formatted[table_name, col_name] = formatted
    - L2054 for (token, target) in total_token_to_target.items():
      - L2055 assign (table_name, col_name) = target
      - L2056 assign value = overall_formatted.get((table_name, col_name), last_totals_per_token.get(token, '0'))
      - L2057 assign html_multi = sub_token(html_multi, token, value)
  - L2060 for (t, s) in LITERALS.items():
    - L2061 assign html_multi = sub_token(html_multi, t, s)
  - L2064 assign ALL_KNOWN_TOKENS = set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())
  - L2065 assign html_multi = blank_known_tokens(html_multi, ALL_KNOWN_TOKENS)
  - L2068 expr OUT_HTML.write_text(html_multi, encoding='utf-8')
  - L2069 expr print('Wrote HTML:', OUT_HTML)
  - L2071 expr print('BATCH_IDS:', len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])
  - L2073 expr asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent))
  - L2074 expr print('Wrote PDF via Playwright:', OUT_PDF)
  - L2076 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': rows_rendered}
- L2080 if __name__ == '__main__':
  - L2081 expr print('Module ready for API integration. Call fill_and_print(...) from your FastAPI endpoint.')

## backend\app\services\reports\ReportGenerateExcel.py
- L2 docstring: "Excel-specific report generation pipeline.\n\nThis module mirrors the baseline `..."
- L9 import asyncio
- L10 import contextlib
- L11 import json
- L12 import re
- L13 from ..dataframes import sqlite_shim as sqlite3
- L14 from collections import defaultdict
- L15 from datetime import datetime
- L16 from decimal import Decimal, InvalidOperation
- L17 from itertools import product
- L18 from pathlib import Path
- L19 from typing import Any, Iterable, Mapping, Sequence
- L21 try:
  - L22 from PIL import Image
  - L23 except ImportError:
    - L24 assign Image = None
- L26 try:
  - L27 import numpy as np
  - L28 except ImportError:
    - L29 assign np = None
- L31 try:
  - L32 import cv2
  - L33 except ImportError:
    - L34 assign cv2 = None
- L36 try:
  - L37 import fitz
  - L38 except ImportError:
    - L39 assign fitz = None
- L41 try:
  - L42 from skimage.metrics import structural_similarity as ssim
  - L43 except ImportError:
    - L44 assign ssim = None
- L46 try:
  - L47 from playwright.async_api import async_playwright
  - L48 except ImportError:
    - L49 assign async_playwright = None
- L51 from .contract_adapter import ContractAdapter, format_decimal_str
- L52 from .date_utils import get_col_type, mk_between_pred_for_date
- L53 from .discovery_excel import discover_batches_and_counts as discover_batches_excel
- L54 from ..dataframes import DuckDBDataFrameQuery, SQLiteDataFrameLoader
- L55 from .common_helpers import _format_for_token, _has_time_component, _parse_date_like, _raise_no_block, _segment_has_any_token, _select_prototype_block, _strip_found_block, _token_regex, _find_or_infer_batch_block, _find_rowish_block, sub_token, blank_known_tokens
- L69 from .common_helpers import _format_for_token, _has_time_component, _parse_date_like, _raise_no_block, _segment_has_any_token, _select_prototype_block, _strip_found_block, _token_regex, _find_or_infer_batch_block, _find_rowish_block, html_without_batch_blocks
- L82 assign _TABLE_RE = re.compile('(?is)<table\\b[^>]*>(?P<body>.*?)</table>')
- L83 assign _ROW_RE = re.compile('(?is)<tr\\b[^>]*>(?P<body>.*?)</tr>')
- L84 assign _CELL_RE = re.compile('(?is)<t(?:d|h)\\b[^>]*>.*?</t(?:d|h)>')
- L85 assign _ROW_ONLY_RE = re.compile('(?is)<tr\\b[^>]*>')
- L86 assign _COLSPAN_RE = re.compile('(?is)colspan\\s*=\\s*["\\\']?(\\d+)["\\\']?')
- L88 assign _EXCEL_PRINT_STYLE_ID = 'excel-print-sizing'
- L89 assign _EXCEL_PRINT_MARGIN_MM = 10
- L90 assign _DEFAULT_EXCEL_PRINT_SCALE = 0.82
- L91 assign _MIN_EXCEL_PRINT_SCALE = 0.4
- L92 assign _MAX_EXCEL_PRINT_SCALE = 0.97
- L93 assign _PAGE_HEIGHT_PX = 980
- L94 assign _BASE_ROW_HEIGHT_PX = 28
- L95 assign _EXCEL_STYLE_BLOCK_RE = re.compile('(?is)<style\\b[^>]*id=["\\\']excel-print-sizing["\\\'][^>]*>.*?</style>')
- L97 assign _DATE_PARAM_START_ALIASES = {'start_ts_utc', 'start_ts', 'start_timestamp', 'start_datetime', 'start_date', 'start_dt', 'start_iso', 'start_date_utc', 'from_ts_utc', 'from_ts', 'from_timestamp', 'from_datetime', 'from_date', 'from_dt', 'from_iso', 'from_date_utc', 'range_start', 'period_start'}
- L118 assign _DATE_PARAM_END_ALIASES = {'end_ts_utc', 'end_ts', 'end_timestamp', 'end_datetime', 'end_date', 'end_dt', 'end_iso', 'end_date_utc', 'to_ts_utc', 'to_ts', 'to_timestamp', 'to_datetime', 'to_date', 'to_dt', 'to_iso', 'to_date_utc', 'range_end', 'period_end'}
- L140 def _clamp_excel_print_scalescale: float | None:
  - L141 if scale is None or not isinstance(scale, (float, int)):
    - L142 return _DEFAULT_EXCEL_PRINT_SCALE
  - L143 return max(_MIN_EXCEL_PRINT_SCALE, min(float(scale), _MAX_EXCEL_PRINT_SCALE))
- L146 def _estimate_excel_print_scalecolumn_count: int:
  - L147 assign count = max(0, int(column_count))
  - L148 if count <= 8:
    - L149 return 0.98
  - L150 if count <= 12:
    - L151 return 0.9
  - L152 if count <= 16:
    - L153 return 0.82
  - L154 if count <= 20:
    - L155 return 0.74
  - L156 if count <= 24:
    - L157 return 0.68
  - L158 if count <= 30:
    - L159 return 0.6
  - L160 if count <= 36:
    - L161 return 0.52
  - L162 if count <= 44:
    - L163 return 0.48
  - L164 return 0.44
- L167 def _count_table_columnshtml_text: str:
  - L168 assign max_cols = 0
  - L169 for table_match in _TABLE_RE.finditer(html_text or ''):
    - L170 assign table_body = table_match.group('body') or ''
    - L171 for row_match in _ROW_RE.finditer(table_body):
      - L172 assign row_html = row_match.group('body') or ''
      - L173 assign total = 0
      - L174 for cell_match in _CELL_RE.finditer(row_html):
        - L175 assign cell_html = cell_match.group(0)
        - L176 assign span_match = _COLSPAN_RE.search(cell_html)
        - L177 assign span = 1
        - L178 if span_match:
          - L179 try:
            - L180 assign span = max(1, int(span_match.group(1)))
            - L181 except (TypeError, ValueError):
              - L182 assign span = 1
        - L183 aug assign total Add span
      - L184 if total:
        - L185 assign max_cols = max(max_cols, total)
    - L186 if max_cols:
      - L187 break
  - L188 return max_cols
- L191 def _count_table_rowshtml_text: str:
  - L192 return len(_ROW_ONLY_RE.findall(html_text or ''))
- L195 def _estimate_rows_per_pagescale: float:
  - L196 assign clamped = _clamp_excel_print_scale(scale)
  - L197 if clamped <= 0:
    - L198 return 1
  - L199 assign capacity = int(_PAGE_HEIGHT_PX / _BASE_ROW_HEIGHT_PX / clamped)
  - L200 return max(1, capacity)
- L203 def _inject_excel_print_styleshtml_text: str, *, scale: float | None=None, rows_per_page: int | None=None:
  - L209 docstring: "\n    Ensure Excel-generated reports include print-specific CSS that\n    switch..."
  - L213 assign scale_value = _clamp_excel_print_scale(scale)
  - L214 assign scale_str = f'{scale_value:.4f}'.rstrip('0').rstrip('.')
  - L215 assign pagination_css = ''
  - L216 if rows_per_page and rows_per_page > 0:
    - L217 assign pagination_css = f'@media print {{\n  tbody tr:nth-of-type({rows_per_page}n+1):not(:first-child) {{\n    break-before: page;\n    page-break-before: always;\n  }}\n}}\n'
  - L226 assign style_block = f'\n<style id="{_EXCEL_PRINT_STYLE_ID}">\n:root {{ --excel-print-scale: {scale_str}; }}\n@page {{\n  size: A4 landscape;\n  margin: {_EXCEL_PRINT_MARGIN_MM}mm;\n}}\nhtml, body {{\n  margin: 0 auto;\n  padding: 0;\n  background: #fff;\n  max-width: calc(100% / var(--excel-print-scale));\n}}\nbody {{\n  width: calc(100% / var(--excel-print-scale));\n  min-height: calc(100% / var(--excel-print-scale));\n  transform-origin: top left;\n}}\ntable {{\n  width: 100% !important;\n  table-layout: fixed;\n  border-collapse: collapse;\n}}\nth, td {{\n  word-break: break-word;\n  white-space: normal;\n}}\n@media screen {{\n  body {{\n    transform: scale(var(--excel-print-scale));\n  }}\n}}\n@media print {{\n  body {{\n    transform: scale(var(--excel-print-scale));\n    zoom: 1;\n  }}\n}}\n{pagination_css}</style>\n'
  - L268 if _EXCEL_STYLE_BLOCK_RE.search(html_text):
    - L269 return _EXCEL_STYLE_BLOCK_RE.sub(style_block, html_text, count=1)
  - L271 assign head_close = re.search('(?is)</head>', html_text)
  - L272 if head_close:
    - L273 assign idx = head_close.start()
    - L274 return f'{html_text[:idx]}{style_block}{html_text[idx:]}'
  - L276 return f'{style_block}{html_text}'
- L284 def fill_and_printOBJ: dict, TEMPLATE_PATH: Path, DB_PATH: Path, OUT_HTML: Path, OUT_PDF: Path, START_DATE: str, END_DATE: str, batch_ids: list[str] | None=None, KEY_VALUES: dict | None=None, GENERATOR_BUNDLE: dict | None=None, __force_single: bool=False:
  - L297 docstring: "\n    DB-driven renderer:\n      - Assumes TEMPLATE_PATH is already the *final s..."
  - L308 for name in ('OBJ', 'TEMPLATE_PATH', 'DB_PATH', 'START_DATE', 'END_DATE'):
    - L309 if locals().get(name) is None:
      - L310 raise NameError(f'Missing required variable: `{name}`')
  - L313 assign OUT_DIR = OUT_HTML.parent
  - L314 expr OUT_DIR.mkdir(parents=True, exist_ok=True)
  - L316 assign log_file_path = Path(__file__).with_name('fill_and_print.log')
  - L318 def _log_debug*parts: object:
    - L319 assign message = ' '.join((str(part) for part in parts))
    - L320 expr print(message)
    - L321 try:
      - L322 with log_file_path.open('a', encoding='utf-8') as fh:
        - L323 expr fh.write(f'[{datetime.now().isoformat()}] {message}\n')
      - L324 except Exception:
        - L325 pass
  - L327 expr _log_debug('=== fill_and_print call ===', 'force_single' if __force_single else 'fanout_root', 'KEY_VALUES raw=', KEY_VALUES or {})
  - L335 assign html = TEMPLATE_PATH.read_text(encoding='utf-8')
  - L337 assign dataframe_loader = SQLiteDataFrameLoader(DB_PATH)
  - L339 assign TOKEN_RE = re.compile('\\{\\{?\\s*([A-Za-z0-9_\\-\\.]+)\\s*\\}\\}?')
  - L340 assign TEMPLATE_TOKENS = {m.group(1) for m in TOKEN_RE.finditer(html)}
  - L343 assign OBJ = OBJ or {}
  - L344 assign contract_adapter = ContractAdapter(OBJ)
  - L345 assign PLACEHOLDER_TO_COL = contract_adapter.mapping
  - L346 assign param_token_set = {token for token in contract_adapter.param_tokens or [] if token}
  - L348 assign join_raw = OBJ.get('join', {}) or {}
  - L349 assign JOIN = {'parent_table': contract_adapter.parent_table or join_raw.get('parent_table', ''), 'child_table': contract_adapter.child_table or join_raw.get('child_table', ''), 'parent_key': contract_adapter.parent_key or join_raw.get('parent_key', ''), 'child_key': contract_adapter.child_key or join_raw.get('child_key', '')}
  - L356 assign DATE_COLUMNS = contract_adapter.date_columns or (OBJ.get('date_columns', {}) or {})
  - L358 assign HEADER_TOKENS = contract_adapter.scalar_tokens or OBJ.get('header_tokens', [])
  - L359 assign ROW_TOKENS = contract_adapter.row_tokens or OBJ.get('row_tokens', [])
  - L360 assign row_token_count = sum((1 for tok in ROW_TOKENS if isinstance(tok, str) and tok.strip()))
  - L361 assign TOTALS = contract_adapter.totals_mapping or OBJ.get('totals', {})
  - L362 assign ROW_ORDER = contract_adapter.row_order or OBJ.get('row_order', ['ROWID'])
  - L363 assign LITERALS = {str(token): '' if value is None else str(value) for token, value in (OBJ.get('literals', {}) or {}).items()}
  - L366 assign FORMATTERS = contract_adapter.formatters
  - L367 annotated assign key_values_map: dict[str, list[str]] = {}
  - L368 if KEY_VALUES:
    - L369 for (token, raw_value) in KEY_VALUES.items():
      - L370 assign name = str(token or '').strip()
      - L371 if not name:
        - L372 continue
      - L373 annotated assign values: list[str] = []
      - L374 if isinstance(raw_value, (list, tuple, set)):
        - L375 assign seen = set()
        - L376 for item in raw_value:
          - L377 assign text = str(item or '').strip()
          - L378 if text and text not in seen:
            - L379 expr seen.add(text)
            - L380 expr values.append(text)
        - L382 else:
          - L382 assign text = str(raw_value or '').strip()
          - L383 if text:
            - L384 assign values = [text]
      - L385 if values:
        - L386 assign key_values_map[name] = values
  - L388 assign _DIRECT_COLUMN_RE = re.compile('^(?P<table>[A-Za-z_][\\w]*)\\.(?P<column>[A-Za-z_][\\w]*)$')
  - L389 assign _SQL_IDENT_RE = re.compile('^[A-Za-z_][A-Za-z0-9_]*$')
  - L391 def _safe_identname: str:
    - L392 if _SQL_IDENT_RE.match(name):
      - L393 return name
    - L394 assign safe = str(name).replace('"', '""')
    - L395 return f'"{safe}"'
  - L397 def _resolve_token_columntoken: str:
    - L398 assign mapping_expr = PLACEHOLDER_TO_COL.get(token)
    - L399 if isinstance(mapping_expr, str):
      - L400 assign match = _DIRECT_COLUMN_RE.match(mapping_expr.strip())
      - L401 if match:
        - L402 return (match.group('table'), match.group('column'))
    - L403 assign required_filters = contract_adapter.required_filters
    - L404 assign optional_filters = contract_adapter.optional_filters
    - L405 assign filter_expr = (required_filters.get(token) or optional_filters.get(token) or '').strip()
    - L406 assign match = _DIRECT_COLUMN_RE.match(filter_expr)
    - L407 if match:
      - L408 return (match.group('table'), match.group('column'))
    - L409 return None
  - L411 def _canonicalize_casetable: str, column: str, raw_value: str:
    - L412 assign normalized_table = str(table or '').strip().lower()
    - L413 assign normalized_column = str(column or '').strip().lower()
    - L414 assign normalized_value = str(raw_value or '').strip()
    - L415 assign cache_key = (normalized_table, normalized_column, normalized_value.lower())
    - L416 if cache_key in _canonicalize_cache:
      - L417 return _canonicalize_cache[cache_key]
    - L418 assign canonical = normalized_value
    - L419 if not normalized_table or not normalized_column or (not normalized_value):
      - L420 assign _canonicalize_cache[cache_key] = canonical
      - L421 return canonical
    - L422 try:
      - L423 assign frame = dataframe_loader.frame(table)
      - L424 except Exception:
        - L425 assign _canonicalize_cache[cache_key] = canonical
        - L426 return canonical
    - L427 if column not in frame.columns:
      - L428 assign _canonicalize_cache[cache_key] = canonical
      - L429 return canonical
    - L430 assign series = frame[column]
    - L431 try:
      - L432 assign matches = series.dropna().astype(str)
      - L433 except Exception:
        - L434 assign matches = series.dropna().apply(lambda v: str(v))
    - L435 assign lower_target = normalized_value.lower()
    - L436 assign mask = matches.str.lower() == lower_target
    - L437 assign filtered = matches[mask]
    - L438 if not filtered.empty:
      - L439 assign canonical = str(filtered.iloc[0])
    - L440 assign _canonicalize_cache[cache_key] = canonical
    - L441 return canonical
  - L443 annotated assign _canonicalize_cache: dict[tuple[str, str, str], str] = {}
  - L445 for (token, values) in list(key_values_map.items()):
    - L446 assign resolved = _resolve_token_column(token)
    - L447 if not resolved:
      - L448 continue
    - L449 assign (table_name, column_name) = resolved
    - L450 if not table_name or not column_name:
      - L451 continue
    - L452 annotated assign updated_values: list[str] = []
    - L453 assign changed = False
    - L454 for value in values:
      - L455 if not isinstance(value, str) or not value.strip():
        - L456 expr updated_values.append(value)
        - L457 continue
      - L458 assign canon = _canonicalize_case(table_name, column_name, value.strip())
      - L459 if canon != value:
        - L460 assign changed = True
      - L461 expr updated_values.append(canon)
    - L462 if changed:
      - L463 assign key_values_map[token] = updated_values
  - L465 for (token, values) in key_values_map.items():
    - L466 assign LITERALS[token] = ', '.join(values)
  - L468 annotated assign alias_link_map: dict[str, str] = {}
  - L469 assign recipe_key_values = key_values_map.get('row_recipe_code')
  - L470 if recipe_key_values:
    - L471 assign alias_link_map = {'recipe_code': 'row_recipe_code', 'filter_recipe_code': 'row_recipe_code'}
    - L475 assign literal_value = ', '.join(recipe_key_values)
    - L476 for alias in alias_link_map.keys():
      - L477 assign LITERALS[alias] = literal_value
  - L479 assign multi_key_selected = any((len(values) > 1 for values in key_values_map.values()))
  - L481 def _first_alias_valuetoken: str:
    - L482 assign source = alias_link_map.get(token)
    - L483 if not source:
      - L484 return None
    - L485 return _first_key_value(key_values_map.get(source, []))
  - L487 def _apply_alias_paramstarget: dict[str, Any]:
    - L488 if not alias_link_map:
      - L489 return None
    - L490 for alias in alias_link_map:
      - L491 if alias in target and str(target[alias] or '').strip():
        - L492 continue
      - L493 assign alias_value = _first_alias_value(alias)
      - L494 if alias_value is not None:
        - L495 assign target[alias] = alias_value
  - L497 expr _log_debug('Normalized key_values_map', key_values_map, 'multi_key_selected', multi_key_selected)
  - L499 def _first_key_valuevalues: list[str]:
    - L500 for val in values:
      - L501 assign text = str(val or '').strip()
      - L502 if text:
        - L503 return text
    - L504 return None
  - L506 def _iter_key_combinationsvalues_map: dict[str, list[str]]:
    - L507 if not values_map:
      - L508 expr (yield {})
      - L509 return None
    - L510 annotated assign tokens: list[str] = []
    - L511 annotated assign value_lists: list[list[str]] = []
    - L512 for (token, raw_values) in values_map.items():
      - L513 annotated assign unique: list[str] = []
      - L514 annotated assign seen_local: set[str] = set()
      - L515 for val in raw_values:
        - L516 assign text = str(val or '').strip()
        - L517 if not text or text in seen_local:
          - L518 continue
        - L519 expr seen_local.add(text)
        - L520 expr unique.append(text)
      - L521 if unique:
        - L522 expr tokens.append(token)
        - L523 expr value_lists.append(unique)
    - L524 if not tokens:
      - L525 expr (yield {})
      - L526 return None
    - L527 for combo in product(*value_lists):
      - L528 expr (yield {token: value for token, value in zip(tokens, combo)})
  - L530 async def html_to_pdf_asynchtml_path: Path, pdf_path: Path, base_dir: Path, pdf_scale: float | None=None:
    - L531 if async_playwright is None:
      - L532 expr print('Playwright not available; skipping PDF generation.')
      - L533 return None
    - L535 assign html_source = html_path.read_text(encoding='utf-8', errors='ignore')
    - L536 assign base_url = (base_dir or html_path.parent).as_uri()
    - L538 async with async_playwright() as p:
      - L539 assign browser = await p.chromium.launch()
      - L540 assign context = None
      - L541 try:
        - L542 assign context = await browser.new_context(base_url=base_url)
        - L543 assign page = await context.new_page()
        - L544 expr await page.set_content(html_source, wait_until='networkidle')
        - L545 expr await page.emulate_media(media='print')
        - L546 assign scale_value = pdf_scale or 1.0
        - L547 if not isinstance(scale_value, (int, float)):
          - L548 assign scale_value = 1.0
        - L549 assign scale_value = max(0.1, min(float(scale_value), 2.0))
        - L550 expr await page.pdf(path=str(pdf_path), format='A4', landscape=True, print_background=True, margin={'top': '10mm', 'right': '10mm', 'bottom': '10mm', 'left': '10mm'}, prefer_css_page_size=True, scale=scale_value)
        - L560 finally:
          - L560 if context is not None:
            - L561 expr await context.close()
          - L562 expr await browser.close()
  - L564 def _combine_html_documentshtml_sections: list[str]:
    - L565 if not html_sections:
      - L566 return ''
    - L567 annotated assign combined_body: list[str] = []
    - L568 assign doc_type = ''
    - L569 assign head_html = ''
    - L571 assign head_pattern = re.compile('(?is)<head\\b[^>]*>(?P<head>.*)</head>')
    - L572 assign body_pattern = re.compile('(?is)<body\\b[^>]*>(?P<body>.*)</body>')
    - L573 assign doctype_pattern = re.compile('(?is)^\\s*<!DOCTYPE[^>]*>', re.MULTILINE)
    - L575 for (idx, raw_html) in enumerate(html_sections):
      - L576 assign text = raw_html or ''
      - L577 if idx == 0:
        - L578 assign doctype_match = doctype_pattern.search(text)
        - L579 if doctype_match:
          - L580 assign doc_type = doctype_match.group(0).strip()
          - L581 assign text = text[doctype_match.end():]
        - L582 assign head_match = head_pattern.search(text)
        - L583 if head_match:
          - L584 assign head_html = head_match.group(0).strip()
        - L585 assign body_match = body_pattern.search(text)
        - L586 if body_match:
          - L587 assign section_body = body_match.group('body').strip()
          - L589 else:
            - L589 assign section_body = text.strip()
        - L590 expr combined_body.append(f'<div class="nr-key-section" data-nr-section="1">\n{section_body}\n</div>')
        - L592 else:
          - L592 assign body_match = body_pattern.search(text)
          - L593 assign section = body_match.group('body').strip() if body_match else text.strip()
          - L594 expr combined_body.append(f'<div class="nr-key-section" data-nr-section="{idx + 1}" style="page-break-before: always;">\n{section}\n</div>')
    - L598 assign doc_lines = []
    - L599 if doc_type:
      - L600 expr doc_lines.append(doc_type)
    - L601 expr doc_lines.append('<html>')
    - L602 if head_html:
      - L603 expr doc_lines.append(head_html)
    - L604 expr doc_lines.append('<body>')
    - L605 expr doc_lines.append('\n\n'.join(combined_body))
    - L606 expr doc_lines.append('</body>')
    - L607 expr doc_lines.append('</html>')
    - L608 return '\n'.join(doc_lines)
  - L610 def _value_has_contentvalue: Any:
    - L611 if value is None:
      - L612 return False
    - L613 if isinstance(value, (int, float, Decimal)):
      - L614 return value != 0
    - L615 assign text = str(value).strip()
    - L616 if not text:
      - L617 return False
    - L618 try:
      - L619 assign num = Decimal(text)
      - L620 except Exception:
        - L621 return True
      - L623 else:
        - L623 return num != 0
  - L625 def _row_has_significant_datarow: Mapping[str, Any], columns: list[str]:
    - L626 return _row_has_any_data(row, (), columns)
  - L628 def _token_values_have_datarow: Mapping[str, Any], tokens: list[str]:
    - L629 return _row_has_any_data(row, tokens, ())
  - L631 def _row_has_any_datarow: Mapping[str, Any], tokens: Sequence[str], columns: Sequence[str]:
    - L632 for token in tokens:
      - L633 if not token:
        - L634 continue
      - L635 if _value_has_content(_value_for_token(row, token)):
        - L636 return True
    - L637 for col in columns:
      - L638 if not col:
        - L639 continue
      - L640 if _value_has_content(row.get(col)):
        - L641 return True
    - L642 return False
  - L644 def _is_counter_fieldname: str | None:
    - L645 if not name:
      - L646 return False
    - L647 if not isinstance(name, str):
      - L648 assign name = str(name)
    - L649 assign normalized = re.sub('[^a-z0-9]', '', name.lower())
    - L650 if not normalized:
      - L651 return False
    - L652 if normalized in {'row', 'rowid', 'rowno', 'rownum', 'rownumber', 'rowindex', 'rowcounter', 'srno', 'sno'}:
      - L663 return True
    - L664 assign counter_markers = ('serial', 'sequence', 'seq', 'counter')
    - L665 if any((marker in normalized for marker in counter_markers)):
      - L666 return True
    - L667 assign counter_suffixes = ('slno', 'srno', 'sno', 'snum', 'snumber', 'sl', 'no', 'num', 'number', 'idx', 'index')
    - L680 return any((normalized.endswith(suffix) and normalized.startswith('row') for suffix in counter_suffixes))
  - L682 def _reindex_serial_fieldsrows: list[dict], tokens: Sequence[str], columns: Sequence[str]:
    - L683 assign serial_tokens = [tok for tok in tokens if tok and _is_counter_field(tok)]
    - L684 assign serial_columns = [col for col in columns if col and _is_counter_field(col)]
    - L685 if not serial_tokens and (not serial_columns):
      - L686 return None
    - L687 for (idx, row) in enumerate(rows, start=1):
      - L688 for tok in serial_tokens:
        - L689 assign row[tok] = idx
      - L690 for col in serial_columns:
        - L691 assign row[col] = idx
  - L693 def _value_for_tokenrow: Mapping[str, Any], token: str:
    - L694 if not token:
      - L695 return None
    - L696 if token in row:
      - L697 return row[token]
    - L698 assign normalized = str(token).lower()
    - L699 for key in row.keys():
      - L700 if isinstance(key, str) and key.lower() == normalized:
        - L701 return row[key]
    - L702 assign mapped = PLACEHOLDER_TO_COL.get(token)
    - L703 if mapped:
      - L704 assign col = _extract_col_name(mapped)
      - L705 if col:
        - L706 if col in row:
          - L707 return row[col]
        - L708 for key in row.keys():
          - L709 if isinstance(key, str) and key.lower() == col.lower():
            - L710 return row[key]
    - L711 return None
  - L713 def _prune_placeholder_rowsrows: Sequence[Mapping[str, Any]], tokens: Sequence[str]:
    - L714 assign material_tokens = [tok for tok in tokens if tok and 'material' in tok.lower()]
    - L715 annotated assign pruned: list[dict[str, Any]] = []
    - L716 for row in rows:
      - L717 assign keep = True
      - L718 for tok in material_tokens:
        - L719 if not _value_has_content(_value_for_token(row, tok)):
          - L720 assign keep = False
          - L721 break
      - L722 if keep:
        - L723 expr pruned.append(dict(row))
    - L724 return pruned if pruned else [dict(row) for row in rows]
  - L726 def _filter_rows_for_renderrows: Sequence[Mapping[str, Any]], row_tokens_template: Sequence[str], row_columns: Sequence[str], *, treat_all_as_data: bool:
    - L733 if not rows:
      - L734 return []
    - L736 if treat_all_as_data:
      - L737 assign prepared = [dict(row) for row in rows]
      - L739 else:
        - L739 assign significant_tokens = [tok for tok in row_tokens_template if tok and (not _is_counter_field(tok))]
        - L740 assign significant_columns = [col for col in row_columns if col and (not _is_counter_field(col))]
        - L741 annotated assign prepared: list[dict[str, Any]] = []
        - L742 for row in rows:
          - L743 if significant_tokens or significant_columns:
            - L744 if not _row_has_any_data(row, significant_tokens, significant_columns):
              - L745 continue
          - L746 expr prepared.append(dict(row))
    - L748 if prepared:
      - L749 expr _reindex_serial_fields(prepared, row_tokens_template, row_columns)
    - L750 return prepared
  - L752 if multi_key_selected and (not __force_single):
    - L753 annotated assign html_sections: list[str] = []
    - L754 annotated assign tmp_outputs: list[tuple[Path, Path]] = []
    - L755 try:
      - L756 for (idx, combo) in enumerate(_iter_key_combinations(key_values_map), start=1):
        - L757 annotated assign selection: dict[str, str] = {token: value for token, value in combo.items()}
        - L758 if alias_link_map:
          - L759 for (alias, source) in alias_link_map.items():
            - L760 if alias not in selection and source in selection:
              - L761 assign selection[alias] = selection[source]
        - L762 expr _log_debug('Fanout iteration', idx, 'selection', selection)
        - L763 assign tmp_html = OUT_HTML.with_name(f'{OUT_HTML.stem}__key{idx}.html')
        - L764 assign tmp_pdf = OUT_PDF.with_name(f'{OUT_PDF.stem}__key{idx}.pdf')
        - L765 assign result = fill_and_print(OBJ=OBJ, TEMPLATE_PATH=TEMPLATE_PATH, DB_PATH=DB_PATH, OUT_HTML=tmp_html, OUT_PDF=tmp_pdf, START_DATE=START_DATE, END_DATE=END_DATE, batch_ids=None, KEY_VALUES=selection or None, GENERATOR_BUNDLE=GENERATOR_BUNDLE, __force_single=True)
        - L778 expr html_sections.append(Path(result['html_path']).read_text(encoding='utf-8', errors='ignore'))
        - L779 expr tmp_outputs.append((Path(result['html_path']), Path(result['pdf_path'])))
      - L781 if not html_sections:
        - L782 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': False}
      - L784 assign combined_html = _combine_html_documents(html_sections)
      - L785 assign column_count = max(row_token_count, _count_table_columns(combined_html))
      - L786 assign excel_print_scale = _estimate_excel_print_scale(column_count)
      - L787 assign row_count = _count_table_rows(combined_html)
      - L788 assign rows_per_page = _estimate_rows_per_page(excel_print_scale)
      - L789 if row_count <= rows_per_page:
        - L790 assign rows_per_page = None
      - L791 assign combined_html = _inject_excel_print_styles(combined_html, scale=excel_print_scale, rows_per_page=rows_per_page)
      - L796 expr OUT_HTML.write_text(combined_html, encoding='utf-8')
      - L797 expr asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent, pdf_scale=excel_print_scale))
      - L805 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': True}
      - L807 finally:
        - L807 for (tmp_html_path, tmp_pdf_path) in tmp_outputs:
          - L808 for path_sel in (tmp_html_path, tmp_pdf_path):
            - L809 with contextlib.suppress(FileNotFoundError):
              - L810 expr path_sel.unlink()
  - L812 def _get_literal_rawtoken: str:
    - L813 if token not in LITERALS:
      - L814 return ''
    - L815 assign raw = LITERALS[token]
    - L816 return '' if raw is None else str(raw)
  - L818 def _literal_has_contenttoken: str:
    - L819 return bool(_get_literal_raw(token).strip())
  - L821 def _first_nonempty_literaltokens: Iterable[str]:
    - L822 for tok in tokens:
      - L823 assign raw = _get_literal_raw(tok)
      - L824 if raw.strip():
        - L825 return (tok, raw)
    - L826 return (None, None)
  - L828 def _record_special_valuetarget: dict[str, str], token: str, value: str:
    - L829 assign existing_raw = _get_literal_raw(token)
    - L830 if existing_raw.strip():
      - L831 assign target[token] = existing_raw
      - L833 else:
        - L833 assign target[token] = value
        - L834 if token in LITERALS:
          - L835 assign LITERALS[token] = value
  - L837 def _filter_tokens_without_literaltokens: set[str]:
    - L838 return {tok for tok in tokens if not _literal_has_content(tok)}
  - L840 assign BEGIN_TAG = '<!-- BEGIN:BATCH (auto) -->'
  - L841 assign END_TAG = '<!-- END:BATCH (auto) -->'
  - L842 try:
    - L843 assign (prototype_block, start0, end_last) = _select_prototype_block(html, ROW_TOKENS)
    - L844 except Exception as exc:
      - L845 expr _raise_no_block(html, exc)
  - L846 assign shell_prefix = html[:start0] + BEGIN_TAG
  - L847 assign shell_suffix = END_TAG + html[end_last:]
  - L849 assign parent_table = JOIN.get('parent_table', '')
  - L850 assign parent_key = JOIN.get('parent_key', '')
  - L851 assign child_table = JOIN.get('child_table', '')
  - L852 assign child_key = JOIN.get('child_key', '')
  - L853 assign parent_date = DATE_COLUMNS.get(parent_table, '')
  - L854 assign child_date = DATE_COLUMNS.get(child_table, '')
  - L855 assign order_col = ROW_ORDER[0] if ROW_ORDER else 'ROWID'
  - L857 def _normalize_token_namename: str:
    - L858 return re.sub('[^a-z0-9]', '', name.lower())
  - L860 annotated assign token_index: dict[str, set[str]] = defaultdict(set)
  - L861 assign all_candidate_tokens = set(TEMPLATE_TOKENS) | set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())
  - L865 def _token_synonym_keysnorm: str:
    - L866 docstring: "\n        Generate lightweight normalization aliases so that abbreviated tokens ..."
    - L871 if not norm:
      - L872 return set()
    - L873 assign aliases = {norm}
    - L874 annotated assign replacements: tuple[tuple[str, str], ...] = (('pg', 'page'), ('num', 'number'), ('no', 'number'), ('cnt', 'count'), ('ttl', 'total'))
    - L881 for (src, dest) in replacements:
      - L882 if src in norm and dest not in norm:
        - L883 expr aliases.add(norm.replace(src, dest))
    - L887 if norm == 'pg':
      - L888 expr aliases.add('page')
    - L889 return {alias for alias in aliases if alias}
  - L891 for tok in all_candidate_tokens:
    - L892 assign norm = _normalize_token_name(tok)
    - L893 for key in _token_synonym_keys(norm):
      - L894 expr token_index[key].add(tok)
  - L896 def _tokens_for_keyskeys: set[str]:
    - L897 annotated assign found: set[str] = set()
    - L898 for key in keys:
      - L899 expr found.update(token_index.get(key, set()))
    - L900 return found
  - L902 def _format_for_dbdt_obj: datetime | None, raw_value, include_time_default: bool:
    - L903 docstring: "\n        Normalize input dates for SQLite bindings:\n          - prefer ISO 860..."
    - L908 if dt_obj:
      - L909 assign include_time = include_time_default or bool(dt_obj.hour or dt_obj.minute or dt_obj.second or dt_obj.microsecond)
      - L912 if include_time:
        - L913 return dt_obj.strftime('%Y-%m-%d %H:%M:%S')
      - L914 return dt_obj.strftime('%Y-%m-%d')
    - L915 return '' if raw_value is None else str(raw_value).strip()
  - L917 def _run_generator_entrypointsentrypoints: dict, sql_params: dict[str, object], df_loader: SQLiteDataFrameLoader:
    - L922 if not entrypoints:
      - L923 return {}
    - L924 assign alias_fix_patterns = [re.compile('\\brows\\.', re.IGNORECASE), re.compile('\\brows_agg\\.', re.IGNORECASE)]
    - L929 def _wrap_date_paramsql_text: str, param: str:
      - L930 assign pattern = re.compile(f':{param}\\b')
      - L932 def _replacematch: re.Match[str]:
        - L933 assign start = match.start()
        - L934 assign prefix = sql_text[:start].rstrip()
        - L935 assign prefix_lower = prefix.lower()
        - L936 if prefix_lower.endswith('date(') or prefix_lower.endswith('datetime('):
          - L937 return match.group(0)
        - L938 return f'DATE({match.group(0)})'
      - L940 return pattern.sub(_replace, sql_text)
    - L942 def _prepare_sqlsql_text: str:
      - L943 assign updated = re.sub('PARAM:([A-Za-z0-9_]+)', ':\\1', sql_text)
      - L944 if 'DATE(' not in updated.upper():
        - L945 return updated
      - L946 for param in ('from_date', 'to_date'):
        - L947 assign updated = _wrap_date_param(updated, param)
      - L948 return updated
    - L950 def _attempt_sql_fixsql_text: str, exc: Exception | None:
      - L951 if exc is None:
        - L952 return None
      - L953 assign message = str(exc).lower()
      - L954 if 'no such column' not in message:
        - L955 return None
      - L956 assign fixed_sql = sql_text
      - L957 for pattern in alias_fix_patterns:
        - L958 assign fixed_sql = pattern.sub('', fixed_sql)
      - L959 return fixed_sql if fixed_sql != sql_text else None
    - L961 assign frames = df_loader.frames()
    - L962 assign query_engine = DuckDBDataFrameQuery(frames)
    - L963 try:
      - L964 annotated assign results: dict[str, list[dict[str, object]]] = {}
      - L965 for name in ('header', 'rows', 'totals'):
        - L966 assign sql = entrypoints.get(name)
        - L967 if not sql:
          - L968 assign results[name] = []
          - L969 continue
        - L970 assign current_sql = _prepare_sql(sql)
        - L971 annotated assign last_error: Exception | None = None
        - L972 for attempt in range(2):
          - L973 try:
            - L974 assign df_rows = query_engine.execute(current_sql, sql_params)
            - L975 except Exception as exc:
              - L976 assign last_error = exc
              - L977 assign fixed_sql = _attempt_sql_fix(current_sql, exc)
              - L978 if fixed_sql is not None and fixed_sql != current_sql:
                - L979 assign current_sql = fixed_sql
                - L980 continue
              - L981 raise
            - L983 else:
              - L983 assign last_error = None
              - L984 assign results[name] = df_rows.to_dict('records')
              - L985 break
          - L987 else:
            - L987 assert last_error is not None
            - L988 raise last_error
      - L989 return results
      - L991 finally:
        - L991 expr query_engine.close()
  - L993 assign start_dt = _parse_date_like(START_DATE)
  - L994 assign end_dt = _parse_date_like(END_DATE)
  - L995 assign print_dt = datetime.now()
  - L997 assign start_has_time = _has_time_component(START_DATE, start_dt)
  - L998 assign end_has_time = _has_time_component(END_DATE, end_dt)
  - L1000 assign START_DATE_KEYS = {'fromdate', 'datefrom', 'startdate', 'periodstart', 'rangefrom', 'fromdt', 'startdt'}
  - L1001 assign END_DATE_KEYS = {'todate', 'dateto', 'enddate', 'periodend', 'rangeto', 'todt', 'enddt'}
  - L1002 assign PRINT_DATE_KEYS = {'printdate', 'printedon', 'printeddate', 'generatedon', 'generateddate', 'rundate', 'runon', 'generatedat'}
  - L1012 assign PAGE_NO_KEYS = {'page', 'pageno', 'pagenum', 'pagenumber', 'pageindex', 'pageidx', 'pagecurrent', 'currentpage', 'currpage', 'pgno', 'pgnum', 'pgnumber', 'pgindex', 'pgcurrent'}
  - L1028 assign PAGE_COUNT_KEYS = {'pagecount', 'pagecounts', 'totalpages', 'pagestotal', 'pages', 'pagetotal', 'totalpage', 'pagecounttotal', 'totalpagecount', 'pagescount', 'countpages', 'lastpage', 'finalpage', 'maxpage', 'pgtotal', 'totalpg', 'pgcount', 'countpg', 'pgs', 'pgscount', 'pgstotal', 'totalpgs'}
  - L1052 assign PAGE_LABEL_KEYS = {'pagelabel', 'pageinfo', 'pagesummary', 'pagefooter', 'pagefootertext', 'pageindicator', 'pagecaption', 'pagefooterlabel', 'pagetext', 'pagefooterinfo', 'pagehint'}
  - L1066 annotated assign special_values: dict[str, str] = {}
  - L1068 assign start_tokens = _tokens_for_keys(START_DATE_KEYS)
  - L1069 assign end_tokens = _tokens_for_keys(END_DATE_KEYS)
  - L1070 assign print_tokens = _tokens_for_keys(PRINT_DATE_KEYS)
  - L1071 assign page_number_tokens = _tokens_for_keys(PAGE_NO_KEYS)
  - L1072 assign page_count_tokens = _tokens_for_keys(PAGE_COUNT_KEYS)
  - L1073 assign page_label_tokens = _tokens_for_keys(PAGE_LABEL_KEYS)
  - L1075 for tok in start_tokens:
    - L1076 expr _record_special_value(special_values, tok, _format_for_token(tok, start_dt, include_time_default=start_has_time))
  - L1081 for tok in end_tokens:
    - L1082 expr _record_special_value(special_values, tok, _format_for_token(tok, end_dt, include_time_default=end_has_time))
  - L1088 assign (_, print_literal_value) = _first_nonempty_literal(print_tokens)
  - L1089 assign parsed_print_dt = _parse_date_like(print_literal_value) if print_literal_value else None
  - L1090 assign print_dt_source = parsed_print_dt or print_dt
  - L1091 assign print_has_time = _has_time_component(print_literal_value, parsed_print_dt)
  - L1093 for tok in print_tokens:
    - L1094 if print_literal_value and (not parsed_print_dt):
      - L1095 assign value = print_literal_value
      - L1097 else:
        - L1097 assign value = _format_for_token(tok, print_dt_source, include_time_default=print_has_time)
    - L1098 expr _record_special_value(special_values, tok, value)
  - L1100 assign page_number_tokens = _filter_tokens_without_literal(page_number_tokens)
  - L1101 assign page_count_tokens = _filter_tokens_without_literal(page_count_tokens)
  - L1102 assign page_label_tokens = _filter_tokens_without_literal(page_label_tokens)
  - L1104 assign post_literal_specials = {tok: val for tok, val in special_values.items() if tok not in LITERALS}
  - L1106 assign _ident_re = re.compile('^[A-Za-z_][A-Za-z0-9_]*$')
  - L1108 def qidentname: str:
    - L1109 if _ident_re.match(name):
      - L1110 return name
    - L1111 assign safe = name.replace('"', '""')
    - L1112 return f'"{safe}"'
  - L1115 def _parse_key_colskey_spec: str:
    - L1116 return [c.strip() for c in str(key_spec).split(',') if c and c.strip()]
  - L1118 def _key_exprcols: list[str]:
    - L1119 assign parts = [f"COALESCE(CAST({qident(c)} AS TEXT),'')" for c in cols]
    - L1120 if not parts:
      - L1121 return "''"
    - L1122 assign expr = parts[0]
    - L1123 for p in parts[1:]:
      - L1124 assign expr = f"{expr} || '|' || {p}"
    - L1125 return expr
  - L1127 def _split_bidbid: str, n: int:
    - L1128 assign parts = str(bid).split('|')
    - L1129 if len(parts) != n:
      - L1130 raise ValueError(f'Composite key mismatch: expected {n} parts, got {len(parts)} in {bid!r}')
    - L1131 return parts
  - L1133 def _looks_like_composite_idx: str, n: int:
    - L1134 return isinstance(x, str) and x.count('|') == n - 1
  - L1136 assign pcols = _parse_key_cols(parent_key)
  - L1137 assign ccols = _parse_key_cols(child_key)
  - L1139 assign has_child = bool(child_table and ccols)
  - L1140 assign parent_table_lc = parent_table.lower()
  - L1141 assign child_table_lc = child_table.lower()
  - L1142 annotated assign parent_filter_map: dict[str, list[str]] = {}
  - L1143 annotated assign child_filter_map: dict[str, list[str]] = {}
  - L1144 if key_values_map:
    - L1145 for (token, values) in key_values_map.items():
      - L1146 assign mapping_value = PLACEHOLDER_TO_COL.get(token)
      - L1147 if not isinstance(mapping_value, str):
        - L1148 continue
      - L1149 assign target = mapping_value.strip()
      - L1150 if not target or target.upper().startswith('PARAM:') or '.' not in target:
        - L1151 continue
      - L1152 assign (table_name, column_name) = target.split('.', 1)
      - L1153 assign table_name = table_name.strip(' "`[]')
      - L1154 assign column_name = column_name.strip(' "`[]')
      - L1155 if not column_name:
        - L1156 continue
      - L1157 assign table_key = table_name.lower()
      - L1158 if table_key in (parent_table_lc, 'header'):
        - L1159 assign bucket = list(parent_filter_map.get(column_name, []))
        - L1160 for val in values:
          - L1161 if val not in bucket:
            - L1162 expr bucket.append(val)
        - L1163 if bucket:
          - L1164 assign parent_filter_map[column_name] = bucket
      - L1165 if has_child and table_key in (child_table_lc, 'rows'):
        - L1166 assign bucket = list(child_filter_map.get(column_name, []))
        - L1167 for val in values:
          - L1168 if val not in bucket:
            - L1169 expr bucket.append(val)
        - L1170 if bucket:
          - L1171 assign child_filter_map[column_name] = bucket
  - L1172 assign parent_filter_items = list(parent_filter_map.items())
  - L1173 assign child_filter_items = list(child_filter_map.items())
  - L1174 annotated assign parent_filter_sqls: list[str] = []
  - L1175 annotated assign parent_filter_values: list[str] = []
  - L1176 for (col, values) in parent_filter_items:
    - L1177 annotated assign normalized: list[str] = []
    - L1178 for val in values:
      - L1179 if not isinstance(val, str):
        - L1180 continue
      - L1181 assign text = val.strip()
      - L1182 if text and text not in normalized:
        - L1183 expr normalized.append(text)
    - L1184 if not normalized:
      - L1185 continue
    - L1186 if len(normalized) == 1:
      - L1187 expr parent_filter_sqls.append(f'{qident(col)} = ?')
      - L1189 else:
        - L1189 assign placeholders = ', '.join(('?' for _ in normalized))
        - L1190 expr parent_filter_sqls.append(f'{qident(col)} IN ({placeholders})')
    - L1191 expr parent_filter_values.extend(normalized)
  - L1192 assign parent_filter_values_tuple = tuple(parent_filter_values)
  - L1194 annotated assign child_filter_sqls: list[str] = []
  - L1195 annotated assign child_filter_values: list[str] = []
  - L1196 for (col, values) in child_filter_items:
    - L1197 annotated assign normalized: list[str] = []
    - L1198 for val in values:
      - L1199 if not isinstance(val, str):
        - L1200 continue
      - L1201 assign text = val.strip()
      - L1202 if text and text not in normalized:
        - L1203 expr normalized.append(text)
    - L1204 if not normalized:
      - L1205 continue
    - L1206 if len(normalized) == 1:
      - L1207 expr child_filter_sqls.append(f'{qident(col)} = ?')
      - L1209 else:
        - L1209 assign placeholders = ', '.join(('?' for _ in normalized))
        - L1210 expr child_filter_sqls.append(f'{qident(col)} IN ({placeholders})')
    - L1211 expr child_filter_values.extend(normalized)
  - L1212 assign child_filter_values_tuple = tuple(child_filter_values)
  - L1214 def _merge_predicatebase_sql: str, extras: list[str]:
    - L1215 if not extras:
      - L1216 return base_sql
    - L1217 assign extras_joined = ' AND '.join(extras)
    - L1218 assign base_sql = (base_sql or '1=1').strip()
    - L1219 return f'({base_sql}) AND {extras_joined}'
  - L1222 assign parent_type = get_col_type(DB_PATH, parent_table, parent_date)
  - L1223 assign child_type = get_col_type(DB_PATH, child_table, child_date)
  - L1224 assign (parent_pred, adapt_parent) = mk_between_pred_for_date(parent_date, parent_type)
  - L1225 assign (child_pred, adapt_child) = mk_between_pred_for_date(child_date, child_type)
  - L1226 assign parent_where_clause = _merge_predicate(parent_pred, parent_filter_sqls)
  - L1227 assign child_where_clause = _merge_predicate(child_pred, child_filter_sqls) if has_child else child_pred
  - L1228 assign db_start = _format_for_db(start_dt, START_DATE, start_has_time)
  - L1229 assign db_end = _format_for_db(end_dt, END_DATE, end_has_time)
  - L1230 assign PDATE = tuple(adapt_parent(db_start, db_end))
  - L1231 assign CDATE = tuple(adapt_child(db_start, db_end)) if has_child else tuple()
  - L1232 assign parent_params_all = tuple(PDATE) + parent_filter_values_tuple
  - L1233 assign child_params_all = tuple(CDATE) + child_filter_values_tuple if has_child else tuple()
  - L1235 annotated assign sql_params: dict[str, object] = {'from_date': db_start, 'to_date': db_end}
  - L1240 for token in contract_adapter.param_tokens:
    - L1241 if token in ('from_date', 'to_date'):
      - L1242 continue
    - L1243 if token in key_values_map:
      - L1244 assign first_value = _first_key_value(key_values_map[token])
      - L1245 if first_value is not None:
        - L1246 assign sql_params[token] = first_value
      - L1247 else:
        - L1247 if alias_link_map.get(token):
          - L1248 assign alias_value = _first_alias_value(token)
          - L1249 if alias_value is not None:
            - L1250 assign sql_params[token] = alias_value
          - L1251 else:
            - L1251 if token in LITERALS:
              - L1252 assign sql_params[token] = LITERALS[token]
              - L1253 else:
                - L1253 if token in special_values:
                  - L1254 assign sql_params[token] = special_values[token]
                  - L1256 else:
                    - L1256 expr sql_params.setdefault(token, '')
  - L1258 expr _apply_alias_params(sql_params)
  - L1260 def _apply_date_param_defaultstarget: dict[str, object]:
    - L1261 if not isinstance(target, dict):
      - L1262 return None
    - L1264 def _injectnames: set[str], default_value: str:
      - L1265 if not default_value:
        - L1266 return None
      - L1267 for alias in names:
        - L1268 if alias not in param_token_set and alias not in target:
          - L1269 continue
        - L1270 assign current = target.get(alias)
        - L1271 if isinstance(current, str):
          - L1272 if current.strip():
            - L1273 continue
          - L1274 else:
            - L1274 if current not in (None, ''):
              - L1275 continue
        - L1276 assign target[alias] = default_value
    - L1278 expr _inject(_DATE_PARAM_START_ALIASES, db_start)
    - L1279 expr _inject(_DATE_PARAM_END_ALIASES, db_end)
  - L1281 expr _apply_date_param_defaults(sql_params)
  - L1283 if 'recipe_code' in sql_params:
    - L1284 expr _log_debug('[multi-debug] generator param binding', 'force_single' if __force_single else 'fanout_root', 'key_values=', key_values_map, 'recipe_code=', sql_params.get('recipe_code'))
  - L1293 if GENERATOR_BUNDLE is None:
    - L1294 assign generator_dir = TEMPLATE_PATH.parent / 'generator'
    - L1295 assign meta_path = generator_dir / 'generator_assets.json'
    - L1296 if meta_path.exists():
      - L1297 try:
        - L1298 assign meta_payload = json.loads(meta_path.read_text(encoding='utf-8'))
        - L1299 except Exception:
          - L1300 assign meta_payload = None
        - L1302 else:
          - L1302 annotated assign bundle: dict[str, object] = {'meta': meta_payload}
          - L1303 assign output_schemas_path = generator_dir / 'output_schemas.json'
          - L1304 if output_schemas_path.exists():
            - L1305 try:
              - L1306 assign bundle['output_schemas'] = json.loads(output_schemas_path.read_text(encoding='utf-8'))
              - L1307 except Exception:
                - L1308 pass
          - L1309 assign GENERATOR_BUNDLE = bundle
  - L1311 annotated assign generator_results: dict[str, list[dict[str, object]]] | None = None
  - L1312 if GENERATOR_BUNDLE and (not multi_key_selected):
    - L1313 assign meta_payload = GENERATOR_BUNDLE.get('meta') or {}
    - L1314 assign entrypoints = meta_payload.get('entrypoints')
    - L1315 if not isinstance(entrypoints, dict):
      - L1316 assign entrypoints = {}
    - L1318 if any(entrypoints.values()):
      - L1319 assign params_spec = meta_payload.get('params') or {}
      - L1320 assign required_params = list(params_spec.get('required') or [])
      - L1321 assign optional_params = list(params_spec.get('optional') or [])
      - L1322 for name in required_params + optional_params:
        - L1323 expr sql_params.setdefault(name, None)
      - L1325 if 'plant_name' in sql_params:
        - L1326 assign sql_params['plant_name'] = LITERALS.get('plant_name') or special_values.get('plant_name', '')
      - L1327 if 'location' in sql_params:
        - L1328 assign sql_params['location'] = LITERALS.get('location') or special_values.get('location', '')
      - L1329 if 'recipe_code' in sql_params:
        - L1330 if 'recipe_code' in key_values_map:
          - L1331 assign sql_params['recipe_code'] = _first_key_value(key_values_map['recipe_code'])
          - L1333 else:
            - L1333 assign fallback_val = LITERALS.get('recipe_code')
            - L1334 assign sql_params['recipe_code'] = fallback_val if fallback_val not in (None, '', []) else None
      - L1335 if 'page_info' in sql_params:
        - L1336 assign sql_params['page_info'] = LITERALS.get('page_info') or ''
      - L1337 if key_values_map:
        - L1338 for (name, values) in key_values_map.items():
          - L1339 assign sql_params[name] = _first_key_value(values)
      - L1341 expr _apply_alias_params(sql_params)
      - L1343 try:
        - L1344 assign generator_results = _run_generator_entrypoints(entrypoints, sql_params, dataframe_loader)
        - L1345 except Exception as exc:
          - L1346 expr print(f'Generator SQL execution failed; falling back to contract mapping: {exc}')
          - L1347 assign generator_results = None
  - L1349 if generator_results is None and (not multi_key_selected):
    - L1350 try:
      - L1351 assign default_sql_pack = contract_adapter.build_default_sql_pack()
      - L1352 except Exception as exc:
        - L1353 expr print(f'Contract-derived SQL synthesis failed: {exc}')
      - L1355 else:
        - L1355 assign default_params = default_sql_pack.get('params') or {}
        - L1356 assign required_default = list(default_params.get('required') or [])
        - L1357 assign optional_default = list(default_params.get('optional') or [])
        - L1358 for name in required_default + optional_default:
          - L1359 expr sql_params.setdefault(name, None)
        - L1360 expr _apply_alias_params(sql_params)
        - L1361 try:
          - L1362 assign fallback_results = _run_generator_entrypoints(default_sql_pack['entrypoints'], sql_params, dataframe_loader)
          - L1367 except Exception as exc:
            - L1368 expr print(f'Contract SQL execution failed; continuing with discovery fallback: {exc}')
          - L1370 else:
            - L1370 if any((fallback_results.get(section) for section in ('rows', 'header', 'totals'))):
              - L1371 assign generator_results = fallback_results
  - L1374 if generator_results is None:
    - L1375 assign need_discover = False
    - L1376 assign existing = batch_ids
    - L1378 if isinstance(existing, str):
      - L1379 assign existing = [existing]
    - L1381 if not existing:
      - L1382 assign need_discover = True
      - L1384 else:
        - L1384 if not isinstance(existing, (list, tuple)):
          - L1385 assign need_discover = True
          - L1387 else:
            - L1387 assign existing = list(existing)
            - L1388 if len(pcols) > 1:
              - L1389 if any((not _looks_like_composite_id(i, len(pcols)) for i in existing)):
                - L1390 expr print('Provided BATCH_IDS do not match composite key format; falling back to auto-discovery.')
                - L1391 assign need_discover = True
    - L1393 if need_discover:
      - L1394 assign discovery_summary = discover_batches_excel(db_path=DB_PATH, contract=OBJ, start_date=START_DATE, end_date=END_DATE, key_values=key_values_map)
      - L1401 assign BATCH_IDS = [str(batch['id']) for batch in discovery_summary.get('batches', [])]
      - L1403 else:
        - L1403 assign BATCH_IDS = existing
    - L1405 else:
      - L1405 assign BATCH_IDS = ['__GENERATOR_SINGLE__']
  - L1407 expr print('BATCH_IDS:', len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])
  - L1409 def format_token_valuetoken: str, raw_value: Any:
    - L1410 return contract_adapter.format_value(token, raw_value)
  - L1413 def _inject_page_counter_spanshtml_in: str, page_tokens: set[str], count_tokens: set[str], label_tokens: set[str] | None=None:
    - L1419 assign tokens_to_remove = {tok for tok in page_tokens if tok} | {tok for tok in count_tokens if tok} | {tok for tok in label_tokens or set() if tok}
    - L1424 if not tokens_to_remove:
      - L1425 return html_in
    - L1427 assign token_pattern = '|'.join((re.escape(tok) for tok in tokens_to_remove))
    - L1428 assign placeholder_pattern = f'(?:\\{{\\{{\\s*(?:{token_pattern})\\s*\\}}\\}}|\\{{\\s*(?:{token_pattern})\\s*\\}})'
    - L1429 assign placeholder_re = re.compile(placeholder_pattern)
    - L1430 assign element_re = re.compile(f'(?is)<(?P<tag>[a-z0-9:_-]+)(?:\\s[^>]*)?>(?:(?!</(?P=tag)>).)*?{placeholder_pattern}(?:(?!</(?P=tag)>).)*?</(?P=tag)>')
    - L1433 assign line_re = re.compile(f'(?im)^[^\\n]*{placeholder_pattern}[^\\n]*\\n?')
    - L1435 def _remove_blockstext: str:
      - L1436 while True:
        - L1437 assign (text, count) = element_re.subn('', text)
        - L1438 if count == 0:
          - L1439 break
      - L1440 assign text = line_re.sub('', text)
      - L1441 assign text = placeholder_re.sub('', text)
      - L1442 return text
    - L1444 return _remove_blocks(html_in)
  - L1447 def best_rows_tbodyinner_html: str, allowed_tokens: set:
    - L1448 assign tbodys = list(re.finditer('(?is)<tbody\\b[^>]*>(.*?)</tbody>', inner_html))
    - L1449 assign best = (None, None, -1)
    - L1450 for m in tbodys:
      - L1451 assign tin = m.group(1)
      - L1452 assign hits = 0
      - L1453 for trm in re.finditer('(?is)<tr\\b[^>]*>.*?</tr>', tin):
        - L1454 assign tr_html = trm.group(0)
        - L1455 assign toks = re.findall('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', tr_html)
        - L1456 assign flat = [a.strip() if a else b.strip() for a, b in toks]
        - L1457 aug assign hits Add sum((1 for t in flat if t in allowed_tokens))
      - L1458 if hits > best[2]:
        - L1459 assign best = (m, tin, hits)
    - L1460 if best[0] is not None:
      - L1461 return (best[0], best[1])
    - L1462 return (tbodys[0], tbodys[0].group(1)) if tbodys else (None, None)
  - L1464 def find_row_templatetbody_inner: str, allowed_tokens: set:
    - L1465 for m in re.finditer('(?is)<tr\\b[^>]*>.*?</tr>', tbody_inner):
      - L1466 assign tr_html = m.group(0)
      - L1467 assign toks = re.findall('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', tr_html)
      - L1468 assign flat = []
      - L1469 for (a, b) in toks:
        - L1470 if a:
          - L1471 expr flat.append(a.strip())
        - L1472 if b:
          - L1473 expr flat.append(b.strip())
      - L1474 assign flat = [t for t in flat if t in allowed_tokens]
      - L1475 if flat:
        - L1476 return (tr_html, (m.start(0), m.end(0)), sorted(set(flat), key=len, reverse=True))
    - L1477 return (None, None, [])
  - L1479 def majority_table_for_tokenstokens, mapping:
    - L1480 from collections import Counter
    - L1482 assign tbls = []
    - L1483 for t in tokens:
      - L1484 assign tc = mapping.get(t, '')
      - L1485 if '.' in tc:
        - L1486 expr tbls.append(tc.split('.', 1)[0])
    - L1487 return Counter(tbls).most_common(1)[0][0] if tbls else None
  - L1490 def _extract_col_namemapping_value: str | None:
    - L1491 if not isinstance(mapping_value, str):
      - L1492 return None
    - L1493 assign target = mapping_value.strip()
    - L1494 if '.' not in target:
      - L1495 return None
    - L1496 return target.split('.', 1)[1].strip() or None
  - L1498 assign header_cols = sorted({col for t in HEADER_TOKENS for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
  - L1499 assign row_cols = sorted({col for t in ROW_TOKENS for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
  - L1501 assign totals_by_table = defaultdict(lambda: defaultdict(list))
  - L1502 assign total_token_to_target = {}
  - L1504 for (token, raw_target) in TOTALS.items():
    - L1505 assign target = (raw_target or PLACEHOLDER_TO_COL.get(token, '')).strip()
    - L1506 if not target or '.' not in target:
      - L1507 continue
    - L1508 assign (table_name, col_name) = [part.strip() for part in target.split('.', 1)]
    - L1509 if not table_name or not col_name:
      - L1510 continue
    - L1511 expr totals_by_table[table_name][col_name].append(token)
    - L1512 assign total_token_to_target[token] = (table_name, col_name)
  - L1514 def _coerce_total_valueraw:
    - L1515 if raw is None:
      - L1516 return (None, '0')
    - L1517 try:
      - L1518 assign decimal_value = Decimal(str(raw).strip())
      - L1519 except (InvalidOperation, ValueError, TypeError, AttributeError):
        - L1520 return (None, '0')
    - L1521 if not decimal_value.is_finite():
      - L1522 return (None, '0')
    - L1523 assign formatted = format_decimal_str(decimal_value, max_decimals=3) or '0'
    - L1524 return (float(decimal_value), formatted)
  - L1526 assign totals_accum = defaultdict(float)
  - L1527 assign last_totals_per_token = {token: '0' for token in TOTALS}
  - L1529 assign child_totals_cols = {col: list(tokens) for col, tokens in totals_by_table.get(child_table, {}).items()}
  - L1531 assign rendered_blocks = []
  - L1532 annotated assign generator_header_replacements: dict[str, str] | None = None
  - L1533 if generator_results is not None:
    - L1534 assign block_html = prototype_block
    - L1536 assign header_rows = generator_results.get('header') or []
    - L1537 assign header_row = header_rows[0] if header_rows else {}
    - L1538 annotated assign header_token_values: dict[str, str] = {}
    - L1539 for t in HEADER_TOKENS:
      - L1540 if t in header_row:
        - L1541 assign value = header_row[t]
        - L1542 assign formatted_value = format_token_value(t, value)
        - L1543 assign block_html = sub_token(block_html, t, formatted_value)
        - L1544 assign header_token_values[t] = formatted_value
    - L1545 if header_token_values:
      - L1546 assign generator_header_replacements = header_token_values
    - L1548 assign allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)
    - L1549 assign rows_data = generator_results.get('rows') or []
    - L1550 annotated assign filtered_rows: list[dict[str, Any]] = []
    - L1551 annotated assign row_tokens_in_template: list[str] = []
    - L1553 if rows_data:
      - L1554 assign (tbody_m, tbody_inner) = best_rows_tbody(block_html, allowed_row_tokens)
      - L1555 if tbody_m and tbody_inner:
        - L1556 assign (row_template, row_span, row_tokens_in_template) = find_row_template(tbody_inner, allowed_row_tokens)
        - L1557 if row_template and row_tokens_in_template:
          - L1558 assign row_columns_template = [_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or '' for tok in row_tokens_in_template]
          - L1561 assign render_columns = list(row_tokens_in_template)
          - L1562 assign filtered_rows = _filter_rows_for_render(rows_data, row_tokens_in_template, render_columns, treat_all_as_data=bool(__force_single))
          - L1568 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
          - L1569 if __force_single:
            - L1570 expr _log_debug(f'[multi-debug] generator rows: total={len(rows_data)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
          - L1573 if filtered_rows:
            - L1574 annotated assign parts: list[str] = []
            - L1575 for row in filtered_rows:
              - L1576 assign tr = row_template
              - L1577 for tok in row_tokens_in_template:
                - L1578 assign val = _value_for_token(row, tok)
                - L1579 assign tr = sub_token(tr, tok, format_token_value(tok, val))
              - L1580 expr parts.append(tr)
            - L1581 assign new_tbody_inner = tbody_inner[:row_span[0]] + '\n'.join(parts) + tbody_inner[row_span[1]:]
            - L1582 assign block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]
        - L1584 else:
          - L1584 assign tr_tokens = [m.group(1) or m.group(2) for m in re.finditer('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', block_html)]
          - L1588 assign row_tokens_in_template = [t.strip() for t in tr_tokens if t and t.strip() in allowed_row_tokens]
          - L1589 if row_tokens_in_template:
            - L1590 assign row_columns_template = [_extract_col_name(PLACEHOLDER_TO_COL.get(tok)) or '' for tok in row_tokens_in_template]
            - L1593 assign render_columns = list(row_tokens_in_template)
            - L1594 assign filtered_rows = _filter_rows_for_render(rows_data, row_tokens_in_template, render_columns, treat_all_as_data=bool(__force_single))
            - L1600 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
            - L1601 if __force_single:
              - L1602 expr _log_debug(f'[multi-debug] generator rows (no tbody): total={len(rows_data)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
            - L1605 if filtered_rows:
              - L1606 assign parts = []
              - L1607 for row in filtered_rows:
                - L1608 assign tr = prototype_block
                - L1609 for tok in row_tokens_in_template:
                  - L1610 assign val = _value_for_token(row, tok)
                  - L1611 assign tr = sub_token(tr, tok, format_token_value(tok, val))
                - L1612 expr parts.append(tr)
              - L1613 assign block_html = '\n'.join(parts)
    - L1615 if filtered_rows:
      - L1616 assign totals_row = (generator_results.get('totals') or [{}])[0]
      - L1617 for token in TOTALS:
        - L1618 assign value = totals_row.get(token)
        - L1619 assign formatted = format_token_value(token, value)
        - L1620 assign block_html = sub_token(block_html, token, formatted)
        - L1621 assign last_totals_per_token[token] = formatted
        - L1622 assign target = total_token_to_target.get(token)
        - L1623 if target:
          - L1624 assign (fv, _formatted) = _coerce_total_value(value)
          - L1625 if fv is not None:
            - L1626 assign totals_accum[target] = totals_accum.get(target, 0.0) + fv
      - L1628 expr rendered_blocks.append(block_html)
      - L1630 else:
        - L1630 expr print('Generator SQL produced no usable row data after filtering; skipping block.')
    - L1632 else:
      - L1632 for batch_id in BATCH_IDS or []:
        - L1633 assign block_html = prototype_block
        - L1636 if header_cols:
          - L1637 if len(pcols) == 1:
            - L1638 assign sql = f"SELECT {', '.join((qident(c) for c in header_cols))} FROM {qident(parent_table)} WHERE {qident(pcols[0])} = ? AND {parent_where_clause} LIMIT 1"
            - L1644 assign hdr_params = (batch_id,) + tuple(PDATE) + parent_filter_values_tuple
            - L1646 else:
              - L1646 assign where = ' AND '.join([f'{qident(c)} = ?' for c in pcols])
              - L1647 assign sql = f"SELECT {', '.join((qident(c) for c in header_cols))} FROM {qident(parent_table)} WHERE {where} AND {parent_where_clause} LIMIT 1"
              - L1653 assign hdr_parts = _split_bid(batch_id, len(pcols))
              - L1654 assign hdr_params = tuple(hdr_parts) + tuple(PDATE) + parent_filter_values_tuple
          - L1656 assign con = sqlite3.connect(str(DB_PATH))
          - L1657 assign con.row_factory = sqlite3.Row
          - L1658 assign cur = con.cursor()
          - L1659 expr cur.execute(sql, hdr_params)
          - L1660 assign row = cur.fetchone()
          - L1661 expr con.close()
          - L1662 if row:
            - L1663 assign r = dict(row)
            - L1664 for t in HEADER_TOKENS:
              - L1665 if t in PLACEHOLDER_TO_COL:
                - L1666 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                - L1667 if not col:
                  - L1668 continue
                - L1669 assign val = r.get(col, '')
                - L1670 assign block_html = sub_token(block_html, t, format_token_value(t, val))
        - L1673 assign allowed_row_tokens = {t for t in PLACEHOLDER_TO_COL.keys() if t not in TOTALS} - set(HEADER_TOKENS)
        - L1676 assign (tbody_m, tbody_inner) = best_rows_tbody(block_html, allowed_row_tokens)
        - L1677 if tbody_m and tbody_inner:
          - L1678 assign (row_template, row_span, row_tokens_in_template) = find_row_template(tbody_inner, allowed_row_tokens)
          - L1679 if row_template and row_tokens_in_template:
            - L1680 assign row_cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
            - L1689 if order_col.upper() != 'ROWID' and order_col not in row_cols_needed:
              - L1690 expr row_cols_needed.append(order_col)
            - L1692 assign order_clause = 'ORDER BY ROWID' if order_col.upper() == 'ROWID' else f'ORDER BY {qident(order_col)}, ROWID'
            - L1696 if len(ccols) == 1:
              - L1697 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause} {order_clause}"
              - L1703 assign row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
              - L1705 else:
                - L1705 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                - L1706 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {where} AND {child_where_clause} {order_clause}"
                - L1712 assign row_parts = _split_bid(batch_id, len(ccols))
                - L1713 assign row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple
            - L1715 assign con = sqlite3.connect(str(DB_PATH))
            - L1716 assign con.row_factory = sqlite3.Row
            - L1717 assign cur = con.cursor()
            - L1718 expr cur.execute(sql, row_params)
            - L1719 assign rows = [dict(r) for r in cur.fetchall()]
            - L1720 expr con.close()
            - L1723 if not rows:
              - L1724 assign maj_table = majority_table_for_tokens(row_tokens_in_template, PLACEHOLDER_TO_COL)
              - L1725 if maj_table:
                - L1726 assign date_col = DATE_COLUMNS.get(maj_table, '')
                - L1727 if date_col:
                  - L1728 assign cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
                  - L1736 if date_col not in cols_needed:
                    - L1737 expr cols_needed.append(date_col)
                  - L1738 assign sql_fb = f"SELECT {', '.join((qident(c) for c in cols_needed))} FROM {qident(maj_table)} WHERE datetime({qident(date_col)}) BETWEEN datetime(?) AND datetime(?) ORDER BY {qident(date_col)} ASC, ROWID ASC"
                  - L1744 assign con = sqlite3.connect(str(DB_PATH))
                  - L1745 assign con.row_factory = sqlite3.Row
                  - L1746 assign cur = con.cursor()
                  - L1747 expr cur.execute(sql_fb, (START_DATE, END_DATE))
                  - L1748 assign rows = [dict(r) for r in cur.fetchall()]
                  - L1749 expr con.close()
                  - L1750 expr print(f'Row fallback used: table={maj_table}, rows={len(rows)}')
            - L1752 if not rows:
              - L1753 expr print(f'No child rows found for batch {batch_id}; skipping block.')
              - L1754 continue
            - L1756 assign significant_cols = [col for col in row_cols_needed if col and (not any((keyword in col.lower() for keyword in ('row', 'serial', 'sl'))))]
            - L1761 assign filtered_rows = []
            - L1762 for r in rows:
              - L1763 if significant_cols and (not _row_has_significant_data(r, significant_cols)):
                - L1764 continue
              - L1765 expr filtered_rows.append(dict(r))
            - L1767 if not filtered_rows and rows:
              - L1768 assign filtered_rows = [dict(r) for r in rows]
            - L1769 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
            - L1770 if __force_single:
              - L1771 expr _log_debug(f'[multi-debug] sql rows: total={len(rows)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
            - L1774 if not filtered_rows:
              - L1775 expr print(f'No significant child rows for batch {batch_id}; skipping block.')
              - L1776 continue
            - L1778 expr _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)
            - L1780 annotated assign parts: list[str] = []
            - L1781 for r in filtered_rows:
              - L1782 assign tr = row_template
              - L1783 for t in row_tokens_in_template:
                - L1784 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                - L1785 if not col:
                  - L1786 continue
                - L1787 assign tr = sub_token(tr, t, format_token_value(t, r.get(col)))
              - L1788 expr parts.append(tr)
            - L1790 assign new_tbody_inner = tbody_inner[:row_span[0]] + '\n'.join(parts) + tbody_inner[row_span[1]:]
            - L1791 assign block_html = block_html[:tbody_m.start(1)] + new_tbody_inner + block_html[tbody_m.end(1):]
          - L1795 else:
            - L1795 assign tr_tokens = [m.group(1) or m.group(2) for m in re.finditer('\\{\\{\\s*([^}\\n]+?)\\s*\\}\\}|\\{\\s*([^}\\n]+?)\\s*\\}', block_html)]
            - L1799 assign tr_tokens = sorted({t.strip() for t in tr_tokens if t}, key=len, reverse=True)
            - L1801 assign row_tokens_in_template = [t for t in tr_tokens if t in allowed_row_tokens]
            - L1802 if row_tokens_in_template:
              - L1803 assign row_cols_needed = sorted({col for t in row_tokens_in_template for col in [_extract_col_name(PLACEHOLDER_TO_COL.get(t))] if col})
              - L1811 if order_col.upper() != 'ROWID' and order_col not in row_cols_needed:
                - L1812 expr row_cols_needed.append(order_col)
              - L1813 assign order_clause = 'ORDER BY ROWID' if order_col.upper() == 'ROWID' else f'ORDER BY {qident(order_col)}, ROWID'
              - L1817 if len(ccols) == 1:
                - L1818 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause} {order_clause}"
                - L1824 assign row_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
                - L1826 else:
                  - L1826 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                  - L1827 assign sql = f"SELECT {', '.join((qident(c) for c in row_cols_needed))} FROM {qident(child_table)} WHERE {where} AND {child_where_clause} {order_clause}"
                  - L1833 assign row_parts = _split_bid(batch_id, len(ccols))
                  - L1834 assign row_params = tuple(row_parts) + tuple(CDATE) + child_filter_values_tuple
              - L1836 assign con = sqlite3.connect(str(DB_PATH))
              - L1837 assign con.row_factory = sqlite3.Row
              - L1838 assign cur = con.cursor()
              - L1839 expr cur.execute(sql, row_params)
              - L1840 assign rows = [dict(r) for r in cur.fetchall()]
              - L1841 expr con.close()
              - L1843 assign significant_cols = [col for col in row_cols_needed if col and (not any((keyword in col.lower() for keyword in ('row', 'serial', 'sl'))))]
              - L1848 assign filtered_rows = []
              - L1849 for r in rows:
                - L1850 if significant_cols and (not _row_has_significant_data(r, significant_cols)):
                  - L1851 continue
                - L1852 expr filtered_rows.append(dict(r))
              - L1854 if not filtered_rows and rows:
                - L1855 assign filtered_rows = [dict(r) for r in rows]
              - L1856 assign filtered_rows = _prune_placeholder_rows(filtered_rows, row_tokens_in_template)
              - L1857 if __force_single:
                - L1858 expr _log_debug(f'[multi-debug] sql rows (no tbody): total={len(rows)}, filtered={len(filtered_rows)}, key_values={KEY_VALUES}')
              - L1861 if not filtered_rows:
                - L1862 expr print(f'No significant child rows (no tbody path) for batch {batch_id}; skipping block.')
                - L1863 continue
              - L1865 expr _reindex_serial_fields(filtered_rows, row_tokens_in_template, row_cols_needed)
              - L1867 assign parts = []
              - L1868 for r in filtered_rows:
                - L1869 assign tr = prototype_block
                - L1870 for t in row_tokens_in_template:
                  - L1871 assign col = _extract_col_name(PLACEHOLDER_TO_COL.get(t))
                  - L1872 if not col:
                    - L1873 continue
                  - L1874 assign tr = sub_token(tr, t, format_token_value(t, r.get(col)))
                - L1875 expr parts.append(tr)
              - L1877 assign block_html = '\n'.join(parts)
        - L1880 assign batch_total_values = {token: '0' for token in TOTALS}
        - L1882 if child_totals_cols:
          - L1883 assign child_cols = sorted(child_totals_cols.keys())
          - L1884 if child_cols:
            - L1885 assign exprs = ', '.join([f'COALESCE(SUM({qident(c)}),0) AS {qident(c)}' for c in child_cols])
            - L1887 if len(ccols) == 1:
              - L1888 assign sql = f'SELECT {exprs} FROM {qident(child_table)} WHERE {qident(ccols[0])} = ? AND {child_where_clause}'
              - L1893 assign tot_params = (batch_id,) + tuple(CDATE) + child_filter_values_tuple
              - L1895 else:
                - L1895 assign where = ' AND '.join([f'{qident(c)} = ?' for c in ccols])
                - L1896 assign sql = f'SELECT {exprs} FROM {qident(child_table)} WHERE {where} AND {child_where_clause}'
                - L1899 assign tot_parts = _split_bid(batch_id, len(ccols))
                - L1900 assign tot_params = tuple(tot_parts) + tuple(CDATE) + child_filter_values_tuple
            - L1902 assign con = sqlite3.connect(str(DB_PATH))
            - L1903 assign con.row_factory = sqlite3.Row
            - L1904 assign cur = con.cursor()
            - L1905 expr cur.execute(sql, tot_params)
            - L1906 assign sums = dict(cur.fetchone() or {})
            - L1907 expr con.close()
            - L1909 for col in child_cols:
              - L1910 assign raw_val = sums.get(col, 0)
              - L1911 assign (fv, formatted) = _coerce_total_value(raw_val)
              - L1912 if fv is not None:
                - L1913 assign key = (child_table, col)
                - L1914 assign totals_accum[key] = totals_accum.get(key, 0.0) + fv
              - L1915 for token in child_totals_cols[col]:
                - L1916 assign batch_total_values[token] = formatted
        - L1918 for (token, value) in batch_total_values.items():
          - L1919 assign block_html = sub_token(block_html, token, value)
          - L1920 assign last_totals_per_token[token] = value
        - L1922 expr rendered_blocks.append(block_html)
  - L1925 assign rows_rendered = bool(rendered_blocks)
  - L1926 if not rows_rendered:
    - L1927 expr print('No rendered blocks generated for this selection.')
  - L1929 assign html_multi = shell_prefix + '\n'.join(rendered_blocks) + shell_suffix
  - L1931 for (tok, val) in post_literal_specials.items():
    - L1932 assign html_multi = sub_token(html_multi, tok, val if val is not None else '')
  - L1934 if page_number_tokens or page_count_tokens or page_label_tokens:
    - L1935 assign html_multi = _inject_page_counter_spans(html_multi, page_number_tokens, page_count_tokens, page_label_tokens)
  - L1937 if total_token_to_target:
    - L1938 assign overall_formatted = {}
    - L1939 for ((table_name, col_name), total) in totals_accum.items():
      - L1940 assign (_, formatted) = _coerce_total_value(total)
      - L1941 assign overall_formatted[table_name, col_name] = formatted
    - L1943 for (token, target) in total_token_to_target.items():
      - L1944 assign (table_name, col_name) = target
      - L1945 assign value = overall_formatted.get((table_name, col_name), last_totals_per_token.get(token, '0'))
      - L1946 assign html_multi = sub_token(html_multi, token, value)
  - L1948 if generator_header_replacements:
    - L1949 for (token, value) in generator_header_replacements.items():
      - L1950 assign html_multi = sub_token(html_multi, token, value)
  - L1953 for (t, s) in LITERALS.items():
    - L1954 assign html_multi = sub_token(html_multi, t, s)
  - L1957 assign ALL_KNOWN_TOKENS = set(HEADER_TOKENS) | set(ROW_TOKENS) | set(TOTALS.keys()) | set(LITERALS.keys())
  - L1958 assign html_multi = blank_known_tokens(html_multi, ALL_KNOWN_TOKENS)
  - L1960 assign column_count = max(row_token_count, _count_table_columns(html_multi))
  - L1961 assign excel_print_scale = _estimate_excel_print_scale(column_count)
  - L1962 assign row_count = _count_table_rows(html_multi)
  - L1963 assign rows_per_page = _estimate_rows_per_page(excel_print_scale)
  - L1964 if row_count <= rows_per_page:
    - L1965 assign rows_per_page = None
  - L1966 assign html_multi = _inject_excel_print_styles(html_multi, scale=excel_print_scale, rows_per_page=rows_per_page)
  - L1973 expr OUT_HTML.write_text(html_multi, encoding='utf-8')
  - L1974 expr print('Wrote HTML:', OUT_HTML)
  - L1976 expr print('BATCH_IDS:', len(BATCH_IDS or []), (BATCH_IDS or [])[:20] if BATCH_IDS else [])
  - L1978 expr asyncio.run(html_to_pdf_async(OUT_HTML, OUT_PDF, TEMPLATE_PATH.parent, pdf_scale=excel_print_scale))
  - L1986 expr print('Wrote PDF via Playwright:', OUT_PDF)
  - L1988 return {'html_path': str(OUT_HTML), 'pdf_path': str(OUT_PDF), 'rows_rendered': rows_rendered}
- L1992 if __name__ == '__main__':
  - L1993 expr print('Module ready for API integration. Call fill_and_print(...) from your FastAPI endpoint.')

## backend\app\services\reports\xlsx_export.py
- L1 from __future__ import annotations
- L3 import logging
- L4 from pathlib import Path
- L5 from typing import Optional
- L7 try:
  - L8 import openpyxl
  - L9 from openpyxl.styles import Alignment, Border, Font, PatternFill, Side
  - L10 from openpyxl.utils import get_column_letter
  - L11 from openpyxl.worksheet.table import Table, TableStyleInfo
  - L12 except ImportError:
    - L13 assign openpyxl = None
    - L14 assign Alignment = None
    - L15 assign Border = None
    - L16 assign Font = None
    - L17 assign PatternFill = None
    - L18 assign Side = None
    - L19 assign get_column_letter = None
    - L20 assign Table = None
    - L21 assign TableStyleInfo = None
- L23 from .html_table_parser import extract_first_table, extract_tables
- L25 assign logger = logging.getLogger('neura.reports.xlsx')
- L28 def _auto_column_widthsworksheet:
  - L29 if get_column_letter is None:
    - L30 return None
  - L31 for col_idx in range(1, worksheet.max_column + 1):
    - L32 assign letter = get_column_letter(col_idx)
    - L33 assign max_len = 0
    - L34 for cell in worksheet[letter]:
      - L35 assign value = cell.value
      - L36 if value is None:
        - L37 continue
      - L38 assign text = str(value)
      - L39 assign max_len = max(max_len, len(text))
    - L40 assign width = min(120, max(12, max_len + 2))
    - L41 assign worksheet.column_dimensions[letter].width = width
- L44 def _table_scoretable: list[list[str]]:
  - L45 if not table:
    - L46 return 0
  - L47 assign row_count = len(table)
  - L48 assign max_cols = max((len(row) for row in table), default=0)
  - L49 assign multi_col_rows = sum((1 for row in table if sum((1 for cell in row if cell.strip())) >= 2))
  - L50 return (multi_col_rows or row_count) * max(1, max_cols)
- L53 def _select_best_table_indextables: list[list[list[str]]]:
  - L54 assign best_idx = 0
  - L55 assign best_score = -1
  - L56 for (idx, table) in enumerate(tables):
    - L57 assign score = _table_score(table)
    - L58 if score > best_score:
      - L59 assign best_idx = idx
      - L60 assign best_score = score
  - L61 return best_idx
- L64 def _looks_like_total_rowrow: list[str]:
  - L65 for cell in row:
    - L66 if cell and cell.strip().lower() == 'total':
      - L67 return True
  - L68 return False
- L71 def html_file_to_xlsxhtml_path: Path, output_path: Path:
  - L72 if openpyxl is None:
    - L73 expr logger.warning('xlsx_export_unavailable', extra={'event': 'xlsx_export_unavailable', 'reason': 'openpyxl not installed', 'html_path': str(html_path)})
    - L81 return None
  - L83 assign html_text = html_path.read_text(encoding='utf-8', errors='ignore')
  - L84 assign tables = extract_tables(html_text)
  - L85 annotated assign rows: list[list[str]] = []
  - L86 annotated assign data_row_positions: list[int] = []
  - L87 annotated assign preface_ranges: list[tuple[int, int]] = []
  - L88 annotated assign data_header_row_idx: int | None = None
  - L90 if tables:
    - L91 assign best_idx = _select_best_table_index(tables)
    - L93 for (idx, table) in enumerate(tables):
      - L94 assign is_data_table = idx == best_idx
      - L95 if not table:
        - L96 continue
      - L97 assign serial_counter = 0
      - L98 assign table_start_idx = len(rows) + 1
      - L99 for (row_idx_in_table, row) in enumerate(table):
        - L100 assign clean_row = [(cell or '').strip() for cell in row]
        - L101 if is_data_table and row_idx_in_table > 0:
          - L102 if _looks_like_total_row(clean_row):
            - L103 if clean_row:
              - L104 assign clean_row[0] = ''
            - L106 else:
              - L106 aug assign serial_counter Add 1
              - L107 if not clean_row[0]:
                - L108 assign clean_row[0] = str(serial_counter)
          - L109 else:
            - L109 if is_data_table and row_idx_in_table == 0:
              - L110 assign data_header_row_idx = len(rows) + 1
        - L111 expr rows.append(clean_row)
        - L112 if is_data_table and row_idx_in_table > 0:
          - L113 expr data_row_positions.append(len(rows))
      - L114 assign table_end_idx = len(rows)
      - L115 if not is_data_table and table_end_idx >= table_start_idx:
        - L116 expr preface_ranges.append((table_start_idx, table_end_idx))
      - L118 expr rows.append([])
    - L120 while rows and (not rows[-1]):
      - L121 expr rows.pop()
    - L123 else:
      - L123 assign rows = [[line.strip()] for line in html_text.splitlines() if line.strip()]
      - L124 if not rows:
        - L125 assign rows = [['Report output unavailable']]
  - L127 assign data_start = data_row_positions[0] if data_row_positions else None
  - L128 assign data_end = data_row_positions[-1] if data_row_positions else None
  - L129 assign data_max_cols = max((len(rows[idx - 1]) for idx in data_row_positions)) if data_row_positions else max((len(r) for r in rows if r), default=1)
  - L134 assign data_max_cols = max(1, data_max_cols)
  - L135 assign sheet_width = max((len(r) for r in rows if r), default=data_max_cols)
  - L137 assign output_path = Path(output_path)
  - L138 expr output_path.parent.mkdir(parents=True, exist_ok=True)
  - L140 assign wb = openpyxl.Workbook()
  - L141 assign ws = wb.active
  - L142 assign ws.title = 'Report'
  - L144 for (r_idx, row) in enumerate(rows, start=1):
    - L145 for (c_idx, value) in enumerate(row, start=1):
      - L146 assign cell = ws.cell(row=r_idx, column=c_idx, value=value)
      - L147 if Alignment is not None:
        - L148 assign cell.alignment = Alignment(horizontal='left', vertical='top', wrap_text=True)
      - L153 if Font is not None and r_idx == 1:
        - L154 assign cell.font = Font(bold=True)
    - L156 if len(row) == 0:
      - L157 continue
  - L159 if PatternFill is not None and Alignment is not None and (Font is not None) and (sheet_width > 0) and preface_ranges:
    - L166 assign header_fill = PatternFill('solid', fgColor='D9E1F2')
    - L167 assign title_fill = PatternFill('solid', fgColor='BDD7EE')
    - L168 assign first_preface_row = preface_ranges[0][0]
    - L169 for (start_idx, end_idx) in preface_ranges:
      - L170 for row_idx in range(start_idx, end_idx + 1):
        - L171 assign row_data = rows[row_idx - 1] if 0 <= row_idx - 1 < len(rows) else []
        - L172 if not row_data or not any((cell for cell in row_data)):
          - L173 continue
        - L174 expr ws.merge_cells(start_row=row_idx, start_column=1, end_row=row_idx, end_column=sheet_width)
        - L175 assign cell = ws.cell(row=row_idx, column=1)
        - L176 assign cell_text = cell.value or ''
        - L177 assign is_title_row = row_idx == first_preface_row and cell_text and (cell_text.upper() == cell_text)
        - L178 assign cell.alignment = Alignment(horizontal='center' if is_title_row else 'left', vertical='center', wrap_text=True)
        - L183 assign cell.font = Font(bold=True, size=14 if is_title_row else 11)
        - L184 assign cell.fill = title_fill if is_title_row else header_fill
  - L186 if PatternFill is not None and Alignment is not None and (Font is not None) and (data_header_row_idx is not None) and (data_max_cols > 0):
    - L193 assign data_header_fill = PatternFill('solid', fgColor='2F75B5')
    - L194 for col_idx in range(1, data_max_cols + 1):
      - L195 assign cell = ws.cell(row=data_header_row_idx, column=col_idx)
      - L196 assign cell.fill = data_header_fill
      - L197 assign cell.font = Font(color='FFFFFF', bold=True)
      - L198 assign cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
  - L200 if Border is not None and Side is not None:
    - L201 assign thin = Side(style='thin', color='FFC0C0C0')
    - L202 assign border = Border(left=thin, right=thin, top=thin, bottom=thin)
    - L203 for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
      - L209 for cell in row:
        - L210 assign cell.border = border
  - L212 if data_header_row_idx:
    - L213 assign freeze_row = data_header_row_idx + 1
    - L215 else:
      - L215 assign freeze_row = data_start + 1 if data_start else 2
  - L216 if ws.max_row >= freeze_row:
    - L217 assign ws.freeze_panes = f'A{freeze_row}'
    - L218 else:
      - L218 if ws.max_row > 1:
        - L219 assign ws.freeze_panes = 'A2'
  - L221 if Table is not None and TableStyleInfo is not None and (ws.max_column > 0) and (data_end is not None):
    - L222 assign table_cols = max(1, min(data_max_cols, ws.max_column))
    - L223 assign table_top = data_header_row_idx if data_header_row_idx is not None else data_start
    - L224 if table_top is not None and table_top <= data_end:
      - L225 assign ref = f'A{table_top}:{ws.cell(row=data_end, column=table_cols).coordinate}'
      - L226 assign table = Table(displayName='ReportTable', ref=ref)
      - L227 assign table.tableStyleInfo = TableStyleInfo(name='TableStyleMedium9', showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False)
      - L234 expr ws.add_table(table)
      - L236 else:
        - L236 assign ws.auto_filter.ref = f'A1:{ws.cell(row=ws.max_row, column=ws.max_column).coordinate}'
    - L238 else:
      - L238 assign ws.auto_filter.ref = f'A1:{ws.cell(row=ws.max_row, column=ws.max_column).coordinate}'
  - L240 expr _auto_column_widths(ws)
  - L242 try:
    - L243 expr wb.save(output_path)
    - L244 except Exception as exc:
      - L245 expr logger.warning('xlsx_export_save_failed', extra={'event': 'xlsx_export_save_failed', 'html_path': str(html_path), 'xlsx_path': str(output_path), 'error': str(exc)})
      - L254 return None
  - L256 expr logger.info('xlsx_export_success', extra={'event': 'xlsx_export_success', 'html_path': str(html_path), 'xlsx_path': str(output_path)})
  - L264 return output_path
- L265 def _table_scoretable: list[list[str]]:
  - L266 if not table:
    - L267 return 0
  - L268 assign row_count = len(table)
  - L269 assign max_cols = max((len(row) for row in table), default=0)
  - L270 assign multi_col_rows = sum((1 for row in table if sum((1 for cell in row if cell.strip())) >= 2))
  - L271 return (multi_col_rows or row_count) * max(1, max_cols)
- L274 def _select_best_table_indextables: list[list[list[str]]]:
  - L275 assign best_idx = 0
  - L276 assign best_score = -1
  - L277 for (idx, table) in enumerate(tables):
    - L278 assign score = _table_score(table)
    - L279 if score > best_score:
      - L280 assign best_idx = idx
      - L281 assign best_score = score
  - L282 return best_idx

## backend\app\services\state\__init__.py
- L1 docstring: "\nUtilities for persisting connection/template state.\n"
- L5 from .store import StateStore, state_store
- L7 assign __all__ = ['StateStore', 'state_store']

## backend\app\services\state\store.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\templates\__init__.py
- L1 docstring: "Templates service placeholder."

## backend\app\services\templates\catalog.py
- L1 from __future__ import annotations
- L3 from typing import Any, Dict, List, Optional, Sequence
- L5 from ..state import state_store
- L6 from .starter_catalog import STARTER_TEMPLATES
- L9 assign TemplateCatalogItem = Dict[str, Any]
- L12 def _normalize_str_listvalues: Optional[Sequence[str]]:
  - L13 if not values:
    - L14 return []
  - L15 annotated assign seen: set[str] = set()
  - L16 annotated assign normalized: list[str] = []
  - L17 for raw in values:
    - L18 assign text = str(raw or '').strip()
    - L19 if not text or text in seen:
      - L20 continue
    - L21 expr seen.add(text)
    - L22 expr normalized.append(text)
  - L23 return normalized
- L26 def build_unified_template_catalog:
  - L27 docstring: "\n    Combine company templates from the persistent state store with the static\..."
  - L39 annotated assign catalog: list[TemplateCatalogItem] = []
  - L40 annotated assign seen_ids: set[str] = set()
  - L43 for rec in state_store.list_templates():
    - L44 assign template_id = str(rec.get('id') or '').strip()
    - L45 if not template_id:
      - L46 continue
    - L47 if template_id in seen_ids:
      - L48 continue
    - L49 assign name = (rec.get('name') or '').strip() or f'Template {template_id[:8]}'
    - L50 assign kind = (rec.get('kind') or 'pdf').strip().lower() or 'pdf'
    - L52 annotated assign item: TemplateCatalogItem = {'id': template_id, 'name': name, 'kind': kind, 'domain': rec.get('domain') or None, 'tags': _normalize_str_list(rec.get('tags') or []), 'useCases': _normalize_str_list(rec.get('useCases') or []), 'primaryMetrics': _normalize_str_list(rec.get('primaryMetrics') or []), 'description': (rec.get('description') or '').strip() or None, 'source': 'company'}
    - L63 expr catalog.append(item)
    - L64 expr seen_ids.add(template_id)
  - L67 for starter in STARTER_TEMPLATES:
    - L68 assign template_id = str(starter.get('id') or '').strip()
    - L69 if not template_id or template_id in seen_ids:
      - L70 continue
    - L71 assign name = (starter.get('name') or '').strip() or template_id
    - L72 assign kind = (starter.get('kind') or 'pdf').strip().lower() or 'pdf'
    - L74 annotated assign item: TemplateCatalogItem = {'id': template_id, 'name': name, 'kind': kind, 'domain': starter.get('domain') or None, 'tags': _normalize_str_list(starter.get('tags') or []), 'useCases': _normalize_str_list(starter.get('useCases') or []), 'primaryMetrics': _normalize_str_list(starter.get('primaryMetrics') or []), 'description': (starter.get('description') or '').strip() or None, 'source': 'starter'}
    - L85 expr catalog.append(item)
    - L86 expr seen_ids.add(template_id)
  - L88 return catalog

## backend\app\services\templates\css_merge.py
- L1 import re
- L3 assign STYLE_RE = re.compile('(?is)<style\\b[^>]*>(.*?)</style>')
- L4 assign HEAD_CLOSE_RE = re.compile('(?i)</head>')
- L7 def _extract_csscss_patch: str:
  - L8 assign match = STYLE_RE.search(css_patch)
  - L9 if match:
    - L10 return match.group(1).strip()
  - L11 return css_patch.strip()
- L14 def merge_css_into_htmlhtml: str, css_patch: str:
  - L15 docstring: "Merge the provided CSS patch into the first <style> block (append to override)."
  - L16 assign rules = _extract_css(css_patch)
  - L17 if not rules:
    - L18 return html
  - L20 assign match = STYLE_RE.search(html)
  - L21 if match:
    - L22 assign (start, end) = match.span(1)
    - L23 assign existing = match.group(1).rstrip()
    - L24 if existing:
      - L25 assign merged = f'{existing}\n{rules}\n'
      - L27 else:
        - L27 assign merged = f'{rules}\n'
    - L28 return html[:start] + merged + html[end:]
  - L30 assign injection = f'<style>\n{rules}\n</style>\n'
  - L31 assign head_match = HEAD_CLOSE_RE.search(html)
  - L32 if head_match:
    - L33 assign idx = head_match.start()
    - L34 return html[:idx] + injection + html[idx:]
  - L35 return injection + html
- L38 def replace_table_colgrouphtml: str, table_id: str, new_colgroup_html: str:
  - L39 docstring: "Replace or insert a <colgroup> for the table with the given id."
  - L40 assign snippet = new_colgroup_html.strip()
  - L41 if not table_id or not snippet:
    - L42 return html
  - L43 if '<colgroup' not in snippet.lower():
    - L44 return html
  - L46 assign table_pattern = re.compile(f"""(<table\\b[^>]*\\bid=["\\']{re.escape(table_id)}["\\'][^>]*>)(?P<body>.*?)(</table>)""", re.I | re.S)
  - L50 assign match = table_pattern.search(html)
  - L51 if not match:
    - L52 return html
  - L54 assign start_tag = match.group(1)
  - L55 assign body = match.group('body')
  - L56 assign end_tag = match.group(3)
  - L58 assign colgroup_pattern = re.compile('<colgroup\\b[^>]*>.*?</colgroup>', re.I | re.S)
  - L59 if colgroup_pattern.search(body):
    - L60 assign new_body = colgroup_pattern.sub(snippet, body, count=1)
    - L62 else:
      - L62 assign prepend = snippet if snippet.endswith('\n') else snippet + '\n'
      - L63 assign new_body = prepend + body
  - L65 assign new_table = start_tag + new_body + end_tag
  - L66 return html[:match.start()] + new_table + html[match.end():]

## backend\app\services\templates\layout_hints.py
- L1 import logging
- L2 from pathlib import Path
- L3 from typing import Any, Dict, Optional
- L5 try:
  - L6 import fitz
  - L7 except ImportError:
    - L8 assign fitz = None
- L10 try:
  - L11 import cv2
  - L12 except ImportError:
    - L13 assign cv2 = None
- L15 try:
  - L16 import numpy as np
  - L17 except ImportError:
    - L18 assign np = None
- L21 assign logger = logging.getLogger('neura.layout_hints')
- L23 assign MM_PER_POINT = 25.4 / 72.0
- L26 def _estimate_table_columnspage:
  - L27 if any((mod is None for mod in (cv2, np, fitz))):
    - L28 return None
  - L30 try:
    - L31 assign pix = page.get_pixmap(matrix=fitz.Matrix(2, 2), alpha=False)
    - L32 assign img = np.frombuffer(pix.samples, dtype='uint8')
    - L33 assign img = img.reshape(pix.height, pix.width, pix.n)
    - L34 if pix.n >= 3:
      - L35 assign rgb = img[:, :, :3]
      - L37 else:
        - L37 assign rgb = img
    - L38 assign gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)
    - L39 assign blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    - L40 assign edges = cv2.Canny(blurred, 50, 150)
    - L42 assign vertical_projection = edges.sum(axis=0)
    - L43 if not np.any(vertical_projection):
      - L44 return None
    - L46 assign threshold = float(vertical_projection.mean() * 1.5)
    - L47 assign indices = np.where(vertical_projection > threshold)[0]
    - L48 if indices.size == 0:
      - L49 return None
    - L51 assign min_gap = max(6, pix.width // 200)
    - L52 assign min_span = max(4, pix.width // 500)
    - L53 annotated assign clusters: list[tuple[int, int]] = []
    - L54 assign start = int(indices[0])
    - L55 assign prev = int(indices[0])
    - L56 for raw_idx in indices[1:]:
      - L57 assign idx = int(raw_idx)
      - L58 if idx - prev > min_gap:
        - L59 expr clusters.append((start, prev))
        - L60 assign start = idx
      - L61 assign prev = idx
    - L62 expr clusters.append((start, prev))
    - L64 assign significant = [(lo, hi) for lo, hi in clusters if hi - lo >= min_span]
    - L65 assign line_count = len(significant)
    - L66 if line_count >= 2:
      - L67 return line_count - 1
    - L68 except Exception:
      - L69 expr logger.debug('layout_hints_column_estimate_failed', exc_info=True)
  - L70 return None
- L73 def get_layout_hintspdf_path: Path, page_index: int=0:
  - L74 docstring: "Best-effort geometry hints for prompts (page size, rough table columns)."
  - L75 if fitz is None:
    - L76 return {}
  - L78 try:
    - L79 with fitz.open(str(pdf_path)) as doc:
      - L80 if page_index < 0 or page_index >= len(doc):
        - L81 return {}
      - L82 assign page = doc[page_index]
      - L83 assign width_mm = round(page.rect.width * MM_PER_POINT, 2)
      - L84 assign height_mm = round(page.rect.height * MM_PER_POINT, 2)
      - L86 annotated assign hints: Dict[str, Any] = {'page_mm': [width_mm, height_mm], 'notes': 'best-effort'}
      - L91 assign est_tables = []
      - L92 assign columns = _estimate_table_columns(page)
      - L93 if columns and columns > 1:
        - L94 expr est_tables.append({'id': 'tbl-1', 'cols': int(columns)})
      - L95 if est_tables:
        - L96 assign hints['est_tables'] = est_tables
      - L97 return hints
    - L98 except Exception:
      - L99 expr logger.debug('layout_hints_failed', exc_info=True, extra={'event': 'layout_hints_failed', 'pdf_path': str(pdf_path), 'page_index': page_index})
  - L104 return {}

## backend\app\services\templates\starter_catalog.py
- L1 from __future__ import annotations
- L3 from typing import Any, Dict, List
- L6 docstring: "\nStatic starter template catalog used to seed the unified template list and\npo..."
- L14 assign StarterTemplate = Dict[str, Any]
- L17 annotated assign STARTER_TEMPLATES: List[StarterTemplate] = [{'id': 'starter_monthly_sales_performance', 'name': 'Monthly Sales Performance Summary', 'kind': 'pdf', 'domain': 'Finance', 'tags': ['sales', 'monthly', 'revenue', 'margin', 'kpi'], 'useCases': ['Share monthly sales KPIs with leadership', 'Track revenue and margin by product line and region'], 'primaryMetrics': ['Total revenue', 'Gross margin %', 'Revenue by product line', 'Top customers by revenue'], 'description': 'Board-ready monthly sales summary with revenue, volume, and margin breakdowns by product line and region.', 'artifacts': {'thumbnail_url': '/starter/starter_monthly_sales_performance.png', 'template_html_url': '/starter/starter_monthly_sales_performance.html'}}, {'id': 'starter_ops_throughput_quality', 'name': 'Operational Throughput & Quality Dashboard', 'kind': 'pdf', 'domain': 'Operations', 'tags': ['operations', 'throughput', 'quality', 'downtime'], 'useCases': ['Monitor daily or weekly production performance', 'Identify bottlenecks and recurring downtime causes'], 'primaryMetrics': ['Units produced', 'Overall equipment effectiveness (OEE)', 'First pass yield', 'Unplanned downtime (minutes)'], 'description': 'Operations dashboard summarising throughput, quality, and downtime with trend charts and top root-cause categories.', 'artifacts': {'thumbnail_url': '/starter/starter_ops_throughput_quality.png', 'template_html_url': '/starter/starter_ops_throughput_quality.html'}}, {'id': 'starter_marketing_campaign_roas', 'name': 'Campaign Performance & ROAS', 'kind': 'pdf', 'domain': 'Marketing', 'tags': ['marketing', 'campaign', 'roas', 'acquisition'], 'useCases': ['Compare acquisition campaigns across channels', 'Report ROAS and conversion performance to stakeholders'], 'primaryMetrics': ['Spend by channel', 'Impressions and clicks', 'Conversions and CPA', 'Revenue and ROAS'], 'description': 'Channel-level campaign report highlighting spend, conversions, and return on ad spend (ROAS) across key marketing channels.', 'artifacts': {'thumbnail_url': '/starter/starter_marketing_campaign_roas.png', 'template_html_url': '/starter/starter_marketing_campaign_roas.html'}}, {'id': 'starter_finance_cashflow_projection', 'name': 'Cashflow Projection & Variance', 'kind': 'excel', 'domain': 'Finance', 'tags': ['cashflow', 'forecast', 'variance', 'finance'], 'useCases': ['Review monthly cash-in and cash-out projections', 'Compare actual vs forecast cash positions'], 'primaryMetrics': ['Opening and closing cash balance', 'Cash-in vs cash-out by category', 'Forecast vs actual variance %'], 'description': 'Tabular cashflow projection with monthly actuals, forecasts, and variance analysis for finance teams.', 'artifacts': {'thumbnail_url': '/starter/starter_finance_cashflow_projection.png', 'template_html_url': '/starter/starter_finance_cashflow_projection.html'}}, {'id': 'starter_product_cohort_retention', 'name': 'Product Cohort Retention', 'kind': 'pdf', 'domain': 'Product', 'tags': ['product', 'cohort', 'retention', 'engagement'], 'useCases': ['Track user retention across signup cohorts', 'Identify churn inflection points by lifecycle stage'], 'primaryMetrics': ['Week N retention %', 'DAU/WAU/MAU', 'Churned users'], 'description': 'Visual cohort grids and charts for retention/engagement by signup date.', 'artifacts': {'thumbnail_url': '/starter/starter_product_cohort_retention.png', 'template_html_url': '/starter/starter_product_cohort_retention.html'}}, {'id': 'starter_supply_chain_fill_rate', 'name': 'Supply Chain Fill Rate & Stockouts', 'kind': 'pdf', 'domain': 'Operations', 'tags': ['supply chain', 'inventory', 'fill rate', 'logistics'], 'useCases': ['Monitor fill rates and backorders by DC/region', 'Spot recurring stockouts and lead-time slippage'], 'primaryMetrics': ['Fill rate %', 'Stockout count', 'Average lead time'], 'description': 'Operational scorecard for fulfillment performance and inventory health.', 'artifacts': {'thumbnail_url': '/starter/starter_supply_chain_fill_rate.png', 'template_html_url': '/starter/starter_supply_chain_fill_rate.html'}}]

## backend\app\services\templates\TemplateVerify.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\app\services\utils\__init__.py
- L1 docstring: "\nUtility helpers shared across backend services.\n\nThis package currently prov..."
- L9 from .artifacts import compute_checksums, write_artifact_manifest
- L10 from .context import get_correlation_id, set_correlation_id
- L13 from .fs import write_json_atomic, write_text_atomic
- L14 from .html import sanitize_html
- L15 from .llm import call_chat_completion
- L16 from .lock import TemplateLockError, acquire_template_lock
- L17 from .prompts import available_prompts, load_prompt
- L18 from .render import render_html_to_png
- L19 from .text import strip_code_fences
- L20 from .tokens import extract_tokens, normalize_token_braces
- L21 from .validation import validate_contract_schema, validate_contract_v2, validate_generator_output_schemas, validate_generator_sql_pack, validate_llm_call_3_5, validate_mapping_inline_v4, validate_mapping_schema, validate_step5_requirements
- L32 assign __all__ = ['write_text_atomic', 'write_json_atomic', 'call_chat_completion', 'load_prompt', 'available_prompts', 'acquire_template_lock', 'TemplateLockError', 'write_artifact_manifest', 'compute_checksums', 'sanitize_html', 'render_html_to_png', 'strip_code_fences', 'normalize_token_braces', 'extract_tokens', 'validate_contract_schema', 'validate_contract_v2', 'validate_mapping_schema', 'validate_mapping_inline_v4', 'validate_llm_call_3_5', 'validate_step5_requirements', 'validate_generator_sql_pack', 'validate_generator_output_schemas', 'get_correlation_id', 'set_correlation_id']

## backend\app\services\utils\artifacts.py
- L1 from __future__ import annotations
- L3 import hashlib
- L4 import json
- L5 import logging
- L6 import os
- L7 from datetime import datetime, timezone
- L8 from pathlib import Path
- L9 from typing import Any, Iterable, Mapping
- L11 from .fs import write_json_atomic
- L13 assign logger = logging.getLogger('neura.artifacts')
- L15 assign MANIFEST_NAME = 'artifact_manifest.json'
- L16 assign MANIFEST_SCHEMA_VERSION = '1.0'
- L19 def compute_checksumsfiles: Mapping[str, Path]:
  - L20 annotated assign checksums: dict[str, str] = {}
  - L21 for (name, path) in files.items():
    - L22 if not path.exists():
      - L23 continue
    - L24 assign h = hashlib.sha256()
    - L25 with path.open('rb') as handle:
      - L26 for chunk in iter(lambda: handle.read(65536), b''):
        - L27 expr h.update(chunk)
    - L28 assign checksums[name] = h.hexdigest()
  - L29 return checksums
- L32 def write_artifact_manifesttemplate_dir: Path, *, step: str, files: Mapping[str, Path], inputs: Iterable[str], correlation_id: str | None=None:
  - L40 docstring: "\n    Persist artifact manifest alongside template artifacts.\n    "
  - L43 assign template_dir = template_dir.resolve()
  - L44 assign manifest_path = template_dir / MANIFEST_NAME
  - L46 annotated assign existing_payload: dict[str, Any] = {}
  - L47 if manifest_path.exists():
    - L48 try:
      - L49 assign existing_payload = json.loads(manifest_path.read_text(encoding='utf-8'))
      - L50 except Exception:
        - L51 expr logger.exception('manifest_read_failed', extra={'event': 'manifest_read_failed', 'path': str(manifest_path)})
        - L55 assign existing_payload = {}
  - L57 assign existing_files = dict(existing_payload.get('files') or {})
  - L58 assign existing_checksums = dict(existing_payload.get('file_checksums') or {})
  - L59 assign existing_inputs = list(existing_payload.get('input_refs') or [])
  - L61 def _store_pathpath: Path:
    - L62 try:
      - L63 return str(path.resolve().relative_to(template_dir))
      - L64 except ValueError:
        - L65 return str(path)
  - L67 assign merged_files = existing_files | {name: _store_path(path) for name, path in files.items()}
  - L68 assign new_checksums = compute_checksums(files)
  - L69 assign merged_checksums = existing_checksums | new_checksums
  - L71 assign input_list = list(inputs)
  - L72 annotated assign merged_inputs: list[str] = []
  - L73 for item in existing_inputs + input_list:
    - L74 if item not in merged_inputs:
      - L75 expr merged_inputs.append(item)
  - L77 assign payload = {'schema_version': MANIFEST_SCHEMA_VERSION, 'produced_at': datetime.now(timezone.utc).isoformat(), 'step': step, 'files': merged_files, 'file_checksums': merged_checksums, 'input_refs': merged_inputs, 'correlation_id': correlation_id, 'pid': os.getpid()}
  - L87 expr write_json_atomic(manifest_path, payload, indent=2, ensure_ascii=False, sort_keys=True, step='artifact_manifest')
  - L88 expr logger.info('artifact_manifest_written', extra={'event': 'artifact_manifest_written', 'template_dir': str(template_dir), 'step': step, 'correlation_id': correlation_id})
  - L97 return manifest_path
- L100 def load_manifesttemplate_dir: Path:
  - L101 assign path = template_dir / MANIFEST_NAME
  - L102 if not path.exists():
    - L103 return None
  - L104 try:
    - L105 return json.loads(path.read_text(encoding='utf-8'))
    - L106 except Exception:
      - L107 expr logger.exception('manifest_load_failed', extra={'event': 'manifest_load_failed', 'path': str(path)})
      - L111 return None

## backend\app\services\utils\context.py
- L1 from __future__ import annotations
- L3 import contextvars
- L4 from typing import Optional
- L6 annotated assign _correlation_id: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar('correlation_id', default=None)
- L9 def set_correlation_idvalue: Optional[str]:
  - L10 expr _correlation_id.set(value)
- L13 def get_correlation_id:
  - L14 return _correlation_id.get()

## backend\app\services\utils\fs.py
- L1 from __future__ import annotations
- L3 import contextlib
- L4 import json
- L5 import logging
- L6 import os
- L7 import tempfile
- L8 from pathlib import Path
- L9 from typing import Any
- L11 assign logger = logging.getLogger('neura.fs')
- L14 def _maybe_failstep: str | None:
  - L15 assign fail_after = os.getenv('NEURA_FAIL_AFTER_STEP')
  - L16 if step and fail_after and (fail_after.strip().lower() == step.strip().lower()):
    - L17 raise RuntimeError(f"Simulated failure after step '{step}'")
- L20 def write_text_atomicpath: Path, data: str | bytes, *, encoding: str='utf-8', step: str | None=None:
  - L21 docstring: "\n    Persist text to `path` atomically:\n      1. Write to a temp file within t..."
  - L27 assign path = path.resolve()
  - L28 expr path.parent.mkdir(parents=True, exist_ok=True)
  - L29 assign (fd, tmp_name) = tempfile.mkstemp(dir=str(path.parent), prefix=f'.{path.name}.', suffix='.tmp')
  - L34 assign tmp_path = Path(tmp_name)
  - L35 assign binary = isinstance(data, (bytes, bytearray))
  - L36 try:
    - L37 if binary:
      - L38 with os.fdopen(fd, 'wb') as tmp_file:
        - L39 expr tmp_file.write(data)
        - L40 expr tmp_file.flush()
        - L41 with contextlib.suppress(OSError):
          - L42 expr os.fsync(tmp_file.fileno())
      - L44 else:
        - L44 with os.fdopen(fd, 'w', encoding=encoding, newline='') as tmp_file:
          - L45 expr tmp_file.write(data)
          - L46 expr tmp_file.flush()
          - L47 with contextlib.suppress(OSError):
            - L48 expr os.fsync(tmp_file.fileno())
    - L49 expr _maybe_fail(step)
    - L50 expr tmp_path.replace(path)
    - L51 except Exception:
      - L52 expr logger.exception('atomic_write_failed', extra={'path': str(path)})
      - L53 raise
    - L55 finally:
      - L55 with contextlib.suppress(FileNotFoundError):
        - L56 expr tmp_path.unlink()
- L59 def write_json_atomicpath: Path, payload: Any, *, encoding: str='utf-8', indent: int | None=2, ensure_ascii: bool=False, sort_keys: bool=False, step: str | None=None:
  - L69 docstring: "\n    Serialize payload to JSON and write atomically.\n    Mirrors json.dumps kw..."
  - L73 assign data = json.dumps(payload, indent=indent, ensure_ascii=ensure_ascii, sort_keys=sort_keys)
  - L79 expr write_text_atomic(path, data, encoding=encoding, step=step)

## backend\app\services\utils\html.py
- L1 from __future__ import annotations
- L3 import re
- L4 from typing import Dict, List
- L6 import bleach
- L7 from bleach.css_sanitizer import CSSSanitizer
- L9 assign ALLOWED_TAGS = {'html', 'head', 'body', 'meta', 'title', 'link', 'style', 'div', 'span', 'section', 'article', 'header', 'footer', 'main', 'table', 'thead', 'tbody', 'tfoot', 'tr', 'th', 'td', 'colgroup', 'col', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'strong', 'em', 'b', 'i', 'u', 'br', 'hr', 'img', 'canvas', 'svg'}
- L55 assign _REPEAT_COMMENT_RE = re.compile('^\\s*(BEGIN:BLOCK_REPEAT\\b.*|END:BLOCK_REPEAT\\b.*)\\s*$', re.IGNORECASE)
- L57 assign ALLOWED_ATTRS = {'*': {'class', 'style', 'id', 'colspan', 'rowspan', 'align', 'valign', 'width', 'height', 'data-title', 'data-index', 'data-name', 'data-value', 'data-label'}, 'img': {'src', 'alt'}, 'meta': {'charset'}, 'link': {'rel', 'href'}, 'svg': {'viewbox', 'xmlns'}, 'canvas': {'width', 'height'}}
- L81 assign _CSS_SANITIZER = CSSSanitizer()
- L82 assign _COMMENT_RE = re.compile('<!--(.*?)-->', re.DOTALL)
- L85 def _bleach_attributes:
  - L86 annotated assign attrs: Dict[str, List[str]] = {}
  - L87 for (tag, allowed) in ALLOWED_ATTRS.items():
    - L88 assign attrs[tag] = sorted(allowed)
  - L89 return attrs
- L92 def _filter_commentshtml: str:
  - L93 def _replacematch: re.Match[str]:
    - L94 assign data = match.group(1)
    - L95 if _REPEAT_COMMENT_RE.match(data):
      - L96 return f'<!--{data}-->'
    - L97 return ''
  - L99 return _COMMENT_RE.sub(_replace, html)
- L102 def sanitize_htmlhtml: str:
  - L103 assign cleaned = bleach.clean(html or '', tags=sorted(ALLOWED_TAGS), attributes=_bleach_attributes(), protocols=['http', 'https', 'data'], strip=True, strip_comments=False, css_sanitizer=_CSS_SANITIZER)
  - L112 return _filter_comments(cleaned)

## backend\app\services\utils\llm.py
- L2 from __future__ import annotations
- L4 import json
- L5 import logging
- L6 import os
- L7 import threading
- L8 import time
- L9 from datetime import datetime
- L10 from pathlib import Path
- L11 from types import SimpleNamespace
- L12 from typing import Any, Callable, Dict, Iterable, Tuple
- L14 try:
  - L15 from openai import RateLimitError
  - L16 except ImportError:
    - L17 assign RateLimitError = None
- L19 assign logger = logging.getLogger('neura.llm')
- L21 assign _TIMEOUT_RAW = os.getenv('OPENAI_REQUEST_TIMEOUT_SECONDS')
- L22 if _TIMEOUT_RAW not in (None, '', '0'):
  - L23 try:
    - L24 assign _DEFAULT_TIMEOUT = float(_TIMEOUT_RAW)
    - L25 except ValueError:
      - L26 expr logger.warning('invalid_openai_timeout', extra={'event': 'invalid_openai_timeout', 'value': _TIMEOUT_RAW})
      - L33 assign _DEFAULT_TIMEOUT = None
  - L35 else:
    - L35 assign _DEFAULT_TIMEOUT = None
- L37 assign _MAX_ATTEMPTS = int(os.getenv('OPENAI_MAX_ATTEMPTS', '3'))
- L38 assign _BACKOFF_INITIAL = float(os.getenv('OPENAI_BACKOFF_SECONDS', '1.5'))
- L39 assign _BACKOFF_MULTIPLIER = float(os.getenv('OPENAI_BACKOFF_MULTIPLIER', '2.0'))
- L40 assign _FORCE_GPT5 = os.getenv('NEURA_FORCE_GPT5', 'true').lower() in {'1', 'true', 'yes'}
- L42 assign _LOG_PATH_ENV = os.getenv('LLM_RAW_OUTPUT_PATH')
- L43 if _LOG_PATH_ENV:
  - L44 assign _RAW_OUTPUT_PATH = Path(_LOG_PATH_ENV).expanduser()
  - L46 else:
    - L46 assign _RAW_OUTPUT_PATH = Path(__file__).resolve().parents[3] / 'llm_raw_outputs.md'
- L48 assign _RAW_OUTPUT_LOCK = threading.Lock()
- L51 def _coerce_jsonablevalue: Any:
  - L52 docstring: "Best-effort conversion of OpenAI responses to JSON-serialisable data."
  - L53 if isinstance(value, (str, int, float, bool)) or value is None:
    - L54 return value
  - L56 if isinstance(value, dict):
    - L57 return {str(k): _coerce_jsonable(v) for k, v in value.items()}
  - L59 if isinstance(value, (list, tuple, set)):
    - L60 return [_coerce_jsonable(v) for v in value]
  - L62 for attr in ('model_dump', 'to_dict', 'dict'):
    - L63 assign method = getattr(value, attr, None)
    - L64 if callable(method):
      - L65 try:
        - L66 return _coerce_jsonable(method())
        - L67 except Exception:
          - L68 continue
  - L70 assign json_method = getattr(value, 'model_dump_json', None)
  - L71 if callable(json_method):
    - L72 try:
      - L73 return _coerce_jsonable(json.loads(json_method()))
      - L74 except Exception:
        - L75 pass
  - L77 return repr(value)
- L80 def _append_raw_outputdescription: str, response: Any:
  - L81 docstring: "Append the raw LLM response to a Markdown log file."
  - L82 assign timestamp = datetime.utcnow().isoformat(timespec='seconds') + 'Z'
  - L83 assign entry = _coerce_jsonable(response)
  - L85 try:
    - L86 with _RAW_OUTPUT_LOCK:
      - L87 expr _RAW_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
      - L88 with _RAW_OUTPUT_PATH.open('a', encoding='utf-8') as handle:
        - L89 expr handle.write(f'## {timestamp} - {description}\n\n')
        - L90 expr handle.write('```json\n')
        - L91 expr handle.write(json.dumps(entry, indent=2))
        - L92 expr handle.write('\n```\n\n')
    - L93 except Exception as exc:
      - L94 expr logger.debug('llm_raw_output_log_failed', extra={'event': 'llm_raw_output_log_failed'}, exc_info=(type(exc), exc, exc.__traceback__))
- L101 def _resolve_createclient: Any, timeout: float | None:
  - L102 docstring: "\n    Return (callable, extra_kwargs) that can be used to issue a chat completio..."
  - L107 assign target = client
  - L108 annotated assign extra_kwargs: Dict[str, Any] = {}
  - L110 if timeout is not None and hasattr(client, 'with_options'):
    - L111 assign target = client.with_options(timeout=timeout)
    - L112 else:
      - L112 if timeout is not None:
        - L113 assign extra_kwargs['timeout'] = timeout
  - L115 if hasattr(target, 'chat_completions'):
    - L116 return (target.chat_completions.create, extra_kwargs)
  - L118 assign chat = getattr(target, 'chat', None)
  - L119 if chat is not None and hasattr(chat, 'completions'):
    - L120 return (chat.completions.create, extra_kwargs)
  - L122 raise AttributeError('OpenAI client does not expose chat completions API')
- L125 def _resolve_responses_createclient: Any, timeout: float | None:
  - L126 assign target = client
  - L127 annotated assign extra_kwargs: Dict[str, Any] = {}
  - L129 if timeout is not None and hasattr(client, 'with_options'):
    - L130 assign target = client.with_options(timeout=timeout)
    - L131 else:
      - L131 if timeout is not None:
        - L132 assign extra_kwargs['timeout'] = timeout
  - L134 assign responses = getattr(target, 'responses', None)
  - L135 if responses is not None and hasattr(responses, 'create'):
    - L136 return (responses.create, extra_kwargs)
  - L138 raise AttributeError('OpenAI client does not expose responses API')
- L141 def _messages_to_responses_inputmessages: Iterable[Dict[str, Any]]:
  - L142 annotated assign converted: list[Dict[str, Any]] = []
  - L143 for message in messages:
    - L144 if not isinstance(message, dict):
      - L145 continue
    - L146 assign role = message.get('role') or 'user'
    - L147 assign content = message.get('content', '')
    - L148 if isinstance(content, list):
      - L149 annotated assign parts: list[Dict[str, Any]] = []
      - L150 for part in content:
        - L151 if isinstance(part, dict):
          - L152 assign part_type = part.get('type')
          - L153 if part_type == 'text':
            - L154 expr parts.append({'type': 'input_text', 'text': part.get('text', '')})
            - L155 continue
          - L156 if part_type == 'image_url':
            - L157 assign image_url = part.get('image_url')
            - L158 if isinstance(image_url, dict):
              - L159 assign image_url = image_url.get('url') or image_url.get('image_url')
            - L160 expr parts.append({'type': 'input_image', 'image_url': image_url})
            - L161 continue
          - L162 expr parts.append(part)
          - L164 else:
            - L164 expr parts.append({'type': 'input_text', 'text': str(part)})
      - L165 assign content = parts
    - L166 expr converted.append({'role': role, 'content': content})
  - L167 return converted
- L170 def _response_output_textresponse: Any:
  - L171 if isinstance(response, dict):
    - L172 assign output_text = response.get('output_text')
    - L173 if isinstance(output_text, str) and output_text.strip():
      - L174 return output_text
    - L175 assign output = response.get('output')
    - L177 else:
      - L177 assign output_text = getattr(response, 'output_text', None)
      - L178 if isinstance(output_text, str) and output_text.strip():
        - L179 return output_text
      - L180 assign output = getattr(response, 'output', None)
  - L182 if isinstance(output, list):
    - L183 annotated assign texts: list[str] = []
    - L184 for item in output:
      - L185 if isinstance(item, dict):
        - L186 assign item_type = item.get('type')
        - L187 assign content = item.get('content') or []
        - L189 else:
          - L189 assign item_type = getattr(item, 'type', None)
          - L190 assign content = getattr(item, 'content', None) or []
      - L191 if item_type != 'message':
        - L192 continue
      - L193 for segment in content:
        - L194 if isinstance(segment, dict):
          - L195 assign seg_type = segment.get('type')
          - L196 assign text = segment.get('text')
          - L198 else:
            - L198 assign seg_type = getattr(segment, 'type', None)
            - L199 assign text = getattr(segment, 'text', None)
        - L200 if seg_type in {'output_text', 'text'} and isinstance(text, str):
          - L201 expr texts.append(text)
    - L202 if texts:
      - L203 return '\n'.join(texts)
  - L204 return ''
- L207 def _response_usage_to_chat_usageresponse: Any:
  - L208 assign usage = response.get('usage') if isinstance(response, dict) else getattr(response, 'usage', None)
  - L209 if usage is None:
    - L210 return None
  - L211 if isinstance(usage, dict):
    - L212 assign input_tokens = usage.get('input_tokens') or usage.get('prompt_tokens') or 0
    - L213 assign output_tokens = usage.get('output_tokens') or usage.get('completion_tokens') or 0
    - L215 else:
      - L215 assign input_tokens = getattr(usage, 'input_tokens', None)
      - L216 if input_tokens is None:
        - L217 assign input_tokens = getattr(usage, 'prompt_tokens', 0)
      - L218 assign output_tokens = getattr(usage, 'output_tokens', None)
      - L219 if output_tokens is None:
        - L220 assign output_tokens = getattr(usage, 'completion_tokens', 0)
  - L221 assign total_tokens = (input_tokens or 0) + (output_tokens or 0)
  - L222 return SimpleNamespace(prompt_tokens=int(input_tokens or 0), completion_tokens=int(output_tokens or 0), total_tokens=int(total_tokens or 0))
- L229 def _responses_to_chat_completionresponse: Any:
  - L230 assign output_text = _response_output_text(response)
  - L231 assign message = SimpleNamespace(content=output_text, role='assistant')
  - L232 assign choice = SimpleNamespace(message=message)
  - L233 assign model = response.get('model') if isinstance(response, dict) else getattr(response, 'model', None)
  - L234 assign response_id = response.get('id') if isinstance(response, dict) else getattr(response, 'id', None)
  - L235 assign usage = _response_usage_to_chat_usage(response)
  - L236 return SimpleNamespace(id=response_id, model=model, choices=[choice], usage=usage)
- L244 def _is_responses_required_errorexc: BaseException:
  - L245 assign (message, code, _, _) = _extract_openai_error_info(exc)
  - L246 assign detail = (message or str(exc)).lower()
  - L247 if (code or '').lower() == 'unsupported_endpoint':
    - L248 return True
  - L249 return 'responses api' in detail or 'responses endpoint' in detail or 'use the responses' in detail
- L252 def _is_temperature_unsupported_errorexc: BaseException:
  - L253 docstring: "Return True when the error indicates temperature overrides are not allowed."
  - L254 assign body = getattr(exc, 'body', None)
  - L255 if isinstance(body, dict):
    - L256 assign error = body.get('error')
    - L257 if isinstance(error, dict):
      - L258 if error.get('param') == 'temperature' and error.get('code') == 'unsupported_value':
        - L259 return True
      - L260 assign message = str(error.get('message') or '')
      - L262 else:
        - L262 assign message = ''
    - L264 else:
      - L264 assign message = ''
  - L266 assign detail = message or str(getattr(exc, 'message', '')) or str(exc)
  - L267 assign detail_lower = detail.lower()
  - L268 return 'temperature' in detail_lower and 'unsupported' in detail_lower
- L271 def _is_quota_exceeded_errorexc: BaseException:
  - L272 docstring: "Return True when the exception represents an OpenAI quota / rate limit exhaustio..."
  - L273 if RateLimitError is not None and isinstance(exc, RateLimitError):
    - L274 return True
  - L276 assign body = getattr(exc, 'body', None)
  - L277 if isinstance(body, dict):
    - L278 assign error = body.get('error')
    - L279 if isinstance(error, dict):
      - L280 assign code = str(error.get('code') or '').lower()
      - L281 if code == 'insufficient_quota':
        - L282 return True
      - L283 assign error_type = str(error.get('type') or '').lower()
      - L284 if error_type == 'insufficient_quota':
        - L285 return True
      - L286 assign message = error.get('message')
      - L287 if isinstance(message, str):
        - L288 assign message_lower = message.lower()
        - L289 if 'insufficient_quota' in message_lower:
          - L290 return True
        - L291 if 'quota' in message_lower and ('exceeded' in message_lower or 'insufficient' in message_lower):
          - L292 return True
  - L294 assign detail = str(exc)
  - L295 assign detail_lower = detail.lower()
  - L296 if 'insufficient_quota' in detail_lower:
    - L297 return True
  - L298 return 'exceeded your current quota' in detail_lower
- L301 def _extract_openai_error_infoexc: BaseException:
  - L302 docstring: "Return (message, code, type, param) from an OpenAI exception when available."
  - L303 assign body = getattr(exc, 'body', None)
  - L304 if isinstance(body, dict):
    - L305 assign error = body.get('error')
    - L306 if isinstance(error, dict):
      - L307 assign message = error.get('message')
      - L308 assign code = error.get('code')
      - L309 assign err_type = error.get('type')
      - L310 assign param = error.get('param')
      - L311 return (str(message or '').strip(), str(code or '').strip() or None, str(err_type or '').strip() or None, str(param or '').strip() or None)
  - L317 assign message = getattr(exc, 'message', None)
  - L318 if isinstance(message, str) and message.strip():
    - L319 return (message.strip(), None, None, None)
  - L320 return (str(exc), None, None, None)
- L323 def _is_model_not_found_errorexc: BaseException:
  - L324 docstring: "Return True when the error indicates an invalid or unavailable model."
  - L325 assign (message, code, _, _) = _extract_openai_error_info(exc)
  - L326 if (code or '').lower() == 'model_not_found':
    - L327 return True
  - L328 assign detail = (message or str(exc)).lower()
  - L329 return 'model' in detail and ('not found' in detail or 'does not exist' in detail or 'do not have access' in detail)
- L332 def _get_fallback_modelsprimary_model: str:
  - L333 assign raw = os.getenv('OPENAI_FALLBACK_MODELS', '')
  - L334 if raw.strip():
    - L335 assign candidates = [m.strip() for m in raw.split(',') if m.strip()]
    - L337 else:
      - L337 assign candidates = []
  - L338 annotated assign out: list[str] = []
  - L339 annotated assign seen: set[str] = set()
  - L340 for model in candidates:
    - L341 if model == primary_model or model in seen:
      - L342 continue
    - L343 expr seen.add(model)
    - L344 expr out.append(model)
  - L345 return out
- L348 def _extract_openai_error_messageexc: BaseException:
  - L349 docstring: "Return the most meaningful error message we can extract from an OpenAI exception..."
  - L350 assign body = getattr(exc, 'body', None)
  - L351 if isinstance(body, dict):
    - L352 assign error = body.get('error')
    - L353 if isinstance(error, dict):
      - L354 assign message = error.get('message')
      - L355 if isinstance(message, str) and message.strip():
        - L356 return message.strip()
  - L358 assign message = getattr(exc, 'message', None)
  - L359 if isinstance(message, str) and message.strip():
    - L360 return message.strip()
  - L361 return str(exc)
- L364 def call_chat_completionclient: Any, *, model: str, messages: Iterable[Dict[str, Any]], description: str, timeout: float | None=None, **kwargs: Any:
  - L373 docstring: "\n    Execute a chat completion with retries, exponential backoff, and timeout s..."
  - L384 assign timeout = timeout if timeout is not None else _DEFAULT_TIMEOUT
  - L385 if _FORCE_GPT5 and (not str(model or '').lower().startswith('gpt-5')):
    - L386 expr logger.warning('llm_model_overridden', extra={'event': 'llm_model_overridden', 'requested': model, 'forced': 'gpt-5'})
    - L390 assign model = 'gpt-5'
  - L391 assign delay = _BACKOFF_INITIAL
  - L392 annotated assign last_exc: BaseException | None = None
  - L393 assign fallback_models = _get_fallback_models(model)
  - L394 assign force_responses = os.getenv('OPENAI_USE_RESPONSES', '').lower() in {'1', 'true', 'yes'}
  - L396 def _wants_responsesmodel_name: str:
    - L397 return force_responses or str(model_name or '').lower().startswith('gpt-5')
  - L399 assign prefer_responses = _wants_responses(model)
  - L401 assign attempt = 1
  - L402 while attempt <= _MAX_ATTEMPTS:
    - L403 assign quota_exceeded = False
    - L404 assign using_responses = False
    - L405 try:
      - L406 if prefer_responses:
        - L407 try:
          - L408 assign (create_fn, extra_kwargs) = _resolve_responses_create(client, timeout)
          - L409 except AttributeError as exc:
            - L410 if _wants_responses(model):
              - L411 raise RuntimeError('OpenAI Responses API is required for this model. Upgrade the openai package to >=1.0.0 to use gpt-5.')
            - L415 assign prefer_responses = False
          - L417 else:
            - L417 assign payload = {'model': model, 'input': _messages_to_responses_input(messages), **kwargs, **extra_kwargs}
            - L423 if 'max_tokens' in payload and 'max_output_tokens' not in payload:
              - L424 assign payload['max_output_tokens'] = payload.pop('max_tokens')
            - L426 expr logger.info('llm_call_start', extra={'event': 'llm_call_start', 'description': description, 'attempt': attempt, 'model': model, 'endpoint': 'responses'})
            - L436 assign using_responses = True
            - L437 assign response = create_fn(**payload)
            - L438 expr _append_raw_output(description, response)
            - L439 expr logger.info('llm_call_success', extra={'event': 'llm_call_success', 'description': description, 'attempt': attempt, 'model': model, 'endpoint': 'responses'})
            - L449 return _responses_to_chat_completion(response)
      - L451 assign (create_fn, extra_kwargs) = _resolve_create(client, timeout)
      - L452 assign payload = {'model': model, 'messages': list(messages), **kwargs, **extra_kwargs}
      - L458 expr logger.info('llm_call_start', extra={'event': 'llm_call_start', 'description': description, 'attempt': attempt, 'model': model})
      - L467 assign response = create_fn(**payload)
      - L468 expr _append_raw_output(description, response)
      - L469 expr logger.info('llm_call_success', extra={'event': 'llm_call_success', 'description': description, 'attempt': attempt, 'model': model})
      - L478 return response
      - L479 except TypeError as exc:
        - L481 if 'timeout' in payload:
          - L482 expr payload.pop('timeout', None)
          - L483 try:
            - L484 assign response = create_fn(**payload)
            - L485 expr _append_raw_output(description, response)
            - L486 expr logger.info('llm_call_success', extra={'event': 'llm_call_success', 'description': description, 'attempt': attempt, 'model': model, 'timeout_dropped': True})
            - L496 return _responses_to_chat_completion(response) if using_responses else response
            - L497 except Exception as inner_exc:
              - L498 assign last_exc = inner_exc
              - L499 assign quota_exceeded = _is_quota_exceeded_error(inner_exc)
          - L501 else:
            - L501 assign last_exc = exc
            - L502 assign quota_exceeded = _is_quota_exceeded_error(exc)
      - L503 except Exception as exc:
        - L504 if not prefer_responses and _is_responses_required_error(exc):
          - L505 assign prefer_responses = True
          - L506 assign last_exc = exc
          - L507 continue
        - L508 if _is_model_not_found_error(exc) and fallback_models:
          - L509 assign next_model = fallback_models.pop(0)
          - L510 expr logger.warning('llm_model_fallback', extra={'event': 'llm_model_fallback', 'description': description, 'from_model': model, 'to_model': next_model})
          - L519 assign model = next_model
          - L520 assign prefer_responses = _wants_responses(model)
          - L521 assign last_exc = exc
          - L522 continue
        - L523 if 'temperature' in kwargs and _is_temperature_unsupported_error(exc):
          - L524 expr logger.info('llm_temperature_override_removed', extra={'event': 'llm_temperature_override_removed', 'description': description, 'attempt': attempt, 'model': model})
          - L533 expr kwargs.pop('temperature', None)
          - L534 continue
        - L535 assign last_exc = exc
        - L536 assign quota_exceeded = _is_quota_exceeded_error(exc)
    - L538 if quota_exceeded:
      - L539 break
    - L541 expr logger.warning('llm_call_retry', extra={'event': 'llm_call_retry', 'description': description, 'attempt': attempt, 'max_attempts': _MAX_ATTEMPTS, 'model': model, 'retry_in': delay if attempt < _MAX_ATTEMPTS else None})
    - L553 if attempt >= _MAX_ATTEMPTS:
      - L554 break
    - L556 expr time.sleep(delay)
    - L557 aug assign delay Mult _BACKOFF_MULTIPLIER
    - L558 aug assign attempt Add 1
  - L560 assert last_exc is not None
  - L561 assign quota_exceeded = _is_quota_exceeded_error(last_exc)
  - L562 expr logger.error('llm_call_failed', extra={'event': 'llm_call_failed', 'description': description, 'attempts': _MAX_ATTEMPTS, 'model': model, 'quota_exceeded': quota_exceeded}, exc_info=last_exc)
  - L573 if quota_exceeded:
    - L574 assign message = _extract_openai_error_message(last_exc)
    - L575 raise RuntimeError(f"{description} failed because the OpenAI API quota was exceeded. {message or 'Please review your API plan and billing details.'}")
  - L579 if _is_model_not_found_error(last_exc):
    - L580 assign (message, code, _, _) = _extract_openai_error_info(last_exc)
    - L581 raise RuntimeError(f"{description} failed because the OpenAI model '{model}' is not available. Set OPENAI_MODEL to a valid model you have access to. {message or code or ''}".strip())
  - L585 assign message = _extract_openai_error_message(last_exc)
  - L586 raise RuntimeError(f'{description} failed after {_MAX_ATTEMPTS} attempts. {message}'.strip())

## backend\app\services\utils\lock.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import os
- L5 import time
- L6 from contextlib import contextmanager
- L7 from pathlib import Path
- L8 from typing import Generator, Optional
- L10 assign logger = logging.getLogger('neura.lock')
- L13 class TemplateLockError(RuntimeError):
  - L14 docstring: "Raised when a template lock cannot be acquired."
  - L16 def __init__self, message: str, lock_holder: Optional[str]=None:
    - L17 expr super().__init__(message)
    - L18 assign self.lock_holder = lock_holder
- L21 def _locks_enabled:
  - L22 assign disable_flag = os.getenv('NEURA_DISABLE_LOCKS', '').lower()
  - L23 if disable_flag in {'1', 'true', 'yes'}:
    - L24 return False
  - L25 assign enable_flag = os.getenv('NEURA_LOCKS_ENABLED', '').lower()
  - L26 if enable_flag in {'1', 'true', 'yes'}:
    - L27 return True
  - L28 if os.getenv('PYTEST_CURRENT_TEST'):
    - L29 return False
  - L30 return True
- L33 class FileLock:
  - L34 docstring: "\n    Cross-platform file-based lock implementation.\n    Uses atomic file creat..."
  - L39 def __init__self, lock_path: Path, timeout: float=30.0, poll_interval: float=0.1, stale_timeout: float=300.0:
    - L46 assign self.lock_path = Path(lock_path)
    - L47 assign self.timeout = timeout
    - L48 assign self.poll_interval = poll_interval
    - L49 assign self.stale_timeout = stale_timeout
    - L50 assign self._locked = False
  - L52 def _get_lock_infoself:
    - L53 docstring: "Read lock file info if it exists."
    - L54 try:
      - L55 if self.lock_path.exists():
        - L56 assign content = self.lock_path.read_text(encoding='utf-8')
        - L57 assign lines = content.strip().split('\n')
        - L58 return {'pid': int(lines[0]) if len(lines) > 0 else 0, 'timestamp': float(lines[1]) if len(lines) > 1 else 0, 'holder': lines[2] if len(lines) > 2 else 'unknown'}
      - L63 except (ValueError, IOError, OSError):
        - L64 pass
    - L65 return None
  - L67 def _is_staleself, lock_info: dict:
    - L68 docstring: "Check if lock is stale (holder process gone or timeout)."
    - L70 if time.time() - lock_info.get('timestamp', 0) > self.stale_timeout:
      - L71 return True
    - L74 assign pid = lock_info.get('pid', 0)
    - L75 if pid > 0:
      - L76 try:
        - L78 expr os.kill(pid, 0)
        - L79 except (OSError, ProcessLookupError):
          - L80 return True
    - L82 return False
  - L84 def _write_lockself, holder: str:
    - L85 docstring: "Write lock file with current process info."
    - L86 assign content = f'{os.getpid()}\n{time.time()}\n{holder}'
    - L87 expr self.lock_path.write_text(content, encoding='utf-8')
  - L89 def acquireself, holder: str='unknown':
    - L90 docstring: "\n        Attempt to acquire the lock.\n        Returns True if lock was acquire..."
    - L94 assign deadline = time.time() + self.timeout
    - L96 while time.time() < deadline:
      - L98 assign lock_info = self._get_lock_info()
      - L100 if lock_info is not None:
        - L102 if self._is_stale(lock_info):
          - L103 expr logger.warning('stale_lock_found', extra={'event': 'stale_lock_found', 'lock_path': str(self.lock_path), 'stale_holder': lock_info.get('holder'), 'stale_pid': lock_info.get('pid')})
          - L113 try:
            - L114 expr self.lock_path.unlink()
            - L115 except (OSError, FileNotFoundError):
              - L116 pass
          - L119 else:
            - L119 expr time.sleep(self.poll_interval)
            - L120 continue
      - L123 try:
        - L125 assign fd = os.open(str(self.lock_path), os.O_CREAT | os.O_EXCL | os.O_WRONLY, 420)
        - L130 expr os.close(fd)
        - L131 expr self._write_lock(holder)
        - L132 assign self._locked = True
        - L133 expr logger.debug('lock_acquired', extra={'event': 'lock_acquired', 'lock_path': str(self.lock_path), 'holder': holder})
        - L141 return True
        - L142 except FileExistsError:
          - L144 expr time.sleep(self.poll_interval)
          - L145 continue
        - L146 except (OSError, IOError) as e:
          - L147 expr logger.warning(f'Lock creation failed: {e}')
          - L148 expr time.sleep(self.poll_interval)
          - L149 continue
    - L151 return False
  - L153 def releaseself:
    - L154 docstring: "Release the lock."
    - L155 if self._locked:
      - L156 try:
        - L157 expr self.lock_path.unlink()
        - L158 expr logger.debug('lock_released', extra={'event': 'lock_released', 'lock_path': str(self.lock_path)})
        - L165 except (OSError, FileNotFoundError):
          - L166 pass
        - L168 finally:
          - L168 assign self._locked = False
- L172 def acquire_template_locktemplate_dir: Path, name: str, correlation_id: str | None=None, timeout: float=30.0:
  - L178 docstring: "\n    Context manager for template locking.\n    Prevents concurrent modificatio..."
  - L191 if not _locks_enabled():
    - L192 expr (yield)
    - L193 return None
  - L195 assign lock_path = Path(template_dir) / f'.lock.{name}'
  - L196 assign holder = f'pid={os.getpid()}'
  - L197 if correlation_id:
    - L198 assign holder = f'{holder},corr={correlation_id}'
  - L200 assign lock = FileLock(lock_path, timeout=timeout)
  - L202 expr logger.info('lock_acquiring', extra={'event': 'lock_acquiring', 'lock': str(lock_path), 'holder': holder, 'correlation_id': correlation_id})
  - L212 if not lock.acquire(holder):
    - L213 assign lock_info = lock._get_lock_info()
    - L214 assign current_holder = lock_info.get('holder') if lock_info else 'unknown'
    - L215 raise TemplateLockError(f"Failed to acquire template lock '{name}' within {timeout}s. Currently held by: {current_holder}", lock_holder=current_holder)
  - L221 try:
    - L222 expr (yield)
    - L224 finally:
      - L224 expr lock.release()
- L228 def try_acquire_template_locktemplate_dir: Path, name: str, correlation_id: str | None=None, timeout: float=5.0:
  - L234 docstring: "\n    Non-blocking version that yields True if lock acquired, False otherwise.\n..."
  - L238 if not _locks_enabled():
    - L239 expr (yield True)
    - L240 return None
  - L242 assign lock_path = Path(template_dir) / f'.lock.{name}'
  - L243 assign holder = f'pid={os.getpid()}'
  - L244 if correlation_id:
    - L245 assign holder = f'{holder},corr={correlation_id}'
  - L247 assign lock = FileLock(lock_path, timeout=timeout)
  - L248 assign acquired = lock.acquire(holder)
  - L250 try:
    - L251 expr (yield acquired)
    - L253 finally:
      - L253 if acquired:
        - L254 expr lock.release()

## backend\app\services\utils\mailer.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import mimetypes
- L5 import os
- L6 import smtplib
- L7 import ssl
- L8 from dataclasses import dataclass
- L9 from email.message import EmailMessage
- L10 from pathlib import Path
- L11 from typing import Iterable, Sequence
- L13 assign logger = logging.getLogger('neura.mailer')
- L16 def _env_boolname: str, default: bool=True:
  - L17 assign raw = os.getenv(name)
  - L18 if raw is None:
    - L19 return default
  - L20 return raw.strip().lower() in {'1', 'true', 'yes', 'on'}
- L23 def _env_intname: str, default: int:
  - L24 assign raw = os.getenv(name)
  - L25 if raw is None:
    - L26 return default
  - L27 try:
    - L28 return int(raw)
    - L29 except ValueError:
      - L30 expr logger.warning('invalid_mail_port', extra={'event': 'invalid_mail_port', 'env': name, 'value': raw})
      - L31 return default
- L35 class MailerConfig:
  - L36 annotated assign host: str | None
  - L37 annotated assign port: int
  - L38 annotated assign username: str | None
  - L39 annotated assign password: str | None
  - L40 annotated assign sender: str | None
  - L41 annotated assign use_tls: bool
  - L42 annotated assign enabled: bool
- L45 def _load_mailer_config:
  - L46 assign host = os.getenv('NEURA_MAIL_HOST')
  - L47 assign sender = os.getenv('NEURA_MAIL_SENDER')
  - L48 assign username = os.getenv('NEURA_MAIL_USERNAME')
  - L49 assign password = os.getenv('NEURA_MAIL_PASSWORD')
  - L50 assign port = _env_int('NEURA_MAIL_PORT', 587)
  - L51 assign use_tls = _env_bool('NEURA_MAIL_USE_TLS', True)
  - L52 assign enabled = bool(host and sender)
  - L53 if not enabled:
    - L54 expr logger.info('mail_disabled', extra={'event': 'mail_disabled', 'reason': 'host_or_sender_missing', 'host': bool(host), 'sender': bool(sender)})
  - L58 return MailerConfig(host=host, port=port, username=username, password=password, sender=sender, use_tls=use_tls, enabled=enabled)
- L69 assign MAILER_CONFIG = _load_mailer_config()
- L72 def refresh_mailer_config:
  - L73 Global
  - L74 assign MAILER_CONFIG = _load_mailer_config()
  - L75 return MAILER_CONFIG
- L78 def _normalize_recipientsrecipients: Iterable[str] | None:
  - L79 if not recipients:
    - L80 return []
  - L81 annotated assign seen: set[str] = set()
  - L82 annotated assign normalized: list[str] = []
  - L83 for raw in recipients:
    - L84 assign text = str(raw or '').strip()
    - L85 if not text or text in seen:
      - L86 continue
    - L87 expr seen.add(text)
    - L88 expr normalized.append(text)
  - L89 return normalized
- L92 def send_report_email*, to_addresses: Sequence[str], subject: str, body: str, attachments: Sequence[Path] | None=None:
  - L99 assign config = MAILER_CONFIG
  - L100 assign recipients = _normalize_recipients(to_addresses)
  - L101 if not recipients:
    - L102 return False
  - L103 if not config.enabled or not config.host or (not config.sender):
    - L104 expr logger.warning('mail_not_configured', extra={'event': 'mail_not_configured', 'recipients': len(recipients)})
    - L108 return False
  - L110 assign message = EmailMessage()
  - L111 assign message['From'] = config.sender
  - L112 assign message['To'] = ', '.join(recipients)
  - L113 assign message['Subject'] = subject
  - L114 expr message.set_content(body)
  - L116 if attachments:
    - L117 for path in attachments:
      - L118 if not path:
        - L119 continue
      - L120 try:
        - L121 assign resolved = Path(path).resolve(strict=True)
        - L122 except (FileNotFoundError, OSError):
          - L123 expr logger.warning('mail_attachment_missing', extra={'event': 'mail_attachment_missing', 'path': str(path)})
          - L127 continue
      - L128 assign (mime_type, encoding) = mimetypes.guess_type(str(resolved))
      - L129 assign maintype = 'application'
      - L130 assign subtype = 'octet-stream'
      - L131 if mime_type and '/' in mime_type:
        - L132 assign (maintype, subtype) = mime_type.split('/', 1)
      - L133 try:
        - L134 assign data = resolved.read_bytes()
        - L135 except OSError:
          - L136 expr logger.warning('mail_attachment_read_failed', extra={'event': 'mail_attachment_read_failed', 'path': str(resolved)})
          - L140 continue
      - L141 expr message.add_attachment(data, maintype=maintype, subtype=subtype, filename=resolved.name)
  - L143 try:
    - L144 if config.use_tls:
      - L145 with smtplib.SMTP(config.host, config.port, timeout=15) as client:
        - L146 assign context = ssl.create_default_context()
        - L147 expr client.starttls(context=context)
        - L148 if config.username and config.password:
          - L149 expr client.login(config.username, config.password)
        - L150 expr client.send_message(message)
      - L152 else:
        - L152 with smtplib.SMTP(config.host, config.port, timeout=15) as client:
          - L153 if config.username and config.password:
            - L154 expr client.login(config.username, config.password)
          - L155 expr client.send_message(message)
    - L156 except Exception:
      - L157 expr logger.exception('mail_send_failed', extra={'event': 'mail_send_failed', 'recipients': len(recipients)})
      - L161 return False
  - L163 expr logger.info('mail_sent', extra={'event': 'mail_sent', 'recipients': len(recipients)})
  - L167 return True

## backend\app\services\utils\prompts.py
- L1 from __future__ import annotations
- L3 from typing import Dict
- L5 from ..prompts.llm_prompts import PROMPT_LIBRARY
- L8 def load_promptkey: str, replacements: Dict[str, str] | None=None:
  - L9 docstring: "\n    Load a prompt by key from the in-memory prompt library and optionally repl..."
  - L12 if key not in PROMPT_LIBRARY:
    - L13 assign available = ', '.join(sorted(PROMPT_LIBRARY))
    - L14 raise KeyError(f"Prompt '{key}' not found. Available keys: {available}")
  - L16 assign prompt = PROMPT_LIBRARY[key]
  - L17 if replacements:
    - L18 for (needle, value) in replacements.items():
      - L19 assign prompt = prompt.replace(needle, value)
  - L20 return prompt
- L23 def available_prompts:
  - L24 docstring: "\n    Return a copy of the prompt library (key -> prompt text).\n    "
  - L27 return dict(PROMPT_LIBRARY)

## backend\app\services\utils\render.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import os
- L5 from pathlib import Path
- L6 from typing import Tuple
- L8 try:
  - L9 from PIL import Image
  - L10 except ImportError:
    - L11 assign Image = None
- L13 try:
  - L14 from playwright.sync_api import sync_playwright
  - L15 except ImportError:
    - L16 assign sync_playwright = None
- L18 assign logger = logging.getLogger('neura.render')
- L20 annotated assign _A4_MM: Tuple[float, float] = (210.0, 297.0)
- L21 assign _MM_PER_INCH = 25.4
- L24 def _page_viewportpage_size: str, dpi: int:
  - L25 docstring: "\n    Compute integer viewport (width, height) for the requested page size at th..."
  - L29 assign size = page_size.upper()
  - L30 if size != 'A4':
    - L31 raise ValueError(f"Unsupported page_size '{page_size}'. Only 'A4' is currently supported.")
  - L32 assign width_px = int(round(_A4_MM[0] / _MM_PER_INCH * dpi))
  - L33 assign height_px = int(round(_A4_MM[1] / _MM_PER_INCH * dpi))
  - L34 return (width_px, height_px)
- L37 def _ensure_dimensionspath: Path, target: Tuple[int, int]:
  - L38 docstring: "\n    Pad/crop the rendered screenshot to exactly match the target dimensions.\n..."
  - L41 if Image is None:
    - L42 expr logger.debug('render_image_adjust_skip', extra={'event': 'render_image_adjust_skip', 'path': str(path)})
    - L46 return None
  - L47 with Image.open(path) as img:
    - L48 if img.size == target:
      - L49 return None
    - L50 assign (width, height) = target
    - L51 if img.width > width or img.height > height:
      - L52 assign img = img.crop((0, 0, width, height))
    - L53 if img.width < width or img.height < height:
      - L54 assign canvas = Image.new('RGB', target, 'white')
      - L55 expr canvas.paste(img, (0, 0))
      - L56 assign img = canvas
    - L57 expr img.save(path)
    - L58 expr logger.info('render_image_adjusted', extra={'event': 'render_image_adjusted', 'path': str(path), 'target_width': width, 'target_height': height})
- L69 def _ensure_playwright_browsers_path:
  - L70 docstring: "\n    Patch in the system-level Playwright browser cache when packaging omits it..."
  - L73 if os.environ.get('PLAYWRIGHT_BROWSERS_PATH'):
    - L74 return None
  - L75 assign local_app = os.getenv('LOCALAPPDATA')
  - L76 if not local_app:
    - L77 return None
  - L78 assign candidate = Path(local_app) / 'ms-playwright'
  - L79 if candidate.exists():
    - L80 assign os.environ['PLAYWRIGHT_BROWSERS_PATH'] = str(candidate)
- L83 def render_html_to_pnghtml_path: Path, out_png_path: Path, *, page_size: str='A4', dpi: int=400, wait_until: str='networkidle':
  - L91 docstring: "\n    Render HTML to a PNG using Playwright's sync API.\n    Ensures determinist..."
  - L95 if sync_playwright is None:
    - L96 raise RuntimeError('playwright is required for HTML rendering. Install with `pip install playwright` and run `playwright install chromium`.')
  - L101 expr _ensure_playwright_browsers_path()
  - L103 assign html_path = Path(html_path).resolve()
  - L104 assign out_png_path = Path(out_png_path).resolve()
  - L105 expr out_png_path.parent.mkdir(parents=True, exist_ok=True)
  - L107 assign viewport = _page_viewport(page_size, dpi)
  - L109 with sync_playwright() as pw:
    - L110 assign browser = pw.chromium.launch()
    - L111 try:
      - L112 assign page = browser.new_page(viewport={'width': viewport[0], 'height': viewport[1]}, device_scale_factor=1)
      - L116 expr page.goto(f'file://{html_path}', wait_until=wait_until)
      - L117 expr page.screenshot(path=str(out_png_path), full_page=True)
      - L119 finally:
        - L119 expr browser.close()
  - L121 expr _ensure_dimensions(out_png_path, viewport)
  - L123 expr logger.info('html_rendered_to_png', extra={'event': 'html_rendered_to_png', 'html_path': str(html_path), 'png_path': str(out_png_path), 'page_size': page_size, 'dpi': dpi})
- L135 assign __all__ = ['render_html_to_png']

## backend\app\services\utils\text.py
- L1 from __future__ import annotations
- L3 import re
- L5 assign _FENCE_PATTERN = re.compile('^\\s*```(?:json)?\\s*([\\s\\S]*?)```', re.IGNORECASE)
- L8 def strip_code_fencestext: str:
  - L9 docstring: "\n    Remove surrounding markdown code fences (plaintext or ```json```),\n    re..."
  - L14 if not text:
    - L15 return ''
  - L16 assign match = _FENCE_PATTERN.search(text)
  - L17 if match:
    - L18 return match.group(1).strip()
  - L19 return text.strip()
- L22 assign __all__ = ['strip_code_fences']

## backend\app\services\utils\tokens.py
- L1 from __future__ import annotations
- L3 import re
- L4 from typing import List
- L6 assign _DOUBLE_BRACE_PATTERN = re.compile('\\{\\{\\s*([^{}]+?)\\s*\\}\\}')
- L7 assign _TOKEN_PATTERN = re.compile('\\{\\{[^{}]+\\}\\}|\\{[^{}]+\\}')
- L8 assign _TOKEN_NAME_PATTERN = re.compile('^[A-Za-z0-9_][A-Za-z0-9_\\-\\.]*$')
- L11 def normalize_token_bracestext: str:
  - L12 docstring: "\n    Convert double-braced placeholders like ``{{token}}`` into single-braced\n..."
  - L17 if not text:
    - L18 return text
  - L20 def _replacematch: re.Match[str]:
    - L21 assign inner = match.group(1).strip()
    - L22 if not inner:
      - L23 return match.group(0)
    - L24 return '{' + inner + '}'
  - L26 return _DOUBLE_BRACE_PATTERN.sub(_replace, text)
- L29 def extract_tokenshtml: str:
  - L30 docstring: "\n    Return a list of token names found in the HTML text.\n    Tokens wrapped i..."
  - L34 if not html:
    - L35 return []
  - L36 assign normalized = normalize_token_braces(html)
  - L37 annotated assign tokens: list[str] = []
  - L38 for match in _TOKEN_PATTERN.findall(normalized):
    - L39 assign token = match.strip()
    - L40 if token.startswith('{{') and token.endswith('}}'):
      - L41 assign token = token[2:-2]
      - L42 else:
        - L42 if token.startswith('{') and token.endswith('}'):
          - L43 assign token = token[1:-1]
    - L44 assign token = token.strip()
    - L45 if not token:
      - L46 continue
    - L47 if not _TOKEN_NAME_PATTERN.match(token):
      - L48 continue
    - L49 expr tokens.append(token)
  - L50 return tokens
- L53 assign __all__ = ['normalize_token_braces', 'extract_tokens']

## backend\app\services\utils\validation.py
- L1 from __future__ import annotations
- L3 import json
- L4 import re
- L5 from collections.abc import Iterable, Mapping
- L6 from copy import deepcopy
- L7 from itertools import zip_longest
- L8 from pathlib import Path
- L9 from typing import Any
- L11 try:
  - L12 from jsonschema import Draft7Validator
  - L13 except ImportError:
    - L14 assign Draft7Validator = None
- L16 assign SCHEMA_DIR = Path(__file__).resolve().parents[2] / 'schemas'
- L17 assign JSON_SCHEMA_DIR = Path(__file__).resolve().parent / 'json_schemas'
- L20 class SchemaValidationError(ValueError):
  - L21 pass
- L24 assign _DIRECT_COLUMN_RE = re.compile('^\\s*(?P<table>[A-Za-z_][\\w]*)\\s*\\.\\s*(?P<column>[A-Za-z_][\\w]*)\\s*$')
- L27 def _infer_parent_table_from_mappingmapping: Mapping[str, Any] | None:
  - L28 if not isinstance(mapping, Mapping):
    - L29 return None
  - L30 for expr in mapping.values():
    - L31 if not isinstance(expr, str):
      - L32 continue
    - L33 assign match = _DIRECT_COLUMN_RE.match(expr.strip())
    - L34 if not match:
      - L35 continue
    - L36 assign table_name = match.group('table').strip(' "`[]')
    - L37 if table_name and (not table_name.lower().startswith('params')):
      - L38 return table_name
  - L39 return None
- L42 def _load_schemaname: str:
  - L43 assign path = SCHEMA_DIR / name
  - L44 return json.loads(path.read_text(encoding='utf-8'))
- L47 assign MAPPING_SCHEMA = _load_schema('mapping_pdf_labels.schema.json')
- L48 assign CONTRACT_SCHEMA = _load_schema('contract.schema.json')
- L51 def _coerce_join_blockdata: Mapping[str, Any]:
  - L52 if not isinstance(data, Mapping):
    - L53 return None
  - L54 assign join_raw = data.get('join')
  - L55 assign join = dict(join_raw) if isinstance(join_raw, Mapping) else {}
  - L56 assign parent_table = str(join.get('parent_table') or '').strip()
  - L57 assign parent_key = str(join.get('parent_key') or '').strip()
  - L58 if not parent_table:
    - L59 assign inferred = _infer_parent_table_from_mapping(data.get('mapping'))
    - L60 if inferred:
      - L61 assign parent_table = inferred
  - L62 if not parent_table:
    - L63 return None
  - L64 if not parent_key:
    - L65 assign parent_key = '__rowid__'
  - L66 assign child_table = str(join.get('child_table') or '').strip()
  - L67 assign child_key = str(join.get('child_key') or '').strip()
  - L68 expr join.update({'parent_table': parent_table, 'parent_key': parent_key, 'child_table': child_table, 'child_key': child_key})
  - L76 return join
- L79 if Draft7Validator is not None:
  - L80 assign _MAPPING_INLINE_V4_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'mapping_inline_v4.schema.json').read_text(encoding='utf-8'))
  - L83 assign _MAPPING_INLINE_V4_VALIDATOR = Draft7Validator(_MAPPING_INLINE_V4_SCHEMA)
  - L84 assign _LLM_CALL_3_5_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'llm_call_3_5.schema.json').read_text(encoding='utf-8'))
  - L85 assign _LLM_CALL_3_5_VALIDATOR = Draft7Validator(_LLM_CALL_3_5_SCHEMA)
  - L86 assign _CONTRACT_V2_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'contract_v2.schema.json').read_text(encoding='utf-8'))
  - L87 assign _CONTRACT_V2_VALIDATOR = Draft7Validator(_CONTRACT_V2_SCHEMA)
  - L88 assign _CONTRACT_V2_OPTIONAL_JOIN_SCHEMA = deepcopy(_CONTRACT_V2_SCHEMA)
  - L89 assign required_fields = _CONTRACT_V2_OPTIONAL_JOIN_SCHEMA.get('required')
  - L90 if isinstance(required_fields, list) and 'join' in required_fields:
    - L91 assign _CONTRACT_V2_OPTIONAL_JOIN_SCHEMA['required'] = [field for field in required_fields if field != 'join']
  - L92 assign join_schema = _CONTRACT_V2_OPTIONAL_JOIN_SCHEMA.get('properties', {}).get('join')
  - L93 if isinstance(join_schema, dict):
    - L94 assign join_required = join_schema.get('required')
    - L95 if isinstance(join_required, list):
      - L96 assign join_schema['required'] = [field for field in join_required if field in ('parent_table', 'parent_key')]
  - L97 assign _CONTRACT_V2_OPTIONAL_JOIN_VALIDATOR = Draft7Validator(_CONTRACT_V2_OPTIONAL_JOIN_SCHEMA)
  - L98 assign _STEP5_REQUIREMENTS_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'step5_requirements.schema.json').read_text(encoding='utf-8'))
  - L101 assign _STEP5_REQUIREMENTS_VALIDATOR = Draft7Validator(_STEP5_REQUIREMENTS_SCHEMA)
  - L102 assign _GENERATOR_SQL_PACK_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'generator_sql_pack.schema.json').read_text(encoding='utf-8'))
  - L105 assign _GENERATOR_SQL_PACK_VALIDATOR = Draft7Validator(_GENERATOR_SQL_PACK_SCHEMA)
  - L106 assign _GENERATOR_OUTPUT_SCHEMAS_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'generator_output_schemas.schema.json').read_text(encoding='utf-8'))
  - L109 assign _GENERATOR_OUTPUT_SCHEMAS_VALIDATOR = Draft7Validator(_GENERATOR_OUTPUT_SCHEMAS_SCHEMA)
  - L110 assign _GENERATOR_LLM_RESPONSE_SCHEMA = json.loads((JSON_SCHEMA_DIR / 'generator_llm_response.schema.json').read_text(encoding='utf-8'))
  - L113 assign _GENERATOR_LLM_RESPONSE_VALIDATOR = Draft7Validator(_GENERATOR_LLM_RESPONSE_SCHEMA)
  - L115 else:
    - L115 assign _MAPPING_INLINE_V4_SCHEMA = None
    - L116 assign _MAPPING_INLINE_V4_VALIDATOR = None
    - L117 assign _LLM_CALL_3_5_SCHEMA = None
    - L118 assign _LLM_CALL_3_5_VALIDATOR = None
    - L119 assign _CONTRACT_V2_SCHEMA = None
    - L120 assign _CONTRACT_V2_VALIDATOR = None
    - L121 assign _CONTRACT_V2_OPTIONAL_JOIN_SCHEMA = None
    - L122 assign _CONTRACT_V2_OPTIONAL_JOIN_VALIDATOR = None
    - L123 assign _STEP5_REQUIREMENTS_SCHEMA = None
    - L124 assign _STEP5_REQUIREMENTS_VALIDATOR = None
    - L125 assign _GENERATOR_SQL_PACK_SCHEMA = None
    - L126 assign _GENERATOR_SQL_PACK_VALIDATOR = None
    - L127 assign _GENERATOR_OUTPUT_SCHEMAS_SCHEMA = None
    - L128 assign _GENERATOR_OUTPUT_SCHEMAS_VALIDATOR = None
    - L129 assign _GENERATOR_LLM_RESPONSE_SCHEMA = None
    - L130 assign _GENERATOR_LLM_RESPONSE_VALIDATOR = None
- L133 def validate_mapping_schemadata: Any:
  - L134 if not isinstance(data, list):
    - L135 raise SchemaValidationError('mapping must be a list')
  - L136 for (idx, item) in enumerate(data):
    - L137 if not isinstance(item, dict):
      - L138 raise SchemaValidationError(f'mapping[{idx}] must be an object')
    - L139 for key in ('header', 'placeholder', 'mapping'):
      - L140 if key not in item or not isinstance(item[key], str) or (not item[key].strip()):
        - L141 raise SchemaValidationError(f'mapping[{idx}].{key} must be a non-empty string')
- L144 def _stringify_scalarvalue: Any:
  - L145 if value is None:
    - L146 return ''
  - L147 if isinstance(value, str):
    - L148 return value.strip()
  - L149 return str(value).strip()
- L152 def _as_sequencevalue: Any:
  - L153 if value is None:
    - L154 return []
  - L155 if isinstance(value, (list, tuple, set)):
    - L156 return list(value)
  - L157 if isinstance(value, Iterable) and (not isinstance(value, (str, bytes, dict))):
    - L158 return list(value)
  - L159 return [value]
- L162 def _flatten_over_valuevalue: Any:
  - L163 if value is None:
    - L164 return []
  - L165 if isinstance(value, dict):
    - L166 annotated assign flattened: list[str] = []
    - L167 for (sub_key, sub_val) in value.items():
      - L168 for item in _flatten_over_value(sub_val):
        - L169 assign entry = f'{sub_key}:{item}' if item else str(sub_key)
        - L170 assign entry = entry.strip()
        - L171 if entry:
          - L172 expr flattened.append(entry)
    - L173 return flattened
  - L174 annotated assign items: list[str] = []
  - L175 for item in _as_sequence(value):
    - L176 if isinstance(item, dict):
      - L177 expr items.extend(_flatten_over_value(item))
      - L178 continue
    - L179 assign entry = _stringify_scalar(item)
    - L180 if entry:
      - L181 expr items.append(entry)
  - L182 return items
- L185 def _normalize_string_listvalues: Any:
  - L186 assign items = _as_sequence(values)
  - L187 annotated assign normalized: list[str] = []
  - L188 annotated assign seen: set[str] = set()
  - L189 for item in items:
    - L190 if isinstance(item, str):
      - L191 assign text = item.strip()
      - L193 else:
        - L193 assign text = str(item or '').strip()
    - L194 if not text or text in seen:
      - L195 continue
    - L196 expr seen.add(text)
    - L197 expr normalized.append(text)
  - L198 return normalized
- L201 def _normalize_hint_valueraw_hint: Any:
  - L202 if not isinstance(raw_hint, dict):
    - L203 assign op = _stringify_scalar(raw_hint) or 'UNKNOWN'
    - L204 return {'op': op, 'over': []}
  - L206 assign op = _stringify_scalar(raw_hint.get('op')) or 'UNKNOWN'
  - L207 annotated assign over_entries: list[str] = []
  - L209 expr over_entries.extend(_flatten_over_value(raw_hint.get('over')))
  - L211 if 'over_a' in raw_hint or 'over_b' in raw_hint:
    - L212 assign seq_a = [_stringify_scalar(item) for item in _as_sequence(raw_hint.get('over_a'))]
    - L213 assign seq_b = [_stringify_scalar(item) for item in _as_sequence(raw_hint.get('over_b'))]
    - L214 for (a, b) in zip_longest(seq_a, seq_b, fillvalue=''):
      - L215 assign a = a.strip()
      - L216 assign b = b.strip()
      - L217 if a and b:
        - L218 expr over_entries.append(f'{a} - {b}')
        - L219 else:
          - L219 if a:
            - L220 expr over_entries.append(a)
            - L221 else:
              - L221 if b:
                - L222 expr over_entries.append(f'- {b}')
  - L224 for key in ('num_ref', 'den_ref', 'formula'):
    - L225 assign val = raw_hint.get(key)
    - L226 assign text = _stringify_scalar(val)
    - L227 if text:
      - L228 expr over_entries.append(f'{key}:{text}')
  - L230 assign allowed_keys = {'op', 'over', 'over_a', 'over_b', 'num_ref', 'den_ref', 'formula'}
  - L231 for (extra_key, extra_value) in raw_hint.items():
    - L232 if extra_key in allowed_keys or extra_value is None:
      - L233 continue
    - L234 if isinstance(extra_value, dict):
      - L235 for (sub_key, sub_value) in extra_value.items():
        - L236 for item in _flatten_over_value({sub_key: sub_value}):
          - L237 expr over_entries.append(f'{extra_key}.{item}')
      - L239 else:
        - L239 for item in _flatten_over_value(extra_value):
          - L240 expr over_entries.append(f'{extra_key}:{item}')
  - L242 annotated assign cleaned: list[str] = []
  - L243 annotated assign seen: set[str] = set()
  - L244 for entry in over_entries:
    - L245 assign normalized = entry.strip()
    - L246 if not normalized or normalized in seen:
      - L247 continue
    - L248 expr seen.add(normalized)
    - L249 expr cleaned.append(normalized)
  - L251 return {'op': op, 'over': cleaned}
- L254 def normalize_mapping_inline_payloadpayload: Any:
  - L255 if not isinstance(payload, dict):
    - L256 return payload
  - L258 assign meta = payload.get('meta')
  - L259 if not isinstance(meta, dict):
    - L260 return payload
  - L262 assign hints = meta.get('hints')
  - L263 if not isinstance(hints, dict):
    - L264 return payload
  - L266 annotated assign normalized: dict[str, dict[str, Any]] = {}
  - L267 for (key, value) in hints.items():
    - L268 assign normalized[str(key)] = _normalize_hint_value(value)
  - L270 assign meta['hints'] = normalized
  - L271 return payload
- L274 def validate_contract_schemadata: Any:
  - L275 if not isinstance(data, dict):
    - L276 raise SchemaValidationError('contract must be an object')
  - L277 if 'literals' not in data:
    - L278 assign data['literals'] = {}
  - L279 assign coerced_join = _coerce_join_block(data)
  - L280 if coerced_join is not None:
    - L281 assign data['join'] = coerced_join
  - L282 assign required = CONTRACT_SCHEMA['required']
  - L283 for key in required:
    - L284 if key not in data:
      - L285 raise SchemaValidationError(f"contract missing key '{key}'")
  - L286 if not isinstance(data['mapping'], dict):
    - L287 raise SchemaValidationError('contract.mapping must be an object')
  - L288 if not isinstance(data['join'], dict):
    - L289 raise SchemaValidationError('contract.join must be an object')
  - L290 assign join = data['join']
  - L291 for key in ('parent_table', 'parent_key'):
    - L292 assign value = join.get(key)
    - L293 if not isinstance(value, str) or not value.strip():
      - L294 raise SchemaValidationError(f'contract.join.{key} must be a non-empty string')
  - L296 assign child_table_raw = join.get('child_table')
  - L297 if child_table_raw is not None and (not isinstance(child_table_raw, str)):
    - L298 raise SchemaValidationError('contract.join.child_table must be a string or null')
  - L299 assign child_key_raw = join.get('child_key')
  - L300 if child_key_raw is not None and (not isinstance(child_key_raw, str)):
    - L301 raise SchemaValidationError('contract.join.child_key must be a string or null')
  - L303 assign child_table_text = child_table_raw.strip() if isinstance(child_table_raw, str) else ''
  - L304 assign child_key_text = child_key_raw.strip() if isinstance(child_key_raw, str) else ''
  - L305 if child_table_text and (not child_key_text):
    - L306 raise SchemaValidationError('contract.join.child_key must be a non-empty string when child_table is provided')
  - L307 for key in ('date_columns', 'totals', 'literals'):
    - L308 if not isinstance(data[key], dict):
      - L309 raise SchemaValidationError(f'contract.{key} must be an object')
  - L310 for key in ('header_tokens', 'row_tokens', 'row_order'):
    - L311 assign arr = data.get(key)
    - L312 if not isinstance(arr, list) or not all((isinstance(item, str) for item in arr)):
      - L313 raise SchemaValidationError(f'contract.{key} must be an array of strings')
- L316 def validate_mapping_inline_v4data: Any:
  - L317 docstring: "\n    Validate LLM Call 3 output against the mapping_inline_v4 schema.\n    "
  - L320 if _MAPPING_INLINE_V4_VALIDATOR is None:
    - L321 raise RuntimeError("jsonschema is required to validate mapping inline payloads. Install the 'jsonschema' dependency.")
  - L325 assign data = normalize_mapping_inline_payload(data)
  - L327 assign errors = sorted(_MAPPING_INLINE_V4_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L328 if errors:
    - L329 assign err = errors[0]
    - L330 assign path = '.'.join((str(p) for p in err.path))
    - L331 assign location = f' at {path}' if path else ''
    - L332 raise SchemaValidationError(f'mapping_inline_v4 validation error{location}: {err.message}')
- L335 def validate_llm_call_3_5data: Any:
  - L336 docstring: "\n    Validate LLM Call 3.5 response against the schema.\n    "
  - L339 if _LLM_CALL_3_5_VALIDATOR is None:
    - L340 raise RuntimeError("jsonschema is required to validate corrections preview payloads. Install the 'jsonschema' dependency.")
  - L344 assign errors = sorted(_LLM_CALL_3_5_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L345 if errors:
    - L346 assign err = errors[0]
    - L347 assign path = '.'.join((str(p) for p in err.path))
    - L348 assign location = f' at {path}' if path else ''
    - L349 raise SchemaValidationError(f'llm_call_3_5 validation error{location}: {err.message}')
- L352 def validate_contract_v2data: Any, *, require_join: bool=True:
  - L353 docstring: "\n    Validate contract.json produced by LLM Call 4.\n    "
  - L356 if _CONTRACT_V2_VALIDATOR is None:
    - L357 raise RuntimeError("jsonschema is required to validate contract v2 payloads. Install the 'jsonschema' dependency.")
  - L361 if require_join or _CONTRACT_V2_OPTIONAL_JOIN_VALIDATOR is None:
    - L362 assign validator = _CONTRACT_V2_VALIDATOR
    - L364 else:
      - L364 assign validator = _CONTRACT_V2_OPTIONAL_JOIN_VALIDATOR
  - L366 assign errors = sorted(validator.iter_errors(data), key=lambda err: list(err.path))
  - L367 if errors:
    - L368 assign err = errors[0]
    - L369 assign path = '.'.join((str(p) for p in err.path))
    - L370 assign location = f' at {path}' if path else ''
    - L371 raise SchemaValidationError(f'contract_v2 validation error{location}: {err.message}')
  - L373 assign reshape_rules = data.get('reshape_rules')
  - L374 if isinstance(reshape_rules, list):
    - L375 assign column_rule_found = False
    - L376 for (idx, rule) in enumerate(reshape_rules):
      - L377 if not isinstance(rule, Mapping):
        - L378 continue
      - L379 assign columns = rule.get('columns')
      - L380 if columns is None:
        - L381 continue
      - L382 if not isinstance(columns, list) or not columns:
        - L383 raise SchemaValidationError(f'contract.reshape_rules[{idx}].columns must be a non-empty array when provided')
      - L386 for (col_idx, column) in enumerate(columns):
        - L387 if not isinstance(column, Mapping):
          - L388 raise SchemaValidationError(f'contract.reshape_rules[{idx}].columns[{col_idx}] must be an object')
        - L389 assign alias = column.get('as')
        - L390 if not isinstance(alias, str) or not alias.strip():
          - L391 raise SchemaValidationError(f'contract.reshape_rules[{idx}].columns[{col_idx}].as must be a non-empty string')
      - L394 assign column_rule_found = True
    - L395 if not column_rule_found:
      - L396 raise SchemaValidationError('contract.reshape_rules must include at least one rule with column definitions')
  - L398 assign join = data.get('join')
  - L399 if isinstance(join, dict):
    - L400 assign parent_table = join.get('parent_table')
    - L401 if not isinstance(parent_table, str) or not parent_table.strip():
      - L402 raise SchemaValidationError('contract.join.parent_table must be a non-empty string')
    - L403 assign parent_key = join.get('parent_key')
    - L404 if not isinstance(parent_key, str) or not parent_key.strip():
      - L405 raise SchemaValidationError('contract.join.parent_key must be a non-empty string')
    - L407 assign child_table = join.get('child_table')
    - L408 assign child_key = join.get('child_key')
    - L409 assign child_table_text = child_table.strip() if isinstance(child_table, str) else ''
    - L410 if child_table_text and (not isinstance(child_key, str) or not child_key.strip()):
      - L411 raise SchemaValidationError('contract.join.child_key must be a non-empty string when child_table is provided')
- L416 def validate_step5_requirementsdata: Any:
  - L417 docstring: "\n    Validate step5_requirements.json produced by LLM Call 4.\n    "
  - L420 if _STEP5_REQUIREMENTS_VALIDATOR is None:
    - L421 raise RuntimeError("jsonschema is required to validate step5 requirements payloads. Install the 'jsonschema' dependency.")
  - L424 assign errors = sorted(_STEP5_REQUIREMENTS_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L425 if errors:
    - L426 assign err = errors[0]
    - L427 assign path = '.'.join((str(p) for p in err.path))
    - L428 assign location = f' at {path}' if path else ''
    - L429 raise SchemaValidationError(f'step5_requirements validation error{location}: {err.message}')
- L432 def validate_generator_sql_packdata: Any:
  - L433 docstring: "\n    Validate the sql_pack section returned by LLM Call 5.\n    "
  - L436 if _GENERATOR_SQL_PACK_VALIDATOR is None:
    - L437 raise RuntimeError("jsonschema is required to validate generator sql pack payloads. Install the 'jsonschema' dependency.")
  - L440 assign errors = sorted(_GENERATOR_SQL_PACK_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L441 if errors:
    - L442 assign err = errors[0]
    - L443 assign path = '.'.join((str(p) for p in err.path))
    - L444 assign location = f' at {path}' if path else ''
    - L445 raise SchemaValidationError(f'generator_sql_pack validation error{location}: {err.message}')
  - L446 assign dialect = str(data.get('dialect') or '').strip().lower()
  - L447 if not dialect or dialect == 'sqlite':
    - L448 raise SchemaValidationError("generator_sql_pack.dialect must be 'duckdb' or 'postgres' (SQLite SQL is no longer supported)")
  - L451 if dialect not in {'duckdb', 'postgres'}:
    - L452 raise SchemaValidationError(f"generator_sql_pack.dialect '{data.get('dialect')}' is not supported")
- L455 def validate_generator_output_schemasdata: Any:
  - L456 docstring: "\n    Validate the output_schemas section returned by LLM Call 5.\n    "
  - L459 if _GENERATOR_OUTPUT_SCHEMAS_VALIDATOR is None:
    - L460 raise RuntimeError("jsonschema is required to validate generator output schemas payloads. Install the 'jsonschema' dependency.")
  - L463 assign errors = sorted(_GENERATOR_OUTPUT_SCHEMAS_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L464 if errors:
    - L465 assign err = errors[0]
    - L466 assign path = '.'.join((str(p) for p in err.path))
    - L467 assign location = f' at {path}' if path else ''
    - L468 raise SchemaValidationError(f'generator_output_schemas validation error{location}: {err.message}')
- L471 def validate_generator_llm_responsedata: Any:
  - L472 docstring: "\n    Validate the full LLM Call 5 response payload before deeper inspection.\n ..."
  - L475 if _GENERATOR_LLM_RESPONSE_VALIDATOR is None:
    - L476 raise RuntimeError("jsonschema is required to validate generator LLM responses. Install the 'jsonschema' dependency.")
  - L480 assign errors = sorted(_GENERATOR_LLM_RESPONSE_VALIDATOR.iter_errors(data), key=lambda err: list(err.path))
  - L481 if errors:
    - L482 assign err = errors[0]
    - L483 assign path = '.'.join((str(p) for p in err.path))
    - L484 assign location = f' at {path}' if path else ''
    - L485 raise SchemaValidationError(f'generator_llm_response validation error{location}: {err.message}')
  - L487 if 'key_tokens' in data:
    - L488 assign tokens = data.get('key_tokens')
    - L489 if not isinstance(tokens, list):
      - L490 raise SchemaValidationError('generator_llm_response.key_tokens must be an array of strings')
    - L491 assign cleaned = _normalize_string_list(tokens)
    - L492 if len(cleaned) != len(tokens):
      - L493 raise SchemaValidationError('generator_llm_response.key_tokens must contain unique, non-empty strings')
    - L494 for (idx, token) in enumerate(tokens):
      - L495 if not isinstance(token, str):
        - L496 raise SchemaValidationError(f'generator_llm_response.key_tokens[{idx}] must be a string')
      - L497 if token.strip() != token:
        - L498 raise SchemaValidationError(f'generator_llm_response.key_tokens[{idx}] must not contain leading or trailing whitespace')

## backend\app\services\utils\zip_tools.py
- L1 from __future__ import annotations
- L3 import io
- L4 import os
- L5 import shutil
- L6 import zipfile
- L7 from pathlib import Path
- L8 from typing import Iterable, Optional, Tuple
- L11 def _is_safe_membermember: zipfile.ZipInfo:
  - L12 assign name = member.filename.replace('\\', '/')
  - L13 if name.startswith(('/', '\\')):
    - L14 return False
  - L15 assign parts = [p for p in name.split('/') if p and p not in ('.', '..')]
  - L16 if not parts:
    - L17 return False
  - L18 if ':' in parts[0]:
    - L19 return False
  - L20 return not any((p in ('..',) for p in parts))
- L23 def _is_within_dirbase_dir: Path, target: Path:
  - L24 try:
    - L25 expr target.resolve().relative_to(base_dir)
    - L26 return True
    - L27 except Exception:
      - L28 return False
- L31 def detect_zip_rootmembers: Iterable[str]:
  - L32 assign roots = []
  - L33 for m in members:
    - L34 assign p = m.replace('\\', '/')
    - L35 if p.endswith('/'):
      - L36 assign p = p[:-1]
    - L37 assign parts = [seg for seg in p.split('/') if seg]
    - L38 if not parts:
      - L39 continue
    - L40 expr roots.append(parts[0])
  - L41 if not roots:
    - L42 return None
  - L43 assign first = roots[0]
  - L44 if all((r == first for r in roots)):
    - L45 return first
  - L46 return None
- L49 def create_zip_from_dirsrc_dir: Path, dest_zip: Path, *, include_root: bool=True:
  - L50 assign src_dir = Path(src_dir).resolve()
  - L51 assign dest_zip = Path(dest_zip).resolve()
  - L52 expr dest_zip.parent.mkdir(parents=True, exist_ok=True)
  - L54 with zipfile.ZipFile(dest_zip, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:
    - L55 assign base_prefix = src_dir.name if include_root else ''
    - L56 for path in src_dir.rglob('*'):
      - L57 if path.is_dir():
        - L58 continue
      - L60 if path.name.startswith('.lock.'):
        - L61 continue
      - L62 assign rel = path.relative_to(src_dir)
      - L63 assign arcname = f'{base_prefix}/{rel.as_posix()}' if base_prefix else rel.as_posix()
      - L64 expr zf.write(path, arcname=arcname)
  - L65 return dest_zip
- L68 def extract_zip_to_dirzip_path: Path, dest_dir: Path, *, strip_root: bool=True, max_entries: int | None=None, max_uncompressed_bytes: int | None=None, max_file_bytes: int | None=None:
  - L77 docstring: "\n    Extract zip into dest_dir safely. Returns (extracted_root, root_name_in_zi..."
  - L83 assign dest_dir = Path(dest_dir).resolve()
  - L84 expr dest_dir.mkdir(parents=True, exist_ok=True)
  - L85 assign base_dir = dest_dir.resolve()
  - L87 with zipfile.ZipFile(zip_path, mode='r') as zf:
    - L88 assign members = list(zf.infolist())
    - L89 assign file_members = [member for member in members if not member.is_dir()]
    - L91 if max_entries is not None and len(file_members) > max_entries:
      - L92 raise ValueError(f'Zip contains {len(file_members)} files, exceeds limit {max_entries}')
    - L94 if max_uncompressed_bytes is not None:
      - L95 assign total_uncompressed = sum((member.file_size for member in file_members))
      - L96 if total_uncompressed > max_uncompressed_bytes:
        - L97 raise ValueError(f'Zip uncompressed size {total_uncompressed} bytes exceeds limit {max_uncompressed_bytes}')
    - L101 if max_file_bytes is not None:
      - L102 for member in file_members:
        - L103 if member.file_size > max_file_bytes:
          - L104 raise ValueError(f'Zip member {member.filename} size {member.file_size} exceeds limit {max_file_bytes}')
    - L108 assign root = detect_zip_root((m.filename for m in members))
    - L109 for member in members:
      - L110 if not _is_safe_member(member):
        - L111 continue
      - L112 assign name = member.filename.replace('\\', '/')
      - L113 if strip_root and root and name.startswith(root + '/'):
        - L114 assign rel = name[len(root) + 1:]
        - L116 else:
          - L116 assign rel = name
      - L117 if not rel:
        - L118 continue
      - L119 assign target = dest_dir / rel
      - L120 if not _is_within_dir(base_dir, target):
        - L121 raise ValueError(f'Zip member {member.filename} would extract outside {base_dir}')
      - L122 if member.is_dir():
        - L123 expr target.mkdir(parents=True, exist_ok=True)
        - L124 continue
      - L125 expr target.parent.mkdir(parents=True, exist_ok=True)
      - L126 with zf.open(member, 'r') as src, open(target, 'wb') as dst:
        - L127 expr shutil.copyfileobj(src, dst, length=1024 * 1024)
  - L128 return (dest_dir, root or '')

## backend\core\__init__.py
- L1 docstring: "Core module - cross-cutting concerns for the entire backend."
- L3 from .errors import NeuraError, ValidationError, NotFoundError, ConflictError, ExternalServiceError, ConfigurationError
- L11 from .result import Result, Ok, Err
- L12 from .events import Event, EventBus, EventHandler
- L13 from .types import EntityId, Timestamp, JSON
- L15 assign __all__ = ['NeuraError', 'ValidationError', 'NotFoundError', 'ConflictError', 'ExternalServiceError', 'ConfigurationError', 'Result', 'Ok', 'Err', 'Event', 'EventBus', 'EventHandler', 'EntityId', 'Timestamp', 'JSON']

## backend\core\errors.py
- L1 docstring: "Unified error hierarchy for the entire backend.\n\nAll domain-specific errors in..."
- L8 from __future__ import annotations
- L10 from typing import Any, Dict, Optional
- L13 class NeuraError(Exception):
  - L14 docstring: "Base error type for all NeuraReport errors."
  - L16 assign code = 'error'
  - L18 def __init__self, message: str, *, details: Optional[Dict[str, Any]]=None, cause: Optional[Exception]=None:
    - L25 expr super().__init__(message)
    - L26 assign self.message = message
    - L27 assign self.details = details
    - L28 assign self.cause = cause
  - L30 def __str__self:
    - L31 if self.details:
      - L32 return f'[{self.code}] {self.message} - {self.details}'
    - L33 return f'[{self.code}] {self.message}'
  - L35 def to_dictself:
    - L36 assign result = {'code': self.code, 'message': self.message}
    - L37 if self.details:
      - L38 assign result['details'] = self.details
    - L39 return result
- L42 class ValidationError(NeuraError):
  - L43 docstring: "Raised when input validation fails."
  - L45 assign code = 'validation_error'
- L48 class NotFoundError(NeuraError):
  - L49 docstring: "Raised when a requested resource does not exist."
  - L51 assign code = 'not_found'
- L54 class ConflictError(NeuraError):
  - L55 docstring: "Raised when an operation conflicts with current state."
  - L57 assign code = 'conflict'
- L60 class ExternalServiceError(NeuraError):
  - L61 docstring: "Raised when an external service (LLM, email, etc.) fails."
  - L63 assign code = 'external_service_error'
  - L65 def __init__self, message: str, *, service: str='unknown', details: Optional[Dict[str, Any]]=None, cause: Optional[Exception]=None:
    - L73 expr super().__init__(message, details=details, cause=cause)
    - L74 assign self.service = service
- L77 class ConfigurationError(NeuraError):
  - L78 docstring: "Raised when system configuration is invalid."
  - L80 assign code = 'configuration_error'
- L83 class PipelineError(NeuraError):
  - L84 docstring: "Raised when a pipeline step fails."
  - L86 assign code = 'pipeline_error'
  - L88 def __init__self, message: str, *, step: str='unknown', details: Optional[Dict[str, Any]]=None, cause: Optional[Exception]=None:
    - L96 expr super().__init__(message, details=details, cause=cause)
    - L97 assign self.step = step
- L100 class DataSourceError(NeuraError):
  - L101 docstring: "Raised when data source operations fail."
  - L103 assign code = 'data_source_error'
- L106 class RenderError(NeuraError):
  - L107 docstring: "Raised when rendering fails."
  - L109 assign code = 'render_error'
  - L111 def __init__self, message: str, *, format: str='unknown', details: Optional[Dict[str, Any]]=None, cause: Optional[Exception]=None:
    - L119 expr super().__init__(message, details=details, cause=cause)
    - L120 assign self.format = format

## backend\core\events.py
- L1 docstring: "Event bus for decoupled communication between components.\n\nThis is an improved..."
- L10 from __future__ import annotations
- L12 import asyncio
- L13 import logging
- L14 import time
- L15 import uuid
- L16 from dataclasses import dataclass, field
- L17 from datetime import datetime, timezone
- L18 from typing import Any, Awaitable, Callable, Dict, List, Optional, Protocol
- L20 assign logger = logging.getLogger('neura.events')
- L24 class Event:
  - L25 docstring: "Immutable event with metadata."
  - L27 annotated assign name: str
  - L28 annotated assign payload: Dict[str, Any]
  - L29 annotated assign event_id: str = field(default_factory=lambda: uuid.uuid4().hex)
  - L30 annotated assign correlation_id: Optional[str] = None
  - L31 annotated assign timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L32 annotated assign source: str = field(default='backend')
  - L34 def to_dictself:
    - L35 return {'event_id': self.event_id, 'name': self.name, 'payload': self.payload, 'correlation_id': self.correlation_id, 'timestamp': self.timestamp.isoformat(), 'source': self.source}
- L45 assign EventHandler = Callable[[Event], Awaitable[None]]
- L46 assign EventMiddleware = Callable[[Event, Callable[[Event], Awaitable[None]]], Awaitable[None]]
- L49 class EventPersistence(Protocol):
  - L50 docstring: "Protocol for event persistence backends."
  - L52 async def persistself, event: Event:
    - L53 docstring: "Persist an event for replay or audit."
    - L54 expr ...
  - L56 async def get_eventsself, *, name: Optional[str]=None, correlation_id: Optional[str]=None, since: Optional[datetime]=None, limit: int=100:
    - L64 docstring: "Retrieve persisted events."
    - L65 expr ...
- L68 class EventBus:
  - L69 docstring: "Async event bus with middleware and persistence support."
  - L71 def __init__self, *, middlewares: Optional[List[EventMiddleware]]=None, persistence: Optional[EventPersistence]=None:
    - L77 annotated assign self._handlers: Dict[str, List[EventHandler]] = {}
    - L78 assign self._middlewares = middlewares or []
    - L79 assign self._persistence = persistence
    - L80 annotated assign self._wildcard_handlers: List[EventHandler] = []
  - L82 def subscribeself, event_name: str, handler: EventHandler:
    - L83 docstring: "Subscribe a handler to an event. Returns unsubscribe function."
    - L84 if event_name == '*':
      - L85 expr self._wildcard_handlers.append(handler)
      - L86 return lambda: self._wildcard_handlers.remove(handler)
    - L88 if event_name not in self._handlers:
      - L89 assign self._handlers[event_name] = []
    - L90 expr self._handlers[event_name].append(handler)
    - L91 return lambda: self._handlers[event_name].remove(handler)
  - L93 async def publishself, event: Event:
    - L94 docstring: "Publish an event to all subscribers."
    - L95 async def dispatchevt: Event:
      - L96 expr await self._dispatch(evt)
    - L99 assign handler = dispatch
    - L100 for middleware in reversed(self._middlewares):
      - L101 assign prev_handler = handler
      - L103 async def make_handlerm: EventMiddleware, h: Callable:
        - L104 async def wrappede: Event:
          - L105 expr await m(e, h)
        - L106 return wrapped
      - L108 assign handler = await make_handler(middleware, prev_handler)
    - L110 expr await handler(event)
    - L113 if self._persistence:
      - L114 try:
        - L115 expr await self._persistence.persist(event)
        - L116 except Exception:
          - L117 expr logger.exception('event_persist_failed', extra={'event': event.name})
  - L119 async def _dispatchself, event: Event:
    - L120 docstring: "Dispatch event to registered handlers."
    - L121 assign handlers = list(self._handlers.get(event.name, []))
    - L122 expr handlers.extend(self._wildcard_handlers)
    - L124 if not handlers:
      - L125 return None
    - L128 assign results = await asyncio.gather(*[self._safe_call(handler, event) for handler in handlers], return_exceptions=True)
    - L133 for result in results:
      - L134 if isinstance(result, Exception):
        - L135 expr logger.exception('event_handler_failed', extra={'event': event.name, 'error': str(result)})
  - L140 async def _safe_callself, handler: EventHandler, event: Event:
    - L141 docstring: "Safely call a handler, catching exceptions."
    - L142 try:
      - L143 expr await handler(event)
      - L144 except Exception as e:
        - L145 expr logger.exception('event_handler_error', extra={'event': event.name, 'handler': handler.__name__})
        - L149 raise
- L152 def logging_middlewarelog: logging.Logger:
  - L153 docstring: "Middleware that logs all events."
  - L155 async def middlewareevent: Event, next_handler: Callable[[Event], Awaitable[None]]:
    - L156 assign start = time.perf_counter()
    - L157 expr log.info('event_published', extra={'event': event.name, 'event_id': event.event_id, 'correlation_id': event.correlation_id})
    - L165 try:
      - L166 expr await next_handler(event)
      - L168 finally:
        - L168 assign elapsed = (time.perf_counter() - start) * 1000
        - L169 expr log.debug('event_handled', extra={'event': event.name, 'event_id': event.event_id, 'elapsed_ms': elapsed})
  - L178 return middleware
- L181 def metrics_middlewarelog: logging.Logger:
  - L182 docstring: "Middleware that tracks event metrics."
  - L184 annotated assign _counts: Dict[str, int] = {}
  - L186 async def middlewareevent: Event, next_handler: Callable[[Event], Awaitable[None]]:
    - L187 assign _counts[event.name] = _counts.get(event.name, 0) + 1
    - L188 expr await next_handler(event)
  - L190 return middleware
- L194 annotated assign _global_bus: Optional[EventBus] = None
- L197 def get_event_bus:
  - L198 docstring: "Get or create the global event bus."
  - L199 Global
  - L200 if _global_bus is None:
    - L201 assign _global_bus = EventBus(middlewares=[logging_middleware(logger), metrics_middleware(logger)])
  - L207 return _global_bus
- L210 def publish_syncevent: Event:
  - L211 docstring: "Publish an event synchronously (for use in sync code)."
  - L212 assign bus = get_event_bus()
  - L213 try:
    - L214 assign loop = asyncio.get_running_loop()
    - L215 expr asyncio.run_coroutine_threadsafe(bus.publish(event), loop)
    - L216 except RuntimeError:
      - L217 expr asyncio.run(bus.publish(event))

## backend\core\result.py
- L1 docstring: "Result type for explicit error handling without exceptions.\n\nInspired by Rust'..."
- L7 from __future__ import annotations
- L9 from dataclasses import dataclass
- L10 from typing import Callable, Generic, TypeVar, Union, overload
- L18 from .errors import NeuraError
- L20 assign T = TypeVar('T')
- L21 assign E = TypeVar('E', bound=NeuraError)
- L22 assign U = TypeVar('U')
- L26 class Ok(Generic[T]):
  - L27 docstring: "Success variant of Result."
  - L29 annotated assign value: T
  - L31 def is_okself:
    - L32 return True
  - L34 def is_errself:
    - L35 return False
  - L37 def unwrapself:
    - L38 return self.value
  - L40 def unwrap_orself, default: T:
    - L41 return self.value
  - L43 def unwrap_or_elseself, f: Callable[[NeuraError], T]:
    - L44 return self.value
  - L46 def mapself, f: Callable[[T], U]:
    - L47 return Ok(f(self.value))
  - L49 def map_errself, f: Callable[[NeuraError], NeuraError]:
    - L50 return self
  - L52 def and_thenself, f: Callable[[T], Result[U, NeuraError]]:
    - L53 return f(self.value)
  - L55 def or_elseself, f: Callable[[NeuraError], Result[T, NeuraError]]:
    - L56 return self
- L60 class Err(Generic[E]):
  - L61 docstring: "Error variant of Result."
  - L63 annotated assign error: E
  - L65 def is_okself:
    - L66 return False
  - L68 def is_errself:
    - L69 return True
  - L71 def unwrapself:
    - L72 raise self.error
  - L74 def unwrap_orself, default: T:
    - L75 return default
  - L77 def unwrap_or_elseself, f: Callable[[E], T]:
    - L78 return f(self.error)
  - L80 def mapself, f: Callable[[T], U]:
    - L81 return self
  - L83 def map_errself, f: Callable[[E], NeuraError]:
    - L84 return Err(f(self.error))
  - L86 def and_thenself, f: Callable[[T], Result[U, E]]:
    - L87 return self
  - L89 def or_elseself, f: Callable[[E], Result[T, E]]:
    - L90 return f(self.error)
- L93 assign Result = Union[Ok[T], Err[E]]
- L96 def result_from_exceptionf: Callable[[], T]:
  - L97 docstring: "Execute a function and wrap its result in a Result type."
  - L98 try:
    - L99 return Ok(f())
    - L100 except NeuraError as e:
      - L101 return Err(e)
    - L102 except Exception as e:
      - L103 from .errors import NeuraError as BaseError
      - L104 return Err(BaseError(code='unexpected_error', message=str(e), cause=e))
- L107 def collect_resultsresults: list[Result[T, E]]:
  - L108 docstring: "Collect a list of Results into a Result of list."
  - L109 assign values = []
  - L110 for r in results:
    - L111 if r.is_err():
      - L112 return r
    - L113 expr values.append(r.unwrap())
  - L114 return Ok(values)

## backend\core\types.py
- L1 docstring: "Core type definitions used throughout the system."
- L3 from __future__ import annotations
- L5 from datetime import datetime
- L6 from typing import Any, Dict, List, NewType, Union
- L8 assign EntityId = NewType('EntityId', str)
- L9 assign Timestamp = NewType('Timestamp', datetime)
- L11 assign JSON = Union[Dict[str, Any], List[Any], str, int, float, bool, None]
- L12 assign JSONObject = Dict[str, Any]
- L13 assign JSONArray = List[Any]

## backend\domain\__init__.py
- L1 docstring: "Domain layer - pure business logic with no IO dependencies."
- L3 from .contracts import Contract, TokenSet, Mapping, ReshapeRule
- L4 from .reports import Report, Batch, RenderRequest, RenderOutput, OutputFormat
- L5 from .templates import Template, TemplateKind, Artifact
- L6 from .connections import Connection, ConnectionStatus
- L7 from .jobs import Job, JobStatus, JobStep, StepStatus, Schedule
- L9 assign __all__ = ['Contract', 'TokenSet', 'Mapping', 'ReshapeRule', 'Report', 'Batch', 'RenderRequest', 'RenderOutput', 'OutputFormat', 'Template', 'TemplateKind', 'Artifact', 'Connection', 'ConnectionStatus', 'Job', 'JobStatus', 'JobStep', 'StepStatus', 'Schedule']

## backend\domain\connections.py
- L1 docstring: "Connection domain entities.\n\nA Connection represents a data source (database) ..."
- L6 from __future__ import annotations
- L8 from dataclasses import dataclass, field
- L9 from datetime import datetime, timezone
- L10 from enum import Enum
- L11 from pathlib import Path
- L12 from typing import Any, Dict, List, Optional
- L13 import uuid
- L16 class ConnectionType(str, Enum):
  - L17 docstring: "Types of database connections."
  - L19 assign SQLITE = 'sqlite'
  - L20 assign POSTGRES = 'postgres'
  - L21 assign MYSQL = 'mysql'
- L24 class ConnectionStatus(str, Enum):
  - L25 docstring: "Health status of a connection."
  - L27 assign UNKNOWN = 'unknown'
  - L28 assign HEALTHY = 'healthy'
  - L29 assign DEGRADED = 'degraded'
  - L30 assign UNAVAILABLE = 'unavailable'
- L34 class TableInfo:
  - L35 docstring: "Information about a database table."
  - L37 annotated assign name: str
  - L38 annotated assign columns: List[str]
  - L39 annotated assign row_count: Optional[int] = None
  - L40 annotated assign primary_key: Optional[str] = None
- L44 class SchemaInfo:
  - L45 docstring: "Database schema information."
  - L47 annotated assign tables: List[TableInfo]
  - L48 annotated assign catalog: List[str]
  - L51 def table_namesself:
    - L52 return [t.name for t in self.tables]
- L56 class ConnectionTest:
  - L57 docstring: "Result of testing a connection."
  - L59 annotated assign success: bool
  - L60 annotated assign latency_ms: Optional[float] = None
  - L61 annotated assign error: Optional[str] = None
  - L62 annotated assign tested_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L63 annotated assign table_count: Optional[int] = None
  - L64 annotated assign total_rows: Optional[int] = None
- L68 class Connection:
  - L69 docstring: "A database connection configuration.\n\n    Connections are used to access data ..."
  - L74 annotated assign connection_id: str
  - L75 annotated assign name: str
  - L76 annotated assign connection_type: ConnectionType
  - L77 annotated assign path: Path
  - L78 annotated assign status: ConnectionStatus = ConnectionStatus.UNKNOWN
  - L79 annotated assign schema_info: Optional[SchemaInfo] = None
  - L80 annotated assign last_test: Optional[ConnectionTest] = None
  - L81 annotated assign created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L82 annotated assign updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L83 annotated assign last_used_at: Optional[datetime] = None
  - L84 annotated assign last_used_template: Optional[str] = None
  - L85 annotated assign description: Optional[str] = None
  - L86 annotated assign tags: List[str] = field(default_factory=list)
  - L89 def create_sqlitecls, name: str, path: Path, connection_id: Optional[str]=None, **kwargs: Any:
    - L96 return cls(connection_id=connection_id or str(uuid.uuid4()), name=name, connection_type=ConnectionType.SQLITE, path=path.resolve(), **kwargs)
  - L104 def record_testself, test: ConnectionTest:
    - L105 assign self.last_test = test
    - L106 assign self.status = ConnectionStatus.HEALTHY if test.success else ConnectionStatus.UNAVAILABLE
    - L111 assign self.updated_at = datetime.now(timezone.utc)
  - L113 def record_useself, template_id: str:
    - L114 assign self.last_used_at = datetime.now(timezone.utc)
    - L115 assign self.last_used_template = template_id
    - L116 assign self.updated_at = datetime.now(timezone.utc)
  - L118 def update_schemaself, schema: SchemaInfo:
    - L119 assign self.schema_info = schema
    - L120 assign self.updated_at = datetime.now(timezone.utc)
  - L123 def catalogself:
    - L124 if self.schema_info:
      - L125 return self.schema_info.catalog
    - L126 return []
  - L128 def to_dictself:
    - L129 return {'connection_id': self.connection_id, 'name': self.name, 'connection_type': self.connection_type.value, 'path': str(self.path), 'status': self.status.value, 'schema_info': {'tables': [{'name': t.name, 'columns': t.columns, 'row_count': t.row_count, 'primary_key': t.primary_key} for t in self.schema_info.tables], 'catalog': self.schema_info.catalog} if self.schema_info else None, 'last_test': {'success': self.last_test.success, 'latency_ms': self.last_test.latency_ms, 'error': self.last_test.error, 'tested_at': self.last_test.tested_at.isoformat(), 'table_count': self.last_test.table_count} if self.last_test else None, 'created_at': self.created_at.isoformat(), 'updated_at': self.updated_at.isoformat(), 'last_used_at': self.last_used_at.isoformat() if self.last_used_at else None, 'last_used_template': self.last_used_template, 'description': self.description, 'tags': self.tags}
  - L167 def from_dictcls, data: Dict[str, Any]:
    - L168 assign schema_data = data.get('schema_info')
    - L169 assign schema_info = None
    - L170 if schema_data:
      - L171 assign tables = [TableInfo(name=t['name'], columns=t['columns'], row_count=t.get('row_count'), primary_key=t.get('primary_key')) for t in schema_data.get('tables', [])]
      - L180 assign schema_info = SchemaInfo(tables=tables, catalog=schema_data.get('catalog', []))
    - L185 assign test_data = data.get('last_test')
    - L186 assign last_test = None
    - L187 if test_data:
      - L188 assign tested_at = test_data.get('tested_at')
      - L189 if isinstance(tested_at, str):
        - L190 assign tested_at = datetime.fromisoformat(tested_at)
      - L191 assign last_test = ConnectionTest(success=test_data['success'], latency_ms=test_data.get('latency_ms'), error=test_data.get('error'), tested_at=tested_at or datetime.now(timezone.utc), table_count=test_data.get('table_count'))
    - L199 assign created_at = data.get('created_at')
    - L200 if isinstance(created_at, str):
      - L201 assign created_at = datetime.fromisoformat(created_at)
    - L203 assign updated_at = data.get('updated_at')
    - L204 if isinstance(updated_at, str):
      - L205 assign updated_at = datetime.fromisoformat(updated_at)
    - L207 assign last_used_at = data.get('last_used_at')
    - L208 if isinstance(last_used_at, str):
      - L209 assign last_used_at = datetime.fromisoformat(last_used_at)
    - L211 return cls(connection_id=data['connection_id'], name=data['name'], connection_type=ConnectionType(data.get('connection_type', 'sqlite')), path=Path(data['path']), status=ConnectionStatus(data.get('status', 'unknown')), schema_info=schema_info, last_test=last_test, created_at=created_at or datetime.now(timezone.utc), updated_at=updated_at or datetime.now(timezone.utc), last_used_at=last_used_at, last_used_template=data.get('last_used_template'), description=data.get('description'), tags=data.get('tags', []))

## backend\domain\contracts.py
- L1 docstring: "Contract domain entities.\n\nA Contract defines how data from a database maps to..."
- L7 from __future__ import annotations
- L9 from dataclasses import dataclass, field
- L10 from datetime import datetime
- L11 from enum import Enum
- L12 from typing import Any, Dict, List, Optional
- L15 class TokenType(str, Enum):
  - L16 docstring: "Types of tokens in a contract."
  - L18 assign SCALAR = 'scalar'
  - L19 assign ROW = 'row'
  - L20 assign TOTAL = 'total'
- L24 class Token:
  - L25 docstring: "A single token/placeholder in a contract."
  - L27 annotated assign name: str
  - L28 annotated assign token_type: TokenType
  - L29 annotated assign expression: Optional[str] = None
  - L30 annotated assign description: Optional[str] = None
  - L32 def __post_init__self:
    - L33 if not self.name or not self.name.strip():
      - L34 raise ValueError('Token name cannot be empty')
- L38 class TokenSet:
  - L39 docstring: "Collection of tokens organized by type."
  - L41 annotated assign scalars: List[str] = field(default_factory=list)
  - L42 annotated assign row_tokens: List[str] = field(default_factory=list)
  - L43 annotated assign totals: List[str] = field(default_factory=list)
  - L45 def all_tokensself:
    - L46 return [*self.scalars, *self.row_tokens, *self.totals]
  - L48 def __contains__self, token: str:
    - L49 return token in self.scalars or token in self.row_tokens or token in self.totals
- L53 class Mapping:
  - L54 docstring: "Mapping from token name to SQL expression."
  - L56 annotated assign token: str
  - L57 annotated assign expression: str
  - L58 annotated assign source_table: Optional[str] = None
  - L59 annotated assign source_column: Optional[str] = None
  - L61 def __post_init__self:
    - L62 if not self.token.strip():
      - L63 raise ValueError('Mapping token cannot be empty')
    - L64 if not self.expression.strip():
      - L65 raise ValueError(f"Mapping expression for '{self.token}' cannot be empty")
- L69 class ReshapeColumn:
  - L70 docstring: "A column in a reshape rule."
  - L72 annotated assign alias: str
  - L73 annotated assign sources: List[str]
- L77 class ReshapeRule:
  - L78 docstring: "Rule for reshaping/transforming data before rendering."
  - L80 annotated assign purpose: str
  - L81 annotated assign strategy: str
  - L82 annotated assign columns: List[ReshapeColumn]
  - L83 annotated assign order_by: List[str] = field(default_factory=list)
  - L84 annotated assign filters: Optional[str] = None
  - L85 annotated assign group_by: Optional[List[str]] = None
  - L86 annotated assign explain: Optional[str] = None
- L90 class JoinSpec:
  - L91 docstring: "Specification for joining tables."
  - L93 annotated assign parent_table: str
  - L94 annotated assign parent_key: str
  - L95 annotated assign child_table: str
  - L96 annotated assign child_key: str
  - L98 def is_validself:
    - L99 return bool(self.parent_table and self.parent_key)
- L103 class OrderSpec:
  - L104 docstring: "Ordering specification for rows."
  - L106 annotated assign rows: List[str] = field(default_factory=list)
- L110 class Contract:
  - L111 docstring: "Complete contract for report generation.\n\n    A contract contains:\n    - Toke..."
  - L121 annotated assign contract_id: str
  - L122 annotated assign template_id: str
  - L123 annotated assign tokens: TokenSet
  - L124 annotated assign mappings: Dict[str, str]
  - L125 annotated assign reshape_rules: List[ReshapeRule] = field(default_factory=list)
  - L126 annotated assign join: Optional[JoinSpec] = None
  - L127 annotated assign order_by: OrderSpec = field(default_factory=OrderSpec)
  - L128 annotated assign row_order: List[str] = field(default_factory=lambda: ['ROWID'])
  - L129 annotated assign literals: Dict[str, Any] = field(default_factory=dict)
  - L130 annotated assign totals_math: Dict[str, str] = field(default_factory=dict)
  - L131 annotated assign row_computed: Dict[str, str] = field(default_factory=dict)
  - L132 annotated assign created_at: Optional[datetime] = None
  - L133 annotated assign version: str = 'v2'
  - L137 def header_tokensself:
    - L138 return list(self.tokens.scalars)
  - L141 def row_tokensself:
    - L142 return list(self.tokens.row_tokens)
  - L145 def totalsself:
    - L146 return {tok: self.mappings.get(tok, '') for tok in self.tokens.totals}
  - L148 def get_mappingself, token: str:
    - L149 return self.mappings.get(token)
  - L151 def validateself:
    - L152 docstring: "Validate contract and return list of issues."
    - L153 assign issues = []
    - L156 for token in self.tokens.all_tokens():
      - L157 if token not in self.mappings:
        - L158 expr issues.append(f"Token '{token}' has no mapping")
    - L161 for rule in self.reshape_rules:
      - L162 if not rule.columns:
        - L163 expr issues.append(f"Reshape rule '{rule.purpose}' has no columns")
    - L166 if self.join and (not self.join.is_valid()):
      - L167 expr issues.append('Join specification is incomplete')
    - L169 return issues
  - L171 def to_dictself:
    - L172 docstring: "Serialize to dict for persistence."
    - L173 return {'contract_id': self.contract_id, 'template_id': self.template_id, 'tokens': {'scalars': list(self.tokens.scalars), 'row_tokens': list(self.tokens.row_tokens), 'totals': list(self.tokens.totals)}, 'mapping': self.mappings, 'reshape_rules': [{'purpose': r.purpose, 'strategy': r.strategy, 'columns': [{'as': c.alias, 'from': c.sources} for c in r.columns], 'order_by': r.order_by, 'filters': r.filters, 'group_by': r.group_by, 'explain': r.explain} for r in self.reshape_rules], 'join': {'parent_table': self.join.parent_table, 'parent_key': self.join.parent_key, 'child_table': self.join.child_table, 'child_key': self.join.child_key} if self.join else None, 'order_by': {'rows': list(self.order_by.rows)}, 'row_order': list(self.row_order), 'literals': self.literals, 'totals_math': self.totals_math, 'row_computed': self.row_computed, 'version': self.version}
  - L211 def from_dictcls, data: Dict[str, Any], template_id: str:
    - L212 docstring: "Deserialize from dict."
    - L213 assign tokens_data = data.get('tokens', {})
    - L214 assign tokens = TokenSet(scalars=list(tokens_data.get('scalars', [])), row_tokens=list(tokens_data.get('row_tokens', [])), totals=list(tokens_data.get('totals', [])))
    - L220 assign reshape_rules = []
    - L221 for rule_data in data.get('reshape_rules', []):
      - L222 assign columns = [ReshapeColumn(alias=c.get('as', ''), sources=c.get('from', [])) for c in rule_data.get('columns', [])]
      - L226 expr reshape_rules.append(ReshapeRule(purpose=rule_data.get('purpose', ''), strategy=rule_data.get('strategy', 'SELECT'), columns=columns, order_by=rule_data.get('order_by', []), filters=rule_data.get('filters'), group_by=rule_data.get('group_by'), explain=rule_data.get('explain')))
    - L238 assign join_data = data.get('join')
    - L239 assign join = None
    - L240 if join_data and isinstance(join_data, dict):
      - L241 assign join = JoinSpec(parent_table=join_data.get('parent_table', ''), parent_key=join_data.get('parent_key', ''), child_table=join_data.get('child_table', ''), child_key=join_data.get('child_key', ''))
    - L248 assign order_data = data.get('order_by', {})
    - L249 assign order_by = OrderSpec(rows=order_data.get('rows', []) if isinstance(order_data, dict) else [])
    - L253 return cls(contract_id=data.get('contract_id', f'contract-{template_id}'), template_id=template_id, tokens=tokens, mappings=dict(data.get('mapping', {})), reshape_rules=reshape_rules, join=join, order_by=order_by, row_order=list(data.get('row_order', ['ROWID'])), literals=dict(data.get('literals', {})), totals_math=dict(data.get('totals_math', {})), row_computed=dict(data.get('row_computed', {})), version=data.get('version', 'v2'))

## backend\domain\jobs.py
- L1 docstring: "Job domain entities.\n\nJobs represent long-running operations that need trackin..."
- L7 from __future__ import annotations
- L9 from dataclasses import dataclass, field
- L10 from datetime import datetime, timedelta, timezone
- L11 from enum import Enum
- L12 from typing import Any, Dict, List, Optional
- L13 import uuid
- L16 class JobStatus(str, Enum):
  - L17 docstring: "Status of a job."
  - L19 assign PENDING = 'pending'
  - L20 assign QUEUED = 'queued'
  - L21 assign RUNNING = 'running'
  - L22 assign SUCCEEDED = 'succeeded'
  - L23 assign FAILED = 'failed'
  - L24 assign CANCELLED = 'cancelled'
  - L27 def is_terminalself:
    - L28 return self in (JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.CANCELLED)
  - L31 def is_activeself:
    - L32 return self in (JobStatus.PENDING, JobStatus.QUEUED, JobStatus.RUNNING)
- L35 class StepStatus(str, Enum):
  - L36 docstring: "Status of a job step."
  - L38 assign PENDING = 'pending'
  - L39 assign RUNNING = 'running'
  - L40 assign SUCCEEDED = 'succeeded'
  - L41 assign FAILED = 'failed'
  - L42 assign SKIPPED = 'skipped'
- L46 class JobStep:
  - L47 docstring: "A step within a job."
  - L49 annotated assign name: str
  - L50 annotated assign label: str
  - L51 annotated assign status: StepStatus = StepStatus.PENDING
  - L52 annotated assign error: Optional[str] = None
  - L53 annotated assign started_at: Optional[datetime] = None
  - L54 annotated assign completed_at: Optional[datetime] = None
  - L55 annotated assign progress_weight: float = 0.0
  - L57 def startself:
    - L58 assign self.status = StepStatus.RUNNING
    - L59 assign self.started_at = datetime.now(timezone.utc)
  - L61 def succeedself:
    - L62 assign self.status = StepStatus.SUCCEEDED
    - L63 assign self.completed_at = datetime.now(timezone.utc)
  - L65 def failself, error: str:
    - L66 assign self.status = StepStatus.FAILED
    - L67 assign self.error = error
    - L68 assign self.completed_at = datetime.now(timezone.utc)
  - L70 def skipself:
    - L71 assign self.status = StepStatus.SKIPPED
    - L72 assign self.completed_at = datetime.now(timezone.utc)
  - L74 def to_dictself:
    - L75 return {'name': self.name, 'label': self.label, 'status': self.status.value, 'error': self.error, 'started_at': self.started_at.isoformat() if self.started_at else None, 'completed_at': self.completed_at.isoformat() if self.completed_at else None}
- L85 class JobType(str, Enum):
  - L86 docstring: "Types of jobs."
  - L88 assign REPORT_GENERATION = 'run_report'
  - L89 assign TEMPLATE_IMPORT = 'import_template'
  - L90 assign TEMPLATE_ANALYSIS = 'analyze_template'
  - L91 assign CONTRACT_BUILD = 'build_contract'
  - L92 assign SCHEMA_DISCOVERY = 'discover_schema'
- L96 class Job:
  - L97 docstring: "A tracked job/operation.\n\n    Jobs have:\n    - Unique ID and correlation ID\n..."
  - L107 annotated assign job_id: str
  - L108 annotated assign job_type: JobType
  - L109 annotated assign status: JobStatus
  - L110 annotated assign steps: List[JobStep] = field(default_factory=list)
  - L111 annotated assign progress: float = 0.0
  - L112 annotated assign result: Optional[Dict[str, Any]] = None
  - L113 annotated assign error: Optional[str] = None
  - L114 annotated assign template_id: Optional[str] = None
  - L115 annotated assign template_name: Optional[str] = None
  - L116 annotated assign template_kind: Optional[str] = None
  - L117 annotated assign connection_id: Optional[str] = None
  - L118 annotated assign schedule_id: Optional[str] = None
  - L119 annotated assign correlation_id: Optional[str] = None
  - L120 annotated assign meta: Dict[str, Any] = field(default_factory=dict)
  - L121 annotated assign created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L122 annotated assign started_at: Optional[datetime] = None
  - L123 annotated assign completed_at: Optional[datetime] = None
  - L126 def createcls, job_type: JobType, steps: Optional[List[JobStep]]=None, **kwargs: Any:
    - L132 return cls(job_id=str(uuid.uuid4()), job_type=job_type, status=JobStatus.PENDING, steps=steps or [], **kwargs)
  - L140 def can_transition_toself, new_status: JobStatus:
    - L141 docstring: "Check if transition to new status is valid."
    - L142 assign valid_transitions = {JobStatus.PENDING: {JobStatus.QUEUED, JobStatus.RUNNING, JobStatus.CANCELLED}, JobStatus.QUEUED: {JobStatus.RUNNING, JobStatus.CANCELLED}, JobStatus.RUNNING: {JobStatus.SUCCEEDED, JobStatus.FAILED, JobStatus.CANCELLED}, JobStatus.SUCCEEDED: set(), JobStatus.FAILED: set(), JobStatus.CANCELLED: set()}
    - L150 return new_status in valid_transitions.get(self.status, set())
  - L152 def startself:
    - L153 if not self.can_transition_to(JobStatus.RUNNING):
      - L154 return None
    - L155 assign self.status = JobStatus.RUNNING
    - L156 assign self.started_at = datetime.now(timezone.utc)
  - L158 def succeedself, result: Optional[Dict[str, Any]]=None:
    - L159 if not self.can_transition_to(JobStatus.SUCCEEDED):
      - L160 return None
    - L161 assign self.status = JobStatus.SUCCEEDED
    - L162 assign self.progress = 100.0
    - L163 assign self.result = result
    - L164 assign self.completed_at = datetime.now(timezone.utc)
  - L166 def failself, error: str:
    - L167 if not self.can_transition_to(JobStatus.FAILED):
      - L168 return None
    - L169 assign self.status = JobStatus.FAILED
    - L170 assign self.error = error
    - L171 assign self.completed_at = datetime.now(timezone.utc)
  - L173 def cancelself:
    - L174 if not self.can_transition_to(JobStatus.CANCELLED):
      - L175 return None
    - L176 assign self.status = JobStatus.CANCELLED
    - L177 assign self.completed_at = datetime.now(timezone.utc)
  - L179 def update_progressself, progress: float:
    - L180 assign self.progress = max(0.0, min(100.0, progress))
  - L182 def get_stepself, name: str:
    - L183 for step in self.steps:
      - L184 if step.name == name:
        - L185 return step
    - L186 return None
  - L188 def step_runningself, name: str:
    - L189 assign step = self.get_step(name)
    - L190 if step:
      - L191 expr step.start()
  - L193 def step_succeededself, name: str, progress: Optional[float]=None:
    - L194 assign step = self.get_step(name)
    - L195 if step:
      - L196 expr step.succeed()
      - L197 if progress is not None:
        - L198 expr self.update_progress(progress)
  - L200 def step_failedself, name: str, error: str:
    - L201 assign step = self.get_step(name)
    - L202 if step:
      - L203 expr step.fail(error)
  - L206 def duration_secondsself:
    - L207 if not self.started_at:
      - L208 return None
    - L209 assign end = self.completed_at or datetime.now(timezone.utc)
    - L210 return (end - self.started_at).total_seconds()
  - L212 def to_dictself:
    - L213 return {'job_id': self.job_id, 'job_type': self.job_type.value, 'status': self.status.value, 'steps': [s.to_dict() for s in self.steps], 'progress': self.progress, 'result': self.result, 'error': self.error, 'template_id': self.template_id, 'template_name': self.template_name, 'template_kind': self.template_kind, 'connection_id': self.connection_id, 'schedule_id': self.schedule_id, 'correlation_id': self.correlation_id, 'meta': self.meta, 'created_at': self.created_at.isoformat(), 'started_at': self.started_at.isoformat() if self.started_at else None, 'completed_at': self.completed_at.isoformat() if self.completed_at else None}
- L235 class Schedule:
  - L236 docstring: "A recurring job schedule.\n\n    Schedules define when jobs should run automatic..."
  - L241 annotated assign schedule_id: str
  - L242 annotated assign name: str
  - L243 annotated assign template_id: str
  - L244 annotated assign connection_id: str
  - L245 annotated assign interval_minutes: int
  - L246 annotated assign active: bool = True
  - L247 annotated assign template_name: Optional[str] = None
  - L248 annotated assign template_kind: str = 'pdf'
  - L249 annotated assign start_date: Optional[str] = None
  - L250 annotated assign end_date: Optional[str] = None
  - L251 annotated assign batch_ids: Optional[List[str]] = None
  - L252 annotated assign key_values: Optional[Dict[str, str]] = None
  - L253 annotated assign docx: bool = False
  - L254 annotated assign xlsx: bool = False
  - L255 annotated assign email_recipients: Optional[List[str]] = None
  - L256 annotated assign email_subject: Optional[str] = None
  - L257 annotated assign email_message: Optional[str] = None
  - L258 annotated assign next_run_at: Optional[datetime] = None
  - L259 annotated assign last_run_at: Optional[datetime] = None
  - L260 annotated assign last_run_status: Optional[str] = None
  - L261 annotated assign last_run_error: Optional[str] = None
  - L262 annotated assign run_count: int = 0
  - L263 annotated assign created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L264 annotated assign updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L267 def createcls, name: str, template_id: str, connection_id: str, interval_minutes: int, **kwargs: Any:
    - L275 assign now = datetime.now(timezone.utc)
    - L276 return cls(schedule_id=str(uuid.uuid4()), name=name, template_id=template_id, connection_id=connection_id, interval_minutes=max(1, interval_minutes), next_run_at=now + timedelta(minutes=max(1, interval_minutes)), **kwargs)
  - L286 def is_dueself, now: Optional[datetime]=None:
    - L287 if not self.active or not self.next_run_at:
      - L288 return False
    - L289 assign now = now or datetime.now(timezone.utc)
    - L290 return self.next_run_at <= now
  - L292 def record_runself, status: str, error: Optional[str]=None, finished_at: Optional[datetime]=None:
    - L298 assign finished = finished_at or datetime.now(timezone.utc)
    - L299 assign self.last_run_at = finished
    - L300 assign self.last_run_status = status
    - L301 assign self.last_run_error = error
    - L302 aug assign self.run_count Add 1
    - L303 assign self.next_run_at = finished + timedelta(minutes=self.interval_minutes)
    - L304 assign self.updated_at = datetime.now(timezone.utc)
  - L306 def pauseself:
    - L307 assign self.active = False
    - L308 assign self.updated_at = datetime.now(timezone.utc)
  - L310 def resumeself:
    - L311 assign self.active = True
    - L312 if not self.next_run_at or self.next_run_at < datetime.now(timezone.utc):
      - L313 assign self.next_run_at = datetime.now(timezone.utc) + timedelta(minutes=self.interval_minutes)
    - L316 assign self.updated_at = datetime.now(timezone.utc)
  - L318 def to_dictself:
    - L319 return {'schedule_id': self.schedule_id, 'id': self.schedule_id, 'name': self.name, 'template_id': self.template_id, 'connection_id': self.connection_id, 'interval_minutes': self.interval_minutes, 'active': self.active, 'template_name': self.template_name, 'template_kind': self.template_kind, 'start_date': self.start_date, 'end_date': self.end_date, 'batch_ids': self.batch_ids, 'key_values': self.key_values, 'docx': self.docx, 'xlsx': self.xlsx, 'email_recipients': self.email_recipients, 'email_subject': self.email_subject, 'email_message': self.email_message, 'next_run_at': self.next_run_at.isoformat() if self.next_run_at else None, 'last_run_at': self.last_run_at.isoformat() if self.last_run_at else None, 'last_run_status': self.last_run_status, 'last_run_error': self.last_run_error, 'run_count': self.run_count, 'created_at': self.created_at.isoformat(), 'updated_at': self.updated_at.isoformat()}
  - L348 def from_dictcls, data: Dict[str, Any]:
    - L349 def parse_dtval: Any:
      - L350 if val is None:
        - L351 return None
      - L352 if isinstance(val, datetime):
        - L353 return val
      - L354 if isinstance(val, str):
        - L355 return datetime.fromisoformat(val)
      - L356 return None
    - L358 return cls(schedule_id=data.get('schedule_id') or data.get('id', str(uuid.uuid4())), name=data['name'], template_id=data['template_id'], connection_id=data['connection_id'], interval_minutes=int(data.get('interval_minutes', 60)), active=bool(data.get('active', True)), template_name=data.get('template_name'), template_kind=data.get('template_kind', 'pdf'), start_date=data.get('start_date'), end_date=data.get('end_date'), batch_ids=data.get('batch_ids'), key_values=data.get('key_values'), docx=bool(data.get('docx')), xlsx=bool(data.get('xlsx')), email_recipients=data.get('email_recipients'), email_subject=data.get('email_subject'), email_message=data.get('email_message'), next_run_at=parse_dt(data.get('next_run_at')), last_run_at=parse_dt(data.get('last_run_at')), last_run_status=data.get('last_run_status'), last_run_error=data.get('last_run_error'), run_count=int(data.get('run_count', 0)), created_at=parse_dt(data.get('created_at')) or datetime.now(timezone.utc), updated_at=parse_dt(data.get('updated_at')) or datetime.now(timezone.utc))

## backend\domain\reports.py
- L1 docstring: "Report domain entities.\n\nA Report represents a generated document from a templ..."
- L6 from __future__ import annotations
- L8 from dataclasses import dataclass, field
- L9 from datetime import datetime, timezone
- L10 from enum import Enum
- L11 from pathlib import Path
- L12 from typing import Any, Dict, List, Optional
- L13 import uuid
- L16 class OutputFormat(str, Enum):
  - L17 docstring: "Supported output formats."
  - L19 assign HTML = 'html'
  - L20 assign PDF = 'pdf'
  - L21 assign DOCX = 'docx'
  - L22 assign XLSX = 'xlsx'
- L25 class ReportStatus(str, Enum):
  - L26 docstring: "Status of a report generation."
  - L28 assign PENDING = 'pending'
  - L29 assign RUNNING = 'running'
  - L30 assign SUCCEEDED = 'succeeded'
  - L31 assign FAILED = 'failed'
  - L32 assign CANCELLED = 'cancelled'
- L36 class Batch:
  - L37 docstring: "A batch of data for report generation.\n\n    Reports can be generated for multi..."
  - L42 annotated assign batch_id: str
  - L43 annotated assign row_count: int
  - L44 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
- L48 class KeyValue:
  - L49 docstring: "A key-value filter for report generation."
  - L51 annotated assign key: str
  - L52 annotated assign value: str
  - L54 def to_sql_conditionself, table: Optional[str]=None:
    - L55 docstring: "Generate SQL WHERE condition for this key-value."
    - L56 assign col = f'{table}.{self.key}' if table else self.key
    - L57 assign safe_value = self.value.replace("'", "''")
    - L58 return f"{col} = '{safe_value}'"
- L62 class RenderRequest:
  - L63 docstring: "Request to render a report.\n\n    This is the input to the report rendering pip..."
  - L68 annotated assign template_id: str
  - L69 annotated assign connection_id: str
  - L70 annotated assign start_date: Optional[str] = None
  - L71 annotated assign end_date: Optional[str] = None
  - L72 annotated assign batch_ids: Optional[List[str]] = None
  - L73 annotated assign key_values: Optional[List[KeyValue]] = None
  - L74 annotated assign output_formats: List[OutputFormat] = field(default_factory=lambda: [OutputFormat.HTML, OutputFormat.PDF])
  - L77 annotated assign email_recipients: Optional[List[str]] = None
  - L78 annotated assign email_subject: Optional[str] = None
  - L79 annotated assign email_message: Optional[str] = None
  - L80 annotated assign correlation_id: Optional[str] = None
  - L82 def __post_init__self:
    - L83 if not self.template_id:
      - L84 raise ValueError('template_id is required')
    - L85 if not self.connection_id:
      - L86 raise ValueError('connection_id is required')
- L90 class RenderOutput:
  - L91 docstring: "Output artifact from rendering.\n\n    Each format produces one RenderOutput.\n ..."
  - L96 annotated assign format: OutputFormat
  - L97 annotated assign path: Path
  - L98 annotated assign size_bytes: int
  - L99 annotated assign checksum: Optional[str] = None
  - L100 annotated assign url: Optional[str] = None
- L104 class Report:
  - L105 docstring: "A generated report.\n\n    This is the result of processing a RenderRequest.\n  ..."
  - L110 annotated assign report_id: str
  - L111 annotated assign template_id: str
  - L112 annotated assign template_name: str
  - L113 annotated assign connection_id: str
  - L114 annotated assign connection_name: Optional[str]
  - L115 annotated assign status: ReportStatus
  - L116 annotated assign outputs: List[RenderOutput] = field(default_factory=list)
  - L117 annotated assign start_date: Optional[str] = None
  - L118 annotated assign end_date: Optional[str] = None
  - L119 annotated assign batch_ids: Optional[List[str]] = None
  - L120 annotated assign key_values: Optional[List[KeyValue]] = None
  - L121 annotated assign error: Optional[str] = None
  - L122 annotated assign started_at: Optional[datetime] = None
  - L123 annotated assign completed_at: Optional[datetime] = None
  - L124 annotated assign correlation_id: Optional[str] = None
  - L125 annotated assign schedule_id: Optional[str] = None
  - L126 annotated assign schedule_name: Optional[str] = None
  - L129 def createcls, template_id: str, template_name: str, connection_id: str, connection_name: Optional[str]=None, **kwargs: Any:
    - L137 return cls(report_id=str(uuid.uuid4()), template_id=template_id, template_name=template_name, connection_id=connection_id, connection_name=connection_name, status=ReportStatus.PENDING, **kwargs)
  - L147 def startself:
    - L148 assign self.status = ReportStatus.RUNNING
    - L149 assign self.started_at = datetime.now(timezone.utc)
  - L151 def succeedself, outputs: List[RenderOutput]:
    - L152 assign self.status = ReportStatus.SUCCEEDED
    - L153 assign self.outputs = outputs
    - L154 assign self.completed_at = datetime.now(timezone.utc)
  - L156 def failself, error: str:
    - L157 assign self.status = ReportStatus.FAILED
    - L158 assign self.error = error
    - L159 assign self.completed_at = datetime.now(timezone.utc)
  - L161 def cancelself:
    - L162 assign self.status = ReportStatus.CANCELLED
    - L163 assign self.completed_at = datetime.now(timezone.utc)
  - L165 def to_dictself:
    - L166 return {'report_id': self.report_id, 'template_id': self.template_id, 'template_name': self.template_name, 'connection_id': self.connection_id, 'connection_name': self.connection_name, 'status': self.status.value, 'outputs': [{'format': o.format.value, 'path': str(o.path), 'size_bytes': o.size_bytes, 'url': o.url} for o in self.outputs], 'start_date': self.start_date, 'end_date': self.end_date, 'batch_ids': self.batch_ids, 'error': self.error, 'started_at': self.started_at.isoformat() if self.started_at else None, 'completed_at': self.completed_at.isoformat() if self.completed_at else None, 'correlation_id': self.correlation_id}
- L193 class DataWindow:
  - L194 docstring: "A window of data for report generation."
  - L196 annotated assign start_date: Optional[str]
  - L197 annotated assign end_date: Optional[str]
  - L198 annotated assign filters: List[KeyValue] = field(default_factory=list)
  - L200 def to_sql_conditionsself, date_column: str='date':
    - L201 docstring: "Generate SQL WHERE conditions."
    - L202 assign conditions = []
    - L203 if self.start_date:
      - L204 expr conditions.append(f"{date_column} >= '{self.start_date}'")
    - L205 if self.end_date:
      - L206 expr conditions.append(f"{date_column} <= '{self.end_date}'")
    - L207 for kv in self.filters:
      - L208 expr conditions.append(kv.to_sql_condition())
    - L209 return conditions

## backend\domain\templates.py
- L1 docstring: "Template domain entities.\n\nA Template is a document blueprint that can be fill..."
- L6 from __future__ import annotations
- L8 from dataclasses import dataclass, field
- L9 from datetime import datetime, timezone
- L10 from enum import Enum
- L11 from pathlib import Path
- L12 from typing import Any, Dict, List, Optional
- L13 import uuid
- L16 class TemplateKind(str, Enum):
  - L17 docstring: "Types of templates."
  - L19 assign PDF = 'pdf'
  - L20 assign EXCEL = 'excel'
- L23 class TemplateStatus(str, Enum):
  - L24 docstring: "Template lifecycle status."
  - L26 assign DRAFT = 'draft'
  - L27 assign ANALYZING = 'analyzing'
  - L28 assign MAPPED = 'mapped'
  - L29 assign APPROVED = 'approved'
  - L30 assign FAILED = 'failed'
- L34 class Artifact:
  - L35 docstring: "A file artifact associated with a template."
  - L37 annotated assign name: str
  - L38 annotated assign path: Path
  - L39 annotated assign artifact_type: str
  - L40 annotated assign size_bytes: Optional[int] = None
  - L41 annotated assign checksum: Optional[str] = None
  - L42 annotated assign created_at: Optional[datetime] = None
- L46 class TemplateSchema:
  - L47 docstring: "Schema extracted from a template."
  - L49 annotated assign scalars: List[str] = field(default_factory=list)
  - L50 annotated assign row_tokens: List[str] = field(default_factory=list)
  - L51 annotated assign totals: List[str] = field(default_factory=list)
  - L52 annotated assign tables_detected: List[str] = field(default_factory=list)
  - L53 annotated assign placeholders_found: int = 0
- L57 class Template:
  - L58 docstring: "A report template.\n\n    Templates contain:\n    - Source document (HTML)\n    ..."
  - L67 annotated assign template_id: str
  - L68 annotated assign name: str
  - L69 annotated assign kind: TemplateKind
  - L70 annotated assign status: TemplateStatus
  - L71 annotated assign schema: Optional[TemplateSchema] = None
  - L72 annotated assign contract_id: Optional[str] = None
  - L73 annotated assign artifacts: List[Artifact] = field(default_factory=list)
  - L74 annotated assign source_file: Optional[str] = None
  - L75 annotated assign description: Optional[str] = None
  - L76 annotated assign created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L77 annotated assign updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
  - L78 annotated assign last_run_at: Optional[datetime] = None
  - L79 annotated assign run_count: int = 0
  - L80 annotated assign tags: List[str] = field(default_factory=list)
  - L83 def createcls, name: str, kind: TemplateKind=TemplateKind.PDF, template_id: Optional[str]=None, **kwargs: Any:
    - L90 return cls(template_id=template_id or str(uuid.uuid4()), name=name, kind=kind, status=TemplateStatus.DRAFT, **kwargs)
  - L98 def record_runself:
    - L99 aug assign self.run_count Add 1
    - L100 assign self.last_run_at = datetime.now(timezone.utc)
    - L101 assign self.updated_at = datetime.now(timezone.utc)
  - L103 def transition_toself, status: TemplateStatus:
    - L104 assign self.status = status
    - L105 assign self.updated_at = datetime.now(timezone.utc)
  - L107 def add_artifactself, artifact: Artifact:
    - L109 assign self.artifacts = [a for a in self.artifacts if a.name != artifact.name]
    - L110 expr self.artifacts.append(artifact)
    - L111 assign self.updated_at = datetime.now(timezone.utc)
  - L113 def get_artifactself, name: str:
    - L114 for artifact in self.artifacts:
      - L115 if artifact.name == name:
        - L116 return artifact
    - L117 return None
  - L119 def to_dictself:
    - L120 return {'template_id': self.template_id, 'name': self.name, 'kind': self.kind.value, 'status': self.status.value, 'schema': {'scalars': self.schema.scalars, 'row_tokens': self.schema.row_tokens, 'totals': self.schema.totals, 'tables_detected': self.schema.tables_detected, 'placeholders_found': self.schema.placeholders_found} if self.schema else None, 'contract_id': self.contract_id, 'artifacts': [{'name': a.name, 'path': str(a.path), 'artifact_type': a.artifact_type, 'size_bytes': a.size_bytes} for a in self.artifacts], 'source_file': self.source_file, 'description': self.description, 'created_at': self.created_at.isoformat(), 'updated_at': self.updated_at.isoformat(), 'last_run_at': self.last_run_at.isoformat() if self.last_run_at else None, 'run_count': self.run_count, 'tags': self.tags}
  - L154 def from_dictcls, data: Dict[str, Any]:
    - L155 assign schema_data = data.get('schema')
    - L156 assign schema = None
    - L157 if schema_data:
      - L158 assign schema = TemplateSchema(scalars=schema_data.get('scalars', []), row_tokens=schema_data.get('row_tokens', []), totals=schema_data.get('totals', []), tables_detected=schema_data.get('tables_detected', []), placeholders_found=schema_data.get('placeholders_found', 0))
    - L166 assign artifacts = []
    - L167 for a in data.get('artifacts', []):
      - L168 expr artifacts.append(Artifact(name=a['name'], path=Path(a['path']), artifact_type=a['artifact_type'], size_bytes=a.get('size_bytes')))
    - L177 assign created_at = data.get('created_at')
    - L178 if isinstance(created_at, str):
      - L179 assign created_at = datetime.fromisoformat(created_at)
    - L181 assign updated_at = data.get('updated_at')
    - L182 if isinstance(updated_at, str):
      - L183 assign updated_at = datetime.fromisoformat(updated_at)
    - L185 assign last_run_at = data.get('last_run_at')
    - L186 if isinstance(last_run_at, str):
      - L187 assign last_run_at = datetime.fromisoformat(last_run_at)
    - L189 return cls(template_id=data['template_id'], name=data['name'], kind=TemplateKind(data.get('kind', 'pdf')), status=TemplateStatus(data.get('status', 'draft')), schema=schema, contract_id=data.get('contract_id'), artifacts=artifacts, source_file=data.get('source_file'), description=data.get('description'), created_at=created_at or datetime.now(timezone.utc), updated_at=updated_at or datetime.now(timezone.utc), last_run_at=last_run_at, run_count=data.get('run_count', 0), tags=data.get('tags', []))

## backend\NeuraCore\_init_.py

## backend\orchestration\__init__.py
- L1 docstring: "Job orchestration - scheduling and execution of background work."
- L3 from .scheduler import Scheduler
- L4 from .executor import JobExecutor, ExecutorConfig
- L5 from .worker import WorkerPool
- L7 assign __all__ = ['Scheduler', 'JobExecutor', 'ExecutorConfig', 'WorkerPool']

## backend\orchestration\executor.py
- L1 docstring: "Job executor - runs jobs with tracking and observability."
- L3 from __future__ import annotations
- L5 import asyncio
- L6 import logging
- L7 import time
- L8 from concurrent.futures import ThreadPoolExecutor
- L9 from dataclasses import dataclass, field
- L10 from datetime import datetime, timezone
- L11 from typing import Any, Callable, Dict, Optional
- L13 from backend.core.events import Event, publish_sync
- L14 from backend.domain.jobs import Job, JobStatus, JobType
- L16 assign logger = logging.getLogger('neura.orchestration.executor')
- L20 class ExecutorConfig:
  - L21 docstring: "Configuration for job executor."
  - L23 annotated assign max_workers: int = 4
  - L24 annotated assign default_timeout_seconds: float = 600.0
  - L25 annotated assign enable_thread_injection_cancel: bool = False
- L29 class JobExecution:
  - L30 docstring: "Tracks a single job execution."
  - L32 annotated assign job: Job
  - L33 annotated assign future: Optional[asyncio.Future] = None
  - L34 annotated assign thread_id: Optional[int] = None
  - L35 annotated assign started_at: Optional[datetime] = None
  - L36 annotated assign child_pids: set = field(default_factory=set)
- L39 assign JobRunner = Callable[[Job, 'JobExecutor'], Any]
- L42 class JobExecutor:
  - L43 docstring: "Executes jobs with tracking and cancellation support.\n\n    Features:\n    - Th..."
  - L52 def __init__self, config: Optional[ExecutorConfig]=None:
    - L53 assign self._config = config or ExecutorConfig()
    - L54 assign self._pool = ThreadPoolExecutor(max_workers=self._config.max_workers, thread_name_prefix='job-executor')
    - L58 annotated assign self._executions: Dict[str, JobExecution] = {}
    - L59 annotated assign self._runners: Dict[JobType, JobRunner] = {}
    - L60 assign self._shutdown = False
  - L62 def register_runnerself, job_type: JobType, runner: JobRunner:
    - L63 docstring: "Register a runner function for a job type."
    - L64 assign self._runners[job_type] = runner
  - L66 async def submitself, job: Job:
    - L67 docstring: "Submit a job for execution."
    - L68 if self._shutdown:
      - L69 raise RuntimeError('Executor is shutting down')
    - L71 if job.job_id in self._executions:
      - L72 raise ValueError(f'Job {job.job_id} is already running')
    - L74 assign runner = self._runners.get(job.job_type)
    - L75 if not runner:
      - L76 raise ValueError(f'No runner registered for job type: {job.job_type}')
    - L78 assign execution = JobExecution(job=job)
    - L79 assign self._executions[job.job_id] = execution
    - L82 expr publish_sync(Event(name='job.submitted', payload={'job_id': job.job_id, 'job_type': job.job_type.value}, correlation_id=job.correlation_id))
    - L94 assign loop = asyncio.get_event_loop()
    - L95 assign future = loop.run_in_executor(self._pool, self._run_job, job, runner, execution)
    - L102 assign execution.future = future
  - L104 def _run_jobself, job: Job, runner: JobRunner, execution: JobExecution:
    - L110 docstring: "Run a job in a worker thread."
    - L111 import threading
    - L113 assign execution.thread_id = threading.get_ident()
    - L114 assign execution.started_at = datetime.now(timezone.utc)
    - L116 expr job.start()
    - L118 expr publish_sync(Event(name='job.started', payload={'job_id': job.job_id}, correlation_id=job.correlation_id))
    - L126 expr logger.info('job_started', extra={'job_id': job.job_id, 'job_type': job.job_type.value, 'correlation_id': job.correlation_id})
    - L135 assign start = time.perf_counter()
    - L137 try:
      - L138 assign result = runner(job, self)
      - L140 assign elapsed = (time.perf_counter() - start) * 1000
      - L141 expr job.succeed({'result': result} if result else None)
      - L143 expr publish_sync(Event(name='job.completed', payload={'job_id': job.job_id, 'status': 'succeeded', 'duration_ms': elapsed}, correlation_id=job.correlation_id))
      - L155 expr logger.info('job_completed', extra={'job_id': job.job_id, 'status': 'succeeded', 'duration_ms': elapsed, 'correlation_id': job.correlation_id})
      - L165 return result
      - L167 except asyncio.CancelledError:
        - L168 expr job.cancel()
        - L169 expr publish_sync(Event(name='job.cancelled', payload={'job_id': job.job_id}, correlation_id=job.correlation_id))
        - L176 expr logger.info('job_cancelled', extra={'job_id': job.job_id, 'correlation_id': job.correlation_id})
        - L183 raise
      - L185 except Exception as e:
        - L186 assign elapsed = (time.perf_counter() - start) * 1000
        - L187 expr job.fail(str(e))
        - L189 expr publish_sync(Event(name='job.failed', payload={'job_id': job.job_id, 'error': str(e), 'duration_ms': elapsed}, correlation_id=job.correlation_id))
        - L201 expr logger.exception('job_failed', extra={'job_id': job.job_id, 'error': str(e), 'duration_ms': elapsed, 'correlation_id': job.correlation_id})
        - L211 raise
      - L214 finally:
        - L214 expr self._executions.pop(job.job_id, None)
  - L216 def cancelself, job_id: str, *, force: bool=False:
    - L217 docstring: "Cancel a running job."
    - L218 assign execution = self._executions.get(job_id)
    - L219 if not execution:
      - L220 return False
    - L222 expr execution.job.cancel()
    - L224 if execution.future and (not execution.future.done()):
      - L225 assign cancelled = execution.future.cancel()
      - L226 if cancelled:
        - L227 return True
    - L229 if force and self._config.enable_thread_injection_cancel:
      - L230 return self._inject_cancel(execution)
    - L232 return False
  - L234 def _inject_cancelself, execution: JobExecution:
    - L235 docstring: "Attempt to cancel via thread exception injection."
    - L236 if not execution.thread_id:
      - L237 return False
    - L239 try:
      - L240 import ctypes
      - L242 assign res = ctypes.pythonapi.PyThreadState_SetAsyncExc(ctypes.c_long(execution.thread_id), ctypes.py_object(asyncio.CancelledError))
      - L246 return res == 1
      - L247 except Exception:
        - L248 return False
  - L250 def get_statusself, job_id: str:
    - L251 docstring: "Get the status of a job."
    - L252 assign execution = self._executions.get(job_id)
    - L253 if execution:
      - L254 return execution.job.status
    - L255 return None
  - L257 def get_active_jobsself:
    - L258 docstring: "Get IDs of all active jobs."
    - L259 return list(self._executions.keys())
  - L261 async def shutdownself, wait: bool=True:
    - L262 docstring: "Shutdown the executor."
    - L263 assign self._shutdown = True
    - L265 if wait:
      - L267 assign futures = [e.future for e in self._executions.values() if e.future and (not e.future.done())]
      - L271 if futures:
        - L272 expr await asyncio.gather(*futures, return_exceptions=True)
    - L274 expr self._pool.shutdown(wait=wait)
- L278 annotated assign _executor: Optional[JobExecutor] = None
- L281 def get_executor:
  - L282 docstring: "Get or create the global job executor."
  - L283 Global
  - L284 if _executor is None:
    - L285 assign _executor = JobExecutor()
  - L286 return _executor

## backend\orchestration\scheduler.py
- L1 docstring: "Job scheduler - triggers scheduled jobs at the right time."
- L3 from __future__ import annotations
- L5 import asyncio
- L6 import logging
- L7 from datetime import datetime, timezone
- L8 from typing import Callable, List, Optional, Protocol
- L10 from backend.core.events import Event, publish_sync
- L11 from backend.domain.jobs import Schedule, Job, JobType, JobStep
- L13 assign logger = logging.getLogger('neura.orchestration.scheduler')
- L16 class ScheduleRepository(Protocol):
  - L17 docstring: "Protocol for schedule storage."
  - L19 def find_dueself, now: Optional[datetime]=None:
    - L20 expr ...
  - L22 def saveself, schedule: Schedule:
    - L23 expr ...
- L26 class JobSubmitter(Protocol):
  - L27 docstring: "Protocol for job submission."
  - L29 async def submitself, job: Job:
    - L30 expr ...
- L33 class Scheduler:
  - L34 docstring: "Scheduler for recurring jobs.\n\n    Polls for due schedules and submits jobs fo..."
  - L39 def __init__self, schedule_repo: ScheduleRepository, job_submitter: JobSubmitter, *, poll_interval_seconds: int=60:
    - L46 assign self._repo = schedule_repo
    - L47 assign self._submitter = job_submitter
    - L48 assign self._poll_interval = max(poll_interval_seconds, 5)
    - L49 annotated assign self._task: Optional[asyncio.Task] = None
    - L50 assign self._stop_event = asyncio.Event()
    - L51 annotated assign self._inflight: set[str] = set()
  - L53 async def startself:
    - L54 docstring: "Start the scheduler."
    - L55 if self._task and (not self._task.done()):
      - L56 return None
    - L58 expr self._stop_event.clear()
    - L59 assign self._task = asyncio.create_task(self._run_loop(), name='scheduler-loop')
    - L64 expr logger.info('scheduler_started', extra={'event': 'scheduler_started'})
    - L66 expr publish_sync(Event(name='scheduler.started', payload={'poll_interval': self._poll_interval}))
  - L73 async def stopself:
    - L74 docstring: "Stop the scheduler."
    - L75 if not self._task:
      - L76 return None
    - L78 expr self._stop_event.set()
    - L79 expr self._task.cancel()
    - L81 try:
      - L82 expr await self._task
      - L83 except asyncio.CancelledError:
        - L84 pass
      - L86 finally:
        - L86 assign self._task = None
    - L88 expr logger.info('scheduler_stopped', extra={'event': 'scheduler_stopped'})
    - L90 expr publish_sync(Event(name='scheduler.stopped', payload={}))
  - L92 async def _run_loopself:
    - L93 docstring: "Main scheduler loop."
    - L94 try:
      - L95 while not self._stop_event.is_set():
        - L96 try:
          - L97 expr await self._check_and_dispatch()
          - L98 except Exception:
            - L99 expr logger.exception('scheduler_tick_failed', extra={'event': 'scheduler_tick_failed'})
        - L104 try:
          - L105 expr await asyncio.wait_for(self._stop_event.wait(), timeout=self._poll_interval)
          - L109 except asyncio.TimeoutError:
            - L110 continue
      - L111 except asyncio.CancelledError:
        - L112 raise
  - L114 async def _check_and_dispatchself:
    - L115 docstring: "Check for due schedules and dispatch jobs."
    - L116 assign now = datetime.now(timezone.utc)
    - L117 assign due_schedules = self._repo.find_due(now)
    - L119 for schedule in due_schedules:
      - L120 if not schedule.active:
        - L121 continue
      - L123 if schedule.schedule_id in self._inflight:
        - L124 continue
      - L126 expr self._inflight.add(schedule.schedule_id)
      - L128 try:
        - L129 expr await self._dispatch_schedule(schedule)
        - L130 except Exception:
          - L131 expr logger.exception('schedule_dispatch_failed', extra={'schedule_id': schedule.schedule_id, 'event': 'schedule_dispatch_failed'})
        - L139 finally:
          - L139 expr self._inflight.discard(schedule.schedule_id)
  - L141 async def _dispatch_scheduleself, schedule: Schedule:
    - L142 docstring: "Create and submit a job for a schedule."
    - L143 assign correlation_id = f'sched-{schedule.schedule_id}-{int(datetime.now().timestamp())}'
    - L146 assign job = Job.create(job_type=JobType.REPORT_GENERATION, template_id=schedule.template_id, template_name=schedule.template_name, template_kind=schedule.template_kind, connection_id=schedule.connection_id, schedule_id=schedule.schedule_id, correlation_id=correlation_id, steps=[JobStep(name='dataLoad', label='Load database'), JobStep(name='contractCheck', label='Prepare contract'), JobStep(name='renderPdf', label='Render PDF'), JobStep(name='finalize', label='Finalize')], meta={'start_date': schedule.start_date, 'end_date': schedule.end_date, 'schedule_name': schedule.name, 'docx': schedule.docx, 'xlsx': schedule.xlsx})
    - L169 expr logger.info('schedule_job_created', extra={'schedule_id': schedule.schedule_id, 'job_id': job.job_id, 'correlation_id': correlation_id, 'event': 'schedule_job_created'})
    - L179 expr publish_sync(Event(name='schedule.triggered', payload={'schedule_id': schedule.schedule_id, 'job_id': job.job_id}, correlation_id=correlation_id))
    - L191 try:
      - L192 expr await self._submitter.submit(job)
      - L195 expr schedule.record_run('triggered')
      - L196 expr self._repo.save(schedule)
      - L198 except Exception as e:
        - L200 expr schedule.record_run('failed', error=str(e))
        - L201 expr self._repo.save(schedule)
        - L202 raise
  - L205 def is_runningself:
    - L206 docstring: "Check if scheduler is running."
    - L207 return self._task is not None and (not self._task.done())

## backend\orchestration\worker.py
- L1 docstring: "Worker pool for job execution."
- L3 from __future__ import annotations
- L5 import asyncio
- L6 import logging
- L7 from dataclasses import dataclass
- L8 from typing import Optional
- L10 from .executor import JobExecutor, ExecutorConfig
- L12 assign logger = logging.getLogger('neura.orchestration.worker')
- L16 class WorkerPoolConfig:
  - L17 docstring: "Configuration for worker pool."
  - L19 annotated assign num_workers: int = 4
  - L20 annotated assign queue_size: int = 100
  - L21 annotated assign shutdown_timeout_seconds: float = 30.0
- L24 class WorkerPool:
  - L25 docstring: "Pool of workers for processing jobs.\n\n    Workers pull jobs from a queue and e..."
  - L30 def __init__self, executor: JobExecutor, config: Optional[WorkerPoolConfig]=None:
    - L35 assign self._executor = executor
    - L36 assign self._config = config or WorkerPoolConfig()
    - L37 annotated assign self._queue: asyncio.Queue = asyncio.Queue(maxsize=self._config.queue_size)
    - L38 annotated assign self._workers: list[asyncio.Task] = []
    - L39 assign self._shutdown = False
  - L41 async def startself:
    - L42 docstring: "Start the worker pool."
    - L43 if self._workers:
      - L44 return None
    - L46 assign self._shutdown = False
    - L48 for i in range(self._config.num_workers):
      - L49 assign worker = asyncio.create_task(self._worker_loop(i), name=f'worker-{i}')
      - L53 expr self._workers.append(worker)
    - L55 expr logger.info('worker_pool_started', extra={'event': 'worker_pool_started', 'num_workers': len(self._workers)})
  - L63 async def stopself:
    - L64 docstring: "Stop the worker pool gracefully."
    - L65 assign self._shutdown = True
    - L68 for worker in self._workers:
      - L69 expr worker.cancel()
    - L72 if self._workers:
      - L73 expr await asyncio.gather(*self._workers, return_exceptions=True)
    - L75 assign self._workers = []
    - L77 expr logger.info('worker_pool_stopped', extra={'event': 'worker_pool_stopped'})
  - L79 async def submitself, job:
    - L80 docstring: "Submit a job to the pool."
    - L81 if self._shutdown:
      - L82 raise RuntimeError('Worker pool is shutting down')
    - L84 expr await self._queue.put(job)
    - L86 expr logger.debug('job_queued', extra={'job_id': job.job_id, 'queue_size': self._queue.qsize()})
  - L94 async def _worker_loopself, worker_id: int:
    - L95 docstring: "Worker loop that processes jobs."
    - L96 expr logger.debug('worker_started', extra={'worker_id': worker_id, 'event': 'worker_started'})
    - L101 try:
      - L102 while not self._shutdown:
        - L103 try:
          - L105 assign job = await asyncio.wait_for(self._queue.get(), timeout=1.0)
          - L109 except asyncio.TimeoutError:
            - L110 continue
        - L112 try:
          - L113 expr await self._executor.submit(job)
          - L114 except Exception:
            - L115 expr logger.exception('worker_job_failed', extra={'worker_id': worker_id, 'job_id': job.job_id})
          - L123 finally:
            - L123 expr self._queue.task_done()
      - L125 except asyncio.CancelledError:
        - L126 pass
      - L128 finally:
        - L128 expr logger.debug('worker_stopped', extra={'worker_id': worker_id, 'event': 'worker_stopped'})
  - L134 def queue_sizeself:
    - L135 docstring: "Get current queue size."
    - L136 return self._queue.qsize()
  - L139 def is_runningself:
    - L140 docstring: "Check if pool is running."
    - L141 return bool(self._workers) and (not self._shutdown)

## backend\pipelines\__init__.py
- L1 docstring: "Pipeline framework for workflow orchestration.\n\nPipelines are declarative, com..."
- L7 from .base import Pipeline, Step, PipelineContext, StepResult
- L8 from .report_pipeline import ReportPipeline, create_report_pipeline
- L9 from .import_pipeline import ImportPipeline, create_import_pipeline
- L11 assign __all__ = ['Pipeline', 'Step', 'PipelineContext', 'StepResult', 'ReportPipeline', 'create_report_pipeline', 'ImportPipeline', 'create_import_pipeline']

## backend\pipelines\base.py
- L1 docstring: "Base pipeline framework.\n\nInspired by Dagster and Prefect, but simplified for ..."
- L11 from __future__ import annotations
- L13 import asyncio
- L14 import logging
- L15 import time
- L16 from abc import ABC, abstractmethod
- L17 from dataclasses import dataclass, field
- L18 from datetime import datetime, timezone
- L19 from enum import Enum
- L20 from typing import Any, Awaitable, Callable, Dict, Generic, List, Optional, TypeVar, Union
- L32 from backend.core.errors import PipelineError
- L33 from backend.core.events import Event, get_event_bus, publish_sync
- L34 from backend.core.result import Result, Ok, Err
- L36 assign logger = logging.getLogger('neura.pipelines')
- L38 assign T = TypeVar('T')
- L39 assign C = TypeVar('C', bound='PipelineContext')
- L42 class StepStatus(str, Enum):
  - L43 docstring: "Status of a pipeline step."
  - L45 assign PENDING = 'pending'
  - L46 assign RUNNING = 'running'
  - L47 assign SUCCEEDED = 'succeeded'
  - L48 assign FAILED = 'failed'
  - L49 assign SKIPPED = 'skipped'
- L53 class StepResult(Generic[T]):
  - L54 docstring: "Result of executing a step."
  - L56 annotated assign status: StepStatus
  - L57 annotated assign data: Optional[T] = None
  - L58 annotated assign error: Optional[str] = None
  - L59 annotated assign duration_ms: float = 0.0
  - L60 annotated assign skipped_reason: Optional[str] = None
  - L63 def successself:
    - L64 return self.status == StepStatus.SUCCEEDED
  - L67 def okcls, data: T, duration_ms: float=0.0:
    - L68 return cls(status=StepStatus.SUCCEEDED, data=data, duration_ms=duration_ms)
  - L71 def failcls, error: str, duration_ms: float=0.0:
    - L72 return cls(status=StepStatus.FAILED, error=error, duration_ms=duration_ms)
  - L75 def skipcls, reason: str:
    - L76 return cls(status=StepStatus.SKIPPED, skipped_reason=reason)
- L80 class PipelineContext:
  - L81 docstring: "Context shared between pipeline steps.\n\n    Contains:\n    - Input parameters\..."
  - L90 annotated assign correlation_id: str
  - L91 annotated assign inputs: Dict[str, Any] = field(default_factory=dict)
  - L92 annotated assign results: Dict[str, Any] = field(default_factory=dict)
  - L93 annotated assign resources: Dict[str, Any] = field(default_factory=dict)
  - L94 annotated assign metadata: Dict[str, Any] = field(default_factory=dict)
  - L95 annotated assign cancelled: bool = False
  - L96 annotated assign started_at: Optional[datetime] = None
  - L97 annotated assign completed_at: Optional[datetime] = None
  - L99 def setself, key: str, value: Any:
    - L100 docstring: "Set a result value."
    - L101 assign self.results[key] = value
  - L103 def getself, key: str, default: Any=None:
    - L104 docstring: "Get a result value."
    - L105 return self.results.get(key, default)
  - L107 def cancelself:
    - L108 docstring: "Mark the context as cancelled."
    - L109 assign self.cancelled = True
  - L111 def is_cancelledself:
    - L112 docstring: "Check if cancelled."
    - L113 return self.cancelled
  - L115 def get_resourceself, name: str:
    - L116 docstring: "Get a shared resource."
    - L117 return self.resources.get(name)
- L120 assign StepFunction = Union[Callable[[PipelineContext], Any], Callable[[PipelineContext], Awaitable[Any]]]
- L124 assign GuardFunction = Callable[[PipelineContext], bool]
- L128 class Step:
  - L129 docstring: "A single step in a pipeline.\n\n    Steps have:\n    - Name: Unique identifier\n..."
  - L138 annotated assign name: str
  - L139 annotated assign fn: StepFunction
  - L140 annotated assign label: Optional[str] = None
  - L141 annotated assign guard: Optional[GuardFunction] = None
  - L142 annotated assign retries: int = 0
  - L143 annotated assign retry_delay_seconds: float = 1.0
  - L144 annotated assign timeout_seconds: Optional[float] = None
  - L145 annotated assign on_error: Optional[Callable[[Exception, PipelineContext], None]] = None
  - L148 def display_nameself:
    - L149 return self.label or self.name
  - L151 def should_runself, ctx: PipelineContext:
    - L152 docstring: "Check if this step should run."
    - L153 if self.guard is None:
      - L154 return True
    - L155 try:
      - L156 return self.guard(ctx)
      - L157 except Exception:
        - L158 return True
  - L160 async def executeself, ctx: PipelineContext:
    - L161 docstring: "Execute this step with retries and timeout."
    - L162 if ctx.is_cancelled():
      - L163 return StepResult.skip('Pipeline cancelled')
    - L165 if not self.should_run(ctx):
      - L166 return StepResult.skip(f'Guard returned false for {self.name}')
    - L168 annotated assign last_error: Optional[Exception] = None
    - L169 assign attempts = self.retries + 1
    - L171 for attempt in range(attempts):
      - L172 if ctx.is_cancelled():
        - L173 return StepResult.skip('Pipeline cancelled')
      - L175 assign start = time.perf_counter()
      - L176 try:
        - L178 if asyncio.iscoroutinefunction(self.fn):
          - L179 if self.timeout_seconds:
            - L180 assign result = await asyncio.wait_for(self.fn(ctx), timeout=self.timeout_seconds)
            - L185 else:
              - L185 assign result = await self.fn(ctx)
          - L187 else:
            - L187 if self.timeout_seconds:
              - L188 assign result = await asyncio.wait_for(asyncio.to_thread(self.fn, ctx), timeout=self.timeout_seconds)
              - L193 else:
                - L193 assign result = self.fn(ctx)
        - L195 assign duration = (time.perf_counter() - start) * 1000
        - L196 return StepResult.ok(result, duration_ms=duration)
        - L198 except asyncio.TimeoutError:
          - L199 assign duration = (time.perf_counter() - start) * 1000
          - L200 return StepResult.fail(f'Step {self.name} timed out after {self.timeout_seconds}s', duration_ms=duration)
        - L204 except Exception as e:
          - L205 assign last_error = e
          - L206 assign duration = (time.perf_counter() - start) * 1000
          - L208 if self.on_error:
            - L209 try:
              - L210 expr self.on_error(e, ctx)
              - L211 except Exception:
                - L212 pass
          - L214 if attempt < attempts - 1:
            - L215 expr logger.warning('step_retry', extra={'step': self.name, 'attempt': attempt + 1, 'max_attempts': attempts, 'error': str(e)})
            - L224 expr await asyncio.sleep(self.retry_delay_seconds)
            - L226 else:
              - L226 return StepResult.fail(str(e), duration_ms=duration)
    - L228 return StepResult.fail(str(last_error) if last_error else 'Unknown error')
- L232 class PipelineResult:
  - L233 docstring: "Result of executing a pipeline."
  - L235 annotated assign success: bool
  - L236 annotated assign steps: Dict[str, StepResult]
  - L237 annotated assign context: PipelineContext
  - L238 annotated assign error: Optional[str] = None
  - L239 annotated assign duration_ms: float = 0.0
  - L241 def get_step_resultself, name: str:
    - L242 return self.steps.get(name)
- L245 class Pipeline:
  - L246 docstring: "A sequence of steps that process data.\n\n    Pipelines are:\n    - Declarative:..."
  - L255 def __init__self, name: str, steps: List[Step], *, on_error: Optional[Callable[[str, Exception, PipelineContext], None]]=None, on_success: Optional[Callable[[PipelineContext], None]]=None, on_step_complete: Optional[Callable[[str, StepResult, PipelineContext], None]]=None:
    - L264 assign self.name = name
    - L265 assign self.steps = steps
    - L266 assign self._on_error = on_error
    - L267 assign self._on_success = on_success
    - L268 assign self._on_step_complete = on_step_complete
  - L270 async def executeself, ctx: PipelineContext:
    - L271 docstring: "Execute all steps in order."
    - L272 assign ctx.started_at = datetime.now(timezone.utc)
    - L273 assign start = time.perf_counter()
    - L274 annotated assign step_results: Dict[str, StepResult] = {}
    - L277 expr publish_sync(Event(name='pipeline.started', payload={'pipeline': self.name, 'steps': [s.name for s in self.steps]}, correlation_id=ctx.correlation_id))
    - L285 try:
      - L286 for step in self.steps:
        - L287 if ctx.is_cancelled():
          - L288 assign step_results[step.name] = StepResult.skip('Pipeline cancelled')
          - L289 continue
        - L291 expr logger.info('step_started', extra={'pipeline': self.name, 'step': step.name, 'correlation_id': ctx.correlation_id})
        - L301 expr publish_sync(Event(name='pipeline.step_started', payload={'pipeline': self.name, 'step': step.name}, correlation_id=ctx.correlation_id))
        - L309 assign result = await step.execute(ctx)
        - L310 assign step_results[step.name] = result
        - L313 if self._on_step_complete:
          - L314 try:
            - L315 expr self._on_step_complete(step.name, result, ctx)
            - L316 except Exception:
              - L317 pass
        - L319 expr logger.info('step_completed', extra={'pipeline': self.name, 'step': step.name, 'status': result.status.value, 'duration_ms': result.duration_ms, 'correlation_id': ctx.correlation_id})
        - L331 expr publish_sync(Event(name='pipeline.step_completed', payload={'pipeline': self.name, 'step': step.name, 'status': result.status.value}, correlation_id=ctx.correlation_id))
        - L344 if result.status == StepStatus.FAILED:
          - L345 if self._on_error:
            - L346 try:
              - L347 expr self._on_error(step.name, PipelineError(message=result.error or 'Step failed', step=step.name), ctx)
              - L355 except Exception:
                - L356 pass
          - L358 assign duration = (time.perf_counter() - start) * 1000
          - L359 assign ctx.completed_at = datetime.now(timezone.utc)
          - L361 expr publish_sync(Event(name='pipeline.failed', payload={'pipeline': self.name, 'failed_step': step.name, 'error': result.error}, correlation_id=ctx.correlation_id))
          - L373 return PipelineResult(success=False, steps=step_results, context=ctx, error=f'Step {step.name} failed: {result.error}', duration_ms=duration)
      - L382 assign duration = (time.perf_counter() - start) * 1000
      - L383 assign ctx.completed_at = datetime.now(timezone.utc)
      - L385 if self._on_success:
        - L386 try:
          - L387 expr self._on_success(ctx)
          - L388 except Exception:
            - L389 pass
      - L391 expr publish_sync(Event(name='pipeline.completed', payload={'pipeline': self.name}, correlation_id=ctx.correlation_id))
      - L399 return PipelineResult(success=True, steps=step_results, context=ctx, duration_ms=duration)
      - L406 except Exception as e:
        - L407 assign duration = (time.perf_counter() - start) * 1000
        - L408 assign ctx.completed_at = datetime.now(timezone.utc)
        - L410 expr logger.exception('pipeline_error', extra={'pipeline': self.name, 'correlation_id': ctx.correlation_id})
        - L418 expr publish_sync(Event(name='pipeline.error', payload={'pipeline': self.name, 'error': str(e)}, correlation_id=ctx.correlation_id))
        - L426 return PipelineResult(success=False, steps=step_results, context=ctx, error=str(e), duration_ms=duration)
  - L434 def execute_syncself, ctx: PipelineContext:
    - L435 docstring: "Execute pipeline synchronously."
    - L436 return asyncio.run(self.execute(ctx))
- L439 def stepname: str, *, label: Optional[str]=None, guard: Optional[GuardFunction]=None, retries: int=0, timeout: Optional[float]=None:
  - L447 docstring: "Decorator to create a step from a function."
  - L449 def decoratorfn: StepFunction:
    - L450 return Step(name=name, fn=fn, label=label or name, guard=guard, retries=retries, timeout_seconds=timeout)
  - L459 return decorator

## backend\pipelines\import_pipeline.py
- L1 docstring: "Template import pipeline.\n\nHandles importing templates from ZIP files or indiv..."
- L6 from __future__ import annotations
- L8 import json
- L9 import logging
- L10 import shutil
- L11 import uuid
- L12 import zipfile
- L13 from dataclasses import dataclass, field
- L14 from datetime import datetime, timezone
- L15 from pathlib import Path
- L16 from typing import Any, Dict, List, Optional
- L18 from backend.core.errors import ValidationError
- L19 from backend.domain.templates import Template, TemplateKind, TemplateStatus, Artifact
- L20 from backend.adapters.extraction import PDFExtractor, ExcelExtractor
- L21 from .base import Pipeline, PipelineContext, Step
- L23 assign logger = logging.getLogger('neura.pipelines.import')
- L27 class ImportPipelineContext(PipelineContext):
  - L28 docstring: "Context specific to template import."
  - L31 annotated assign source_path: Optional[Path] = None
  - L32 annotated assign template_name: Optional[str] = None
  - L33 annotated assign template_kind: TemplateKind = TemplateKind.PDF
  - L34 annotated assign output_dir: Optional[Path] = None
  - L37 annotated assign extracted_dir: Optional[Path] = None
  - L38 annotated assign source_files: List[Path] = field(default_factory=list)
  - L39 annotated assign html_content: Optional[str] = None
  - L40 annotated assign extracted_tables: List[Dict[str, Any]] = field(default_factory=list)
  - L41 annotated assign detected_tokens: List[str] = field(default_factory=list)
  - L44 annotated assign template: Optional[Template] = None
  - L45 annotated assign artifacts: List[Artifact] = field(default_factory=list)
- L51 def validate_importctx: ImportPipelineContext:
  - L52 docstring: "Validate the import request."
  - L53 if not ctx.source_path:
    - L54 raise ValidationError(message='No source path provided')
  - L56 if not ctx.source_path.exists():
    - L57 raise ValidationError(message=f'Source file not found: {ctx.source_path}')
  - L59 if not ctx.template_name:
    - L60 assign ctx.template_name = ctx.source_path.stem
  - L63 assign suffix = ctx.source_path.suffix.lower()
  - L64 if suffix in ('.xlsx', '.xls', '.xlsm'):
    - L65 assign ctx.template_kind = TemplateKind.EXCEL
    - L66 else:
      - L66 if suffix == '.pdf':
        - L67 assign ctx.template_kind = TemplateKind.PDF
        - L68 else:
          - L68 if suffix == '.zip':
            - L70 pass
  - L72 expr logger.info('import_validated', extra={'source': str(ctx.source_path), 'kind': ctx.template_kind.value, 'correlation_id': ctx.correlation_id})
- L82 def extract_archivectx: ImportPipelineContext:
  - L83 docstring: "Extract ZIP archive if needed."
  - L84 if ctx.source_path.suffix.lower() != '.zip':
    - L86 assign ctx.source_files = [ctx.source_path]
    - L87 return ctx.source_path
  - L90 assign extract_dir = ctx.output_dir / f'import_{uuid.uuid4().hex[:8]}'
  - L91 expr extract_dir.mkdir(parents=True, exist_ok=True)
  - L93 with zipfile.ZipFile(ctx.source_path, 'r') as zf:
    - L94 expr zf.extractall(extract_dir)
  - L96 assign ctx.extracted_dir = extract_dir
  - L99 assign ctx.source_files = list(extract_dir.glob('**/*'))
  - L100 assign ctx.source_files = [f for f in ctx.source_files if f.is_file()]
  - L103 for f in ctx.source_files:
    - L104 if f.suffix.lower() in ('.xlsx', '.xls'):
      - L105 assign ctx.template_kind = TemplateKind.EXCEL
      - L106 break
      - L107 else:
        - L107 if f.suffix.lower() == '.pdf':
          - L108 assign ctx.template_kind = TemplateKind.PDF
          - L109 break
          - L110 else:
            - L110 if f.suffix.lower() == '.html':
              - L111 assign ctx.template_kind = TemplateKind.PDF
              - L112 break
  - L114 expr logger.info('archive_extracted', extra={'files': len(ctx.source_files), 'kind': ctx.template_kind.value, 'correlation_id': ctx.correlation_id})
  - L123 return extract_dir
- L126 def extract_contentctx: ImportPipelineContext:
  - L127 docstring: "Extract content from source files."
  - L128 if ctx.template_kind == TemplateKind.EXCEL:
    - L129 return _extract_excel(ctx)
    - L131 else:
      - L131 return _extract_pdf(ctx)
- L134 def _extract_pdfctx: ImportPipelineContext:
  - L135 docstring: "Extract content from PDF source."
  - L137 assign html_files = [f for f in ctx.source_files if f.suffix.lower() == '.html']
  - L138 if html_files:
    - L139 assign ctx.html_content = html_files[0].read_text(encoding='utf-8')
    - L140 return {'html': ctx.html_content}
  - L143 assign pdf_files = [f for f in ctx.source_files if f.suffix.lower() == '.pdf']
  - L144 if not pdf_files:
    - L145 raise ValidationError(message='No PDF or HTML file found')
  - L147 assign extractor = PDFExtractor()
  - L148 assign result = extractor.extract(pdf_files[0])
  - L150 assign ctx.extracted_tables = [t.to_dict() for t in result.tables]
  - L153 assign ctx.html_content = _build_html_from_extraction(result)
  - L155 return {'html': ctx.html_content, 'tables': ctx.extracted_tables, 'page_count': result.page_count}
- L162 def _extract_excelctx: ImportPipelineContext:
  - L163 docstring: "Extract content from Excel source."
  - L164 assign excel_files = [f for f in ctx.source_files if f.suffix.lower() in ('.xlsx', '.xls', '.xlsm')]
  - L168 if not excel_files:
    - L169 raise ValidationError(message='No Excel file found')
  - L171 assign extractor = ExcelExtractor()
  - L172 assign result = extractor.extract(excel_files[0])
  - L174 assign ctx.extracted_tables = [t.to_dict() for t in result.tables]
  - L177 assign ctx.html_content = _build_html_from_extraction(result)
  - L179 return {'html': ctx.html_content, 'tables': ctx.extracted_tables, 'sheet_count': len(result.tables)}
- L186 def _build_html_from_extractionresult:
  - L187 docstring: "Build HTML document from extraction result."
  - L188 assign html_parts = ['<!DOCTYPE html>', '<html>', '<head>', '<meta charset="UTF-8">', '<style>', 'table { border-collapse: collapse; width: 100%; }', 'th, td { border: 1px solid black; padding: 8px; text-align: left; }', 'th { background-color: #f2f2f2; }', '</style>', '</head>', '<body>']
  - L202 for table in result.tables:
    - L203 expr html_parts.append('<table>')
    - L204 expr html_parts.append('<thead><tr>')
    - L205 for header in table.headers:
      - L206 expr html_parts.append(f'<th>{header}</th>')
    - L207 expr html_parts.append('</tr></thead>')
    - L208 expr html_parts.append('<tbody>')
    - L209 for row in table.rows:
      - L210 expr html_parts.append('<tr>')
      - L211 for cell in row:
        - L212 expr html_parts.append(f'<td>{cell}</td>')
      - L213 expr html_parts.append('</tr>')
    - L214 expr html_parts.append('</tbody>')
    - L215 expr html_parts.append('</table>')
    - L216 expr html_parts.append('<br>')
  - L218 expr html_parts.extend(['</body>', '</html>'])
  - L219 return '\n'.join(html_parts)
- L222 def detect_tokensctx: ImportPipelineContext:
  - L223 docstring: "Detect placeholder tokens in the template."
  - L224 import re
  - L226 if not ctx.html_content:
    - L227 return []
  - L230 assign pattern = '\\{\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}\\}'
  - L231 assign matches = re.findall(pattern, ctx.html_content)
  - L233 assign ctx.detected_tokens = list(set(matches))
  - L235 expr logger.info('tokens_detected', extra={'count': len(ctx.detected_tokens), 'tokens': ctx.detected_tokens[:10], 'correlation_id': ctx.correlation_id})
  - L244 return ctx.detected_tokens
- L247 def create_template_recordctx: ImportPipelineContext:
  - L248 docstring: "Create the template record."
  - L249 assign template_id = str(uuid.uuid4())
  - L252 assign template_dir = ctx.output_dir / template_id
  - L253 expr template_dir.mkdir(parents=True, exist_ok=True)
  - L256 assign html_path = template_dir / 'template_p1.html'
  - L257 if ctx.html_content:
    - L258 expr html_path.write_text(ctx.html_content, encoding='utf-8')
    - L259 expr ctx.artifacts.append(Artifact(name='template_p1.html', path=html_path, artifact_type='html', size_bytes=html_path.stat().st_size, created_at=datetime.now(timezone.utc)))
  - L270 if ctx.source_path and ctx.source_path.exists():
    - L271 assign source_copy = template_dir / f'original{ctx.source_path.suffix}'
    - L272 expr shutil.copy2(ctx.source_path, source_copy)
    - L273 expr ctx.artifacts.append(Artifact(name=source_copy.name, path=source_copy, artifact_type='source', size_bytes=source_copy.stat().st_size))
  - L283 from backend.domain.templates import TemplateSchema
  - L285 assign ctx.template = Template(template_id=template_id, name=ctx.template_name, kind=ctx.template_kind, status=TemplateStatus.DRAFT, schema=TemplateSchema(scalars=[t for t in ctx.detected_tokens if not t.startswith('row_')], row_tokens=[t for t in ctx.detected_tokens if t.startswith('row_')], totals=[t for t in ctx.detected_tokens if t.startswith('total_')], placeholders_found=len(ctx.detected_tokens)), artifacts=ctx.artifacts, source_file=ctx.source_path.name if ctx.source_path else None)
  - L301 assign meta_path = template_dir / 'template_meta.json'
  - L302 expr meta_path.write_text(json.dumps(ctx.template.to_dict(), indent=2, default=str), encoding='utf-8')
  - L307 expr logger.info('template_created', extra={'template_id': template_id, 'name': ctx.template_name, 'kind': ctx.template_kind.value, 'artifacts': len(ctx.artifacts), 'correlation_id': ctx.correlation_id})
  - L318 return ctx.template
- L324 class ImportPipeline:
  - L325 docstring: "Template import pipeline wrapper."
  - L327 def __init__self, output_dir: Path:
    - L328 assign self._output_dir = output_dir
    - L329 assign self._pipeline = create_import_pipeline()
  - L331 def executeself, source_path: Path, *, template_name: Optional[str]=None, correlation_id: Optional[str]=None:
    - L338 docstring: "Execute the import pipeline."
    - L339 assign ctx = ImportPipelineContext(correlation_id=correlation_id or str(uuid.uuid4()), source_path=source_path, template_name=template_name, output_dir=self._output_dir)
    - L346 assign result = self._pipeline.execute_sync(ctx)
    - L348 if not result.success:
      - L349 raise Exception(f'Import pipeline failed: {result.error}')
    - L351 return ctx.template
- L354 def create_import_pipeline:
  - L355 docstring: "Create the template import pipeline."
  - L356 return Pipeline(name='template_import', steps=[Step(name='validate', fn=validate_import, label='Validate import'), Step(name='extract_archive', fn=extract_archive, label='Extract archive'), Step(name='extract_content', fn=extract_content, label='Extract content'), Step(name='detect_tokens', fn=detect_tokens, label='Detect tokens'), Step(name='create_template', fn=create_template_record, label='Create template')])

## backend\pipelines\report_pipeline.py
- L1 docstring: "Report generation pipeline.\n\nThis replaces the monolithic ReportGenerate.fill_..."
- L7 from __future__ import annotations
- L9 import json
- L10 import logging
- L11 import uuid
- L12 from dataclasses import dataclass, field
- L13 from datetime import datetime, timezone
- L14 from pathlib import Path
- L15 from typing import Any, Dict, List, Optional
- L17 from backend.core.errors import ValidationError, NotFoundError
- L18 from backend.domain.contracts import Contract
- L19 from backend.domain.reports import OutputFormat, RenderRequest, RenderOutput, Report
- L20 from backend.adapters.databases import DataSource, SQLiteDataSource
- L21 from backend.adapters.rendering import HTMLRenderer, PDFRenderer, DOCXRenderer, XLSXRenderer, RenderContext
- L28 from .base import Pipeline, PipelineContext, Step, StepResult
- L30 assign logger = logging.getLogger('neura.pipelines.report')
- L34 class ReportPipelineContext(PipelineContext):
  - L35 docstring: "Context specific to report generation."
  - L38 annotated assign request: Optional[RenderRequest] = None
  - L39 annotated assign template_path: Optional[Path] = None
  - L40 annotated assign contract_path: Optional[Path] = None
  - L41 annotated assign db_path: Optional[Path] = None
  - L44 annotated assign contract: Optional[Contract] = None
  - L45 annotated assign template_html: Optional[str] = None
  - L46 annotated assign data_scalars: Dict[str, Any] = field(default_factory=dict)
  - L47 annotated assign data_rows: List[Dict[str, Any]] = field(default_factory=list)
  - L48 annotated assign data_totals: Dict[str, Any] = field(default_factory=dict)
  - L49 annotated assign filled_html: Optional[str] = None
  - L52 annotated assign outputs: List[RenderOutput] = field(default_factory=list)
  - L53 annotated assign report: Optional[Report] = None
- L59 def validate_requestctx: ReportPipelineContext:
  - L60 docstring: "Validate the render request."
  - L61 if not ctx.request:
    - L62 raise ValidationError(message='No render request provided')
  - L64 if not ctx.request.template_id:
    - L65 raise ValidationError(message='template_id is required')
  - L67 if not ctx.request.connection_id:
    - L68 raise ValidationError(message='connection_id is required')
  - L70 expr logger.info('report_request_validated', extra={'template_id': ctx.request.template_id, 'connection_id': ctx.request.connection_id, 'correlation_id': ctx.correlation_id})
- L80 def load_contractctx: ReportPipelineContext:
  - L81 docstring: "Load and parse the contract file."
  - L82 if not ctx.contract_path or not ctx.contract_path.exists():
    - L83 raise NotFoundError(message='Contract file not found')
  - L85 assign contract_data = json.loads(ctx.contract_path.read_text(encoding='utf-8'))
  - L86 assign contract = Contract.from_dict(contract_data, ctx.request.template_id)
  - L89 assign issues = contract.validate()
  - L90 if issues:
    - L91 expr logger.warning('contract_validation_warnings', extra={'issues': issues, 'correlation_id': ctx.correlation_id})
  - L96 assign ctx.contract = contract
  - L97 expr logger.info('contract_loaded', extra={'template_id': ctx.request.template_id, 'tokens': len(contract.tokens.all_tokens()), 'correlation_id': ctx.correlation_id})
  - L105 return contract
- L108 def load_templatectx: ReportPipelineContext:
  - L109 docstring: "Load the template HTML."
  - L110 if not ctx.template_path or not ctx.template_path.exists():
    - L111 raise NotFoundError(message='Template HTML file not found')
  - L113 assign ctx.template_html = ctx.template_path.read_text(encoding='utf-8')
  - L114 expr logger.info('template_loaded', extra={'path': str(ctx.template_path), 'size_bytes': len(ctx.template_html), 'correlation_id': ctx.correlation_id})
  - L122 return ctx.template_html
- L125 def load_datactx: ReportPipelineContext:
  - L126 docstring: "Load data from the database using the contract."
  - L127 if not ctx.db_path or not ctx.db_path.exists():
    - L128 raise NotFoundError(message='Database file not found')
  - L130 if not ctx.contract:
    - L131 raise ValidationError(message='Contract not loaded')
  - L133 assign datasource = SQLiteDataSource(ctx.db_path)
  - L135 try:
    - L137 assign ctx.data_scalars = _load_scalars(datasource, ctx.contract, ctx.request)
    - L140 assign ctx.data_rows = _load_rows(datasource, ctx.contract, ctx.request)
    - L143 assign ctx.data_totals = _calculate_totals(ctx.data_rows, ctx.contract)
    - L145 expr logger.info('data_loaded', extra={'scalars': len(ctx.data_scalars), 'rows': len(ctx.data_rows), 'totals': len(ctx.data_totals), 'correlation_id': ctx.correlation_id})
    - L155 return {'scalars': ctx.data_scalars, 'rows': ctx.data_rows, 'totals': ctx.data_totals}
    - L161 finally:
      - L161 expr datasource.close()
- L164 def _load_scalarsdatasource: DataSource, contract: Contract, request: RenderRequest:
  - L169 docstring: "Load scalar values from database."
  - L170 assign scalars = {}
  - L172 for token in contract.tokens.scalars:
    - L173 assign expr = contract.get_mapping(token)
    - L174 if not expr:
      - L175 continue
    - L178 try:
      - L179 assign result = datasource.execute_query(f'SELECT {expr} AS value')
      - L180 if result.rows:
        - L181 assign scalars[token] = result.rows[0][0]
      - L182 except Exception as e:
        - L183 expr logger.warning('scalar_load_failed', extra={'token': token, 'error': str(e)})
        - L187 assign scalars[token] = None
  - L190 if request.start_date:
    - L191 assign scalars['START_DATE'] = request.start_date
  - L192 if request.end_date:
    - L193 assign scalars['END_DATE'] = request.end_date
  - L195 return scalars
- L198 def _load_rowsdatasource: DataSource, contract: Contract, request: RenderRequest:
  - L203 docstring: "Load row data from database."
  - L204 if not contract.tokens.row_tokens:
    - L205 return []
  - L208 assign select_parts = []
  - L209 for token in contract.tokens.row_tokens:
    - L210 assign expr = contract.get_mapping(token)
    - L211 if expr:
      - L212 expr select_parts.append(f'{expr} AS {token}')
  - L214 if not select_parts:
    - L215 return []
  - L218 assign query = f"SELECT {', '.join(select_parts)}"
  - L221 for expr in contract.mappings.values():
    - L222 if '.' in expr:
      - L223 assign table = expr.split('.')[0].strip()
      - L224 aug assign query Add f' FROM [{table}]'
      - L225 break
  - L228 assign conditions = []
  - L229 if request.start_date:
    - L230 expr conditions.append(f"date >= '{request.start_date}'")
  - L231 if request.end_date:
    - L232 expr conditions.append(f"date <= '{request.end_date}'")
  - L234 if conditions:
    - L235 aug assign query Add ' WHERE ' + ' AND '.join(conditions)
  - L238 if contract.row_order:
    - L239 aug assign query Add f" ORDER BY {', '.join(contract.row_order)}"
  - L241 try:
    - L242 assign result = datasource.execute_query(query)
    - L243 return result.to_dicts()
    - L244 except Exception as e:
      - L245 expr logger.warning('row_load_failed', extra={'error': str(e)})
      - L246 return []
- L249 def _calculate_totalsrows: List[Dict[str, Any]], contract: Contract:
  - L253 docstring: "Calculate totals from row data."
  - L254 assign totals = {}
  - L256 for token in contract.tokens.totals:
    - L257 assign expr = contract.totals_math.get(token)
    - L258 if not expr:
      - L260 assign row_token = token.replace('total_', 'row_')
      - L261 if row_token in contract.tokens.row_tokens:
        - L262 try:
          - L263 assign totals[token] = sum((float(row.get(row_token, 0) or 0) for row in rows))
          - L266 except (ValueError, TypeError):
            - L267 assign totals[token] = 0
      - L270 else:
        - L270 assign totals[token] = _eval_total_expr(expr, rows, totals)
  - L272 return totals
- L275 def _eval_total_exprexpr: str, rows: List[Dict[str, Any]], totals: Dict[str, Any]:
  - L280 docstring: "Evaluate a totals expression safely."
  - L282 if expr.upper().startswith('SUM('):
    - L283 assign col = expr[4:-1].strip()
    - L284 try:
      - L285 return sum((float(row.get(col, 0) or 0) for row in rows))
      - L286 except (ValueError, TypeError):
        - L287 return 0
  - L288 return expr
- L291 def render_htmlctx: ReportPipelineContext:
  - L292 docstring: "Render filled HTML from template and data."
  - L293 if not ctx.template_html:
    - L294 raise ValidationError(message='Template HTML not loaded')
  - L296 assign renderer = HTMLRenderer()
  - L297 assign output_dir = ctx.template_path.parent
  - L298 assign output_path = output_dir / f'filled_{int(datetime.now().timestamp())}.html'
  - L301 assign all_data = {**ctx.data_scalars, **{f'row_{i}_{k}': v for i, row in enumerate(ctx.data_rows) for k, v in row.items()}, **ctx.data_totals, 'ROW_COUNT': len(ctx.data_rows), 'GENERATED_AT': datetime.now(timezone.utc).isoformat()}
  - L309 assign render_ctx = RenderContext(template_html=ctx.template_html, data=all_data, output_format=OutputFormat.HTML, output_path=output_path)
  - L316 assign result = renderer.render(render_ctx)
  - L317 if not result.success:
    - L318 raise Exception(f'HTML rendering failed: {result.error}')
  - L320 assign ctx.filled_html = output_path.read_text(encoding='utf-8')
  - L321 expr ctx.outputs.append(RenderOutput(format=OutputFormat.HTML, path=output_path, size_bytes=result.size_bytes))
  - L329 expr logger.info('html_rendered', extra={'path': str(output_path), 'size_bytes': result.size_bytes, 'correlation_id': ctx.correlation_id})
  - L338 return output_path
- L341 def render_pdfctx: ReportPipelineContext:
  - L342 docstring: "Render PDF from HTML."
  - L343 if not ctx.filled_html:
    - L344 raise ValidationError(message='HTML not rendered')
  - L346 if OutputFormat.PDF not in ctx.request.output_formats:
    - L347 return None
  - L349 assign renderer = PDFRenderer()
  - L350 assign output_dir = ctx.template_path.parent
  - L351 assign output_path = output_dir / f'filled_{int(datetime.now().timestamp())}.pdf'
  - L353 assign render_ctx = RenderContext(template_html=ctx.filled_html, data={}, output_format=OutputFormat.PDF, output_path=output_path)
  - L360 assign result = renderer.render(render_ctx)
  - L361 if not result.success:
    - L362 expr logger.error('pdf_render_failed', extra={'error': result.error})
    - L363 return None
  - L365 expr ctx.outputs.append(RenderOutput(format=OutputFormat.PDF, path=output_path, size_bytes=result.size_bytes))
  - L373 expr logger.info('pdf_rendered', extra={'path': str(output_path), 'size_bytes': result.size_bytes, 'correlation_id': ctx.correlation_id})
  - L382 return output_path
- L385 def render_docxctx: ReportPipelineContext:
  - L386 docstring: "Render DOCX from HTML."
  - L387 if OutputFormat.DOCX not in ctx.request.output_formats:
    - L388 return None
  - L390 if not ctx.filled_html:
    - L391 return None
  - L393 assign renderer = DOCXRenderer()
  - L394 assign output_dir = ctx.template_path.parent
  - L395 assign output_path = output_dir / f'filled_{int(datetime.now().timestamp())}.docx'
  - L397 assign render_ctx = RenderContext(template_html=ctx.filled_html, data={}, output_format=OutputFormat.DOCX, output_path=output_path)
  - L404 assign result = renderer.render(render_ctx)
  - L405 if not result.success:
    - L406 expr logger.warning('docx_render_failed', extra={'error': result.error})
    - L407 return None
  - L409 expr ctx.outputs.append(RenderOutput(format=OutputFormat.DOCX, path=output_path, size_bytes=result.size_bytes))
  - L417 return output_path
- L420 def render_xlsxctx: ReportPipelineContext:
  - L421 docstring: "Render XLSX from HTML tables."
  - L422 if OutputFormat.XLSX not in ctx.request.output_formats:
    - L423 return None
  - L425 if not ctx.filled_html:
    - L426 return None
  - L428 assign renderer = XLSXRenderer()
  - L429 assign output_dir = ctx.template_path.parent
  - L430 assign output_path = output_dir / f'filled_{int(datetime.now().timestamp())}.xlsx'
  - L432 assign render_ctx = RenderContext(template_html=ctx.filled_html, data={'scalars': ctx.data_scalars, 'rows': ctx.data_rows}, output_format=OutputFormat.XLSX, output_path=output_path)
  - L439 assign result = renderer.render(render_ctx)
  - L440 if not result.success:
    - L441 expr logger.warning('xlsx_render_failed', extra={'error': result.error})
    - L442 return None
  - L444 expr ctx.outputs.append(RenderOutput(format=OutputFormat.XLSX, path=output_path, size_bytes=result.size_bytes))
  - L452 return output_path
- L455 def finalize_reportctx: ReportPipelineContext:
  - L456 docstring: "Create the final report record."
  - L457 assign report = Report(report_id=str(uuid.uuid4()), template_id=ctx.request.template_id, template_name=ctx.request.template_id, connection_id=ctx.request.connection_id, connection_name=None, status='succeeded', outputs=ctx.outputs, start_date=ctx.request.start_date, end_date=ctx.request.end_date, correlation_id=ctx.correlation_id, started_at=ctx.started_at, completed_at=datetime.now(timezone.utc))
  - L472 assign ctx.report = report
  - L473 return report
- L479 class ReportPipeline:
  - L480 docstring: "Report generation pipeline wrapper."
  - L482 def __init__self:
    - L483 assign self._pipeline = create_report_pipeline()
  - L485 def executeself, request: RenderRequest, template_path: Path, contract_path: Path, db_path: Path, *, correlation_id: Optional[str]=None:
    - L494 docstring: "Execute the report pipeline."
    - L495 assign ctx = ReportPipelineContext(correlation_id=correlation_id or str(uuid.uuid4()), request=request, template_path=template_path, contract_path=contract_path, db_path=db_path)
    - L503 assign result = self._pipeline.execute_sync(ctx)
    - L505 if not result.success:
      - L506 raise Exception(f'Report pipeline failed: {result.error}')
    - L508 return ctx.report
- L511 def create_report_pipeline:
  - L512 docstring: "Create the report generation pipeline."
  - L513 return Pipeline(name='report_generation', steps=[Step(name='validate', fn=validate_request, label='Validate request'), Step(name='load_contract', fn=load_contract, label='Load contract'), Step(name='load_template', fn=load_template, label='Load template'), Step(name='load_data', fn=load_data, label='Load data', retries=1), Step(name='render_html', fn=render_html, label='Render HTML'), Step(name='render_pdf', fn=render_pdf, label='Render PDF', timeout_seconds=120.0, guard=lambda ctx: OutputFormat.PDF in ctx.request.output_formats), Step(name='render_docx', fn=render_docx, label='Render DOCX', guard=lambda ctx: OutputFormat.DOCX in ctx.request.output_formats), Step(name='render_xlsx', fn=render_xlsx, label='Render XLSX', guard=lambda ctx: OutputFormat.XLSX in ctx.request.output_formats), Step(name='finalize', fn=finalize_report, label='Finalize report')])

## backend\scripts\raster_html.py
- L1 import argparse
- L2 import pathlib
- L4 from backend.app.services.render.html_raster import rasterize_html_to_png, save_png
- L7 def main:
  - L8 assign parser = argparse.ArgumentParser(description='Rasterize HTML to a 400-DPI A4 PNG.')
  - L9 expr parser.add_argument('--in', dest='inp', required=True, help='Path to HTML file')
  - L10 expr parser.add_argument('--out', dest='out', required=True, help='Output PNG path')
  - L11 expr parser.add_argument('--method', default='pdf', choices=['pdf', 'screenshot'])
  - L12 expr parser.add_argument('--dpi', type=int, default=400)
  - L13 expr parser.add_argument('--selector', default='.page', help='Used only for screenshot method')
  - L14 assign args = parser.parse_args()
  - L16 assign html = pathlib.Path(args.inp).read_text(encoding='utf-8')
  - L17 assign png = rasterize_html_to_png(html, dpi=args.dpi, method=args.method, selector=args.selector)
  - L18 expr save_png(png, args.out)
  - L19 expr print(f'OK -> {args.out}')
- L22 if __name__ == '__main__':
  - L23 expr main()

## backend\tests\__init__.py
- L1 docstring: "Test package for backend."

## backend\tests\conftest.py
- L1 import warnings
- L4 expr warnings.filterwarnings('ignore', category=DeprecationWarning)

## backend\tests\test_adapters.py
- L1 docstring: "Tests for backend adapters."
- L2 from __future__ import annotations
- L4 import os
- L5 import sys
- L6 import tempfile
- L7 import types
- L8 from pathlib import Path
- L9 from unittest.mock import MagicMock, patch, AsyncMock
- L11 import pytest
- L14 assign fernet_module = types.ModuleType('cryptography.fernet')
- L17 class _DummyFernet:
  - L18 def __init__self, key:
    - L19 assign self.key = key
  - L22 def generate_key:
    - L23 return b'A' * 44
  - L25 def encryptself, payload: bytes:
    - L26 return payload
  - L28 def decryptself, token: bytes:
    - L29 return token
- L32 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L33 expr setattr(fernet_module, 'InvalidToken', Exception)
- L34 assign crypto_module = types.ModuleType('cryptography')
- L35 expr setattr(crypto_module, 'fernet', fernet_module)
- L36 expr sys.modules.setdefault('cryptography', crypto_module)
- L37 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L45 class TestSQLiteConnectionPool:
  - L46 docstring: "Tests for SQLite connection pooling (now DataFrame-based)."
  - L48 def test_pool_creationself, tmp_path:
    - L49 docstring: "Connection pool should be created with proper size."
    - L50 from backend.adapters.databases.sqlite import SQLiteConnectionPool
    - L52 assign db_path = tmp_path / 'test.db'
    - L54 import sqlite3
    - L55 assign conn = sqlite3.connect(db_path)
    - L56 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY)')
    - L57 expr conn.close()
    - L59 assign pool = SQLiteConnectionPool(db_path, readonly=True, pool_size=3)
    - L61 assign status = pool.status()
    - L62 assert status['pool_size'] == 3
    - L63 assert status['tables_loaded'] == 1
    - L64 assert not status['closed']
    - L66 expr pool.close()
  - L68 def test_pool_acquire_and_releaseself, tmp_path:
    - L69 docstring: "Connections should be acquired and released properly."
    - L70 from backend.adapters.databases.sqlite import SQLiteConnectionPool
    - L71 from backend.app.services.dataframes import sqlite_shim
    - L73 assign db_path = tmp_path / 'test.db'
    - L74 import sqlite3
    - L75 assign conn = sqlite3.connect(db_path)
    - L76 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY, value TEXT)')
    - L77 expr conn.execute("INSERT INTO test VALUES (1, 'hello')")
    - L78 expr conn.commit()
    - L79 expr conn.close()
    - L81 assign pool = SQLiteConnectionPool(db_path, readonly=True, pool_size=2)
    - L83 with pool.acquire() as conn:
      - L84 assign conn.row_factory = sqlite_shim.Row
      - L85 assign cursor = conn.execute('SELECT id, value FROM test')
      - L86 assign rows = cursor.fetchall()
      - L87 assert len(rows) == 1
      - L88 assert rows[0]['value'] == 'hello'
    - L91 assign status = pool.status()
    - L92 assert status['active_connections'] == 0
    - L94 expr pool.close()
  - L96 def test_pool_concurrent_acquireself, tmp_path:
    - L97 docstring: "Pool should handle concurrent connections (DataFrame-based)."
    - L98 from backend.adapters.databases.sqlite import SQLiteConnectionPool
    - L100 assign db_path = tmp_path / 'test.db'
    - L101 import sqlite3
    - L102 assign conn = sqlite3.connect(db_path)
    - L103 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY)')
    - L104 expr conn.close()
    - L106 assign pool = SQLiteConnectionPool(db_path, readonly=True, pool_size=1, max_overflow=2)
    - L109 with pool.acquire():
      - L110 with pool.acquire():
        - L111 assign status = pool.status()
        - L113 assert status['active_connections'] == 2
    - L115 expr pool.close()
- L118 class TestSQLiteDataSource:
  - L119 docstring: "Tests for SQLite data source."
  - L121 def test_test_connection_successself, tmp_path:
    - L122 docstring: "test_connection should return success for valid database."
    - L123 from backend.adapters.databases.sqlite import SQLiteDataSource
    - L125 assign db_path = tmp_path / 'test.db'
    - L126 import sqlite3
    - L127 assign conn = sqlite3.connect(db_path)
    - L128 expr conn.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')
    - L129 expr conn.execute('CREATE TABLE orders (id INTEGER PRIMARY KEY)')
    - L130 expr conn.close()
    - L132 assign ds = SQLiteDataSource(db_path, readonly=True)
    - L133 assign result = ds.test_connection()
    - L135 assert result.success is True
    - L136 assert result.table_count == 2
    - L137 assert result.error is None
    - L138 assert result.latency_ms > 0
    - L140 expr ds.close()
  - L142 def test_test_connection_empty_databaseself, tmp_path:
    - L143 docstring: "test_connection should succeed for empty database with 0 tables."
    - L144 from backend.adapters.databases.sqlite import SQLiteDataSource
    - L146 assign db_path = tmp_path / 'empty.db'
    - L148 import sqlite3
    - L149 assign conn = sqlite3.connect(db_path)
    - L150 expr conn.close()
    - L152 assign ds = SQLiteDataSource(db_path, readonly=True)
    - L153 assign result = ds.test_connection()
    - L156 assert result.success is True
    - L157 assert result.table_count == 0
    - L158 assert result.error is None
    - L160 expr ds.close()
  - L162 def test_execute_queryself, tmp_path:
    - L163 docstring: "execute_query should return proper results."
    - L164 from backend.adapters.databases.sqlite import SQLiteDataSource
    - L166 assign db_path = tmp_path / 'test.db'
    - L167 import sqlite3
    - L168 assign conn = sqlite3.connect(db_path)
    - L169 expr conn.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')
    - L170 expr conn.execute("INSERT INTO users VALUES (1, 'Alice')")
    - L171 expr conn.execute("INSERT INTO users VALUES (2, 'Bob')")
    - L172 expr conn.commit()
    - L173 expr conn.close()
    - L175 assign ds = SQLiteDataSource(db_path, readonly=True)
    - L176 assign result = ds.execute_query('SELECT id, name FROM users ORDER BY id')
    - L178 assert result.row_count == 2
    - L180 assert 'id' in result.columns
    - L181 assert 'name' in result.columns
    - L183 assign id_idx = result.columns.index('id')
    - L184 assign name_idx = result.columns.index('name')
    - L185 assert result.rows[0][id_idx] == 1
    - L186 assert result.rows[0][name_idx] == 'Alice'
    - L187 assert result.rows[1][id_idx] == 2
    - L188 assert result.rows[1][name_idx] == 'Bob'
    - L189 assert result.execution_time_ms > 0
    - L191 expr ds.close()
  - L193 def test_with_connection_poolself, tmp_path:
    - L194 docstring: "DataSource should work with connection pooling enabled."
    - L195 from backend.adapters.databases.sqlite import SQLiteDataSource
    - L197 assign db_path = tmp_path / 'test.db'
    - L198 import sqlite3
    - L199 assign conn = sqlite3.connect(db_path)
    - L200 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY)')
    - L201 expr conn.close()
    - L203 assign ds = SQLiteDataSource(db_path, readonly=True, use_pool=True, pool_size=2)
    - L205 assign status = ds.pool_status()
    - L206 assert status is not None
    - L207 assert status['pool_size'] == 2
    - L209 assign result = ds.execute_query('SELECT 1 as value')
    - L210 assert result.rows[0][0] == 1
    - L212 expr ds.close()
- L215 class TestSQLiteSchemaDiscovery:
  - L216 docstring: "Tests for SQLite schema discovery."
  - L218 def test_discover_tables_batchself, tmp_path:
    - L219 docstring: "Schema discovery should batch queries efficiently."
    - L220 from backend.adapters.databases.sqlite import SQLiteDataSource
    - L222 assign db_path = tmp_path / 'test.db'
    - L223 import sqlite3
    - L224 assign conn = sqlite3.connect(db_path)
    - L226 for i in range(5):
      - L227 expr conn.execute(f'CREATE TABLE table_{i} (id INTEGER PRIMARY KEY, value TEXT)')
      - L228 expr conn.execute(f"INSERT INTO table_{i} VALUES (1, 'test')")
    - L229 expr conn.commit()
    - L230 expr conn.close()
    - L232 assign ds = SQLiteDataSource(db_path, readonly=True)
    - L233 assign schema = ds.discover_schema()
    - L236 assert len(schema.tables) == 5
    - L237 for table in schema.tables:
      - L238 assert table.row_count == 1
      - L239 assert 'id' in table.columns
      - L240 assert 'value' in table.columns
    - L242 expr ds.close()
- L250 class TestOpenAIClient:
  - L251 docstring: "Tests for OpenAI LLM client."
  - L253 def test_api_key_validation_missingself, monkeypatch:
    - L254 docstring: "Client should raise error when API key is missing."
    - L255 expr monkeypatch.delenv('OPENAI_API_KEY', raising=False)
    - L257 from backend.adapters.llm.openai import OpenAIClient
    - L259 assign client = OpenAIClient(api_key=None)
    - L261 with pytest.raises(ValueError, match='API key is required'):
      - L262 expr client._get_client()
  - L264 def test_api_key_validation_warningself, monkeypatch, caplog:
    - L265 docstring: "Client should warn on invalid API key format."
    - L266 expr monkeypatch.delenv('OPENAI_API_KEY', raising=False)
    - L268 from backend.adapters.llm.openai import OpenAIClient
    - L269 import logging
    - L271 with patch('openai.OpenAI'):
      - L272 expr caplog.set_level(logging.WARNING)
      - L273 assign client = OpenAIClient(api_key='invalid-key-format')
      - L274 expr client._get_client()
      - L277 assert any(('may be invalid' in r.message for r in caplog.records))
  - L279 def test_force_gpt5_disabled_by_defaultself, monkeypatch:
    - L280 docstring: "GPT-5 forcing should be disabled by default."
    - L281 expr monkeypatch.delenv('NEURA_FORCE_GPT5', raising=False)
    - L284 import importlib
    - L285 from backend.adapters.llm import openai
    - L286 expr importlib.reload(openai)
    - L289 assert openai._FORCE_GPT5 is False
  - L291 def test_force_gpt5_can_be_enabledself, monkeypatch:
    - L292 docstring: "GPT-5 forcing can be enabled via env var."
    - L293 expr monkeypatch.setenv('NEURA_FORCE_GPT5', 'true')
    - L295 import importlib
    - L296 from backend.adapters.llm import openai
    - L297 expr importlib.reload(openai)
    - L299 assert openai._FORCE_GPT5 is True
  - L301 def test_model_not_overridden_when_disabledself, monkeypatch:
    - L302 docstring: "Model should not be overridden when forcing is disabled."
    - L303 expr monkeypatch.setenv('NEURA_FORCE_GPT5', 'false')
    - L305 import importlib
    - L306 from backend.adapters.llm import openai
    - L307 expr importlib.reload(openai)
    - L309 assign result = openai._force_gpt5('gpt-4')
    - L310 assert result == 'gpt-4'
  - L312 def test_complete_prepares_messagesself, monkeypatch:
    - L313 docstring: "complete should prepare messages correctly."
    - L314 expr monkeypatch.setenv('OPENAI_API_KEY', 'sk-test-key')
    - L315 expr monkeypatch.setenv('NEURA_FORCE_GPT5', 'false')
    - L317 from backend.adapters.llm.openai import OpenAIClient
    - L318 from backend.adapters.llm.base import LLMMessage, LLMRole
    - L320 assign mock_response = MagicMock()
    - L321 assign mock_response.choices = [MagicMock()]
    - L322 assign mock_response.choices[0].message.content = 'Test response'
    - L323 assign mock_response.choices[0].finish_reason = 'stop'
    - L324 assign mock_response.model = 'gpt-4'
    - L325 assign mock_response.usage = MagicMock()
    - L326 assign mock_response.usage.prompt_tokens = 10
    - L327 assign mock_response.usage.completion_tokens = 5
    - L328 assign mock_response.usage.total_tokens = 15
    - L330 assign mock_client = MagicMock()
    - L331 assign mock_client.chat.completions.create.return_value = mock_response
    - L333 with patch('openai.OpenAI', return_value=mock_client):
      - L334 assign client = OpenAIClient(api_key='sk-test-key', default_model='gpt-4')
      - L335 assign messages = [LLMMessage(role=LLMRole.SYSTEM, content='You are helpful'), LLMMessage(role=LLMRole.USER, content='Hello')]
      - L339 assign result = client.complete(messages, model='gpt-4')
      - L341 assert result.content == 'Test response'
      - L342 assert result.model == 'gpt-4'
      - L343 assert result.usage['total_tokens'] == 15
- L351 class TestRateLimiter:
  - L352 docstring: "Tests for the rate limiter."
  - L354 def test_allows_requests_under_limitself:
    - L355 docstring: "Should allow requests under the rate limit."
    - L356 from backend.api import RateLimiter
    - L358 assign limiter = RateLimiter(requests_per_minute=10, requests_per_second=5)
    - L360 assign mock_request = MagicMock()
    - L361 assign mock_request.headers = {}
    - L362 assign mock_request.client.host = '127.0.0.1'
    - L365 assign (allowed, info) = limiter.is_allowed(mock_request)
    - L366 assert allowed is True
    - L367 assert info['remaining'] == 9
  - L369 def test_blocks_requests_over_limitself:
    - L370 docstring: "Should block requests over the rate limit."
    - L371 from backend.api import RateLimiter
    - L373 assign limiter = RateLimiter(requests_per_minute=3, requests_per_second=10)
    - L375 assign mock_request = MagicMock()
    - L376 assign mock_request.headers = {}
    - L377 assign mock_request.client.host = '127.0.0.1'
    - L380 for _ in range(3):
      - L381 assign (allowed, _) = limiter.is_allowed(mock_request)
      - L382 assert allowed is True
    - L385 assign (allowed, info) = limiter.is_allowed(mock_request)
    - L386 assert allowed is False
    - L387 assert info['remaining'] == 0
  - L389 def test_respects_forwarded_headerself:
    - L390 docstring: "Should use X-Forwarded-For header for client identification."
    - L391 from backend.api import RateLimiter
    - L393 assign limiter = RateLimiter(requests_per_minute=2, requests_per_second=10)
    - L396 assign mock_request1 = MagicMock()
    - L397 assign mock_request1.headers = {'x-forwarded-for': '10.0.0.1'}
    - L398 assign mock_request1.client.host = '127.0.0.1'
    - L400 assign mock_request2 = MagicMock()
    - L401 assign mock_request2.headers = {'x-forwarded-for': '10.0.0.2'}
    - L402 assign mock_request2.client.host = '127.0.0.1'
    - L405 for _ in range(2):
      - L406 assign (allowed, _) = limiter.is_allowed(mock_request1)
      - L407 assert allowed is True
    - L409 for _ in range(2):
      - L410 assign (allowed, _) = limiter.is_allowed(mock_request2)
      - L411 assert allowed is True
    - L414 assign (allowed1, _) = limiter.is_allowed(mock_request1)
    - L415 assign (allowed2, _) = limiter.is_allowed(mock_request2)
    - L416 assert allowed1 is False
    - L417 assert allowed2 is False
- L420 if __name__ == '__main__':
  - L421 expr pytest.main([__file__, '-v'])

## backend\tests\test_ai_integrations.py
- L2 docstring: "\nComprehensive Test Suite for Open Source AI Integrations.\n\nTests:\n1. Multi-..."
- L14 import json
- L15 import os
- L16 import sys
- L17 import tempfile
- L18 from pathlib import Path
- L19 from typing import Any, Dict, List
- L22 expr sys.path.insert(0, str(Path(__file__).parent.parent.parent))
- L25 def print_headertitle: str:
  - L26 docstring: "Print a formatted header."
  - L27 expr print('\n' + '=' * 60)
  - L28 expr print(f'  {title}')
  - L29 expr print('=' * 60)
- L32 def print_resulttest_name: str, success: bool, details: str='', skip: bool=False:
  - L33 docstring: "Print test result."
  - L34 if skip:
    - L35 assign status = '[SKIP]'
    - L37 else:
      - L37 assign status = '[PASS]' if success else '[FAIL]'
  - L38 expr print(f'{status} {test_name}')
  - L39 if details:
    - L40 for line in details.split('\n'):
      - L41 expr print(f'       {line}')
- L44 def test_llm_config:
  - L45 docstring: "Test LLM configuration detection."
  - L46 expr print_header('1. LLM Configuration')
  - L48 try:
    - L49 from backend.app.services.llm import get_llm_config, LLMProvider
    - L51 assign config = get_llm_config()
    - L52 expr print_result('Configuration loaded', True, f'Provider: {config.provider.value}\nModel: {config.model}\nTimeout: {config.timeout_seconds}s\nMax retries: {config.max_retries}')
    - L60 return True
    - L61 except Exception as e:
      - L62 expr print_result('Configuration loaded', False, str(e))
      - L63 return False
- L66 def test_llm_providers:
  - L67 docstring: "Test LLM provider availability."
  - L68 expr print_header('2. LLM Providers')
  - L70 assign results = {}
  - L73 try:
    - L74 from backend.app.services.llm.providers import OpenAIProvider
    - L75 from backend.app.services.llm.config import LLMConfig, LLMProvider as LP
    - L77 assign config = LLMConfig(provider=LP.OPENAI, model='gpt-4o', api_key=os.getenv('OPENAI_API_KEY', 'test-key'))
    - L82 assign provider = OpenAIProvider(config)
    - L83 assign has_key = bool(os.getenv('OPENAI_API_KEY'))
    - L84 expr print_result('OpenAI Provider (Primary)', has_key, 'API key configured' if has_key else 'No API key (OPENAI_API_KEY)')
    - L85 assign results['openai'] = has_key
    - L86 except Exception as e:
      - L87 expr print_result('OpenAI Provider (Primary)', False, str(e))
      - L88 assign results['openai'] = False
  - L91 try:
    - L92 from backend.app.services.llm.providers import OllamaProvider
    - L93 from backend.app.services.llm.config import LLMConfig, LLMProvider as LP
    - L95 assign config = LLMConfig(provider=LP.OLLAMA, model='llama3.2', base_url='http://localhost:11434')
    - L100 assign provider = OllamaProvider(config)
    - L101 assign available = provider.health_check()
    - L103 expr print_result('Ollama Provider (Optional)', available, 'Local models available' if available else 'Not running - using OpenAI instead', skip=not available)
    - L106 assign results['ollama'] = True if available else None
    - L108 if available:
      - L109 assign models = provider.list_models()
      - L110 expr print(f"       Available models: {models[:5]}{('...' if len(models) > 5 else '')}")
    - L111 except Exception as e:
      - L112 expr print_result('Ollama Provider (Optional)', False, str(e), skip=True)
      - L113 assign results['ollama'] = None
  - L116 try:
    - L117 from backend.app.services.llm.providers import DeepSeekProvider
    - L118 from backend.app.services.llm.config import LLMConfig, LLMProvider as LP
    - L120 assign api_key = os.getenv('DEEPSEEK_API_KEY')
    - L121 if api_key:
      - L122 assign config = LLMConfig(provider=LP.DEEPSEEK, model='deepseek-chat', api_key=api_key)
      - L127 assign provider = DeepSeekProvider(config)
      - L128 expr print_result('DeepSeek Provider (Optional)', True, 'API key configured')
      - L129 assign results['deepseek'] = True
      - L132 else:
        - L132 expr print_result('DeepSeek Provider (Optional)', False, 'No API key - using OpenAI instead', skip=True)
        - L134 assign results['deepseek'] = None
    - L135 except Exception as e:
      - L136 expr print_result('DeepSeek Provider (Optional)', False, str(e), skip=True)
      - L137 assign results['deepseek'] = None
  - L139 return results
- L142 def test_pdf_extractors:
  - L143 docstring: "Test PDF extraction tools."
  - L144 expr print_header('3. PDF Extraction Tools')
  - L146 from backend.app.services.extraction import get_available_extractors
  - L148 assign available = get_available_extractors()
  - L149 expr print(f'Available extractors: {available}')
  - L151 assign results = {}
  - L154 assign extractors_to_test = ['pymupdf', 'pdfplumber', 'tabula', 'camelot']
  - L156 for name in extractors_to_test:
    - L157 assign is_available = name in available
    - L158 expr print_result(f'{name.capitalize()} Extractor', is_available)
    - L159 assign results[name] = is_available
  - L161 return results
- L164 def test_excel_extractor:
  - L165 docstring: "Test Excel extraction."
  - L166 expr print_header('4. Excel Extraction')
  - L168 try:
    - L169 from backend.app.services.extraction import ExcelExtractor
    - L171 assign extractor = ExcelExtractor()
    - L174 with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
      - L175 expr f.write('Name,Age,City\n')
      - L176 expr f.write('Alice,30,New York\n')
      - L177 expr f.write('Bob,25,San Francisco\n')
      - L178 expr f.write('Charlie,35,Chicago\n')
      - L179 assign csv_path = f.name
    - L182 assign result = extractor.extract(csv_path)
    - L184 assign success = len(result.sheets) > 0 and result.sheets[0].row_count == 3
    - L185 expr print_result('CSV Extraction', success, f'Sheets: {len(result.sheets)}, Rows: {(result.sheets[0].row_count if result.sheets else 0)}')
    - L192 expr os.unlink(csv_path)
    - L194 return success
    - L195 except Exception as e:
      - L196 expr print_result('CSV Extraction', False, str(e))
      - L197 return False
- L200 def test_vision_language_model:
  - L201 docstring: "Test VLM integration."
  - L202 expr print_header('5. Vision-Language Models')
  - L204 try:
    - L205 from backend.app.services.llm import get_vlm, get_llm_config
    - L207 assign config = get_llm_config()
    - L208 assign vlm = get_vlm()
    - L210 expr print_result('VLM Initialization', True, f'Using model: {vlm.model}\nVision support: {config.supports_vision}')
    - L218 from backend.app.services.llm.config import VISION_MODELS
    - L219 for (provider, models) in VISION_MODELS.items():
      - L220 if models:
        - L221 expr print(f'       {provider.value}: {models[:3]}...')
    - L223 return True
    - L224 except Exception as e:
      - L225 expr print_result('VLM Initialization', False, str(e))
      - L226 return False
- L229 def test_multi_agent_system:
  - L230 docstring: "Test multi-agent orchestration."
  - L231 expr print_header('6. Multi-Agent System')
  - L233 try:
    - L234 from backend.app.services.llm import Agent, AgentConfig, AgentRole, Task, create_document_analyzer_agent, create_document_processing_crew
    - L244 assign agent = create_document_analyzer_agent()
    - L245 expr print_result('Agent Creation', True, f'Role: {agent.role}\nGoal: {agent.config.goal[:50]}...')
    - L253 assign crew = create_document_processing_crew(verbose=False)
    - L254 expr print_result('Crew Creation', True, f'Agents: {len(crew.agents)}\nTasks: {len(crew.tasks)}')
    - L261 return True
    - L262 except Exception as e:
      - L263 expr print_result('Multi-Agent System', False, str(e))
      - L264 return False
- L267 def test_text_to_sql:
  - L268 docstring: "Test Text-to-SQL generation."
  - L269 expr print_header('7. Text-to-SQL Generation')
  - L271 try:
    - L272 from backend.app.services.llm import TextToSQL, TableSchema
    - L274 assign t2sql = TextToSQL(dialect='duckdb')
    - L277 expr t2sql.add_table_schema(TableSchema(name='orders', columns=[{'name': 'id', 'type': 'INTEGER', 'description': 'Order ID'}, {'name': 'customer_id', 'type': 'INTEGER', 'description': 'Customer ID'}, {'name': 'amount', 'type': 'DECIMAL', 'description': 'Order amount'}, {'name': 'order_date', 'type': 'DATE', 'description': 'Order date'}], primary_key='id'))
    - L288 expr t2sql.add_table_schema(TableSchema(name='customers', columns=[{'name': 'id', 'type': 'INTEGER', 'description': 'Customer ID'}, {'name': 'name', 'type': 'VARCHAR', 'description': 'Customer name'}, {'name': 'email', 'type': 'VARCHAR', 'description': 'Email address'}], primary_key='id'))
    - L298 expr print_result('Schema Registration', True, f'Tables: {list(t2sql._schemas.keys())}')
    - L305 expr print_result('SQL Generator Ready', True, f'Dialect: {t2sql.dialect}\nModel: {t2sql.model}')
    - L312 return True
    - L313 except Exception as e:
      - L314 expr print_result('Text-to-SQL', False, str(e))
      - L315 return False
- L318 def test_rag_framework:
  - L319 docstring: "Test RAG framework."
  - L320 expr print_header('8. RAG Framework')
  - L322 try:
    - L323 from backend.app.services.llm import RAGRetriever, create_retriever, BM25Index
    - L326 assign index = BM25Index()
    - L327 from backend.app.services.llm.rag import Document
    - L329 assign docs = [Document(id='1', content='Python is a programming language.'), Document(id='2', content='Machine learning uses Python extensively.'), Document(id='3', content='JavaScript is used for web development.')]
    - L334 expr index.add_documents(docs)
    - L336 assign results = index.search('Python programming', top_k=2)
    - L337 expr print_result('BM25 Search', len(results) == 2, f"Found {len(results)} documents\nTop result: {(results[0][0].id if results else 'None')}")
    - L345 assign retriever = create_retriever(use_embeddings=False)
    - L346 expr retriever.add_document('The quick brown fox jumps over the lazy dog.', doc_id='fox')
    - L347 expr retriever.add_document('Machine learning is transforming industries.', doc_id='ml')
    - L349 assign result = retriever.retrieve('machine learning', top_k=1)
    - L350 expr print_result('RAG Retriever', len(result.documents) > 0, f'Documents indexed: 2\nMethod: {result.method}')
    - L357 return True
    - L358 except Exception as e:
      - L359 expr print_result('RAG Framework', False, str(e))
      - L360 return False
- L363 def test_chart_generation:
  - L364 docstring: "Test chart generation."
  - L365 expr print_header('9. Chart Generation (QuickChart)')
  - L367 try:
    - L368 from backend.app.services.charts import QuickChartClient, create_bar_chart, create_line_chart, generate_chart_url
    - L375 assign client = QuickChartClient()
    - L378 assign config = create_bar_chart(labels=['Jan', 'Feb', 'Mar', 'Apr'], datasets=[{'label': 'Sales', 'data': [10, 20, 30, 25]}], title='Monthly Sales')
    - L383 assign url = client.get_chart_url(config)
    - L385 expr print_result('Bar Chart URL', url.startswith('https://quickchart.io'), f'URL length: {len(url)} chars')
    - L392 assign config = create_line_chart(labels=['Q1', 'Q2', 'Q3', 'Q4'], datasets=[{'label': '2023', 'data': [100, 120, 140, 160]}, {'label': '2024', 'data': [110, 130, 150, 180]}], title='Quarterly Revenue')
    - L400 assign url = client.get_chart_url(config)
    - L402 expr print_result('Line Chart URL', url.startswith('https://quickchart.io'), 'Multi-dataset support working')
    - L409 assign url = generate_chart_url('pie', labels=['A', 'B', 'C'], data=[30, 50, 20], title='Distribution')
    - L416 expr print_result('Pie Chart (Quick Function)', url.startswith('https://quickchart.io'))
    - L421 return True
    - L422 except Exception as e:
      - L423 expr print_result('Chart Generation', False, str(e))
      - L424 return False
- L427 def test_document_extractor:
  - L428 docstring: "Test enhanced document extractor."
  - L429 expr print_header('10. Enhanced Document Extractor')
  - L431 try:
    - L432 from backend.app.services.llm import EnhancedDocumentExtractor
    - L434 assign extractor = EnhancedDocumentExtractor(use_vlm=False)
    - L437 with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
      - L438 expr f.write('Product,Price,Quantity\n')
      - L439 expr f.write('Widget,10.99,100\n')
      - L440 expr f.write('Gadget,24.99,50\n')
      - L441 assign csv_path = f.name
    - L443 assign result = extractor.extract(csv_path)
    - L445 assign success = len(result.tables) > 0
    - L446 expr print_result('Document Extraction', success, f'Tables found: {len(result.tables)}\nColumns: {(result.tables[0].headers if result.tables else [])}')
    - L454 if result.tables:
      - L455 assign schemas = extractor.infer_schema(result.tables[0])
      - L456 expr print_result('Schema Inference', len(schemas) > 0, f'Fields: {[s.name for s in schemas]}\nTypes: {[s.data_type for s in schemas]}')
    - L463 expr os.unlink(csv_path)
    - L464 return success
    - L465 except Exception as e:
      - L466 expr print_result('Document Extractor', False, str(e))
      - L467 return False
- L470 def run_all_tests:
  - L471 docstring: "Run all integration tests."
  - L472 expr print('\n' + '=' * 60)
  - L473 expr print('   NEURA REPORT - AI INTEGRATION TEST SUITE')
  - L474 expr print('=' * 60)
  - L476 assign results = {'llm_config': test_llm_config(), 'llm_providers': test_llm_providers(), 'pdf_extractors': test_pdf_extractors(), 'excel_extractor': test_excel_extractor(), 'vlm': test_vision_language_model(), 'multi_agent': test_multi_agent_system(), 'text_to_sql': test_text_to_sql(), 'rag': test_rag_framework(), 'chart_generation': test_chart_generation(), 'document_extractor': test_document_extractor()}
  - L490 expr print_header('TEST SUMMARY')
  - L492 assign passed = 0
  - L493 assign failed = 0
  - L494 assign skipped = 0
  - L496 for (test_name, result) in results.items():
    - L497 if isinstance(result, dict):
      - L499 for (sub_name, sub_result) in result.items():
        - L500 if sub_result is None:
          - L501 aug assign skipped Add 1
          - L502 else:
            - L502 if sub_result:
              - L503 aug assign passed Add 1
              - L505 else:
                - L505 aug assign failed Add 1
      - L507 else:
        - L507 if result is None:
          - L508 aug assign skipped Add 1
          - L509 else:
            - L509 if result:
              - L510 aug assign passed Add 1
              - L512 else:
                - L512 aug assign failed Add 1
  - L514 expr print(f'\nTotal Tests: {passed + failed + skipped}')
  - L515 expr print(f'Passed: {passed}')
  - L516 expr print(f'Failed: {failed}')
  - L517 expr print(f'Skipped: {skipped} (optional providers)')
  - L518 expr print(f'\nSuccess Rate: {passed / (passed + failed) * 100:.1f}%' if passed + failed > 0 else '\nSuccess Rate: 100%')
  - L520 return failed == 0
- L523 if __name__ == '__main__':
  - L524 assign success = run_all_tests()
  - L525 expr sys.exit(0 if success else 1)

## backend\tests\test_api_endpoints_comprehensive.py
- L1 docstring: "Comprehensive API endpoint tests.\n\nTests all major API endpoints for correct b..."
- L6 from __future__ import annotations
- L8 import sys
- L9 import types
- L10 from pathlib import Path
- L11 from unittest.mock import MagicMock, patch, AsyncMock
- L12 import json
- L14 import pytest
- L15 from fastapi.testclient import TestClient
- L18 assign fernet_module = types.ModuleType('cryptography.fernet')
- L21 class _DummyFernet:
  - L22 def __init__self, key:
    - L23 assign self.key = key
  - L26 def generate_key:
    - L27 return b'A' * 44
  - L29 def encryptself, payload: bytes:
    - L30 return payload
  - L32 def decryptself, token: bytes:
    - L33 return token
- L36 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L37 expr setattr(fernet_module, 'InvalidToken', Exception)
- L38 assign crypto_module = types.ModuleType('cryptography')
- L39 expr setattr(crypto_module, 'fernet', fernet_module)
- L40 expr sys.modules.setdefault('cryptography', crypto_module)
- L41 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L43 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L45 from .. import api
- L46 from ..app.services.state import store as state_store_module
- L50 def fresh_statetmp_path, monkeypatch:
  - L51 docstring: "Create a fresh state store for each test."
  - L52 assign base_dir = tmp_path / 'state'
  - L53 assign store = state_store_module.StateStore(base_dir=base_dir)
  - L54 assign state_store_module.state_store = store
  - L55 assign api.state_store = store
  - L58 assign api.SCHEDULER_DISABLED = True
  - L59 assign api.SCHEDULER = None
  - L62 assign upload_root = tmp_path / 'uploads'
  - L63 expr upload_root.mkdir(parents=True, exist_ok=True)
  - L64 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', upload_root)
  - L65 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', upload_root.resolve())
  - L67 assign excel_root = tmp_path / 'excel-uploads'
  - L68 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L69 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L70 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L72 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (upload_root.resolve(), '/uploads'))
  - L73 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L75 return store
- L79 def clientfresh_state:
  - L80 docstring: "Create test client."
  - L81 return TestClient(api.app)
- L84 class TestHealthEndpoints:
  - L85 docstring: "Test health check endpoints."
  - L87 def test_health_endpoint_returns_okself, client:
    - L88 docstring: "Test /health returns OK status."
    - L89 assign resp = client.get('/health')
    - L90 assert resp.status_code == 200
    - L91 assign data = resp.json()
    - L92 assert data.get('status') == 'ok'
  - L94 def test_healthz_endpoint_returns_okself, client:
    - L95 docstring: "Test /healthz returns detailed health info."
    - L96 assign resp = client.get('/healthz')
    - L97 assert resp.status_code == 200
    - L98 assign data = resp.json()
    - L99 assert 'status' in data
  - L101 def test_readyz_endpoint_returns_statusself, client:
    - L102 docstring: "Test /readyz returns readiness status."
    - L103 assign resp = client.get('/readyz')
    - L105 assert resp.status_code in (200, 503)
- L108 class TestConnectionEndpoints:
  - L109 docstring: "Test connection management endpoints."
  - L111 def test_list_connections_emptyself, client, fresh_state:
    - L112 docstring: "Test listing connections when none exist."
    - L113 assign resp = client.get('/connections')
    - L114 assert resp.status_code == 200
    - L115 assign data = resp.json()
    - L116 assert data['status'] == 'ok'
    - L117 assert data['connections'] == []
  - L119 def test_list_connections_with_dataself, client, fresh_state:
    - L120 docstring: "Test listing connections with existing data."
    - L121 expr fresh_state.upsert_connection(conn_id='conn-1', name='Test Connection', db_type='sqlite', database_path='/path/to/db.sqlite', secret_payload={'token': 'secret'})
    - L129 assign resp = client.get('/connections')
    - L130 assert resp.status_code == 200
    - L131 assign data = resp.json()
    - L132 assert len(data['connections']) == 1
    - L133 assert data['connections'][0]['id'] == 'conn-1'
    - L134 assert data['connections'][0]['name'] == 'Test Connection'
  - L136 def test_test_connection_valid_sqliteself, client, fresh_state, tmp_path:
    - L137 docstring: "Test connection testing with valid SQLite DB."
    - L139 assign db_path = tmp_path / 'test.db'
    - L140 import sqlite3
    - L141 assign conn = sqlite3.connect(str(db_path))
    - L142 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY)')
    - L143 expr conn.close()
    - L145 assign resp = client.post('/connections/test', json={'db_url': str(db_path), 'db_type': 'sqlite'})
    - L150 assert resp.status_code == 200
    - L151 assign data = resp.json()
    - L152 assert data.get('ok') is True or data.get('status') == 'ok'
  - L154 def test_test_connection_invalid_pathself, client:
    - L155 docstring: "Test connection testing with invalid path."
    - L156 assign resp = client.post('/connections/test', json={'db_url': '/nonexistent/path/db.sqlite', 'db_type': 'sqlite'})
    - L162 assign data = resp.json()
    - L164 assert resp.status_code >= 400 or data.get('ok') is False or data.get('status') == 'error'
  - L166 def test_upsert_connection_createself, client, fresh_state, tmp_path:
    - L167 docstring: "Test creating a new connection."
    - L168 assign resp = client.post('/connections', json={'name': 'New Connection', 'db_type': 'sqlite', 'db_url': str(tmp_path / 'new.db')})
    - L174 assert resp.status_code == 200
    - L175 assign data = resp.json()
    - L176 assert data['status'] == 'ok'
    - L177 assert data['connection']['name'] == 'New Connection'
  - L179 def test_upsert_connection_updateself, client, fresh_state, tmp_path:
    - L180 docstring: "Test updating an existing connection."
    - L182 assign conn = fresh_state.upsert_connection(conn_id='conn-update', name='Original Name', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L191 assign resp = client.post('/connections', json={'id': 'conn-update', 'name': 'Updated Name', 'db_type': 'sqlite', 'db_url': str(tmp_path / 'db.sqlite')})
    - L198 assert resp.status_code == 200
    - L199 assign data = resp.json()
    - L200 assert data['connection']['name'] == 'Updated Name'
  - L202 def test_delete_connection_existingself, client, fresh_state, tmp_path:
    - L203 docstring: "Test deleting an existing connection."
    - L204 expr fresh_state.upsert_connection(conn_id='conn-delete', name='To Delete', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L212 assign resp = client.delete('/connections/conn-delete')
    - L213 assert resp.status_code == 200
    - L214 assign data = resp.json()
    - L215 assert data['status'] == 'ok'
    - L218 assign connections = fresh_state.list_connections()
    - L219 assert len(connections) == 0
  - L221 def test_delete_connection_nonexistentself, client:
    - L222 docstring: "Test deleting a nonexistent connection returns 404."
    - L223 assign resp = client.delete('/connections/nonexistent-conn')
    - L224 assert resp.status_code == 404
  - L226 def test_healthcheck_connectionself, client, fresh_state, tmp_path:
    - L227 docstring: "Test connection health check."
    - L229 assign db_path = tmp_path / 'health.db'
    - L230 import sqlite3
    - L231 assign conn = sqlite3.connect(str(db_path))
    - L232 expr conn.execute('CREATE TABLE test (id INTEGER PRIMARY KEY)')
    - L233 expr conn.close()
    - L235 expr fresh_state.upsert_connection(conn_id='conn-health', name='Health Check Connection', db_type='sqlite', database_path=str(db_path), secret_payload=None)
    - L243 assign resp = client.post('/connections/conn-health/health')
    - L245 assert resp.status_code in (200, 400, 404, 500)
- L248 class TestTemplateEndpoints:
  - L249 docstring: "Test template management endpoints."
  - L251 def test_list_templates_emptyself, client, fresh_state:
    - L252 docstring: "Test listing templates when none exist."
    - L253 assign resp = client.get('/templates')
    - L254 assert resp.status_code == 200
    - L255 assign data = resp.json()
    - L256 assert 'templates' in data
    - L257 assert data['templates'] == []
  - L259 def test_list_templates_with_dataself, client, fresh_state:
    - L260 docstring: "Test listing templates with existing data."
    - L261 expr fresh_state.upsert_template('tpl-1', name='Test Template', status='approved')
    - L267 assign resp = client.get('/templates')
    - L268 assert resp.status_code == 200
    - L269 assign data = resp.json()
    - L270 assert len(data['templates']) == 1
    - L271 assert data['templates'][0]['id'] == 'tpl-1'
  - L273 def test_list_templates_with_status_filterself, client, fresh_state:
    - L274 docstring: "Test filtering templates by status."
    - L275 expr fresh_state.upsert_template('tpl-approved', name='Approved', status='approved')
    - L276 expr fresh_state.upsert_template('tpl-pending', name='Pending', status='pending')
    - L278 assign resp = client.get('/templates', params={'status': 'approved'})
    - L279 assert resp.status_code == 200
    - L280 assign data = resp.json()
    - L282 for tpl in data['templates']:
      - L283 assert tpl['status'] == 'approved'
  - L285 def test_delete_template_existingself, client, fresh_state:
    - L286 docstring: "Test deleting an existing template."
    - L287 expr fresh_state.upsert_template('tpl-delete', name='To Delete', status='approved')
    - L289 assign resp = client.delete('/templates/tpl-delete')
    - L290 assert resp.status_code == 200
    - L293 assign templates = fresh_state.list_templates()
    - L294 assert len(templates) == 0
  - L296 def test_delete_template_nonexistentself, client:
    - L297 docstring: "Test deleting a nonexistent template."
    - L298 assign resp = client.delete('/templates/nonexistent-tpl')
    - L300 assert resp.status_code in (404, 400, 500)
  - L302 def test_bootstrap_stateself, client, fresh_state, tmp_path:
    - L303 docstring: "Test bootstrap state endpoint returns app state."
    - L305 expr fresh_state.upsert_connection(conn_id='conn-boot', name='Boot Connection', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L312 expr fresh_state.upsert_template('tpl-boot', name='Boot Template', status='approved')
    - L313 expr fresh_state.set_last_used('conn-boot', 'tpl-boot')
    - L315 assign resp = client.get('/state/bootstrap')
    - L316 assert resp.status_code == 200
    - L317 assign data = resp.json()
    - L318 assert 'connections' in data
    - L319 assert 'templates' in data
    - L320 assert 'last_used' in data
- L323 class TestJobEndpoints:
  - L324 docstring: "Test job management endpoints."
  - L326 def test_list_jobs_emptyself, client, fresh_state:
    - L327 docstring: "Test listing jobs when none exist."
    - L328 assign resp = client.get('/jobs')
    - L329 assert resp.status_code == 200
    - L330 assign data = resp.json()
    - L331 assert data['jobs'] == []
  - L333 def test_list_jobs_with_dataself, client, fresh_state:
    - L334 docstring: "Test listing jobs with existing data."
    - L335 expr fresh_state.upsert_template('tpl-job', name='Job Template', status='approved')
    - L336 expr fresh_state.create_job(job_type='run_report', template_id='tpl-job', template_name='Job Template', template_kind='pdf')
    - L343 assign resp = client.get('/jobs')
    - L344 assert resp.status_code == 200
    - L345 assign data = resp.json()
    - L346 assert len(data['jobs']) == 1
  - L348 def test_list_jobs_with_status_filterself, client, fresh_state:
    - L349 docstring: "Test filtering jobs by status."
    - L350 expr fresh_state.upsert_template('tpl-filter', name='Filter Template', status='approved')
    - L353 assign queued = fresh_state.create_job(job_type='run_report', template_id='tpl-filter', template_name='Filter Template')
    - L358 assign running = fresh_state.create_job(job_type='run_report', template_id='tpl-filter', template_name='Filter Template')
    - L363 expr fresh_state.record_job_start(running['id'])
    - L365 assign resp = client.get('/jobs', params={'status': 'running'})
    - L366 assert resp.status_code == 200
    - L367 assign data = resp.json()
    - L368 assert all((job['status'] == 'running' for job in data['jobs']))
  - L370 def test_list_jobs_active_onlyself, client, fresh_state:
    - L371 docstring: "Test listing only active jobs."
    - L372 expr fresh_state.upsert_template('tpl-active', name='Active Template', status='approved')
    - L375 assign active = fresh_state.create_job(job_type='run_report', template_id='tpl-active', template_name='Active Template')
    - L380 assign completed = fresh_state.create_job(job_type='run_report', template_id='tpl-active', template_name='Active Template')
    - L385 expr fresh_state.record_job_completion(completed['id'], status='succeeded')
    - L387 assign resp = client.get('/jobs', params={'active_only': 'true'})
    - L388 assert resp.status_code == 200
    - L389 assign data = resp.json()
    - L390 assign job_ids = [job['id'] for job in data['jobs']]
    - L391 assert active['id'] in job_ids
    - L392 assert completed['id'] not in job_ids
  - L394 def test_get_job_existingself, client, fresh_state:
    - L395 docstring: "Test getting a specific job."
    - L396 expr fresh_state.upsert_template('tpl-get', name='Get Template', status='approved')
    - L397 assign job = fresh_state.create_job(job_type='run_report', template_id='tpl-get', template_name='Get Template')
    - L403 assign resp = client.get(f"/jobs/{job['id']}")
    - L404 assert resp.status_code == 200
    - L405 assign data = resp.json()
    - L406 assert data['job']['id'] == job['id']
  - L408 def test_get_job_nonexistentself, client:
    - L409 docstring: "Test getting a nonexistent job."
    - L410 assign resp = client.get('/jobs/nonexistent-job-id')
    - L411 assert resp.status_code == 200
    - L412 assign data = resp.json()
    - L413 assert data['job'] is None
  - L415 def test_cancel_jobself, client, fresh_state, monkeypatch:
    - L416 docstring: "Test canceling a job."
    - L417 expr fresh_state.upsert_template('tpl-cancel', name='Cancel Template', status='approved')
    - L418 assign job = fresh_state.create_job(job_type='run_report', template_id='tpl-cancel', template_name='Cancel Template')
    - L425 class MockReportService:
      - L427 def force_cancel_jobjob_id, *, force=False:
        - L428 return True
    - L430 assign api.report_service = MockReportService()
    - L432 assign resp = client.post(f"/jobs/{job['id']}/cancel")
    - L433 assert resp.status_code == 200
    - L436 assign updated = fresh_state.get_job(job['id'])
    - L437 assert updated['status'] == 'cancelled'
- L440 class TestReportEndpoints:
  - L441 docstring: "Test report generation endpoints."
  - L443 def test_run_report_missing_templateself, client, fresh_state:
    - L444 docstring: "Test running report with missing template."
    - L445 assign resp = client.post('/reports/run', json={'template_id': 'nonexistent', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'})
    - L452 assert resp.status_code in (400, 404, 500)
  - L454 def test_run_report_job_creates_jobself, client, fresh_state, monkeypatch:
    - L455 docstring: "Test that run-report job endpoint creates a job."
    - L456 expr fresh_state.upsert_template('tpl-run', name='Run Template', status='approved')
    - L459 assign scheduled = []
    - L461 def mock_schedulejob_id, payload_data, kind, correlation_id, step_progress:
      - L462 expr scheduled.append(job_id)
    - L464 expr monkeypatch.setattr(api, '_schedule_report_job', mock_schedule)
    - L466 assign resp = client.post('/jobs/run-report', json={'template_id': 'tpl-run', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'})
    - L472 assert resp.status_code == 200
    - L473 assign data = resp.json()
    - L474 assert 'job_id' in data
    - L475 assert data['job_id'] in scheduled
- L478 class TestScheduleEndpoints:
  - L479 docstring: "Test schedule management endpoints."
  - L481 def test_list_schedules_emptyself, client, fresh_state:
    - L482 docstring: "Test listing schedules when none exist."
    - L483 assign resp = client.get('/reports/schedules')
    - L484 assert resp.status_code == 200
    - L485 assign data = resp.json()
    - L486 assert data['schedules'] == []
  - L488 def test_create_and_list_scheduleself, client, fresh_state, tmp_path:
    - L489 docstring: "Test creating and listing schedules."
    - L491 expr fresh_state.upsert_template('tpl-sched', name='Schedule Template', status='approved')
    - L492 expr fresh_state.upsert_connection(conn_id='conn-sched', name='Schedule Connection', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L500 assign resp = client.post('/reports/schedules', json={'template_id': 'tpl-sched', 'connection_id': 'conn-sched', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59', 'frequency': 'daily', 'name': 'Daily Report'})
    - L509 assert resp.status_code == 200
    - L510 assign data = resp.json()
    - L511 assert 'schedule' in data or 'id' in data
  - L513 def test_delete_scheduleself, client, fresh_state, tmp_path:
    - L514 docstring: "Test deleting a schedule."
    - L515 expr fresh_state.upsert_template('tpl-del-sched', name='Delete Schedule', status='approved')
    - L517 assign schedule = fresh_state.create_schedule(name='To Delete', template_id='tpl-del-sched', template_name='Delete Schedule', template_kind='pdf', connection_id=None, connection_name=None, start_date='2024-01-01 00:00:00', end_date='2024-01-31 23:59:59', key_values=None, batch_ids=None, docx=False, xlsx=False, email_recipients=None, email_subject=None, email_message=None, frequency='daily', interval_minutes=1440, next_run_at='2024-01-01T00:00:00Z', first_run_at='2024-01-01T00:00:00Z')
    - L539 assign resp = client.delete(f"/reports/schedules/{schedule['id']}")
    - L540 assert resp.status_code == 200
    - L543 assign schedules = fresh_state.list_schedules()
    - L544 assert len(schedules) == 0
- L547 class TestCorrelationId:
  - L548 docstring: "Test correlation ID propagation."
  - L550 def test_correlation_id_in_responseself, client, fresh_state:
    - L551 docstring: "Test that correlation ID is included in responses."
    - L552 assign resp = client.get('/connections')
    - L553 assert resp.status_code == 200
    - L554 assign data = resp.json()
    - L555 assert 'correlation_id' in data
  - L557 def test_custom_correlation_id_preservedself, client, fresh_state:
    - L558 docstring: "Test that custom correlation ID header is preserved."
    - L559 assign custom_id = 'test-correlation-123'
    - L560 assign resp = client.get('/connections', headers={'x-correlation-id': custom_id})
    - L564 assert resp.status_code == 200
    - L566 assign data = resp.json()
    - L567 assert data.get('correlation_id') == custom_id or 'X-Correlation-ID' in resp.headers

## backend\tests\test_api_mapping_approve_contract_v2.py
- L1 from __future__ import annotations
- L3 import base64
- L4 import importlib
- L5 import json
- L6 import os
- L7 from pathlib import Path
- L9 import pytest
- L10 from fastapi.testclient import TestClient
- L12 expr os.environ.setdefault('NEURA_ALLOW_MISSING_OPENAI', 'true')
- L14 assign api = importlib.import_module('backend.api')
- L15 assign utils_module = importlib.import_module('backend.app.services.utils')
- L16 assign write_artifact_manifest = utils_module.write_artifact_manifest
- L17 assign write_json_atomic = utils_module.write_json_atomic
- L19 assign PNG_BYTES = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==')
- L25 def client:
  - L26 return TestClient(api.app)
- L29 def _make_template_dirroot: Path, template_id: str:
  - L30 assign tdir = root / template_id
  - L31 expr tdir.mkdir(parents=True, exist_ok=True)
  - L33 assign html_content = '<html>{material_name} {qty}</html>'
  - L34 expr (tdir / 'report_final.html').write_text(html_content, encoding='utf-8')
  - L35 expr (tdir / 'template_p1.html').write_text(html_content, encoding='utf-8')
  - L37 assign schema_payload = {'scalars': ['report_title'], 'row_tokens': ['material_name', 'qty'], 'totals': ['total_qty'], 'notes': ''}
  - L43 expr write_json_atomic(tdir / 'schema_ext.json', schema_payload, ensure_ascii=False, indent=2)
  - L44 assign mapping_step3 = {'mapping': {'report_title': 'batches.title', 'material_name': 'lines.material', 'qty': 'lines.qty', 'total_qty': 'lines.qty'}, 'meta': {'unresolved': []}}
  - L55 expr write_json_atomic(tdir / 'mapping_step3.json', mapping_step3, ensure_ascii=False, indent=2)
  - L56 return tdir
- L59 def test_mapping_approve_emits_contract_stagemonkeypatch, tmp_path, client:
  - L60 assign template_id = '00000000-0000-0000-0000-000000000001'
  - L61 assign uploads_root = tmp_path
  - L62 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', uploads_root)
  - L63 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', uploads_root)
  - L65 assign template_dir = _make_template_dir(uploads_root, template_id)
  - L68 assign db_path = tmp_path / 'db.sqlite'
  - L69 expr db_path.touch()
  - L71 expr monkeypatch.setattr(api, '_db_path_from_payload_or_default', lambda connection_id: db_path)
  - L72 expr monkeypatch.setattr(api, 'get_parent_child_info', lambda _: {'parent table': 'batches', 'child table': 'lines', 'parent_columns': ['batch_id', 'batch_date'], 'child_columns': ['batch_id', 'line_date', 'material', 'qty']})
  - L82 expr monkeypatch.setattr(api, 'compute_db_signature', lambda _: 'sig')
  - L84 def fake_build_or_load_contract_v2*, template_dir: Path, catalog, final_template_html: str, schema, auto_mapping_proposal, mapping_override, user_instructions: str, dialect_hint: str | None, db_signature: str | None=None, key_tokens=None, prompt_builder=None, prompt_version=None:
    - L99 assign overview_path = template_dir / 'overview.md'
    - L100 assign step5_path = template_dir / 'step5_requirements.json'
    - L101 assign meta_path = template_dir / 'contract_v2_meta.json'
    - L103 assign contract_payload = {'tokens': schema, 'mapping': auto_mapping_proposal.get('mapping', {}), 'join': {'parent_table': 'batches', 'parent_key': 'batch_id', 'child_table': 'lines', 'child_key': 'batch_id'}, 'date_columns': {'batches': 'batch_date', 'lines': 'line_date'}, 'reshape_rules': [], 'row_computed': {}, 'totals_math': {}, 'formatters': {}, 'order_by': {'rows': ['material_name ASC']}, 'filters': {}}
    - L120 expr overview_path.write_text('# Contract Overview\n\n- Details here.', encoding='utf-8')
    - L121 expr write_json_atomic(step5_path, {'datasets': {}, 'parameters': {}, 'transformations': [], 'edge_cases': [], 'dialect_notes': [], 'artifact_expectations': {}}, ensure_ascii=False, indent=2)
    - L134 expr write_json_atomic(meta_path, {'assumptions': [], 'warnings': [], 'validation': {}, 'contract_payload': contract_payload, 'overview_path': 'overview.md', 'step5_requirements_path': 'step5_requirements.json'}, ensure_ascii=False, indent=2)
    - L148 expr write_artifact_manifest(template_dir, step='contract_build_v2_test', files={'overview.md': overview_path, 'step5_requirements.json': step5_path, 'contract_v2_meta.json': meta_path}, inputs=[], correlation_id=None)
    - L160 return {'contract': contract_payload, 'overview_md': '# Contract Overview\n\n- Details here.', 'step5_requirements': {}, 'assumptions': [], 'warnings': [], 'validation': {'unknown_columns': [], 'unknown_tokens': [], 'token_coverage': {}}, 'artifacts': {'overview': overview_path, 'step5_requirements': step5_path, 'meta': meta_path}, 'meta': {'assumptions': [], 'warnings': [], 'validation': {}, 'contract_payload': contract_payload}, 'cached': False}
  - L185 expr monkeypatch.setattr(api, 'build_or_load_contract_v2', fake_build_or_load_contract_v2)
  - L186 expr monkeypatch.setattr(api, 'render_html_to_png', lambda *_, **__: None)
  - L188 def fake_panelhtml_path: Path, dest_png: Path, **kwargs:
    - L189 expr dest_png.write_bytes(PNG_BYTES)
    - L190 return dest_png
  - L192 expr monkeypatch.setattr(api, 'render_panel_preview', fake_panel)
  - L194 def fake_build_generator_assets_from_payload*, template_dir: Path, step4_output, final_template_html: str, reference_pdf_image, catalog_allowlist, dialect: str='duckdb', params_spec=None, sample_params=None, force_rebuild: bool=False, key_tokens=None, require_contract_join: bool=True:
    - L208 assign generator_dir = template_dir / 'generator'
    - L209 expr generator_dir.mkdir(parents=True, exist_ok=True)
    - L210 assign contract_path = template_dir / 'contract.json'
    - L211 assign sql_pack_path = generator_dir / 'sql_pack.sql'
    - L212 assign output_schemas_path = generator_dir / 'output_schemas.json'
    - L213 assign assets_meta_path = generator_dir / 'generator_assets.json'
    - L214 expr write_json_atomic(contract_path, step4_output.get('contract') or {}, ensure_ascii=False, indent=2)
    - L220 expr sql_pack_path.write_text('-- test sql pack', encoding='utf-8')
    - L221 expr write_json_atomic(output_schemas_path, {'header': [], 'rows': [], 'totals': []}, ensure_ascii=False, indent=2)
    - L227 expr write_json_atomic(assets_meta_path, {'needs_user_fix': [], 'invalid': False, 'dialect': dialect, 'params': {'required': params_spec or [], 'optional': []}, 'summary': {}, 'dry_run': {'sample_params': sample_params or {}}}, ensure_ascii=False, indent=2)
    - L240 return {'artifacts': {'contract': contract_path, 'sql_pack': sql_pack_path, 'output_schemas': output_schemas_path, 'generator_assets': assets_meta_path}, 'needs_user_fix': [], 'invalid': False, 'dialect': dialect, 'params': {'required': params_spec or [], 'optional': []}, 'summary': {}, 'dry_run': {'sample_params': sample_params or {}}, 'cached': False}
  - L256 expr monkeypatch.setattr(api, 'build_generator_assets_from_payload', fake_build_generator_assets_from_payload)
  - L262 assign payload = {'mapping': {'report_title': 'batches.title', 'material_name': 'lines.material', 'qty': 'lines.qty', 'total_qty': 'lines.qty'}, 'connection_id': 'test-connection', 'user_values_text': '', 'user_instructions': 'Summarize contract for testing.'}
  - L274 assign response = client.post(f'/templates/{template_id}/mapping/approve', json=payload, headers={'x-correlation-id': 'test-correlation'})
  - L279 assert response.status_code == 200
  - L280 assign lines = [line for line in response.content.decode('utf-8').splitlines() if line.strip()]
  - L281 assign events = [json.loads(line) for line in lines]
  - L283 assign stage_events = [evt for evt in events if evt.get('event') == 'stage' and evt.get('stage') == 'contract_build_v2']
  - L284 assert stage_events
  - L285 assert stage_events[0]['contract_ready'] is False
  - L286 assert stage_events[0].get('blueprint_ready') is True
  - L287 assert 'overview_md' in stage_events[0]
  - L288 assert stage_events[-1]['contract_ready'] is True
  - L289 assign generator_stage_events = [evt for evt in events if evt.get('event') == 'stage' and evt.get('stage') == 'generator_assets_v1']
  - L292 assert generator_stage_events
  - L293 assert 'contract' in (generator_stage_events[-1].get('artifacts') or {})
  - L295 assign result_events = [evt for evt in events if evt.get('event') == 'result']
  - L296 assert result_events
  - L297 assign final_event = result_events[-1]
  - L298 assert final_event.get('contract_stage', {}).get('stage') == 'contract_build_v2'
  - L299 assert final_event.get('generator_stage', {}).get('stage') == 'generator_assets_v1'
  - L300 assert final_event.get('contract_stage', {}).get('contract_ready') is True
  - L301 assert (template_dir / 'contract.json').exists()

## backend\tests\test_chart_suggestions.py
- L1 from __future__ import annotations
- L3 import json
- L4 from types import SimpleNamespace
- L6 import pytest
- L8 from backend.app.features.generate.schemas.charts import ChartSuggestPayload
- L9 from backend.app.features.generate.services import chart_suggestions_service as svc
- L10 from backend.app.services.reports.discovery_metrics import build_batch_field_catalog_and_stats
- L13 class DummyResponse:
  - L14 def __init__self, content: str:
    - L15 assign self.choices = [SimpleNamespace(message=SimpleNamespace(content=content))]
- L20 def _logger:
  - L21 return SimpleNamespace(info=lambda *_, **__: None, warning=lambda *_, **__: None, exception=lambda *_, **__: None)
- L28 def _build_field_catalogbatches:
  - L29 return build_batch_field_catalog_and_stats(batches)
- L32 def _build_metricsbatches, metadata, limit:
  - L33 assign rows = []
  - L34 for (idx, batch) in enumerate(batches[:limit]):
    - L35 expr rows.append({'batch_index': idx + 1, 'batch_id': batch.get('id'), 'rows': batch.get('rows', 0), 'parent': batch.get('parent', 0), 'rows_per_parent': batch.get('rows', 0) / (batch.get('parent', 1) or 1) if batch.get('parent') else batch.get('rows', 0)})
  - L46 return rows
- L49 def _call**_kwargs:
  - L50 return DummyResponse('{"charts":[{"id":"chart_1","type":"bar","xField":"batch_index","yFields":["rows"]}]}')
- L53 def _make_summarybatches, batch_metadata=None:
  - L54 return {'batches': batches, 'batches_count': len(batches), 'rows_total': sum((int(item.get('rows', 0)) for item in batches)), 'batch_metadata': batch_metadata or {}}
- L62 def _suggesttemplate_id, payload, summary_ref, template_dir, db_path, *, metrics_fn=_build_metrics, call_fn=_call:
  - L63 assign discover = lambda **_kwargs: summary_ref
  - L64 return svc.suggest_charts(template_id, payload, kind='pdf', correlation_id=None, template_dir_fn=lambda *_: template_dir, db_path_fn=lambda *_: db_path, load_contract_fn=lambda *_: None, clean_key_values_fn=lambda kv: kv, discover_fn=discover, build_field_catalog_fn=_build_field_catalog, build_metrics_fn=metrics_fn, build_prompt_fn=lambda **_kwargs: '{}', call_chat_completion_fn=call_fn, model='gpt', strip_code_fences_fn=lambda x: x, logger=_logger())
- L85 def chart_suggest_setupmonkeypatch, tmp_path:
  - L86 assign template_dir = tmp_path / 'tpl'
  - L87 expr template_dir.mkdir()
  - L88 expr (template_dir / 'contract.json').write_text('{}')
  - L89 assign db_path = template_dir / 'db.sqlite'
  - L90 expr db_path.touch()
  - L92 annotated assign batches: list[dict[str, object]] = [{'id': 'batch_1', 'rows': 25, 'parent': 5}, {'id': 'batch_2', 'rows': 30, 'parent': 3}]
  - L96 assign summary = _make_summary(batches, {'batch_1': {'time': '2024-01-01', 'category': 'North'}})
  - L98 expr monkeypatch.setattr(svc.state_store, 'set_last_used', lambda *_args, **_kwargs: None)
  - L100 return (summary, template_dir, db_path)
- L103 def test_sample_data_only_returned_when_requestedchart_suggest_setup:
  - L104 assign (summary, template_dir, db_path) = chart_suggest_setup
  - L105 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='Show rows')
  - L111 assign response = _suggest('tpl_1', payload, summary, template_dir, db_path)
  - L112 assert response.sample_data is None
  - L114 assign payload_with_sample = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='Show rows', include_sample_data=True)
  - L121 assign response_with_sample = _suggest('tpl_1', payload_with_sample, summary, template_dir, db_path)
  - L122 assert isinstance(response_with_sample.sample_data, list)
  - L123 assert len(response_with_sample.sample_data) == 2
  - L124 assign first_row = response_with_sample.sample_data[0]
  - L125 for field in ('batch_index', 'batch_id', 'rows', 'parent', 'rows_per_parent'):
    - L126 assert field in first_row
  - L127 assert response_with_sample.sample_data[0]['batch_index'] == 1
- L130 def test_sample_data_is_limited_to_100_rowschart_suggest_setup:
  - L131 assign (summary, template_dir, db_path) = chart_suggest_setup
  - L132 assign large_batches = [{'id': f'batch_{i}', 'rows': i, 'parent': i % 3 + 1} for i in range(150)]
  - L133 expr summary.update(_make_summary(large_batches))
  - L135 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='limit test', include_sample_data=True)
  - L142 assign response = _suggest('tpl_limit', payload, summary, template_dir, db_path)
  - L143 assert response.sample_data is not None
  - L144 assert len(response.sample_data) == 100
  - L145 assert response.sample_data[0]['batch_index'] == 1
  - L146 assert response.sample_data[-1]['batch_index'] == 100
- L149 def test_sample_data_failure_does_not_block_responsechart_suggest_setup:
  - L150 assign (summary, template_dir, db_path) = chart_suggest_setup
  - L152 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='error tolerance', include_sample_data=True)
  - L159 assign response = _suggest('tpl_error', payload, summary, template_dir, db_path, metrics_fn=lambda *_args, **_kwargs: (_ for _ in ()).throw(RuntimeError('boom')))
  - L167 assert response.sample_data is None
  - L168 assert isinstance(response.charts, list)
  - L169 assert response.charts
- L172 def test_malformed_chart_payload_is_sanitizedchart_suggest_setup:
  - L173 assign (summary, template_dir, db_path) = chart_suggest_setup
  - L175 def _bad_call**_kwargs:
    - L176 assign payload = {'charts': [{'id': None, 'type': 'LineChart', 'xField': 'BATCH_INDEX', 'yFields': ['rows', 'category'], 'aggregation': 'Sum', 'style': {'color': 'blue'}, 'unknown': 'junk'}, {'type': 'pie', 'xField': 'rows', 'yFields': ['category']}, {'type': 'bar', 'xField': 'missing', 'yFields': ['rows']}]}
    - L191 return DummyResponse(json.dumps(payload))
  - L193 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='malformed', include_sample_data=False)
  - L200 assign response = _suggest('tpl_malformed', payload, summary, template_dir, db_path, call_fn=_bad_call)
  - L202 assert len(response.charts) == 1
  - L203 assign chart = response.charts[0]
  - L204 assert chart.type == 'line'
  - L205 assert chart.xField == 'batch_index'
  - L206 assert chart.yFields == ['rows']
  - L207 assert chart.aggregation == 'sum'
  - L208 assert chart.style == {'color': 'blue'}
- L211 def test_template_ids_are_validated_against_catalogchart_suggest_setup:
  - L212 assign (summary, template_dir, db_path) = chart_suggest_setup
  - L214 def _template_call**_kwargs:
    - L215 assign payload = {'charts': [{'type': 'bar', 'xField': 'category', 'yFields': ['rows'], 'chartTemplateId': 'top_n_categories'}, {'type': 'line', 'xField': 'batch_index', 'yFields': ['rows'], 'chartTemplateId': 'does_not_exist'}, {'type': 'pie', 'xField': 'category', 'yFields': ['rows'], 'chartTemplateId': 'distribution_histogram'}]}
    - L237 return DummyResponse(json.dumps(payload))
  - L239 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', key_values=None, question='template ids')
  - L245 assign response = _suggest('tpl_tplid', payload, summary, template_dir, db_path, call_fn=_template_call)
  - L247 assert len(response.charts) == 3
  - L248 assign template_ids = [chart.chartTemplateId for chart in response.charts]
  - L249 assert template_ids[0] == 'top_n_categories'
  - L250 assert template_ids[1] is None
  - L251 assert template_ids[2] is None

## backend\tests\test_chart_suggestions_fallback.py
- L1 from __future__ import annotations
- L3 from types import SimpleNamespace
- L5 from backend.app.features.generate.services.chart_suggestions_service import suggest_charts
- L6 from backend.app.features.generate.schemas.charts import ChartSuggestPayload
- L9 def test_chart_suggestions_fallback_when_llm_returns_emptytmp_path:
  - L11 assign template_id = 'tpl-1'
  - L12 assign payload = ChartSuggestPayload(start_date='2024-01-01', end_date='2024-01-31', question='', include_sample_data=False)
  - L20 def discover_fn**_kwargs:
    - L21 return {'batches': [{'id': 'b1', 'rows': 10, 'parent': 2}, {'id': 'b2', 'rows': 5, 'parent': 1}], 'batch_metadata': {'b1': {'category': 'North'}, 'b2': {'category': 'South'}}, 'field_catalog': [{'name': 'rows', 'type': 'number'}, {'name': 'parent', 'type': 'number'}, {'name': 'category', 'type': 'categorical'}, {'name': 'time', 'type': 'datetime'}]}
  - L35 def build_field_catalog_fnbatches:
    - L37 assign rows_vals = [b.get('rows', 0) for b in batches]
    - L38 return ([{'name': 'rows', 'type': 'number'}, {'name': 'parent', 'type': 'number'}, {'name': 'category', 'type': 'categorical'}, {'name': 'time', 'type': 'datetime'}], {'rows': {'min': min(rows_vals), 'max': max(rows_vals), 'avg': sum(rows_vals) / len(rows_vals)}})
  - L47 def build_metrics_fnbatches, metadata, limit=100:
    - L48 assign metrics = []
    - L49 for (idx, batch) in enumerate(batches[:limit], start=1):
      - L50 assign bid = batch.get('id') or idx
      - L51 assign meta = metadata.get(str(bid), {})
      - L52 expr metrics.append({'batch_index': idx, 'batch_id': bid, 'rows': batch.get('rows', 0), 'parent': batch.get('parent', 0), 'category': meta.get('category')})
    - L61 return metrics
  - L64 def build_prompt_fn**_kwargs:
    - L65 return 'prompt'
  - L67 def call_chat_completion_fn**_kwargs:
    - L68 return SimpleNamespace(choices=[SimpleNamespace(message=SimpleNamespace(content='{"charts": []}'))])
  - L70 assign db_path = tmp_path / 'db.sqlite'
  - L71 expr db_path.write_bytes(b'')
  - L72 assign contract_path = tmp_path / 'contract.json'
  - L73 expr contract_path.write_text('{}')
  - L75 class _Logger:
    - L76 def infoself, *args, **kwargs:
      - L77 return None
    - L79 def warningself, *args, **kwargs:
      - L80 return None
    - L82 def exceptionself, *args, **kwargs:
      - L83 return None
  - L85 assign logger = _Logger()
  - L87 assign charts_response = suggest_charts(template_id, payload, kind='pdf', correlation_id=None, template_dir_fn=lambda tpl, kind='pdf': tmp_path, db_path_fn=lambda conn_id: db_path, load_contract_fn=lambda *_: None, clean_key_values_fn=lambda kv: kv, discover_fn=discover_fn, build_field_catalog_fn=build_field_catalog_fn, build_metrics_fn=build_metrics_fn, build_prompt_fn=build_prompt_fn, call_chat_completion_fn=call_chat_completion_fn, model='mock', strip_code_fences_fn=lambda text: text, logger=logger)
  - L107 assert charts_response.charts
  - L108 assign chart_types = {chart.type for chart in charts_response.charts}
  - L109 assert chart_types & {'bar', 'line'}

## backend\tests\test_contract_builder_sql.py
- L1 import pytest
- L3 from backend.app.services.contract.ContractBuilderV2 import ContractBuilderError, _normalize_sql_mapping_sections
- L9 def test_normalize_sql_mapping_sections_rejects_legacy_prefix:
  - L10 assign contract = {'mapping': {'total_set': 'DERIVED:SUM(recipes.bin1_sp)'}, 'totals': {'total_set': 'DERIVED:SUM(recipes.bin1_sp)'}}
  - L14 with pytest.raises(ContractBuilderError):
    - L15 expr _normalize_sql_mapping_sections(contract, allow_list=['recipes.bin1_sp'])
- L18 def test_normalize_sql_mapping_sections_keeps_sql_fragment:
  - L19 assign contract = {'mapping': {'total_error': 'SUM(recipes.bin1_act) - SUM(recipes.bin1_sp)'}}
  - L22 expr _normalize_sql_mapping_sections(contract, allow_list=['recipes.bin1_act', 'recipes.bin1_sp'])
  - L23 assert contract['mapping']['total_error'] == 'SUM(recipes.bin1_act) - SUM(recipes.bin1_sp)'
- L26 def test_normalize_sql_mapping_sections_rejects_unknown_columns:
  - L27 assign contract = {'mapping': {'bad': 'SUM(recipes.missing_col)'}}
  - L28 with pytest.raises(ContractBuilderError):
    - L29 expr _normalize_sql_mapping_sections(contract, allow_list=['recipes.bin1_sp'])
- L32 def test_normalize_sql_mapping_sections_rejects_subqueries:
  - L33 assign contract = {'mapping': {'bad': 'SELECT * FROM recipes'}}
  - L34 with pytest.raises(ContractBuilderError):
    - L35 expr _normalize_sql_mapping_sections(contract, allow_list=['recipes.bin1_sp'])

## backend\tests\test_contract_schema_validation.py
- L1 from __future__ import annotations
- L3 import pytest
- L5 from backend.app.services.utils.validation import SchemaValidationError, validate_contract_schema
- L11 def _base_contract:
  - L12 return {'mapping': {}, 'join': {'parent_table': 'recipes', 'parent_key': 'id', 'child_table': '', 'child_key': ''}, 'date_columns': {}, 'header_tokens': [], 'row_tokens': [], 'totals': {}, 'row_order': [], 'literals': {}}
- L29 def test_contract_schema_allows_empty_child_join:
  - L30 assign payload = _base_contract()
  - L31 expr validate_contract_schema(payload)
- L34 def test_contract_schema_allows_null_child_join:
  - L35 assign payload = _base_contract()
  - L36 assign payload['join']['child_table'] = None
  - L37 assign payload['join']['child_key'] = None
  - L38 expr validate_contract_schema(payload)
- L41 def test_contract_schema_rejects_non_string_child_join:
  - L42 assign payload = _base_contract()
  - L43 assign payload['join']['child_table'] = 123
  - L44 with pytest.raises(SchemaValidationError):
    - L45 expr validate_contract_schema(payload)
- L48 def test_contract_schema_requires_child_key_when_table_present:
  - L49 assign payload = _base_contract()
  - L50 assign payload['join']['child_table'] = 'line_items'
  - L51 assign payload['join']['child_key'] = ''
  - L52 with pytest.raises(SchemaValidationError):
    - L53 expr validate_contract_schema(payload)
- L56 def test_contract_schema_infers_join_from_mapping_when_missing:
  - L57 assign payload = _base_contract()
  - L58 expr payload.pop('join')
  - L59 assign payload['mapping'] = {'row_value': 'flowmeters.value'}
  - L60 expr validate_contract_schema(payload)
  - L61 assign join = payload.get('join') or {}
  - L62 assert join.get('parent_table') == 'flowmeters'
  - L63 assert join.get('parent_key') == '__rowid__'

## backend\tests\test_contract_v2_schema.py
- L1 from __future__ import annotations
- L3 from copy import deepcopy
- L5 import pytest
- L7 from backend.app.services.utils.validation import SchemaValidationError, validate_contract_v2, validate_step5_requirements
- L14 def _sample_contract:
  - L15 return {'tokens': {'scalars': ['report_title'], 'row_tokens': ['material_name', 'qty'], 'totals': ['total_qty']}, 'mapping': {'report_title': 'batches.title', 'material_name': 'lines.material', 'qty': 'lines.qty', 'total_qty': 'lines.qty'}, 'unresolved': [], 'join': {'parent_table': 'batches', 'parent_key': 'batch_id', 'child_table': 'lines', 'child_key': 'batch_id'}, 'date_columns': {'batches': 'batch_date', 'lines': 'line_date'}, 'reshape_rules': [{'purpose': 'rows', 'strategy': 'NONE', 'columns': [{'as': 'material_name', 'from': ['lines.material']}, {'as': 'qty', 'from': ['lines.qty']}]}], 'row_computed': {'qty_formatted': 'ROUND(qty, 2)'}, 'totals_math': {'total_qty': 'SUM(lines.qty)'}, 'formatters': {'qty': 'number(2)'}, 'order_by': {'rows': ['material_name ASC']}, 'filters': {}, 'assumptions': ['Totals exclude inactive batches.'], 'warnings': [], 'validation': {'unknown_tokens': [], 'unknown_columns': [], 'token_coverage': {'scalars_mapped_pct': 100, 'row_tokens_mapped_pct': 100, 'totals_mapped_pct': 100}}}
- L67 def _sample_step5_requirements:
  - L68 return {'datasets': {'header': {'description': 'Single-row header dataset', 'columns': ['report_title']}, 'rows': {'description': 'Detail rows dataset', 'columns': ['material_name', 'qty'], 'grouping': ['material_name'], 'ordering': ['material_name ASC']}, 'totals': {'description': 'Aggregate totals dataset', 'columns': ['total_qty']}}, 'parameters': {'required': [{'name': 'from_date', 'type': 'date'}], 'optional': [{'name': 'plant', 'type': 'string'}], 'semantics': 'Filter rows by provided date range and optional plant parameter.'}, 'transformations': ['Rows use raw values from lines table.'], 'edge_cases': ['Division by zero yields NULL.'], 'dialect_notes': ['Use sqlite-compatible syntax.'], 'artifact_expectations': {'output_schemas': 'Rows output must include material_name and qty.', 'sql_pack': 'Provide SQL scripts zipped together.'}}
- L106 def test_contract_v2_schema_valid:
  - L107 expr validate_contract_v2(_sample_contract())
- L110 def test_contract_v2_schema_allows_base_filter_rule_without_columns:
  - L111 assign contract = _sample_contract()
  - L112 expr contract['reshape_rules'].insert(0, {'purpose': 'Filter base recipes', 'strategy': 'BASE_FILTER', 'where': ['recipes.start_time >= :from_date', 'recipes.start_time <= :to_date']})
  - L120 expr validate_contract_v2(contract)
- L123 def test_contract_v2_schema_requires_column_rule:
  - L124 assign contract = _sample_contract()
  - L125 assign contract['reshape_rules'] = [{'purpose': 'Filter base recipes', 'strategy': 'BASE_FILTER', 'where': ['recipes.start_time >= :from_date']}]
  - L132 with pytest.raises(SchemaValidationError) as excinfo:
    - L133 expr validate_contract_v2(contract)
  - L134 assert 'reshape_rules' in str(excinfo.value)
- L137 def test_step5_requirements_schema_valid:
  - L138 expr validate_step5_requirements(_sample_step5_requirements())
- L141 def test_contract_v2_schema_invalid_type:
  - L142 assign invalid = deepcopy(_sample_contract())
  - L143 assign invalid['tokens']['scalars'] = 'report_title'
  - L144 with pytest.raises(SchemaValidationError):
    - L145 expr validate_contract_v2(invalid)

## backend\tests\test_critical_bugs.py
- L1 docstring: "Tests for critical bugs identified in code review.\n\nThis module tests for crit..."
- L9 from __future__ import annotations
- L11 import asyncio
- L12 import inspect
- L13 import sys
- L14 import types
- L15 from pathlib import Path
- L16 from unittest.mock import MagicMock, patch
- L18 import pytest
- L21 assign fernet_module = types.ModuleType('cryptography.fernet')
- L24 class _DummyFernet:
  - L25 def __init__self, key:
    - L26 assign self.key = key
  - L29 def generate_key:
    - L30 return b'A' * 44
  - L32 def encryptself, payload: bytes:
    - L33 return payload
  - L35 def decryptself, token: bytes:
    - L36 return token
- L39 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L40 expr setattr(fernet_module, 'InvalidToken', Exception)
- L41 assign crypto_module = types.ModuleType('cryptography')
- L42 expr setattr(crypto_module, 'fernet', fernet_module)
- L43 expr sys.modules.setdefault('cryptography', crypto_module)
- L44 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L46 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L49 class TestTemplateEndpointNamingBugs:
  - L50 docstring: "Test for function naming bugs that could cause infinite recursion."
  - L52 def test_export_template_zip_function_names_should_not_shadow_importself:
    - L53 docstring: "\n        CRITICAL BUG: templates.py:73-75 defines export_template_zip route han..."
    - L60 from src.endpoints import templates as templates_module
    - L64 assign route_functions = [name for name, obj in inspect.getmembers(templates_module) if callable(obj) and (not name.startswith('_'))]
    - L71 assert 'export_template_zip' in route_functions
    - L74 assign func = getattr(templates_module, 'export_template_zip')
    - L75 assign source = inspect.getsource(func)
    - L79 if 'return export_template_zip(template_id, request)' in source:
      - L80 expr pytest.fail('CRITICAL BUG DETECTED: export_template_zip calls itself causing infinite recursion. The route handler shadows the imported function from template_service.py. Fix: Rename the route handler to export_template_zip_route or similar.')
  - L86 def test_import_template_zip_function_names_should_not_shadow_importself:
    - L87 docstring: "\n        CRITICAL BUG: templates.py:78-80 defines import_template_zip route han..."
    - L91 from src.endpoints import templates as templates_module
    - L93 assign func = getattr(templates_module, 'import_template_zip', None)
    - L94 if func is None:
      - L95 return None
    - L97 assign source = inspect.getsource(func)
    - L100 if 'return await import_template_zip(' in source and '@router.post("/templates/import-zip")' in source:
      - L101 expr pytest.fail('CRITICAL BUG DETECTED: import_template_zip calls itself causing infinite recursion. The route handler shadows the imported function from template_service.py. Fix: Rename the route handler to import_template_zip_route or similar.')
- L108 class TestReportServiceJobCancellation:
  - L109 docstring: "Test for dangerous job cancellation patterns."
  - L111 def test_inject_thread_cancel_is_dangerousself:
    - L112 docstring: "\n        The _inject_thread_cancel function uses ctypes to inject exceptions in..."
    - L117 from src.services import report_service
    - L120 assign func = getattr(report_service, '_inject_thread_cancel', None)
    - L121 if func is None:
      - L122 return None
    - L124 assign source = inspect.getsource(func)
    - L127 if 'PyThreadState_SetAsyncExc' in source:
      - L129 assert 'Best-effort' in func.__doc__ or 'dangerous' in (func.__doc__ or '').lower()
- L133 class TestStateStoreThreadSafety:
  - L134 docstring: "Test state store thread safety."
  - L137 def fresh_stateself, tmp_path:
    - L138 from backend.app.services.state import store as state_store_module
    - L139 assign base_dir = tmp_path / 'state'
    - L140 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L141 return store
  - L143 def test_state_store_uses_lockself, fresh_state:
    - L144 docstring: "Verify state store uses threading lock for operations."
    - L145 import threading
    - L147 assert hasattr(fresh_state, '_lock')
    - L148 assert isinstance(fresh_state._lock, type(threading.RLock()))
  - L151 def test_concurrent_connection_upsertsself, fresh_state:
    - L152 docstring: "Test that concurrent connection updates don't corrupt state."
    - L153 import threading
    - L154 import time
    - L156 assign errors = []
    - L157 assign results = []
    - L159 def upsert_connectionconn_id:
      - L160 try:
        - L161 assign result = fresh_state.upsert_connection(conn_id=conn_id, name=f'Connection {conn_id}', db_type='sqlite', database_path=f'/path/to/db_{conn_id}.sqlite', secret_payload={'token': f'secret_{conn_id}'})
        - L168 expr results.append(result)
        - L169 except Exception as e:
          - L170 expr errors.append(e)
    - L172 assign threads = [threading.Thread(target=upsert_connection, args=(f'conn-{i}',)) for i in range(10)]
    - L177 for t in threads:
      - L178 expr t.start()
    - L179 for t in threads:
      - L180 expr t.join()
    - L182 assert not errors
    - L183 assert len(results) == 10
    - L186 assign connections = fresh_state.list_connections()
    - L187 assert len(connections) == 10
  - L189 def test_concurrent_template_upsertsself, fresh_state:
    - L190 docstring: "Test that concurrent template updates don't corrupt state."
    - L191 import threading
    - L193 assign errors = []
    - L194 assign results = []
    - L196 def upsert_templatetemplate_id:
      - L197 try:
        - L198 assign result = fresh_state.upsert_template(template_id, name=f'Template {template_id}', status='approved')
        - L203 expr results.append(result)
        - L204 except Exception as e:
          - L205 expr errors.append(e)
    - L207 assign threads = [threading.Thread(target=upsert_template, args=(f'tpl-{i}',)) for i in range(10)]
    - L212 for t in threads:
      - L213 expr t.start()
    - L214 for t in threads:
      - L215 expr t.join()
    - L217 assert not errors
    - L218 assert len(results) == 10
    - L221 assign templates = fresh_state.list_templates()
    - L222 assert len(templates) == 10
- L225 class TestAPIContractConsistency:
  - L226 docstring: "Test API contract between frontend and backend."
  - L228 def test_excel_routes_existself:
    - L229 docstring: "Verify all Excel routes that frontend expects exist in backend."
    - L230 from src import routes
    - L233 assign all_routes = []
    - L234 for route in routes.router.routes:
      - L235 if hasattr(route, 'path'):
        - L236 expr all_routes.append(route.path)
    - L239 assign expected_excel_routes = ['/excel/verify', '/excel/{template_id}/mapping/preview', '/excel/{template_id}/mapping/approve', '/excel/{template_id}/mapping/corrections-preview', '/excel/{template_id}/generator-assets/v1', '/excel/{template_id}/keys/options', '/excel/reports/run']
    - L249 assign missing_routes = []
    - L250 for expected in expected_excel_routes:
      - L252 assign normalized = expected.replace('{template_id}', '{template_id}')
      - L253 assign found = any((normalized in r for r in all_routes))
      - L254 if not found:
        - L255 expr missing_routes.append(expected)
    - L258 if missing_routes:
      - L259 expr pytest.skip(f'Missing Excel routes (may be expected): {missing_routes}')
  - L261 def test_discover_endpoint_existsself:
    - L262 docstring: "Verify the discover endpoint exists (client.js has placeholder)."
    - L263 from src import routes
    - L265 assign all_routes = []
    - L266 for route in routes.router.routes:
      - L267 if hasattr(route, 'path'):
        - L268 expr all_routes.append(route.path)
    - L272 assert any(('/reports/discover' in r for r in all_routes))
- L276 class TestMockModeDefault:
  - L277 docstring: "Test frontend mock mode configuration."
  - L279 def test_mock_mode_should_not_be_default_in_productionself:
    - L280 docstring: "\n        BUG: client.js line 29 sets isMock = true by default.\n        This co..."
    - L286 assign frontend_client_path = Path(__file__).resolve().parents[2] / 'frontend' / 'src' / 'api' / 'client.js'
    - L288 if frontend_client_path.exists():
      - L289 assign content = frontend_client_path.read_text()
      - L292 if "VITE_USE_MOCK || 'true'" in content:
        - L293 expr pytest.skip("INFO: Mock mode is ON by default in client.js. Consider changing default to 'false' for production safety. Current: (runtimeEnv.VITE_USE_MOCK || 'true') === 'true'")
- L300 class TestConnectionsEndpoint:
  - L301 docstring: "Test connection endpoint behaviors."
  - L303 def test_connections_import_locationself:
    - L304 docstring: "\n        BUG: connections.py:44 imports HTTPException inside function body.\n  ..."
    - L308 from src.endpoints import connections
    - L311 assign module_imports = dir(connections)
    - L315 assign func = getattr(connections, 'delete_connection_route', None)
    - L316 if func:
      - L317 assign source = inspect.getsource(func)
      - L318 if 'from fastapi import HTTPException' in source:
        - L319 expr pytest.skip('STYLE: HTTPException import inside function at connections.py:44. Consider moving to module level imports.')
- L325 class TestSubprocessMonkeypatching:
  - L326 docstring: "Test for dangerous subprocess monkeypatching."
  - L328 def test_subprocess_popen_patching_is_localizedself:
    - L329 docstring: "\n        BUG: report_service.py:951 globally patches subprocess.Popen\n        ..."
    - L333 from src.services import report_service
    - L336 assign source = inspect.getsource(report_service)
    - L338 if 'subprocess.Popen = _job_popen' in source:
      - L340 if '@contextlib.contextmanager' in source and '_patch_subprocess_tracking' in source:
        - L342 pass
        - L344 else:
          - L344 expr pytest.fail('CRITICAL: subprocess.Popen is being patched without proper context management. This could cause issues in concurrent scenarios.')

## backend\tests\test_css_merge.py
- L1 from backend.app.services.templates.css_merge import merge_css_into_html, replace_table_colgroup
- L7 def test_merge_css_appends_rules_to_existing_style:
  - L8 assign html = '<html><head><style>.foo{color:red;}</style></head><body></body></html>'
  - L9 assign result = merge_css_into_html(html, '.bar{color:blue;}')
  - L10 assign expected = '<html><head><style>.foo{color:red;}\n.bar{color:blue;}\n</style></head><body></body></html>'
  - L11 assert result == expected
- L14 def test_merge_css_inserts_new_style_block_when_missing:
  - L15 assign html = '<html><head></head><body></body></html>'
  - L16 assign result = merge_css_into_html(html, '<style>.foo{}</style>')
  - L17 assign expected = '<html><head><style>\n.foo{}\n</style>\n</head><body></body></html>'
  - L18 assert result == expected
- L21 def test_replace_table_colgroup_swaps_existing_definition:
  - L22 assign html = "<table id='tbl-1'><colgroup><col style='width:50%'></colgroup><tbody></tbody></table>"
  - L23 assign result = replace_table_colgroup(html, 'tbl-1', "<colgroup><col style='width:40%'><col style='width:60%'></colgroup>")
  - L28 assert result == "<table id='tbl-1'><colgroup><col style='width:40%'><col style='width:60%'></colgroup><tbody></tbody></table>"
- L34 def test_replace_table_colgroup_inserts_when_missing:
  - L35 assign html = "<table id='tbl-2'><tbody><tr></tr></tbody></table>"
  - L36 assign result = replace_table_colgroup(html, 'tbl-2', "<colgroup><col style='width:50%'></colgroup>")
  - L37 assert result == "<table id='tbl-2'><colgroup><col style='width:50%'></colgroup>\n<tbody><tr></tr></tbody></table>"

## backend\tests\test_discovery.py
- L1 from __future__ import annotations
- L3 import sqlite3
- L4 from pathlib import Path
- L6 from backend.app.services.reports.discovery import discover_batches_and_counts
- L9 def _build_dbpath: Path, schema_statements: list[str], data_statements: list[tuple[str, tuple]]:
  - L10 expr path.parent.mkdir(parents=True, exist_ok=True)
  - L11 with sqlite3.connect(str(path)) as con:
    - L12 assign cur = con.cursor()
    - L13 for stmt in schema_statements:
      - L14 expr cur.execute(stmt)
    - L15 for (stmt, params) in data_statements:
      - L16 expr cur.execute(stmt, params)
    - L17 expr con.commit()
- L20 def test_discover_handles_contract_without_child_tabletmp_path: Path:
  - L21 assign db_path = tmp_path / 'db.sqlite'
  - L22 expr _build_db(db_path, schema_statements=['CREATE TABLE recipes (id INTEGER PRIMARY KEY, start_time TEXT, plant TEXT)'], data_statements=[('INSERT INTO recipes (id, start_time, plant) VALUES (?, ?, ?)', (1, '2025-01-05', 'A')), ('INSERT INTO recipes (id, start_time, plant) VALUES (?, ?, ?)', (2, '2024-12-31', 'B'))])
  - L31 assign contract = {'join': {'parent_table': 'recipes', 'child_table': '', 'parent_key': 'id', 'child_key': ''}, 'date_columns': {'recipes': 'start_time'}}
  - L43 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-01-01', end_date='2025-12-31')
  - L50 assert summary['batches_count'] == 1
  - L51 assert summary['rows_total'] == 1
  - L52 assert summary['batches'][0]['id'] == '1'
  - L53 assert summary['batches'][0]['parent'] == 1
  - L54 assert summary['batches'][0]['rows'] == 1
- L57 def test_discover_supports_list_keys_and_child_tabletmp_path: Path:
  - L58 assign db_path = tmp_path / 'db.sqlite'
  - L59 expr _build_db(db_path, schema_statements=['CREATE TABLE orders (plant_id INTEGER, batch_no TEXT, start_ts TEXT)', 'CREATE TABLE order_items (plant_id INTEGER, batch_no TEXT, line_no INTEGER, start_ts TEXT)'], data_statements=[('INSERT INTO orders VALUES (?, ?, ?)', (101, 'A', '2025-03-01')), ('INSERT INTO orders VALUES (?, ?, ?)', (101, 'B', '2025-03-02')), ('INSERT INTO orders VALUES (?, ?, ?)', (101, 'C', '2024-12-31')), ('INSERT INTO order_items VALUES (?, ?, ?, ?)', (101, 'A', 1, '2025-03-01')), ('INSERT INTO order_items VALUES (?, ?, ?, ?)', (101, 'A', 2, '2025-03-01')), ('INSERT INTO order_items VALUES (?, ?, ?, ?)', (101, 'B', 1, '2025-03-02')), ('INSERT INTO order_items VALUES (?, ?, ?, ?)', (101, 'C', 1, '2024-12-31'))])
  - L76 assign contract = {'join': {'parent_table': 'orders', 'child_table': 'order_items', 'parent_key': ['plant_id', 'batch_no'], 'child_key': ['plant_id', 'batch_no']}, 'date_columns': {'orders': 'start_ts', 'order_items': 'start_ts'}}
  - L89 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-03-01', end_date='2025-03-31')
  - L97 assign ids = {batch['id']: batch for batch in summary['batches']}
  - L98 assert ids == {'101|A': {'id': '101|A', 'parent': 1, 'rows': 2}, '101|B': {'id': '101|B', 'parent': 1, 'rows': 1}}
  - L102 assert summary['batches_count'] == 2
  - L103 assert summary['rows_total'] == 3
  - L104 assign metadata = summary['batch_metadata']
  - L105 assert metadata['101|A']['category'] == '101'
  - L106 assert metadata['101|A']['plant_id'] == '101'
  - L107 assert metadata['101|A']['batch_no'] == 'A'
  - L108 assert str(metadata['101|A']['time']).startswith('2025-03-01')
  - L109 assign metric_map = {entry['batch_id']: entry for entry in summary['batch_metrics']}
  - L110 assert metric_map['101|A']['plant_id'] == '101'
  - L111 assert metric_map['101|A']['batch_no'] == 'A'
  - L112 assign catalog = {field['name']: field for field in summary['field_catalog']}
  - L113 assert catalog['time']['source'] == 'orders.start_ts'
  - L114 assert catalog['plant_id']['type'] == 'categorical'
  - L115 assert catalog['plant_id']['source'] == 'orders.plant_id'
  - L116 assert 'numeric_bins' in summary
  - L117 assert isinstance(summary['numeric_bins'].get('rows'), list)
- L120 def test_discover_infers_parent_key_from_child_when_missingtmp_path: Path:
  - L121 assign db_path = tmp_path / 'db.sqlite'
  - L122 expr _build_db(db_path, schema_statements=['CREATE TABLE orders (order_id INTEGER PRIMARY KEY, start_ts TEXT)', 'CREATE TABLE order_items (order_id INTEGER, line_no INTEGER, start_ts TEXT)'], data_statements=[('INSERT INTO orders VALUES (?, ?)', (1, '2025-04-01')), ('INSERT INTO orders VALUES (?, ?)', (2, '2025-04-02')), ('INSERT INTO orders VALUES (?, ?)', (3, '2024-12-31')), ('INSERT INTO order_items VALUES (?, ?, ?)', (1, 1, '2025-04-01')), ('INSERT INTO order_items VALUES (?, ?, ?)', (1, 2, '2025-04-01')), ('INSERT INTO order_items VALUES (?, ?, ?)', (2, 1, '2025-04-02')), ('INSERT INTO order_items VALUES (?, ?, ?)', (3, 1, '2024-12-31'))])
  - L139 assign contract = {'join': {'parent_table': 'orders', 'child_table': 'order_items', 'parent_key': '', 'child_key': 'order_id'}, 'date_columns': {'orders': 'start_ts', 'order_items': 'start_ts'}}
  - L152 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-04-01', end_date='2025-04-30')
  - L159 assign ids = {batch['id']: batch for batch in summary['batches']}
  - L160 assert ids == {'1': {'id': '1', 'parent': 1, 'rows': 2}, '2': {'id': '2', 'parent': 1, 'rows': 1}}
  - L164 assert summary['batches_count'] == 2
  - L165 assert summary['rows_total'] == 3
- L168 def test_discover_falls_back_to_rowid_when_no_keystmp_path: Path:
  - L169 assign db_path = tmp_path / 'db.sqlite'
  - L170 expr _build_db(db_path, schema_statements=['CREATE TABLE readings (start_ts TEXT, metric REAL)'], data_statements=[('INSERT INTO readings VALUES (?, ?)', ('2025-05-01', 1.1)), ('INSERT INTO readings VALUES (?, ?)', ('2025-05-15', 2.2)), ('INSERT INTO readings VALUES (?, ?)', ('2024-12-31', 3.3))])
  - L180 assign contract = {'join': {'parent_table': 'readings', 'child_table': '', 'parent_key': '', 'child_key': ''}, 'date_columns': {'readings': 'start_ts'}}
  - L192 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-05-01', end_date='2025-05-31')
  - L199 assign ids = {batch['id']: batch for batch in summary['batches']}
  - L201 assert ids == {'1': {'id': '1', 'parent': 1, 'rows': 1}, '2': {'id': '2', 'parent': 1, 'rows': 1}}
  - L205 assert summary['batches_count'] == 2
  - L206 assert summary['rows_total'] == 2
- L209 def test_discover_infers_parent_table_from_mapping_when_join_missingtmp_path: Path:
  - L210 assign db_path = tmp_path / 'db.sqlite'
  - L211 expr _build_db(db_path, schema_statements=['CREATE TABLE readings (start_ts TEXT, metric REAL)'], data_statements=[('INSERT INTO readings VALUES (?, ?)', ('2025-05-01', 1.1)), ('INSERT INTO readings VALUES (?, ?)', ('2025-05-15', 2.2))])
  - L220 assign contract = {'mapping': {'row_metric': 'readings.metric'}, 'date_columns': {'readings': 'start_ts'}}
  - L225 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-05-01', end_date='2025-05-31')
  - L232 assert summary['batches_count'] == 2
  - L233 assert summary['rows_total'] == 2

## backend\tests\test_discovery_excel.py
- L1 from __future__ import annotations
- L3 import sqlite3
- L4 from pathlib import Path
- L6 from backend.app.services.reports.discovery_excel import discover_batches_and_counts
- L9 def _build_dbpath: Path, schema_statements: list[str], data_statements: list[tuple[str, tuple]]:
  - L10 expr path.parent.mkdir(parents=True, exist_ok=True)
  - L11 with sqlite3.connect(str(path)) as con:
    - L12 assign cur = con.cursor()
    - L13 for stmt in schema_statements:
      - L14 expr cur.execute(stmt)
    - L15 for (stmt, params) in data_statements:
      - L16 expr cur.execute(stmt, params)
    - L17 expr con.commit()
- L20 def test_excel_discovery_infers_parent_table_from_mappingtmp_path: Path:
  - L21 assign db_path = tmp_path / 'excel.db'
  - L22 expr _build_db(db_path, schema_statements=['CREATE TABLE flowmeters (timestamp_utc TEXT, value REAL)'], data_statements=[('INSERT INTO flowmeters VALUES (?, ?)', ('2025-06-01', 10.0)), ('INSERT INTO flowmeters VALUES (?, ?)', ('2025-06-02', 20.0))])
  - L31 assign contract = {'mapping': {'row_value': 'flowmeters.value'}, 'date_columns': {'flowmeters': 'timestamp_utc'}}
  - L36 assign summary = discover_batches_and_counts(db_path=db_path, contract=contract, start_date='2025-06-01', end_date='2025-06-30')
  - L43 assert summary['batches_count'] == 2
  - L44 assert summary['rows_total'] == 2

## backend\tests\test_discovery_metrics_helpers.py
- L1 from __future__ import annotations
- L3 from backend.app.services.reports.discovery_metrics import bin_numeric_metric, build_discovery_schema, build_resample_support, group_metrics_by_field
- L11 def test_build_discovery_schema_classifies_fields_and_defaults:
  - L12 assign catalog = [{'name': 'time', 'type': 'datetime'}, {'name': 'category', 'type': 'string'}, {'name': 'rows', 'type': 'number', 'description': 'row count'}, {'name': 'revenue', 'type': 'numeric'}]
  - L19 assign schema = build_discovery_schema(catalog)
  - L21 assert schema['defaults']['dimension'] == 'time'
  - L22 assert schema['defaults']['metric'] == 'rows'
  - L24 assign metrics = {item['name']: item for item in schema['metrics']}
  - L25 assert metrics['rows']['bucketable'] is True
  - L26 assert metrics['revenue']['bucketable'] is True
  - L28 assign dims = {item['name']: item for item in schema['dimensions']}
  - L29 assert dims['time']['kind'] == 'temporal'
  - L30 assert dims['time']['bucketable'] is True
  - L31 assert dims['category']['kind'] == 'categorical'
  - L32 assert dims['category']['bucketable'] is False
  - L33 assert dims['rows']['kind'] == 'numeric'
  - L34 assert dims['rows']['bucketable'] is True
- L37 def test_bin_numeric_metric_builds_expected_buckets:
  - L38 assign metrics = [{'batch_id': 'a', 'rows': 1}, {'batch_id': 'b', 'rows': 3}, {'batch_id': 'c', 'rows': 8}]
  - L44 assign buckets = bin_numeric_metric(metrics, 'rows', bucket_count=2)
  - L46 assert len(buckets) == 2
  - L47 assign (first, second) = buckets
  - L49 assert first['count'] == 2
  - L50 assert set(first['batch_ids']) == {'a', 'b'}
  - L51 assert second['count'] == 1
  - L52 assert second['batch_ids'] == ['c']
  - L53 assert first['sum'] + second['sum'] == 12
- L56 def test_group_metrics_by_field_supports_average:
  - L57 assign metrics = [{'batch_id': 'a', 'rows': 5, 'region': 'east'}, {'batch_id': 'b', 'rows': 15, 'region': 'east'}, {'batch_id': 'c', 'rows': 10, 'region': 'west'}]
  - L63 assign groups = group_metrics_by_field(metrics, 'region', metric_field='rows', aggregation='avg')
  - L65 assign grouped = {item['key']: item for item in groups}
  - L66 assert grouped['east']['value'] == 10.0
  - L67 assert grouped['east']['count'] == 2
  - L68 assert grouped['west']['value'] == 10.0
  - L69 assert grouped['west']['batch_ids'] == ['c']
- L72 def test_build_resample_support_buckets_and_groups:
  - L73 assign catalog = [{'name': 'rows', 'type': 'number'}, {'name': 'region', 'type': 'string'}]
  - L77 assign metrics = [{'batch_id': 'a', 'rows': 5, 'region': 'north'}, {'batch_id': 'b', 'rows': 15, 'region': 'north'}, {'batch_id': 'c', 'rows': 10, 'region': 'south'}]
  - L83 assign support = build_resample_support(catalog, metrics, default_metric='rows', bucket_count=2)
  - L85 assert 'rows' in support['numeric_bins']
  - L86 assert len(support['numeric_bins']['rows']) == 2
  - L87 assert 'region' in support['category_groups']
  - L88 assign grouped = {row['key']: row for row in support['category_groups']['region']}
  - L89 assert grouped['north']['value'] == 20.0

## backend\tests\test_docqa_feedback.py
- L1 docstring: "Tests for Document Q&A feedback and regenerate functionality."
- L2 import pytest
- L3 from unittest.mock import patch, MagicMock
- L4 from datetime import datetime
- L6 from backend.app.domain.docqa.schemas import AskRequest, AskResponse, ChatMessage, Citation, DocQASession, DocumentReference, FeedbackRequest, FeedbackType, MessageFeedback, MessageRole, RegenerateRequest
- L19 from backend.app.domain.docqa.service import DocumentQAService
- L22 class TestFeedbackSchemas:
  - L23 docstring: "Test feedback-related schemas."
  - L25 def test_feedback_type_enumself:
    - L26 docstring: "Test FeedbackType enum values."
    - L27 assert FeedbackType.HELPFUL.value == 'helpful'
    - L28 assert FeedbackType.NOT_HELPFUL.value == 'not_helpful'
  - L30 def test_message_feedback_modelself:
    - L31 docstring: "Test MessageFeedback model."
    - L32 assign feedback = MessageFeedback(feedback_type=FeedbackType.HELPFUL, comment='Great answer!')
    - L36 assert feedback.feedback_type == FeedbackType.HELPFUL
    - L37 assert feedback.comment == 'Great answer!'
    - L38 assert feedback.timestamp is not None
  - L40 def test_chat_message_with_feedbackself:
    - L41 docstring: "Test ChatMessage model with feedback field."
    - L42 assign feedback = MessageFeedback(feedback_type=FeedbackType.NOT_HELPFUL)
    - L45 assign message = ChatMessage(id='msg-123', role=MessageRole.ASSISTANT, content='Test response', feedback=feedback)
    - L51 assert message.feedback is not None
    - L52 assert message.feedback.feedback_type == FeedbackType.NOT_HELPFUL
  - L54 def test_chat_message_without_feedbackself:
    - L55 docstring: "Test ChatMessage model without feedback field."
    - L56 assign message = ChatMessage(id='msg-123', role=MessageRole.ASSISTANT, content='Test response')
    - L61 assert message.feedback is None
  - L63 def test_feedback_request_modelself:
    - L64 docstring: "Test FeedbackRequest model."
    - L65 assign request = FeedbackRequest(feedback_type=FeedbackType.HELPFUL, comment='Useful information')
    - L69 assert request.feedback_type == FeedbackType.HELPFUL
    - L70 assert request.comment == 'Useful information'
  - L72 def test_regenerate_request_modelself:
    - L73 docstring: "Test RegenerateRequest model."
    - L74 assign request = RegenerateRequest(include_citations=False, max_response_length=1500)
    - L78 assert request.include_citations is False
    - L79 assert request.max_response_length == 1500
  - L81 def test_regenerate_request_defaultsself:
    - L82 docstring: "Test RegenerateRequest default values."
    - L83 assign request = RegenerateRequest()
    - L84 assert request.include_citations is True
    - L85 assert request.max_response_length == 2000
- L88 class TestDocQAServiceFeedback:
  - L89 docstring: "Test feedback functionality in DocumentQAService."
  - L92 def mock_state_storeself:
    - L93 docstring: "Create a mock state store."
    - L94 assign mock_store = MagicMock()
    - L95 assign mock_store._lock = MagicMock()
    - L96 assign mock_store._lock.__enter__ = MagicMock(return_value=None)
    - L97 assign mock_store._lock.__exit__ = MagicMock(return_value=None)
    - L98 return mock_store
  - L101 def sample_sessionself:
    - L102 docstring: "Create a sample session with messages."
    - L103 return {'id': 'session-123', 'name': 'Test Session', 'documents': [{'id': 'doc-1', 'name': 'test.txt', 'content_preview': 'Test content...', 'full_content': 'Test content for document', 'page_count': 1, 'added_at': datetime.utcnow().isoformat()}], 'messages': [{'id': 'msg-user-1', 'role': 'user', 'content': 'What is in the document?', 'citations': [], 'timestamp': datetime.utcnow().isoformat(), 'metadata': {}, 'feedback': None}, {'id': 'msg-assistant-1', 'role': 'assistant', 'content': 'The document contains test content.', 'citations': [], 'timestamp': datetime.utcnow().isoformat(), 'metadata': {'confidence': 0.9}, 'feedback': None}], 'context_window': 10, 'created_at': datetime.utcnow().isoformat(), 'updated_at': datetime.utcnow().isoformat()}
  - L141 def test_submit_feedback_helpfulself, mock_state_store, sample_session:
    - L142 docstring: "Test submitting helpful feedback."
    - L143 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L144 assign mock_state_store._read_state.return_value = {'docqa_sessions': {'session-123': sample_session}}
      - L148 assign service = DocumentQAService()
      - L149 assign request = FeedbackRequest(feedback_type=FeedbackType.HELPFUL)
      - L151 assign result = service.submit_feedback('session-123', 'msg-assistant-1', request)
      - L153 assert result is not None
      - L154 assert result.feedback is not None
      - L155 assert result.feedback.feedback_type == FeedbackType.HELPFUL
  - L157 def test_submit_feedback_not_helpfulself, mock_state_store, sample_session:
    - L158 docstring: "Test submitting not helpful feedback."
    - L159 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L160 assign mock_state_store._read_state.return_value = {'docqa_sessions': {'session-123': sample_session}}
      - L164 assign service = DocumentQAService()
      - L165 assign request = FeedbackRequest(feedback_type=FeedbackType.NOT_HELPFUL, comment='Answer was not accurate')
      - L170 assign result = service.submit_feedback('session-123', 'msg-assistant-1', request)
      - L172 assert result is not None
      - L173 assert result.feedback is not None
      - L174 assert result.feedback.feedback_type == FeedbackType.NOT_HELPFUL
      - L175 assert result.feedback.comment == 'Answer was not accurate'
  - L177 def test_submit_feedback_session_not_foundself, mock_state_store:
    - L178 docstring: "Test submitting feedback for non-existent session."
    - L179 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L180 assign mock_state_store._read_state.return_value = {'docqa_sessions': {}}
      - L182 assign service = DocumentQAService()
      - L183 assign request = FeedbackRequest(feedback_type=FeedbackType.HELPFUL)
      - L185 assign result = service.submit_feedback('nonexistent', 'msg-1', request)
      - L187 assert result is None
  - L189 def test_submit_feedback_message_not_foundself, mock_state_store, sample_session:
    - L190 docstring: "Test submitting feedback for non-existent message."
    - L191 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L192 assign mock_state_store._read_state.return_value = {'docqa_sessions': {'session-123': sample_session}}
      - L196 assign service = DocumentQAService()
      - L197 assign request = FeedbackRequest(feedback_type=FeedbackType.HELPFUL)
      - L199 assign result = service.submit_feedback('session-123', 'nonexistent-msg', request)
      - L201 assert result is None
- L204 class TestDocQAServiceRegenerate:
  - L205 docstring: "Test regenerate functionality in DocumentQAService."
  - L208 def mock_state_storeself:
    - L209 docstring: "Create a mock state store."
    - L210 assign mock_store = MagicMock()
    - L211 assign mock_store._lock = MagicMock()
    - L212 assign mock_store._lock.__enter__ = MagicMock(return_value=None)
    - L213 assign mock_store._lock.__exit__ = MagicMock(return_value=None)
    - L214 return mock_store
  - L217 def sample_session_with_docsself:
    - L218 docstring: "Create a sample session with documents and messages."
    - L219 return {'id': 'session-456', 'name': 'Test Session with Docs', 'documents': [{'id': 'doc-1', 'name': 'test.txt', 'content_preview': 'Test content...', 'full_content': 'This is the full content of the test document.', 'page_count': 1, 'added_at': datetime.utcnow().isoformat()}], 'messages': [{'id': 'msg-user-1', 'role': 'user', 'content': 'What is in the document?', 'citations': [], 'timestamp': datetime.utcnow().isoformat(), 'metadata': {}, 'feedback': None}, {'id': 'msg-assistant-1', 'role': 'assistant', 'content': 'The document contains test content.', 'citations': [], 'timestamp': datetime.utcnow().isoformat(), 'metadata': {'confidence': 0.9}, 'feedback': None}], 'context_window': 10, 'created_at': datetime.utcnow().isoformat(), 'updated_at': datetime.utcnow().isoformat()}
  - L257 def test_regenerate_session_not_foundself, mock_state_store:
    - L258 docstring: "Test regenerating response for non-existent session."
    - L259 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L260 assign mock_state_store._read_state.return_value = {'docqa_sessions': {}}
      - L262 assign service = DocumentQAService()
      - L263 assign request = RegenerateRequest()
      - L265 assign result = service.regenerate_response('nonexistent', 'msg-1', request)
      - L267 assert result is None
  - L269 def test_regenerate_message_not_foundself, mock_state_store, sample_session_with_docs:
    - L270 docstring: "Test regenerating response for non-existent message."
    - L271 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L272 assign mock_state_store._read_state.return_value = {'docqa_sessions': {'session-456': sample_session_with_docs}}
      - L276 assign service = DocumentQAService()
      - L277 assign request = RegenerateRequest()
      - L279 assign result = service.regenerate_response('session-456', 'nonexistent-msg', request)
      - L281 assert result is None
  - L283 def test_regenerate_with_mock_llmself, mock_state_store, sample_session_with_docs:
    - L284 docstring: "Test regenerating response with mocked LLM."
    - L285 assign mock_llm_response = {'choices': [{'message': {'content': '{"answer": "Regenerated answer.", "citations": [], "confidence": 0.85, "follow_up_questions": []}'}}], 'usage': {'total_tokens': 100}}
    - L296 with patch('backend.app.domain.docqa.service._state_store', return_value=mock_state_store):
      - L297 assign mock_state_store._read_state.return_value = {'docqa_sessions': {'session-456': sample_session_with_docs}}
      - L301 assign service = DocumentQAService()
      - L302 assign mock_client = MagicMock()
      - L303 assign mock_client.complete.return_value = mock_llm_response
      - L304 assign service._llm_client = mock_client
      - L306 assign request = RegenerateRequest(include_citations=False)
      - L307 assign result = service.regenerate_response('session-456', 'msg-assistant-1', request)
      - L309 assert result is not None
      - L310 assert result.message.content == 'Regenerated answer.'
      - L311 assert result.message.metadata.get('regenerated') is True
      - L312 assert result.message.feedback is None
- L315 class TestDocQAAPIEndpoints:
  - L316 docstring: "Test API endpoint integration."
  - L318 def test_feedback_request_validationself:
    - L319 docstring: "Test feedback request validation."
    - L321 assign valid_request = FeedbackRequest(feedback_type=FeedbackType.HELPFUL)
    - L322 assert valid_request.feedback_type == FeedbackType.HELPFUL
    - L325 with pytest.raises(ValueError):
      - L326 expr FeedbackRequest(feedback_type='invalid')
  - L328 def test_regenerate_request_validationself:
    - L329 docstring: "Test regenerate request validation."
    - L331 assign valid_request = RegenerateRequest(max_response_length=2000)
    - L332 assert valid_request.max_response_length == 2000
    - L335 with pytest.raises(ValueError):
      - L336 expr RegenerateRequest(max_response_length=50)
    - L339 with pytest.raises(ValueError):
      - L340 expr RegenerateRequest(max_response_length=20000)
- L343 if __name__ == '__main__':
  - L344 expr pytest.main([__file__, '-v'])

## backend\tests\test_enrichment_cache_stats.py
- L1 docstring: "Tests for Enrichment Cache hit/miss tracking."
- L2 import pytest
- L3 from unittest.mock import MagicMock, patch
- L4 from datetime import datetime, timezone, timedelta
- L6 from backend.app.domain.enrichment.cache import EnrichmentCache, _compute_cache_key
- L9 class TestCacheHitMissTracking:
  - L10 docstring: "Test cache hit/miss tracking functionality."
  - L13 def mock_state_storeself:
    - L14 docstring: "Create a mock state store."
    - L15 assign mock_store = MagicMock()
    - L16 assign mock_store._lock = MagicMock()
    - L17 assign mock_store._lock.__enter__ = MagicMock(return_value=None)
    - L18 assign mock_store._lock.__exit__ = MagicMock(return_value=None)
    - L19 return mock_store
  - L21 def test_cache_miss_on_empty_cacheself, mock_state_store:
    - L22 docstring: "Test that cache miss is recorded when cache is empty."
    - L23 assign mock_state_store._read_state.return_value = {'enrichment_cache': {}}
    - L25 assign cache = EnrichmentCache(mock_state_store)
    - L26 assign result = cache.get('source-1', 'lookup-value')
    - L28 assert result is None
    - L29 assert cache._misses == 1
    - L30 assert cache._hits == 0
  - L32 def test_cache_hit_on_valid_entryself, mock_state_store:
    - L33 docstring: "Test that cache hit is recorded on valid entry."
    - L34 assign cache_key = _compute_cache_key('source-1', 'lookup-value')
    - L35 assign now = datetime.now(timezone.utc)
    - L37 assign mock_state_store._read_state.return_value = {'enrichment_cache': {cache_key: {'source_id': 'source-1', 'lookup_value': 'lookup-value', 'data': {'result': 'cached'}, 'ttl_hours': 24, 'cached_at': now.isoformat()}}}
    - L49 assign cache = EnrichmentCache(mock_state_store)
    - L50 assign result = cache.get('source-1', 'lookup-value', max_age_hours=24)
    - L52 assert result == {'result': 'cached'}
    - L53 assert cache._hits == 1
    - L54 assert cache._misses == 0
  - L56 def test_cache_miss_on_expired_entryself, mock_state_store:
    - L57 docstring: "Test that cache miss is recorded on expired entry."
    - L58 assign cache_key = _compute_cache_key('source-1', 'lookup-value')
    - L59 assign old_time = datetime.now(timezone.utc) - timedelta(hours=48)
    - L61 assign mock_state_store._read_state.return_value = {'enrichment_cache': {cache_key: {'source_id': 'source-1', 'lookup_value': 'lookup-value', 'data': {'result': 'expired'}, 'ttl_hours': 24, 'cached_at': old_time.isoformat()}}}
    - L73 assign cache = EnrichmentCache(mock_state_store)
    - L74 assign result = cache.get('source-1', 'lookup-value', max_age_hours=24)
    - L76 assert result is None
    - L77 assert cache._misses == 1
    - L78 assert cache._hits == 0
  - L80 def test_multiple_hits_and_missesself, mock_state_store:
    - L81 docstring: "Test tracking multiple hits and misses."
    - L82 assign cache_key = _compute_cache_key('source-1', 'exists')
    - L83 assign now = datetime.now(timezone.utc)
    - L85 assign mock_state_store._read_state.return_value = {'enrichment_cache': {cache_key: {'source_id': 'source-1', 'lookup_value': 'exists', 'data': {'result': 'cached'}, 'ttl_hours': 24, 'cached_at': now.isoformat()}}}
    - L97 assign cache = EnrichmentCache(mock_state_store)
    - L100 expr cache.get('source-1', 'exists')
    - L101 expr cache.get('source-1', 'exists')
    - L104 expr cache.get('source-1', 'not-exists')
    - L105 expr cache.get('source-2', 'something')
    - L107 assert cache._hits == 2
    - L108 assert cache._misses == 2
  - L110 def test_get_stats_returns_hit_rateself, mock_state_store:
    - L111 docstring: "Test that get_stats returns hit_rate."
    - L112 assign cache_key = _compute_cache_key('source-1', 'value')
    - L113 assign now = datetime.now(timezone.utc)
    - L115 assign mock_state_store._read_state.return_value = {'enrichment_cache': {cache_key: {'source_id': 'source-1', 'lookup_value': 'value', 'data': {'result': 'cached'}, 'ttl_hours': 24, 'cached_at': now.isoformat()}}}
    - L127 assign cache = EnrichmentCache(mock_state_store)
    - L130 expr cache.get('source-1', 'value')
    - L131 expr cache.get('source-1', 'value')
    - L132 expr cache.get('source-1', 'missing')
    - L134 assign stats = cache.get_stats()
    - L136 assert stats['hits'] == 2
    - L137 assert stats['misses'] == 1
    - L138 assert stats['hit_rate'] == pytest.approx(2 / 3, rel=0.01)
  - L140 def test_get_stats_returns_size_bytesself, mock_state_store:
    - L141 docstring: "Test that get_stats returns size_bytes."
    - L142 assign cache_key = _compute_cache_key('source-1', 'value')
    - L143 assign now = datetime.now(timezone.utc)
    - L145 assign mock_state_store._read_state.return_value = {'enrichment_cache': {cache_key: {'source_id': 'source-1', 'lookup_value': 'value', 'data': {'result': 'some data here'}, 'ttl_hours': 24, 'cached_at': now.isoformat()}}}
    - L157 assign cache = EnrichmentCache(mock_state_store)
    - L158 assign stats = cache.get_stats()
    - L160 assert 'size_bytes' in stats
    - L161 assert stats['size_bytes'] > 0
  - L163 def test_get_stats_with_zero_requestsself, mock_state_store:
    - L164 docstring: "Test hit_rate with zero requests."
    - L165 assign mock_state_store._read_state.return_value = {'enrichment_cache': {}}
    - L167 assign cache = EnrichmentCache(mock_state_store)
    - L168 assign stats = cache.get_stats()
    - L170 assert stats['hits'] == 0
    - L171 assert stats['misses'] == 0
    - L172 assert stats['hit_rate'] == 0.0
  - L174 def test_get_stats_includes_all_expected_fieldsself, mock_state_store:
    - L175 docstring: "Test that get_stats returns all expected fields."
    - L176 assign mock_state_store._read_state.return_value = {'enrichment_cache': {}}
    - L178 assign cache = EnrichmentCache(mock_state_store)
    - L179 assign stats = cache.get_stats()
    - L181 assign expected_fields = ['total_entries', 'expired_entries', 'entries_by_source', 'hits', 'misses', 'hit_rate', 'size_bytes']
    - L191 for field in expected_fields:
      - L192 assert field in stats
- L195 class TestCacheKeyComputation:
  - L196 docstring: "Test cache key computation."
  - L198 def test_cache_key_consistencyself:
    - L199 docstring: "Test that same input produces same cache key."
    - L200 assign key1 = _compute_cache_key('source-1', 'value')
    - L201 assign key2 = _compute_cache_key('source-1', 'value')
    - L202 assert key1 == key2
  - L204 def test_cache_key_uniquenessself:
    - L205 docstring: "Test that different inputs produce different cache keys."
    - L206 assign key1 = _compute_cache_key('source-1', 'value1')
    - L207 assign key2 = _compute_cache_key('source-1', 'value2')
    - L208 assign key3 = _compute_cache_key('source-2', 'value1')
    - L210 assert key1 != key2
    - L211 assert key1 != key3
    - L212 assert key2 != key3
- L215 if __name__ == '__main__':
  - L216 expr pytest.main([__file__, '-v'])

## backend\tests\test_enrichment_sources.py
- L1 docstring: "Comprehensive tests for enrichment sources."
- L2 from __future__ import annotations
- L4 import asyncio
- L5 import sys
- L6 import types
- L7 from pathlib import Path
- L8 from unittest.mock import AsyncMock, MagicMock, patch
- L10 import pytest
- L13 assign fernet_module = types.ModuleType('cryptography.fernet')
- L16 class _DummyFernet:
  - L17 def __init__self, key:
    - L18 assign self.key = key
  - L21 def generate_key:
    - L22 return b'A' * 44
  - L24 def encryptself, payload: bytes:
    - L25 return payload
  - L27 def decryptself, token: bytes:
    - L28 return token
- L31 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L32 expr setattr(fernet_module, 'InvalidToken', Exception)
- L33 assign crypto_module = types.ModuleType('cryptography')
- L34 expr setattr(crypto_module, 'fernet', fernet_module)
- L35 expr sys.modules.setdefault('cryptography', crypto_module)
- L36 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L38 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L40 from backend.app.domain.enrichment.sources.base import EnrichmentSourceBase
- L41 from backend.app.domain.enrichment.sources.company import CompanyInfoSource
- L42 from backend.app.domain.enrichment.sources.address import AddressSource
- L43 from backend.app.domain.enrichment.sources.exchange import ExchangeRateSource, FALLBACK_RATES_USD, clear_exchange_rate_cache
- L55 class TestExchangeRateSource:
  - L56 docstring: "Tests for ExchangeRateSource with live API and fallback support."
  - L59 def clear_cacheself:
    - L60 docstring: "Clear the exchange rate cache before each test."
    - L61 expr clear_exchange_rate_cache()
    - L62 expr (yield)
    - L63 expr clear_exchange_rate_cache()
  - L66 def sourceself:
    - L67 docstring: "Create a default exchange rate source."
    - L68 return ExchangeRateSource({'base_currency': 'USD', 'target_currency': 'EUR'})
  - L74 def source_no_liveself:
    - L75 docstring: "Create an exchange rate source with live rates disabled."
    - L76 return ExchangeRateSource({'base_currency': 'USD', 'target_currency': 'EUR', 'use_live_rates': False})
  - L83 async def test_same_currency_returns_identityself, source:
    - L84 docstring: "Converting same currency should return 1.0 rate."
    - L85 assign result = await source.lookup({'amount': 100, 'from_currency': 'USD', 'to_currency': 'USD'})
    - L90 assert result is not None
    - L91 assert result['converted_amount'] == 100.0
    - L92 assert result['exchange_rate'] == 1.0
    - L93 assert result['rate_source'] == 'identity'
  - L96 async def test_fallback_rates_usd_to_eurself, source_no_live:
    - L97 docstring: "Test conversion using fallback rates."
    - L98 assign result = await source_no_live.lookup({'amount': 100, 'from_currency': 'USD', 'to_currency': 'EUR'})
    - L103 assert result is not None
    - L104 assert result['source_currency'] == 'USD'
    - L105 assert result['target_currency'] == 'EUR'
    - L106 assert result['rate_source'] == 'fallback'
    - L107 assign expected_rate = FALLBACK_RATES_USD['EUR']
    - L108 assert result['exchange_rate'] == round(expected_rate, 6)
    - L109 assert result['converted_amount'] == round(100 * expected_rate, 2)
  - L112 async def test_fallback_rates_eur_to_usdself, source_no_live:
    - L113 docstring: "Test inverse conversion using fallback rates."
    - L114 assign source_no_live.target_currency = 'USD'
    - L115 assign result = await source_no_live.lookup({'amount': 100, 'from_currency': 'EUR', 'to_currency': 'USD'})
    - L120 assert result is not None
    - L121 assert result['source_currency'] == 'EUR'
    - L122 assert result['target_currency'] == 'USD'
    - L123 assert result['rate_source'] == 'fallback'
    - L124 assign expected_rate = 1.0 / FALLBACK_RATES_USD['EUR']
    - L125 assert result['exchange_rate'] == pytest.approx(expected_rate, rel=0.0001)
  - L128 async def test_fallback_rates_cross_currencyself, source_no_live:
    - L129 docstring: "Test cross-currency conversion via USD."
    - L130 assign result = await source_no_live.lookup({'amount': 100, 'from_currency': 'EUR', 'to_currency': 'GBP'})
    - L135 assert result is not None
    - L136 assert result['source_currency'] == 'EUR'
    - L137 assert result['target_currency'] == 'GBP'
    - L138 assert result['rate_source'] == 'fallback'
    - L140 assign expected_rate = FALLBACK_RATES_USD['GBP'] / FALLBACK_RATES_USD['EUR']
    - L141 assert result['exchange_rate'] == pytest.approx(expected_rate, rel=0.0001)
  - L144 async def test_numeric_inputself, source_no_live:
    - L145 docstring: "Test conversion with numeric input."
    - L146 assign result = await source_no_live.lookup(100)
    - L147 assert result is not None
    - L148 assert result['source_currency'] == 'USD'
    - L149 assert result['target_currency'] == 'EUR'
    - L150 assert result['converted_amount'] == round(100 * FALLBACK_RATES_USD['EUR'], 2)
  - L153 async def test_string_input_amount_currencyself, source_no_live:
    - L154 docstring: "Test parsing '100 EUR' string format."
    - L155 assign result = await source_no_live.lookup('100 EUR')
    - L156 assert result is not None
    - L157 assert result['source_currency'] == 'EUR'
    - L158 assert result['target_currency'] == 'EUR'
  - L161 async def test_string_input_currency_amountself, source_no_live:
    - L162 docstring: "Test parsing 'EUR 100' string format."
    - L163 assign result = await source_no_live.lookup('EUR 100')
    - L164 assert result is not None
    - L165 assert result['source_currency'] == 'EUR'
  - L168 async def test_pipe_separated_formatself, source_no_live:
    - L169 docstring: "Test parsing '100|EUR' pipe-separated format."
    - L170 assign result = await source_no_live.lookup('100|EUR')
    - L171 assert result is not None
    - L172 assert result['source_currency'] == 'EUR'
  - L175 async def test_none_input_returns_noneself, source:
    - L176 docstring: "Null input should return None."
    - L177 assign result = await source.lookup(None)
    - L178 assert result is None
  - L181 async def test_invalid_currency_returns_noneself, source_no_live:
    - L182 docstring: "Invalid currency should return None."
    - L183 assign result = await source_no_live.lookup({'amount': 100, 'from_currency': 'INVALID', 'to_currency': 'EUR'})
    - L188 assert result is None
  - L191 async def test_confidence_for_live_ratesself, source:
    - L192 docstring: "Live rates should have higher confidence."
    - L193 assign result = {'exchange_rate': 0.92, 'rate_source': 'live'}
    - L197 assign confidence = source.get_confidence(result)
    - L198 assert confidence == 0.99
  - L201 async def test_confidence_for_fallback_ratesself, source:
    - L202 docstring: "Fallback rates should have lower confidence."
    - L203 assign result = {'exchange_rate': 0.92, 'rate_source': 'fallback'}
    - L207 assign confidence = source.get_confidence(result)
    - L208 assert confidence == 0.85
  - L211 async def test_confidence_for_identityself, source:
    - L212 docstring: "Identity conversion should have 1.0 confidence."
    - L213 assign result = {'exchange_rate': 1.0, 'rate_source': 'identity'}
    - L217 assign confidence = source.get_confidence(result)
    - L218 assert confidence == 1.0
  - L221 async def test_confidence_for_empty_resultself, source:
    - L222 docstring: "Empty result should have 0 confidence."
    - L223 assign confidence = source.get_confidence({})
    - L224 assert confidence == 0.0
    - L225 assign confidence = source.get_confidence(None)
    - L226 assert confidence == 0.0
  - L229 async def test_supported_fieldsself, source:
    - L230 docstring: "Verify supported fields are correct."
    - L231 assign fields = source.get_supported_fields()
    - L232 assert 'converted_amount' in fields
    - L233 assert 'exchange_rate' in fields
    - L234 assert 'source_currency' in fields
    - L235 assert 'target_currency' in fields
    - L236 assert 'rate_date' in fields
    - L237 assert 'rate_source' in fields
  - L240 async def test_live_api_successself, source:
    - L241 docstring: "Test successful live API call."
    - L243 try:
      - L244 import httpx
      - L245 assign http_lib = 'httpx'
      - L246 except ImportError:
        - L247 try:
          - L248 import aiohttp
          - L249 assign http_lib = 'aiohttp'
          - L250 except ImportError:
            - L251 expr pytest.skip('No HTTP library available for live API test')
            - L252 return None
    - L255 assign result = await source.lookup({'amount': 100, 'from_currency': 'USD', 'to_currency': 'EUR'})
    - L261 assert result is not None
    - L262 assert result['source_currency'] == 'USD'
    - L263 assert result['target_currency'] == 'EUR'
    - L264 assert result['rate_source'] in ('live', 'fallback')
  - L267 async def test_live_api_fallback_on_errorself, source_no_live:
    - L268 docstring: "Test fallback when live rates are disabled."
    - L269 assign result = await source_no_live.lookup({'amount': 100, 'from_currency': 'USD', 'to_currency': 'EUR'})
    - L275 assert result is not None
    - L276 assert result['rate_source'] == 'fallback'
  - L278 def test_validate_configself, source:
    - L279 docstring: "Config validation should pass."
    - L280 assert source.validate_config() is True
- L288 class TestCompanyInfoSource:
  - L289 docstring: "Tests for CompanyInfoSource."
  - L292 def sourceself:
    - L293 docstring: "Create a company info source."
    - L294 return CompanyInfoSource({})
  - L297 async def test_none_input_returns_noneself, source:
    - L298 docstring: "Null input should return None."
    - L299 assign result = await source.lookup(None)
    - L300 assert result is None
  - L303 async def test_empty_string_returns_noneself, source:
    - L304 docstring: "Empty string should return None."
    - L305 assign result = await source.lookup('')
    - L306 assert result is None
  - L308 def test_supported_fieldsself, source:
    - L309 docstring: "Verify supported fields include company info."
    - L310 assign fields = source.get_supported_fields()
    - L311 assert 'industry' in fields
    - L312 assert 'sector' in fields
    - L313 assert 'company_size' in fields
    - L314 assert 'founded_year' in fields
    - L315 assert 'headquarters_city' in fields
    - L316 assert 'website' in fields
  - L318 def test_confidence_with_resultself, source:
    - L319 docstring: "Confidence should be based on fields populated."
    - L321 assign full_result = {'industry': 'Technology', 'sector': 'Software', 'company_size': 'Large', 'founded_year': 1998, 'headquarters_city': 'Mountain View', 'headquarters_country': 'USA', 'website': 'https://google.com', 'description': 'Search engine company'}
    - L331 assign confidence = source.get_confidence(full_result)
    - L332 assert confidence >= 0.8
    - L335 assign partial_result = {'industry': 'Technology'}
    - L338 assign partial_confidence = source.get_confidence(partial_result)
    - L339 assert partial_confidence < confidence
  - L342 async def test_lookup_with_mock_llmself, source:
    - L343 docstring: "Test lookup with mocked LLM response."
    - L344 assign mock_response = {'industry': 'Technology', 'sector': 'Internet', 'company_size': 'Large', 'founded_year': 1998, 'headquarters_city': 'Mountain View', 'headquarters_country': 'USA', 'website': 'https://google.com', 'description': 'Search engine and technology company'}
    - L355 with patch.object(source, 'lookup', return_value=mock_response):
      - L356 assign result = await source.lookup('Google')
      - L357 assert result is not None
      - L358 assert result.get('industry') == 'Technology'
- L366 class TestAddressSource:
  - L367 docstring: "Tests for AddressSource."
  - L370 def sourceself:
    - L371 docstring: "Create an address source."
    - L372 return AddressSource({})
  - L375 async def test_none_input_returns_noneself, source:
    - L376 docstring: "Null input should return None."
    - L377 assign result = await source.lookup(None)
    - L378 assert result is None
  - L381 async def test_empty_string_returns_noneself, source:
    - L382 docstring: "Empty string should return None."
    - L383 assign result = await source.lookup('')
    - L384 assert result is None
  - L386 def test_supported_fieldsself, source:
    - L387 docstring: "Verify supported fields include address components."
    - L388 assign fields = source.get_supported_fields()
    - L389 assert 'street_address' in fields
    - L390 assert 'city' in fields
    - L391 assert 'state_province' in fields
    - L392 assert 'postal_code' in fields
    - L393 assert 'country' in fields
    - L394 assert 'country_code' in fields
    - L395 assert 'formatted_address' in fields
  - L397 def test_confidence_with_resultself, source:
    - L398 docstring: "Confidence should be based on fields populated."
    - L399 assign full_result = {'street_address': '1600 Amphitheatre Parkway', 'city': 'Mountain View', 'state_province': 'California', 'postal_code': '94043', 'country': 'United States', 'country_code': 'US', 'formatted_address': '1600 Amphitheatre Parkway, Mountain View, CA 94043, USA'}
    - L408 assign confidence = source.get_confidence(full_result)
    - L409 assert confidence >= 0.8
  - L412 async def test_lookup_with_mock_llmself, source:
    - L413 docstring: "Test lookup with mocked LLM response."
    - L414 assign mock_response = {'street_address': '1600 Amphitheatre Parkway', 'city': 'Mountain View', 'state_province': 'California', 'postal_code': '94043', 'country': 'United States', 'country_code': 'US', 'formatted_address': '1600 Amphitheatre Parkway, Mountain View, CA 94043, USA', 'address_type': 'commercial'}
    - L425 with patch.object(source, 'lookup', return_value=mock_response):
      - L426 assign result = await source.lookup('1600 Amphitheatre Parkway, Mountain View, CA')
      - L427 assert result is not None
      - L428 assert result.get('city') == 'Mountain View'
- L436 class TestEnrichmentServiceIntegration:
  - L437 docstring: "Integration tests for enrichment service."
  - L440 def mock_state_storeself, tmp_path, monkeypatch:
    - L441 docstring: "Create a mock state store."
    - L442 from backend.app.services.state import store as state_store_module
    - L444 assign base_dir = tmp_path / 'state'
    - L445 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L446 assign state_store_module.state_store = store
    - L447 return store
  - L450 async def test_service_creates_sourceself, mock_state_store:
    - L451 docstring: "Test creating an enrichment source via service."
    - L452 from backend.app.domain.enrichment.service import EnrichmentService
    - L453 from backend.app.domain.enrichment.schemas import EnrichmentSourceCreate, EnrichmentSourceType
    - L458 assign service = EnrichmentService()
    - L460 assign request = EnrichmentSourceCreate(name='Test Exchange', type=EnrichmentSourceType.EXCHANGE_RATE, description='Test exchange rate source', config={'base_currency': 'USD', 'target_currency': 'EUR'})
    - L467 assign source = service.create_source(request)
    - L468 assert source is not None
    - L469 assert source.name == 'Test Exchange'
    - L470 assert source.type == EnrichmentSourceType.EXCHANGE_RATE
  - L473 async def test_service_lists_sourcesself, mock_state_store:
    - L474 docstring: "Test listing enrichment sources."
    - L475 from backend.app.domain.enrichment.service import EnrichmentService
    - L476 from backend.app.domain.enrichment.schemas import EnrichmentSourceCreate, EnrichmentSourceType
    - L481 assign service = EnrichmentService()
    - L484 assign request = EnrichmentSourceCreate(name='Test Source', type=EnrichmentSourceType.COMPANY_INFO, description='Test source', config={})
    - L490 expr service.create_source(request)
    - L492 assign sources = service.list_sources()
    - L493 assert len(sources) >= 1
  - L495 def test_get_available_source_typesself, mock_state_store:
    - L496 docstring: "Test getting available source types."
    - L497 from backend.app.domain.enrichment.service import EnrichmentService
    - L499 assign service = EnrichmentService()
    - L500 assign types = service.get_available_source_types()
    - L502 assert len(types) == 3
    - L503 assign type_names = [t['type'] for t in types]
    - L504 assert 'company_info' in type_names
    - L505 assert 'address' in type_names
    - L506 assert 'exchange_rate' in type_names
- L509 if __name__ == '__main__':
  - L510 expr pytest.main([__file__, '-v'])

## backend\tests\test_excel_verify_db_mapping.py
- L1 from __future__ import annotations
- L3 import sqlite3
- L4 from pathlib import Path
- L6 import pytest
- L8 from backend.app.services.excel import ExcelVerify as excel_verify_module
- L9 from backend.app.services.excel.ExcelVerify import xlsx_to_html_preview
- L11 assign openpyxl = pytest.importorskip('openpyxl')
- L14 def _make_sample_dbdb_path: Path:
  - L15 assign conn = sqlite3.connect(db_path)
  - L16 try:
    - L17 expr conn.execute('\n            CREATE TABLE flows (\n                timestamp_utc TEXT,\n                di_raw_inlet_fm_3 REAL\n            )\n            ')
    - L25 expr conn.execute('\n            CREATE TABLE meta (\n                plant TEXT,\n                operator TEXT\n            )\n            ')
    - L33 expr conn.commit()
    - L35 finally:
      - L35 expr conn.close()
- L38 def _make_sample_xlsxxlsx_path: Path:
  - L39 assign wb = openpyxl.Workbook()
  - L40 assign ws = wb.active
  - L41 assign ws.title = 'Sample Sheet'
  - L42 expr ws.append(['DATE', 'DI_RAW_INLET_FM_3'])
  - L43 expr ws.append(['2024-01-01', 123.45])
  - L44 expr wb.save(xlsx_path)
- L47 def test_excel_preview_injects_db_placeholderstmp_path, monkeypatch:
  - L48 assign db_path = tmp_path / 'sample.db'
  - L49 expr _make_sample_db(db_path)
  - L51 assign xlsx_path = tmp_path / 'sample.xlsx'
  - L52 expr _make_sample_xlsx(xlsx_path)
  - L54 annotated assign render_calls: list[Path] = []
  - L56 def fake_render_html_to_pnghtml_path: Path, out_png_path: Path, *, page_size: str='A4', dpi: int=144:
    - L57 expr render_calls.append(html_path)
    - L58 expr out_png_path.write_bytes(b'fake')
  - L60 expr monkeypatch.setattr(excel_verify_module, 'render_html_to_png', fake_render_html_to_png)
  - L62 assign out_dir = tmp_path / 'out'
  - L63 assign result = xlsx_to_html_preview(xlsx_path, out_dir, db_path=db_path)
  - L65 assert result.html_path.exists()
  - L66 assert result.png_path and result.png_path.exists()
  - L67 assign html_text = result.html_path.read_text(encoding='utf-8')
  - L69 assert '<th data-label="date">DATE</th>' in html_text
  - L70 assert '<th data-label="di_raw_inlet_fm_3">DI_RAW_INLET_FM_3</th>' in html_text
  - L71 assert '{row_date}' in html_text
  - L72 assert '{row_di_raw_inlet_fm_3}' in html_text
  - L73 assert '2024-01-01' not in html_text
  - L74 assert html_text.count('<tr>') == 2
  - L75 assert render_calls

## backend\tests\test_federation.py
- L1 docstring: "Comprehensive tests for Cross-Database Federation feature.\n\nThis module tests:..."
- L11 from __future__ import annotations
- L13 import sys
- L14 import types
- L15 from pathlib import Path
- L16 from unittest.mock import MagicMock, patch, AsyncMock
- L18 import pytest
- L21 assign fernet_module = types.ModuleType('cryptography.fernet')
- L24 class _DummyFernet:
  - L25 def __init__self, key:
    - L26 assign self.key = key
  - L29 def generate_key:
    - L30 return b'A' * 44
  - L32 def encryptself, payload: bytes:
    - L33 return payload
  - L35 def decryptself, token: bytes:
    - L36 return token
- L39 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L40 expr setattr(fernet_module, 'InvalidToken', Exception)
- L41 assign crypto_module = types.ModuleType('cryptography')
- L42 expr setattr(crypto_module, 'fernet', fernet_module)
- L43 expr sys.modules.setdefault('cryptography', crypto_module)
- L44 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L46 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L54 class TestFederationService:
  - L55 docstring: "Tests for FederationService."
  - L58 def mock_state_storeself, tmp_path, monkeypatch:
    - L59 docstring: "Create a mock state store."
    - L60 from backend.app.services.state import store as state_store_module
    - L62 assign base_dir = tmp_path / 'state'
    - L63 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L64 assign state_store_module.state_store = store
    - L65 return store
  - L68 def serviceself, mock_state_store:
    - L69 docstring: "Create a FederationService instance."
    - L70 from backend.app.domain.federation.service import FederationService
    - L71 return FederationService()
  - L73 def test_extract_table_names_simple_selectself, service:
    - L74 docstring: "Extract table names from simple SELECT."
    - L75 assign sql = 'SELECT * FROM customers'
    - L76 assign tables = service._extract_table_names(sql)
    - L77 assert 'customers' in tables
  - L79 def test_extract_table_names_select_with_aliasself, service:
    - L80 docstring: "Extract table names from SELECT with alias."
    - L81 assign sql = 'SELECT c.name FROM customers c'
    - L82 assign tables = service._extract_table_names(sql)
    - L83 assert 'customers' in tables
  - L85 def test_extract_table_names_joinself, service:
    - L86 docstring: "Extract table names from JOIN query."
    - L87 assign sql = 'SELECT * FROM orders o JOIN customers c ON o.customer_id = c.id'
    - L88 assign tables = service._extract_table_names(sql)
    - L89 assert 'orders' in tables
    - L90 assert 'customers' in tables
  - L92 def test_extract_table_names_multiple_joinsself, service:
    - L93 docstring: "Extract table names from multiple JOINs."
    - L94 assign sql = '\n            SELECT * FROM orders\n            JOIN customers ON orders.customer_id = customers.id\n            JOIN products ON orders.product_id = products.id\n            LEFT JOIN shipping ON orders.id = shipping.order_id\n        '
    - L100 assign tables = service._extract_table_names(sql)
    - L101 assert 'orders' in tables
    - L102 assert 'customers' in tables
    - L103 assert 'products' in tables
    - L104 assert 'shipping' in tables
  - L106 def test_extract_table_names_comma_separatedself, service:
    - L107 docstring: "Extract table names from comma-separated tables."
    - L108 assign sql = 'SELECT * FROM orders, customers WHERE orders.customer_id = customers.id'
    - L109 assign tables = service._extract_table_names(sql)
    - L110 assert 'orders' in tables
    - L111 assert 'customers' in tables
  - L113 def test_extract_table_names_insertself, service:
    - L114 docstring: "Extract table name from INSERT statement."
    - L115 assign sql = 'INSERT INTO orders (customer_id, product_id) VALUES (1, 2)'
    - L116 assign tables = service._extract_table_names(sql)
    - L117 assert 'orders' in tables
  - L119 def test_extract_table_names_updateself, service:
    - L120 docstring: "Extract table name from UPDATE statement."
    - L121 assign sql = "UPDATE customers SET name = 'John' WHERE id = 1"
    - L122 assign tables = service._extract_table_names(sql)
    - L123 assert 'customers' in tables
- L126 class TestVirtualSchemaManagement:
  - L127 docstring: "Tests for virtual schema CRUD operations."
  - L130 def mock_state_storeself, tmp_path, monkeypatch:
    - L131 docstring: "Create a mock state store."
    - L132 from backend.app.services.state import store as state_store_module
    - L134 assign base_dir = tmp_path / 'state'
    - L135 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L136 assign state_store_module.state_store = store
    - L137 return store
  - L140 def serviceself, mock_state_store:
    - L141 docstring: "Create a FederationService instance."
    - L142 from backend.app.domain.federation.service import FederationService
    - L143 return FederationService()
  - L146 def mock_connection_schemaself, monkeypatch:
    - L147 docstring: "Mock connection schema retrieval."
    - L148 def mock_get_schemaconn_id, **kwargs:
      - L149 assign schemas = {'conn-1': {'tables': [{'name': 'customers', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}, {'name': 'orders', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}]}, 'conn-2': {'tables': [{'name': 'products', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}, {'name': 'inventory', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}]}}
      - L163 return schemas.get(conn_id, {'tables': []})
    - L165 expr monkeypatch.setattr('src.services.connection_inspector.get_connection_schema', mock_get_schema)
  - L170 def test_create_virtual_schemaself, service, mock_connection_schema:
    - L171 docstring: "Test creating a virtual schema."
    - L172 from backend.app.domain.federation.schemas import VirtualSchemaCreate
    - L174 assign request = VirtualSchemaCreate(name='Test Schema', description='A test schema', connection_ids=['conn-1', 'conn-2'])
    - L180 assign schema = service.create_virtual_schema(request)
    - L182 assert schema is not None
    - L183 assert schema.name == 'Test Schema'
    - L184 assert schema.description == 'A test schema'
    - L185 assert len(schema.connections) == 2
    - L186 assert len(schema.tables) == 4
  - L188 def test_list_virtual_schemasself, service, mock_connection_schema:
    - L189 docstring: "Test listing virtual schemas."
    - L190 from backend.app.domain.federation.schemas import VirtualSchemaCreate
    - L193 assign request = VirtualSchemaCreate(name='Schema 1', connection_ids=['conn-1'])
    - L197 assign created = service.create_virtual_schema(request)
    - L199 assign schemas = service.list_virtual_schemas()
    - L200 assert len(schemas) == 1
    - L201 assert schemas[0].id == created.id
  - L203 def test_get_virtual_schemaself, service, mock_connection_schema:
    - L204 docstring: "Test getting a specific virtual schema."
    - L205 from backend.app.domain.federation.schemas import VirtualSchemaCreate
    - L207 assign request = VirtualSchemaCreate(name='Schema to Get', connection_ids=['conn-1'])
    - L211 assign created = service.create_virtual_schema(request)
    - L213 assign retrieved = service.get_virtual_schema(created.id)
    - L214 assert retrieved is not None
    - L215 assert retrieved.id == created.id
    - L216 assert retrieved.name == 'Schema to Get'
  - L218 def test_get_nonexistent_schema_returns_noneself, service:
    - L219 docstring: "Getting nonexistent schema should return None."
    - L220 assign result = service.get_virtual_schema('nonexistent-id')
    - L221 assert result is None
  - L223 def test_delete_virtual_schemaself, service, mock_connection_schema:
    - L224 docstring: "Test deleting a virtual schema."
    - L225 from backend.app.domain.federation.schemas import VirtualSchemaCreate
    - L227 assign request = VirtualSchemaCreate(name='Schema to Delete', connection_ids=['conn-1'])
    - L231 assign created = service.create_virtual_schema(request)
    - L233 assign result = service.delete_virtual_schema(created.id)
    - L234 assert result is True
    - L237 assign retrieved = service.get_virtual_schema(created.id)
    - L238 assert retrieved is None
  - L240 def test_delete_nonexistent_schema_returns_falseself, service:
    - L241 docstring: "Deleting nonexistent schema should return False."
    - L242 assign result = service.delete_virtual_schema('nonexistent-id')
    - L243 assert result is False
- L246 class TestTableToConnectionMapping:
  - L247 docstring: "Tests for table-to-connection mapping."
  - L250 def mock_state_storeself, tmp_path, monkeypatch:
    - L251 docstring: "Create a mock state store."
    - L252 from backend.app.services.state import store as state_store_module
    - L254 assign base_dir = tmp_path / 'state'
    - L255 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L256 assign state_store_module.state_store = store
    - L257 return store
  - L260 def serviceself, mock_state_store:
    - L261 docstring: "Create a FederationService instance."
    - L262 from backend.app.domain.federation.service import FederationService
    - L263 return FederationService()
  - L265 def test_map_tables_to_connectionsself, service:
    - L266 docstring: "Test mapping tables to their connections."
    - L267 from backend.app.domain.federation.schemas import VirtualSchema, TableReference
    - L269 assign schema = VirtualSchema(id='test-schema', name='Test', description=None, connections=['conn-1', 'conn-2'], tables=[TableReference(connection_id='conn-1', table_name='customers', alias='conn_customers'), TableReference(connection_id='conn-1', table_name='orders', alias='conn_orders'), TableReference(connection_id='conn-2', table_name='products', alias='conn_products')], joins=[], created_at='2024-01-01T00:00:00Z', updated_at='2024-01-01T00:00:00Z')
    - L284 assign mapping = service._map_tables_to_connections(['customers', 'products'], schema)
    - L289 assert 'conn-1' in mapping
    - L290 assert 'conn-2' in mapping
    - L291 assert 'customers' in mapping['conn-1']
    - L292 assert 'products' in mapping['conn-2']
  - L294 def test_map_tables_by_aliasself, service:
    - L295 docstring: "Test mapping tables by their alias."
    - L296 from backend.app.domain.federation.schemas import VirtualSchema, TableReference
    - L298 assign schema = VirtualSchema(id='test-schema', name='Test', description=None, connections=['conn-1'], tables=[TableReference(connection_id='conn-1', table_name='customers', alias='c1_customers')], joins=[], created_at='2024-01-01T00:00:00Z', updated_at='2024-01-01T00:00:00Z')
    - L312 assign mapping = service._map_tables_to_connections(['c1_customers'], schema)
    - L313 assert 'conn-1' in mapping
- L316 class TestResultMerging:
  - L317 docstring: "Tests for merging results from multiple connections."
  - L320 def mock_state_storeself, tmp_path, monkeypatch:
    - L321 docstring: "Create a mock state store."
    - L322 from backend.app.services.state import store as state_store_module
    - L324 assign base_dir = tmp_path / 'state'
    - L325 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L326 assign state_store_module.state_store = store
    - L327 return store
  - L330 def serviceself, mock_state_store:
    - L331 docstring: "Create a FederationService instance."
    - L332 from backend.app.domain.federation.service import FederationService
    - L333 return FederationService()
  - L335 def test_merge_single_resultself, service:
    - L336 docstring: "Single result should be returned as-is."
    - L337 assign results = [{'columns': ['id', 'name'], 'rows': [[1, 'Alice'], [2, 'Bob']]}]
    - L341 assign merged = service._merge_results(results)
    - L343 assert merged['columns'] == ['id', 'name']
    - L344 assert len(merged['rows']) == 2
  - L346 def test_merge_union_resultsself, service:
    - L347 docstring: "Multiple results should be unioned without join keys."
    - L348 assign results = [{'columns': ['id', 'name'], 'rows': [[1, 'Alice']]}, {'columns': ['id', 'name'], 'rows': [[2, 'Bob']]}]
    - L353 assign merged = service._merge_results(results, None)
    - L355 assert merged['columns'] == ['id', 'name']
    - L356 assert len(merged['rows']) == 2
    - L357 assert merged['merge_type'] == 'union'
  - L359 def test_merge_results_different_columnsself, service:
    - L360 docstring: "Merging results with different columns should include all."
    - L361 assign results = [{'columns': ['id', 'name'], 'rows': [[1, 'Alice']]}, {'columns': ['id', 'email'], 'rows': [[2, 'bob@example.com']]}]
    - L366 assign merged = service._merge_results(results, None)
    - L368 assert 'id' in merged['columns']
    - L369 assert 'name' in merged['columns']
    - L370 assert 'email' in merged['columns']
  - L372 def test_merge_with_join_keyself, service:
    - L373 docstring: "Merging with join key should perform join."
    - L374 assign results = [{'columns': ['id', 'name'], 'rows': [[1, 'Alice'], [2, 'Bob']]}, {'columns': ['id', 'email'], 'rows': [[1, 'alice@example.com'], [3, 'charlie@example.com']]}]
    - L379 assign merged = service._merge_results(results, ['id'])
    - L381 assert merged['merge_type'] == 'join'
    - L383 assert len(merged['rows']) == 1
    - L384 assert merged['rows'][0][0] == 1
    - L385 assert merged['rows'][0][1] == 'Alice'
  - L387 def test_merge_empty_resultsself, service:
    - L388 docstring: "Merging empty results should return empty."
    - L389 assign merged = service._merge_results([])
    - L391 assert merged['columns'] == []
    - L392 assert merged['rows'] == []
    - L393 assert merged['row_count'] == 0
- L396 class TestFederatedQueryExecution:
  - L397 docstring: "Tests for federated query execution."
  - L400 def mock_state_storeself, tmp_path, monkeypatch:
    - L401 docstring: "Create a mock state store."
    - L402 from backend.app.services.state import store as state_store_module
    - L404 assign base_dir = tmp_path / 'state'
    - L405 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L406 assign state_store_module.state_store = store
    - L407 return store
  - L410 def serviceself, mock_state_store:
    - L411 docstring: "Create a FederationService instance."
    - L412 from backend.app.domain.federation.service import FederationService
    - L413 return FederationService()
  - L416 def mock_connection_schemaself, monkeypatch:
    - L417 docstring: "Mock connection schema retrieval."
    - L418 def mock_get_schemaconn_id, **kwargs:
      - L419 assign schemas = {'conn-1': {'tables': [{'name': 'customers', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}]}, 'conn-2': {'tables': [{'name': 'orders', 'columns': [{'name': 'id', 'type': 'INTEGER'}]}]}}
      - L431 return schemas.get(conn_id, {'tables': []})
    - L433 expr monkeypatch.setattr('src.services.connection_inspector.get_connection_schema', mock_get_schema)
  - L438 def test_execute_query_single_connectionself, service, mock_connection_schema, monkeypatch:
    - L439 docstring: "Query on single connection should execute directly."
    - L440 from backend.app.domain.federation.schemas import VirtualSchemaCreate, FederatedQueryRequest
    - L446 assign request = VirtualSchemaCreate(name='Single Connection Schema', connection_ids=['conn-1'])
    - L450 assign schema = service.create_virtual_schema(request)
    - L453 def mock_executeconnection_id, sql, limit=None:
      - L454 return {'columns': ['id', 'name'], 'rows': [[1, 'Alice']]}
    - L459 expr monkeypatch.setattr('backend.app.services.connections.db_connection.execute_query', mock_execute)
    - L464 assign query_request = FederatedQueryRequest(virtual_schema_id=schema.id, sql='SELECT * FROM customers', limit=100)
    - L470 assign result = service.execute_query(query_request)
    - L472 assert result is not None
    - L473 assert result['routing'] == 'single'
    - L474 assert len(result['executed_on']) == 1
  - L476 def test_execute_query_schema_not_foundself, service:
    - L477 docstring: "Query on nonexistent schema should raise error."
    - L478 from backend.app.domain.federation.schemas import FederatedQueryRequest
    - L479 from backend.app.core.errors import AppError
    - L481 assign query_request = FederatedQueryRequest(virtual_schema_id='nonexistent', sql='SELECT * FROM customers')
    - L486 with pytest.raises(AppError) as exc_info:
      - L487 expr service.execute_query(query_request)
    - L489 assert exc_info.value.code == 'schema_not_found'
  - L491 def test_execute_query_no_connectionsself, service, mock_state_store:
    - L492 docstring: "Query on schema with no connections should raise error."
    - L493 from backend.app.domain.federation.schemas import FederatedQueryRequest
    - L494 from backend.app.core.errors import AppError
    - L497 assign store = mock_state_store
    - L498 with store._lock:
      - L499 assign state = store._read_state()
      - L500 assign state.setdefault('virtual_schemas', {})['empty-schema'] = {'id': 'empty-schema', 'name': 'Empty', 'connections': [], 'tables': [], 'joins': [], 'created_at': '2024-01-01T00:00:00Z', 'updated_at': '2024-01-01T00:00:00Z'}
      - L509 expr store._write_state(state)
    - L511 assign query_request = FederatedQueryRequest(virtual_schema_id='empty-schema', sql='SELECT * FROM customers')
    - L516 with pytest.raises(AppError) as exc_info:
      - L517 expr service.execute_query(query_request)
    - L519 assert exc_info.value.code == 'no_connections'
- L522 class TestJoinSuggestion:
  - L523 docstring: "Tests for AI-powered join suggestion."
  - L526 def mock_state_storeself, tmp_path, monkeypatch:
    - L527 docstring: "Create a mock state store."
    - L528 from backend.app.services.state import store as state_store_module
    - L530 assign base_dir = tmp_path / 'state'
    - L531 assign store = state_store_module.StateStore(base_dir=base_dir)
    - L532 assign state_store_module.state_store = store
    - L533 return store
  - L536 def serviceself, mock_state_store:
    - L537 docstring: "Create a FederationService instance."
    - L538 from backend.app.domain.federation.service import FederationService
    - L539 return FederationService()
  - L542 def mock_connection_schemaself, monkeypatch:
    - L543 docstring: "Mock connection schema retrieval."
    - L544 def mock_get_schemaconn_id, **kwargs:
      - L545 assign schemas = {'conn-1': {'tables': [{'name': 'customers', 'columns': [{'name': 'id', 'type': 'INTEGER'}, {'name': 'name', 'type': 'TEXT'}]}]}, 'conn-2': {'tables': [{'name': 'orders', 'columns': [{'name': 'id', 'type': 'INTEGER'}, {'name': 'customer_id', 'type': 'INTEGER'}]}]}}
      - L569 return schemas.get(conn_id, {'tables': []})
    - L571 expr monkeypatch.setattr('src.services.connection_inspector.get_connection_schema', mock_get_schema)
  - L576 def test_suggest_joins_with_mock_llmself, service, mock_connection_schema, monkeypatch:
    - L577 docstring: "Test join suggestion with mocked LLM response."
    - L578 assign mock_llm = MagicMock()
    - L579 assign mock_llm.complete.return_value = {'choices': [{'message': {'content': '[\n                        {\n                            "left_connection_id": "conn-1",\n                            "left_table": "customers",\n                            "left_column": "id",\n                            "right_connection_id": "conn-2",\n                            "right_table": "orders",\n                            "right_column": "customer_id",\n                            "confidence": 0.95,\n                            "reason": "customer_id in orders references id in customers"\n                        }\n                    ]'}}]}
    - L598 expr monkeypatch.setattr(service, '_get_llm_client', lambda: mock_llm)
    - L600 assign suggestions = service.suggest_joins(['conn-1', 'conn-2'])
    - L602 assert len(suggestions) == 1
    - L603 assert suggestions[0].left_table == 'customers'
    - L604 assert suggestions[0].right_table == 'orders'
    - L605 assert suggestions[0].confidence == 0.95
  - L607 def test_suggest_joins_insufficient_connectionsself, service, mock_connection_schema:
    - L608 docstring: "Join suggestion with < 2 connections should return empty."
    - L609 assign suggestions = service.suggest_joins(['conn-1'])
    - L610 assert suggestions == []
  - L612 def test_suggest_joins_llm_errorself, service, mock_connection_schema, monkeypatch:
    - L613 docstring: "Join suggestion should handle LLM errors gracefully."
    - L614 assign mock_llm = MagicMock()
    - L615 assign mock_llm.complete.side_effect = Exception('LLM error')
    - L617 expr monkeypatch.setattr(service, '_get_llm_client', lambda: mock_llm)
    - L619 assign suggestions = service.suggest_joins(['conn-1', 'conn-2'])
    - L620 assert suggestions == []
- L623 if __name__ == '__main__':
  - L624 expr pytest.main([__file__, '-v'])

## backend\tests\test_fix_html.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import base64
- L5 import io
- L6 import json
- L7 from pathlib import Path
- L8 from types import SimpleNamespace
- L10 import pytest
- L12 import backend.api as api
- L13 from backend.app.services.templates import TemplateVerify as tv
- L15 assign PNG_BYTES = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==')
- L20 def _write_pngpath: Path:
  - L21 expr path.write_bytes(PNG_BYTES)
- L24 def _make_initial_html:
  - L25 return '<!DOCTYPE html>\n<html>\n<head>\n<style>\ntable { width: 100%; border-collapse: collapse; }\n</style>\n</head>\n<body>\n<table id="data-table">\n<thead>\n  <tr><th data-label="amount">{amount_label}</th></tr>\n</thead>\n<tbody>\n<!-- BEGIN:BLOCK_REPEAT ROW -->\n  <tr><td>{amount}</td></tr>\n<!-- END:BLOCK_REPEAT ROW -->\n</tbody>\n</table>\n</body>\n</html>'
- L47 def test_request_fix_html_successmonkeypatch, tmp_path: Path:
  - L48 assign html_path = tmp_path / 'template_p1.html'
  - L49 expr html_path.write_text(_make_initial_html(), encoding='utf-8')
  - L51 assign reference_png = tmp_path / 'reference_p1.png'
  - L52 assign render_png = tmp_path / 'render_p1.png'
  - L53 expr _write_png(reference_png)
  - L54 expr _write_png(render_png)
  - L56 assign schema_path = tmp_path / 'schema_ext.json'
  - L57 expr schema_path.write_text(json.dumps({'scalars': ['{amount}']}), encoding='utf-8')
  - L59 assign refined_html = _make_initial_html().replace('border-collapse: collapse;', 'border-collapse: collapse; border: 0.2mm solid #000;')
  - L64 def fake_call_chat_completionclient, model, messages, description:
    - L65 assign content = f'<!--BEGIN_HTML-->{refined_html}<!--END_HTML-->'
    - L66 return SimpleNamespace(choices=[SimpleNamespace(message=SimpleNamespace(content=content))])
  - L68 annotated assign render_calls: list[Path] = []
  - L70 def fake_render_html_to_pnghtml_path_arg: Path, out_png_path: Path, *, page_size: str='A4':
    - L71 expr render_calls.append(out_png_path)
    - L72 expr _write_png(out_png_path)
  - L74 expr monkeypatch.setattr(tv, 'get_openai_client', lambda: object())
  - L75 expr monkeypatch.setattr(tv, 'call_chat_completion', fake_call_chat_completion)
  - L76 expr monkeypatch.setattr(tv, 'render_html_to_png', fake_render_html_to_png)
  - L78 def fake_panel_previewhtml_path_arg: Path, dest_png: Path, **kwargs:
    - L79 expr dest_png.write_bytes(PNG_BYTES)
    - L80 return dest_png
  - L82 expr monkeypatch.setattr(tv, 'render_panel_preview', fake_panel_preview)
  - L83 assign result = tv.request_fix_html(tmp_path, html_path, schema_path, reference_png, render_png, 0.9123)
  - L92 assert result['accepted'] is True
  - L93 assert render_calls
  - L94 assign refined_text = html_path.read_text(encoding='utf-8')
  - L95 assert '0.2mm solid' in refined_text
  - L97 annotated assign metrics_path: Path = result['metrics_path']
  - L98 assert metrics_path.exists()
  - L99 assign metrics = json.loads(metrics_path.read_text(encoding='utf-8'))
  - L100 assert metrics == {'accepted': True, 'rejected_reason': None}
  - L104 annotated assign render_after: Path = result['render_after_path']
  - L105 assert render_after.exists()
- L108 def test_request_fix_html_reject_token_driftmonkeypatch, tmp_path: Path:
  - L109 assign html_path = tmp_path / 'template_p1.html'
  - L110 expr html_path.write_text(_make_initial_html(), encoding='utf-8')
  - L112 assign reference_png = tmp_path / 'reference_p1.png'
  - L113 assign render_png = tmp_path / 'render_p1.png'
  - L114 expr _write_png(reference_png)
  - L115 expr _write_png(render_png)
  - L117 assign drifted_html = _make_initial_html().replace('{amount}', '{amount_new}')
  - L119 def fake_call_chat_completionclient, model, messages, description:
    - L120 assign content = f'<!--BEGIN_HTML-->{drifted_html}<!--END_HTML-->'
    - L121 return SimpleNamespace(choices=[SimpleNamespace(message=SimpleNamespace(content=content))])
  - L123 expr monkeypatch.setattr(tv, 'get_openai_client', lambda: object())
  - L124 expr monkeypatch.setattr(tv, 'call_chat_completion', fake_call_chat_completion)
  - L125 expr monkeypatch.setattr(tv, 'render_html_to_png', lambda *args, **kwargs: pytest.fail('render_html_to_png should not run on rejection'))
  - L130 expr monkeypatch.setattr(tv, 'render_panel_preview', lambda *args, **kwargs: pytest.fail('render_panel_preview should not run on rejection'))
  - L135 assign result = tv.request_fix_html(tmp_path, html_path, None, reference_png, render_png, 0.91)
  - L144 assert result['accepted'] is False
  - L145 assert result['rejected_reason'] == 'token_drift'
  - L146 assign metrics = json.loads(result['metrics_path'].read_text(encoding='utf-8'))
  - L147 assert metrics == {'accepted': False, 'rejected_reason': 'token_drift'}
  - L148 assert not (tmp_path / 'render_p1_after.png').exists()
  - L149 assign roundtrip_html = html_path.read_text(encoding='utf-8')
  - L150 assert '{amount}' in roundtrip_html
- L153 def test_fix_html_refine_stage_runs_even_when_target_metmonkeypatch, tmp_path: Path:
  - L155 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', tmp_path)
  - L156 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', tmp_path.resolve())
  - L159 class _DummyState:
    - L160 def upsert_templateself, *args, **kwargs:
      - L161 return None
    - L163 def set_last_usedself, *args, **kwargs:
      - L164 return None
  - L166 expr monkeypatch.setattr(api, 'state_store', _DummyState())
  - L169 def fake_pdf_to_pngspdf_path: Path, out_dir: Path, dpi: int=400:
    - L170 assign target = out_dir / 'reference_p1.png'
    - L171 expr _write_png(target)
    - L172 return [target]
  - L174 expr monkeypatch.setattr(api, 'pdf_to_pngs', fake_pdf_to_pngs)
  - L175 expr monkeypatch.setattr(api, 'get_layout_hints', lambda *args, **kwargs: {})
  - L177 assign initial_html = _make_initial_html()
  - L178 assign initial_result = SimpleNamespace(html=initial_html, schema=None)
  - L179 expr monkeypatch.setattr(api, 'request_initial_html', lambda *args, **kwargs: initial_result)
  - L180 expr monkeypatch.setattr(api, 'save_html', lambda path, text: Path(path).write_text(text, encoding='utf-8'))
  - L182 expr monkeypatch.setattr(api, 'rasterize_html_to_png', lambda *args, **kwargs: PNG_BYTES)
  - L184 def fake_save_pngdata: bytes, out_path: str:
    - L185 expr Path(out_path).write_bytes(data)
    - L186 return out_path
  - L188 expr monkeypatch.setattr(api, 'save_png', fake_save_png)
  - L190 annotated assign render_calls: list[Path] = []
  - L192 def fake_render_html_to_pnghtml_path: Path, out_png_path: Path, *, page_size: str='A4':
    - L193 expr render_calls.append(out_png_path)
    - L194 expr _write_png(out_png_path)
  - L196 expr monkeypatch.setattr(api, 'render_html_to_png', fake_render_html_to_png)
  - L198 def fake_panelhtml_path: Path, dest_png: Path, **kwargs:
    - L199 expr dest_png.write_bytes(PNG_BYTES)
    - L200 return dest_png
  - L202 expr monkeypatch.setattr(api, 'render_panel_preview', fake_panel)
  - L204 annotated assign fix_calls: list[tuple] = []
  - L206 def fake_request_fix_htmltemplate_dir: Path, html_path: Path, schema_path, reference_png: Path, render_png: Path, ssim_before: float:
    - L214 expr fix_calls.append((template_dir, html_path, reference_png, render_png, ssim_before))
    - L215 assign render_after = template_dir / 'render_p1_after.png'
    - L216 assign render_after_full = template_dir / 'render_p1_after_full.png'
    - L217 expr _write_png(render_after)
    - L218 expr _write_png(render_after_full)
    - L219 assign metrics_path = template_dir / 'fix_metrics.json'
    - L220 expr metrics_path.write_text('{}', encoding='utf-8')
    - L221 return {'accepted': True, 'rejected_reason': None, 'render_after_path': render_after, 'render_after_full_path': render_after_full, 'metrics_path': metrics_path, 'raw_response': '<!--BEGIN_HTML--><!--END_HTML-->'}
  - L230 expr monkeypatch.setattr(api, 'request_fix_html', fake_request_fix_html)
  - L232 annotated assign manifest_calls: list[dict] = []
  - L234 def fake_write_artifact_manifesttemplate_dir: Path, *, step: str, files: dict, inputs, correlation_id:
    - L235 expr manifest_calls.append(files)
    - L236 assign target = template_dir / 'artifact_manifest.json'
    - L237 expr target.write_text('{}', encoding='utf-8')
    - L238 return target
  - L240 expr monkeypatch.setattr(api, 'write_artifact_manifest', fake_write_artifact_manifest)
  - L242 expr monkeypatch.setenv('VERIFY_FIX_HTML_ENABLED', 'true')
  - L243 expr monkeypatch.setenv('MAX_FIX_PASSES', '1')
  - L245 async def _run_verification:
    - L246 assign upload = api.UploadFile(file=io.BytesIO(b'%PDF-1.4\n'), filename='source.pdf')
    - L247 assign response = await api.verify_template(file=upload, connection_id='conn-1', refine_iters=0, request=None)
    - L249 assign chunks = [chunk async for chunk in response.body_iterator]
    - L250 assign events = []
    - L251 for chunk in chunks:
      - L252 assign text = chunk.decode('utf-8').strip()
      - L253 if not text:
        - L254 continue
      - L255 for part in text.split('`n'):
        - L256 assign part = part.strip()
        - L257 if part:
          - L258 expr events.append(json.loads(part))
    - L260 assign fix_stage = next((ev for ev in events if ev.get('event') == 'stage' and ev.get('label') == 'Refining HTML layout fidelity...' and (ev.get('status') == 'complete')))
    - L267 assert fix_calls
    - L268 assert fix_stage['skipped'] is False
    - L269 assert fix_stage['fix_attempted'] is True
    - L270 assert fix_stage['fix_accepted'] is True
    - L272 assign result_event = next((ev for ev in events if ev.get('event') == 'result'))
    - L273 assert 'render_png_url' in result_event['artifacts']
    - L274 assert 'render_after_png_url' in result_event['artifacts']
    - L276 assert manifest_calls
    - L277 assign manifest_files = manifest_calls[-1]
    - L278 assert 'render_p1.png' in manifest_files
    - L279 assert 'render_p1_llm.png' in manifest_files
    - L280 assert 'render_p1_after.png' in manifest_files
    - L281 assert 'render_p1_after_full.png' in manifest_files
    - L282 assert 'fix_metrics.json' in manifest_files
  - L284 expr asyncio.run(_run_verification())

## backend\tests\test_generator_assets_v1_api.py
- L1 from __future__ import annotations
- L3 import base64
- L4 import json
- L5 import os
- L6 from pathlib import Path
- L7 from typing import Any, Dict
- L9 import pytest
- L10 from fastapi.testclient import TestClient
- L12 assign PNG_BYTES = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==')
- L17 def _write_pngpath: Path:
  - L18 expr path.write_bytes(PNG_BYTES)
- L21 expr os.environ.setdefault('NEURA_ALLOW_MISSING_OPENAI', 'true')
- L23 from backend import api
- L24 from backend.app.services.state import StateStore
- L25 from backend.app.services.utils import write_artifact_manifest, write_json_atomic
- L32 def clienttmp_path, monkeypatch:
  - L33 assign uploads_root = tmp_path / 'uploads'
  - L34 expr uploads_root.mkdir(parents=True, exist_ok=True)
  - L35 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', uploads_root)
  - L36 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', uploads_root)
  - L37 assign state_dir = tmp_path / 'state'
  - L38 expr state_dir.mkdir(parents=True, exist_ok=True)
  - L39 assign test_state_store = StateStore(state_dir)
  - L40 expr monkeypatch.setattr(api, 'state_store', test_state_store)
  - L41 expr monkeypatch.setattr('backend.app.services.state.state_store', test_state_store)
  - L42 expr monkeypatch.setattr('backend.app.services.state.store.state_store', test_state_store)
  - L43 return TestClient(api.app)
- L46 def _setup_template_dirroot: Path, template_id: str:
  - L47 assign tdir = root / template_id
  - L48 expr tdir.mkdir(parents=True, exist_ok=True)
  - L50 expr (tdir / 'overview.md').write_text('# Overview', encoding='utf-8')
  - L51 expr write_json_atomic(tdir / 'step5_requirements.json', {'datasets': {}, 'parameters': {}, 'transformations': [], 'edge_cases': [], 'dialect_notes': [], 'artifact_expectations': {}}, ensure_ascii=False, indent=2)
  - L64 expr _write_png(tdir / 'reference_p1.png')
  - L65 return tdir
- L68 def _fake_servicetemplate_dir: Path, *, dialect: str, invalid: bool, needs: list[str]:
  - L69 assign generator_dir = template_dir / 'generator'
  - L70 expr generator_dir.mkdir(parents=True, exist_ok=True)
  - L71 assign contract_path = template_dir / 'contract.json'
  - L72 expr write_json_atomic(contract_path, {'tokens': {'scalars': ['h1'], 'row_tokens': ['r1'], 'totals': ['t1']}}, ensure_ascii=False, indent=2)
  - L78 assign sql_path = generator_dir / 'sql_pack.sql'
  - L79 expr sql_path.write_text('-- SQL SCRIPT', encoding='utf-8')
  - L80 assign output_schemas_path = generator_dir / 'output_schemas.json'
  - L81 assign output_payload = {'header': ['h1'], 'rows': ['r1'], 'totals': ['t1']}
  - L82 expr write_json_atomic(output_schemas_path, output_payload, ensure_ascii=False, indent=2)
  - L83 assign meta_path = generator_dir / 'generator_assets.json'
  - L84 assign meta_payload = {'dialect': dialect, 'params': {'required': ['from_date', 'to_date'], 'optional': []}, 'entrypoints': {'header': 'SELECT 1 AS h1', 'rows': 'SELECT 1 AS r1', 'totals': 'SELECT 1 AS t1'}, 'needs_user_fix': needs, 'notes': '', 'invalid': invalid, 'dry_run': None, 'summary': {'mock': True}}
  - L94 expr write_json_atomic(meta_path, meta_payload, ensure_ascii=False, indent=2)
  - L96 expr write_artifact_manifest(template_dir, step='generator_assets_v1_test', files={'contract.json': contract_path, 'sql_pack.sql': sql_path, 'output_schemas.json': output_schemas_path, 'generator_assets.json': meta_path}, inputs=['test'], correlation_id=None)
  - L109 return {'artifacts': {'contract': contract_path, 'sql_pack': sql_path, 'output_schemas': output_schemas_path, 'generator_assets': meta_path}, 'needs_user_fix': needs, 'invalid': invalid, 'dialect': dialect, 'params': meta_payload['params'], 'dry_run': None, 'cached': False, 'summary': meta_payload['summary']}
- L126 def _parse_eventsresponse:
  - L127 assign lines = [line for line in response.content.decode('utf-8').splitlines() if line.strip()]
  - L128 return [json.loads(line) for line in lines]
- L131 def _request_payload:
  - L132 return {'step4_output': {'contract': {'tokens': {'scalars': ['h1'], 'row_tokens': ['r1'], 'totals': ['t1']}}, 'overview_md': '# Overview', 'step5_requirements': {}}, 'final_template_html': '<html>{h1}</html>', 'reference_pdf_image': None, 'catalog': ['batches.h1'], 'dialect': 'duckdb', 'params': ['from_date', 'to_date'], 'sample_params': {'from_date': '2025-01-01', 'to_date': '2025-01-31'}, 'force_rebuild': False}
- L148 def test_generator_assets_v1_happy_pathtmp_path, client, monkeypatch:
  - L149 assign template_id = '00000000-0000-0000-0000-000000000002'
  - L150 assign template_dir = _setup_template_dir(api.UPLOAD_ROOT, template_id)
  - L152 def fake_build*, template_dir: Path, dialect: str, **kwargs: Any:
    - L153 return _fake_service(template_dir, dialect=dialect, invalid=False, needs=[])
  - L155 expr monkeypatch.setattr(api, 'build_generator_assets_from_payload', fake_build)
  - L157 assign response = client.post(f'/templates/{template_id}/generator-assets/v1', json=_request_payload())
  - L158 assert response.status_code == 200
  - L159 assign events = _parse_events(response)
  - L160 assign result_event = next((evt for evt in events if evt.get('event') == 'result'))
  - L161 assign data = result_event
  - L162 assert data['invalid'] is False
  - L163 assert data['needs_user_fix'] == []
  - L164 assert data['dialect'] == 'duckdb'
  - L165 assert data['params']['required'] == ['from_date', 'to_date']
  - L167 assert data['artifacts'].get('contract')
  - L168 assign generator_dir = template_dir / 'generator'
  - L169 assert (generator_dir / 'sql_pack.sql').exists()
  - L170 assert (generator_dir / 'output_schemas.json').exists()
  - L171 assert (generator_dir / 'generator_assets.json').exists()
  - L172 assert (template_dir / 'contract.json').exists()
  - L173 assign manifest_path = template_dir / 'artifact_manifest.json'
  - L174 assert manifest_path.exists()
  - L175 assign manifest = json.loads(manifest_path.read_text(encoding='utf-8'))
  - L176 assert 'contract.json' in manifest['files']
  - L177 assert 'sql_pack.sql' in manifest['files']
  - L178 assign templates_state = api.state_store.list_templates()
  - L179 assign tpl_record = next((t for t in templates_state if t['id'] == template_id))
  - L180 assert tpl_record['artifacts'].get('contract_url')
  - L181 assert tpl_record['artifacts'].get('generator_sql_pack_url')
  - L182 assign generator_meta = tpl_record.get('generator') or {}
  - L183 assert generator_meta.get('dialect') == 'duckdb'
  - L184 assert generator_meta.get('invalid') is False
  - L185 assert generator_meta.get('needsUserFix') == []
- L188 def test_generator_assets_v1_allowlist_violationtmp_path, client, monkeypatch:
  - L189 assign template_id = '00000000-0000-0000-0000-000000000003'
  - L190 expr _setup_template_dir(api.UPLOAD_ROOT, template_id)
  - L192 def fake_build*, template_dir: Path, dialect: str, **kwargs: Any:
    - L193 return _fake_service(template_dir, dialect=dialect, invalid=True, needs=['catalog_violation:lines.unknown'])
  - L195 expr monkeypatch.setattr(api, 'build_generator_assets_from_payload', fake_build)
  - L197 assign response = client.post(f'/templates/{template_id}/generator-assets/v1', json=_request_payload())
  - L198 assert response.status_code == 200
  - L199 assign events = _parse_events(response)
  - L200 assign result_event = next((evt for evt in events if evt.get('event') == 'result'))
  - L201 assign data = result_event
  - L202 assert data['invalid'] is True
  - L203 assert 'catalog_violation:lines.unknown' in data['needs_user_fix']
  - L204 assign templates_state = api.state_store.list_templates()
  - L205 assign tpl_record = next((t for t in templates_state if t['id'] == template_id))
  - L206 assign generator_meta = tpl_record.get('generator') or {}
  - L207 assert generator_meta.get('invalid') is True
  - L208 assert any(('catalog_violation:lines.unknown' in item for item in generator_meta.get('needsUserFix') or []))
- L211 def test_generator_assets_v1_missing_contractclient:
  - L212 assign template_id = '00000000-0000-0000-0000-000000000004'
  - L213 expr _setup_template_dir(api.UPLOAD_ROOT, template_id)
  - L214 assign response = client.post(f'/templates/{template_id}/generator-assets/v1', json={'final_template_html': '<html></html>'})
  - L218 assert response.status_code == 422

## backend\tests\test_generator_assets_validation.py
- L1 from __future__ import annotations
- L3 import base64
- L4 import json
- L5 from pathlib import Path
- L6 from typing import Any
- L8 import pytest
- L10 from backend.app.services.generator import GeneratorAssetsV1 as generator_assets
- L11 from backend.app.services.utils.validation import SchemaValidationError, validate_contract_v2, validate_generator_output_schemas, validate_generator_sql_pack
- L18 assign PNG_BYTES = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGMAAQAABQABDQottAAAAABJRU5ErkJggg==')
- L23 def _write_pngpath: Path:
  - L24 expr path.write_bytes(PNG_BYTES)
- L27 def _make_contracttokens: dict[str, list[str]]:
  - L28 annotated assign tokens_copy: dict[str, Any] = {key: list(value) if isinstance(value, list) else value for key, value in tokens.items()}
  - L31 assign row_tokens = list(tokens_copy.get('row_tokens') or [])
  - L32 assign columns = [{'as': token, 'from': [f'lines.{token}']} for token in row_tokens[:2] if isinstance(token, str) and token]
  - L35 if not columns:
    - L36 assign columns = [{'as': 'row_value', 'from': ['lines.row_value']}]
    - L37 if not row_tokens:
      - L38 assign row_tokens = ['row_value']
    - L39 else:
      - L39 if not row_tokens:
        - L40 assign row_tokens = [col['as'] for col in columns]
  - L42 assign tokens_copy['row_tokens'] = row_tokens
  - L43 return {'tokens': tokens_copy, 'mapping': {}, 'join': {'parent_table': 'batches', 'parent_key': 'id', 'child_table': 'lines', 'child_key': 'batch_id'}, 'date_columns': {}, 'filters': {}, 'reshape_rules': [{'purpose': 'rows', 'strategy': 'UNION_ALL', 'columns': columns}], 'row_computed': {}, 'totals_math': {}, 'formatters': {}, 'order_by': {'rows': []}, 'notes': ''}
- L64 class FakeResponse:
  - L65 def __init__self, content: str:
    - L66 assign self.choices = [type('Choice', (), {'message': type('Msg', (), {'content': content})})()]
- L69 def test_validate_generator_sql_pack_schema:
  - L70 assign valid_payload = {'dialect': 'duckdb', 'script': 'SELECT 1;', 'entrypoints': {'header': 'SELECT 1', 'rows': 'SELECT 1', 'totals': 'SELECT 1'}, 'params': {'required': ['from_date', 'to_date'], 'optional': []}, 'notes': ''}
  - L77 expr validate_generator_sql_pack(valid_payload)
  - L79 with pytest.raises(SchemaValidationError):
    - L80 expr validate_generator_sql_pack({'dialect': 'duckdb'})
  - L82 with pytest.raises(SchemaValidationError) as excinfo:
    - L83 assign invalid = dict(valid_payload)
    - L84 assign invalid['dialect'] = 'sqlite'
    - L85 expr validate_generator_sql_pack(invalid)
  - L86 assert 'duckdb' in str(excinfo.value).lower()
- L89 def test_validate_generator_output_schemas_schema:
  - L90 expr validate_generator_output_schemas({'header': [], 'rows': [], 'totals': []})
  - L91 with pytest.raises(SchemaValidationError):
    - L92 expr validate_generator_output_schemas({'header': [], 'rows': []})
- L95 def test_validate_contract_v2_rejects_blank_parent_key:
  - L96 assign contract = _make_contract({'scalars': [], 'row_tokens': [], 'totals': []})
  - L97 assign contract['join']['parent_key'] = ''
  - L98 with pytest.raises(SchemaValidationError) as excinfo:
    - L99 expr validate_contract_v2(contract)
  - L100 assert 'contract.join.parent_key' in str(excinfo.value)
- L103 def test_validate_contract_v2_requires_child_key_when_child_table_present:
  - L104 assign contract = _make_contract({'scalars': [], 'row_tokens': [], 'totals': []})
  - L105 assign contract['join']['child_table'] = 'child_tbl'
  - L106 assign contract['join']['child_key'] = '   '
  - L107 with pytest.raises(SchemaValidationError) as excinfo:
    - L108 expr validate_contract_v2(contract)
  - L109 assert 'contract.join.child_key' in str(excinfo.value)
- L112 def test_build_generator_assets_detects_column_mismatchmonkeypatch, tmp_path:
  - L113 assign template_dir = tmp_path / 'tpl'
  - L114 expr template_dir.mkdir()
  - L115 expr _write_png(template_dir / 'reference_p1.png')
  - L117 assign tokens = {'scalars': ['h1'], 'row_tokens': ['r1'], 'totals': ['t1']}
  - L118 assign contract_blueprint = _make_contract(tokens)
  - L119 assign step4_output = {'contract': contract_blueprint, 'overview_md': '#', 'step5_requirements': {}}
  - L125 def fake_response*args, **kwargs:
    - L126 assign payload = {'contract': step4_output['contract'], 'sql_pack': {'dialect': 'duckdb', 'script': 'SELECT 1 AS h1;\nSELECT 1 AS r1;\nSELECT 1 AS t1;', 'entrypoints': {'header': 'SELECT 1 AS wrong_alias', 'rows': 'SELECT 1 AS r1', 'totals': 'SELECT 1 AS t1'}, 'params': {'required': ['from_date', 'to_date'], 'optional': []}, 'notes': ''}, 'output_schemas': {'header': ['h1'], 'rows': ['r1'], 'totals': ['t1']}, 'needs_user_fix': [], 'dry_run': None, 'summary': {}}
    - L144 return FakeResponse(json.dumps(payload, ensure_ascii=False))
  - L146 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L147 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L149 assign result = generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=['from_date', 'to_date'])
  - L157 assert result['invalid'] is True
  - L158 assert any(('schema_mismatch' in item for item in result['needs_user_fix']))
- L161 def test_build_generator_assets_accepts_legacy_sql_packmonkeypatch, tmp_path:
  - L162 assign template_dir = tmp_path / 'tpl'
  - L163 expr template_dir.mkdir()
  - L164 expr _write_png(template_dir / 'report_final.png')
  - L166 assign tokens = {'scalars': ['plant_name'], 'row_tokens': ['row_material_name'], 'totals': ['total_set_wt']}
  - L171 assign contract_blueprint = _make_contract(tokens)
  - L172 assign step4_output = {'contract': contract_blueprint, 'overview_md': '#', 'step5_requirements': {}}
  - L178 assign legacy_sql_pack = {'header': "SELECT\n  :plant_name AS plant_name,\n  :location AS location,\n  DATE('now') AS print_date,\n  :from_date AS from_date,\n  :to_date AS to_date,\n  :recipe_code AS recipe_code;", 'rows': "WITH filtered_recipes AS (\n  SELECT *\n  FROM recipes\n  WHERE (:from_date IS NULL OR DATE(start_time) >= :from_date)\n    AND (:to_date IS NULL OR DATE(start_time) <= :to_date)\n    AND (:recipe_code IS NULL OR TRIM(:recipe_code) = '' OR recipe_name = :recipe_code)\n),\nagg AS (\n  SELECT\n    bin1_content AS row_material_name,\n    COALESCE(bin1_sp, 0) AS row_set_wt,\n    COALESCE(bin1_act, 0) AS row_ach_wt\n  FROM filtered_recipes\n  WHERE bin1_content IS NOT NULL AND TRIM(bin1_content) <> ''\n)\nSELECT\n  ROW_NUMBER() OVER (ORDER BY row_material_name ASC) AS row_sl_no,\n  row_material_name,\n  row_set_wt,\n  row_ach_wt,\n  row_ach_wt - row_set_wt AS row_error_kg,\n  CASE WHEN row_set_wt = 0 THEN NULL ELSE (row_ach_wt - row_set_wt) / row_set_wt END AS row_error_pct\nFROM agg\nORDER BY row_material_name ASC;", 'totals': "WITH filtered_recipes AS (\n  SELECT *\n  FROM recipes\n  WHERE (:from_date IS NULL OR DATE(start_time) >= :from_date)\n    AND (:to_date IS NULL OR DATE(start_time) <= :to_date)\n    AND (:recipe_code IS NULL OR TRIM(:recipe_code) = '' OR recipe_name = :recipe_code)\n),\nagg AS (\n  SELECT\n    COALESCE(bin1_sp, 0) AS row_set_wt,\n    COALESCE(bin1_act, 0) AS row_ach_wt\n  FROM filtered_recipes\n)\nSELECT\n  SUM(row_set_wt) AS total_set_wt,\n  SUM(row_ach_wt) AS total_ach_wt,\n  SUM(row_ach_wt) - SUM(row_set_wt) AS total_error_kg,\n  (SUM(row_ach_wt) - SUM(row_set_wt)) / NULLIF(SUM(row_set_wt), 0) AS total_error_pct\nFROM agg;", 'params': ['from_date', 'to_date', 'plant_name', 'location', 'recipe_code']}
  - L231 assign legacy_payload = {'contract': step4_output['contract'], 'sql_pack': legacy_sql_pack, 'output_schemas': {'header': ['plant_name', 'location', 'print_date', 'from_date', 'to_date', 'recipe_code'], 'rows': ['row_sl_no', 'row_material_name', 'row_set_wt', 'row_ach_wt', 'row_error_kg', 'row_error_pct'], 'totals': ['total_set_wt', 'total_ach_wt', 'total_error_kg', 'total_error_pct']}, 'needs_user_fix': [], 'dry_run': None, 'summary': {}}
  - L244 def fake_response*args, **kwargs:
    - L245 return FakeResponse(json.dumps(legacy_payload, ensure_ascii=False))
  - L247 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L248 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L250 assign result = generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=['from_date', 'to_date', 'plant_name', 'location', 'recipe_code'])
  - L259 assert result['invalid'] is False
  - L260 assert result['needs_user_fix'] == []
  - L261 assert result['dialect'] == 'duckdb'
  - L262 assert result['params']['required'] == ['from_date', 'to_date', 'plant_name', 'location', 'recipe_code']
  - L263 assert result['params']['optional'] == []
- L266 def test_normalise_sql_pack_fills_missing_entrypointsmonkeypatch, tmp_path:
  - L267 assign template_dir = tmp_path / 'tpl'
  - L268 expr template_dir.mkdir()
  - L269 expr _write_png(template_dir / 'report_final.png')
  - L271 assign tokens = {'scalars': [], 'row_tokens': [], 'totals': []}
  - L272 assign contract_blueprint = _make_contract(tokens)
  - L273 assign step4_output = {'contract': contract_blueprint, 'overview_md': '#', 'step5_requirements': {}}
  - L279 assign payload_with_empty_entrypoints = {'contract': step4_output['contract'], 'sql_pack': {'script': 'SELECT 1 AS col;', 'entrypoints': {}, 'params': {'required': [], 'optional': []}}, 'output_schemas': {'header': [], 'rows': [], 'totals': []}, 'needs_user_fix': [], 'dry_run': None, 'summary': {}}
  - L292 def fake_response*args, **kwargs:
    - L293 return FakeResponse(json.dumps(payload_with_empty_entrypoints, ensure_ascii=False))
  - L295 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L296 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L298 assign result = generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=[])
  - L307 assert result['invalid'] is False
- L310 def test_generator_assets_requires_step5_contractmonkeypatch, tmp_path:
  - L311 assign template_dir = tmp_path / 'tpl'
  - L312 expr template_dir.mkdir()
  - L313 expr _write_png(template_dir / 'report_final.png')
  - L315 assign step4_output = {'contract': _make_contract({'scalars': ['h1'], 'row_tokens': ['r1'], 'totals': ['t1']}), 'overview_md': '#', 'step5_requirements': {}}
  - L321 assign payload_without_contract = {'sql_pack': {'script': 'SELECT 1 AS h1;', 'entrypoints': {'header': 'SELECT 1 AS h1', 'rows': 'SELECT 1 AS r1', 'totals': 'SELECT 1 AS t1'}, 'params': {'required': [], 'optional': []}}, 'output_schemas': {'header': ['h1'], 'rows': ['r1'], 'totals': ['t1']}, 'needs_user_fix': [], 'summary': {}}
  - L332 def fake_response*args, **kwargs:
    - L333 return FakeResponse(json.dumps(payload_without_contract, ensure_ascii=False))
  - L335 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L336 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L338 with pytest.raises(generator_assets.GeneratorAssetsError) as excinfo:
    - L339 expr generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=[])
  - L347 assert 'contract payload' in str(excinfo.value)
- L350 def test_generator_assets_validates_step5_contractmonkeypatch, tmp_path:
  - L351 assign template_dir = tmp_path / 'tpl'
  - L352 expr template_dir.mkdir()
  - L353 expr _write_png(template_dir / 'report_final.png')
  - L355 assign good_contract = _make_contract({'scalars': ['h1'], 'row_tokens': ['r1'], 'totals': ['t1']})
  - L356 assign bad_contract = dict(good_contract)
  - L357 assign bad_contract['reshape_rules'] = [{'purpose': 'rows', 'strategy': 'UNION_ALL', 'columns': []}]
  - L359 assign step4_output = {'contract': good_contract, 'overview_md': '#', 'step5_requirements': {}}
  - L365 assign payload_with_bad_contract = {'contract': bad_contract, 'sql_pack': {'script': 'SELECT 1 AS h1;', 'entrypoints': {'header': 'SELECT 1 AS h1', 'rows': 'SELECT 1 AS r1', 'totals': 'SELECT 1 AS t1'}, 'params': {'required': [], 'optional': []}}, 'output_schemas': {'header': ['h1'], 'rows': ['r1'], 'totals': ['t1']}, 'needs_user_fix': [], 'summary': {}}
  - L377 def fake_response*args, **kwargs:
    - L378 return FakeResponse(json.dumps(payload_with_bad_contract, ensure_ascii=False))
  - L380 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L381 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L383 with pytest.raises(generator_assets.GeneratorAssetsError) as excinfo:
    - L384 expr generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=[])
  - L392 assert 'Generator contract failed validation' in str(excinfo.value)
- L395 def test_generator_assets_backfills_row_order_and_purposemonkeypatch, tmp_path:
  - L396 assign template_dir = tmp_path / 'tpl'
  - L397 expr template_dir.mkdir()
  - L398 expr _write_png(template_dir / 'report_final.png')
  - L400 assign tokens = {'scalars': [], 'row_tokens': ['row_material_name'], 'totals': []}
  - L401 assign contract_blueprint = _make_contract(tokens)
  - L402 assign contract_blueprint['order_by'] = {'rows': ['rows.row_material_name ASC']}
  - L403 expr contract_blueprint['reshape_rules'][0].pop('purpose', None)
  - L405 assign step4_output = {'contract': contract_blueprint, 'overview_md': '#', 'step5_requirements': {}}
  - L411 assign response_payload = {'contract': json.loads(json.dumps(contract_blueprint)), 'sql_pack': {'script': '-- HEADER SELECT --\nSELECT 1;\n-- ROWS SELECT --\nSELECT 1 AS row_material_name;\n-- TOTALS SELECT --\nSELECT 1;', 'entrypoints': {'header': 'SELECT 1', 'rows': 'SELECT 1 AS row_material_name', 'totals': 'SELECT 1'}, 'params': {'required': [], 'optional': []}}, 'output_schemas': {'header': [], 'rows': ['row_material_name'], 'totals': []}, 'needs_user_fix': [], 'summary': {}}
  - L427 def fake_response*args, **kwargs:
    - L428 return FakeResponse(json.dumps(response_payload, ensure_ascii=False))
  - L430 expr monkeypatch.setattr(generator_assets, 'call_chat_completion', lambda *args, **kwargs: fake_response())
  - L431 expr monkeypatch.setattr(generator_assets, 'get_openai_client', lambda: object())
  - L433 assign result = generator_assets.build_generator_assets_from_payload(template_dir=template_dir, step4_output=step4_output, final_template_html='<html></html>', reference_pdf_image=None, catalog_allowlist=None, params_spec=[])
  - L442 assert result['invalid'] is False
  - L444 assign written_contract = json.loads((template_dir / 'contract.json').read_text(encoding='utf-8'))
  - L445 assert written_contract['row_order'] == ['rows.row_material_name ASC']
  - L446 assert written_contract['reshape_rules'][0]['purpose']

## backend\tests\test_html_sanitizer.py
- L1 import os
- L3 expr os.environ.setdefault('OPENAI_API_KEY', 'test-key')
- L5 from backend.app.services.utils.html import sanitize_html
- L8 def test_sanitizer_strips_disallowed_tags:
  - L9 assign html = '<div><script>alert("x")</script><span onclick="evil()">ok</span></div>'
  - L10 assign sanitized = sanitize_html(html)
  - L11 assert '<script' not in sanitized
  - L12 assert 'onclick' not in sanitized
  - L13 assert sanitized.count('<div>') == 1
  - L14 assert '</div>' in sanitized
- L17 def test_sanitizer_preserves_allowed_attributes:
  - L18 assign html = '<table style="width:100%"><tr><td data-name="val">x</td></tr></table>'
  - L19 assign sanitized = sanitize_html(html)
  - L20 assert 'style="width:100%"' in sanitized
  - L21 assert 'data-name="val"' in sanitized
  - L22 assert '<table' in sanitized and '</table>' in sanitized

## backend\tests\test_jobs.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import sys
- L5 import types
- L6 from pathlib import Path
- L8 import pytest
- L9 from fastapi import HTTPException
- L10 from fastapi.testclient import TestClient
- L12 assign fernet_module = types.ModuleType('cryptography.fernet')
- L15 class _DummyFernet:
  - L16 def __init__self, key:
    - L17 assign self.key = key
  - L20 def generate_key:
    - L21 return b'A' * 44
  - L23 def encryptself, payload: bytes:
    - L24 return payload
  - L26 def decryptself, token: bytes:
    - L27 return token
- L30 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L31 expr setattr(fernet_module, 'InvalidToken', Exception)
- L32 assign crypto_module = types.ModuleType('cryptography')
- L33 expr setattr(crypto_module, 'fernet', fernet_module)
- L34 expr sys.modules.setdefault('cryptography', crypto_module)
- L35 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L37 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L39 from .. import api
- L40 from ..app.services.connections import db_connection as db_conn_module
- L41 from ..app.services.state import store as state_store_module
- L45 def fresh_statetmp_path, monkeypatch:
  - L46 assign base_dir = tmp_path / 'state'
  - L47 assign store = state_store_module.StateStore(base_dir=base_dir)
  - L48 assign state_store_module.state_store = store
  - L49 assign api.state_store = store
  - L50 assign db_conn_module.state_store = store
  - L51 assign api.SCHEDULER_DISABLED = True
  - L52 assign api.SCHEDULER = None
  - L54 assign upload_root = tmp_path / 'uploads'
  - L55 expr upload_root.mkdir(parents=True, exist_ok=True)
  - L56 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', upload_root)
  - L57 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', upload_root.resolve())
  - L58 assign excel_root = tmp_path / 'excel-uploads'
  - L59 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L60 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L61 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L62 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (upload_root.resolve(), '/uploads'))
  - L63 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L64 return store
- L68 def clientfresh_state:
  - L69 return TestClient(api.app)
- L72 def _seed_templatestore, template_id='tpl-job', name='Job Template', kind='pdf':
  - L73 expr store.upsert_template(template_id, name=name, status='approved', template_type=kind)
- L76 def test_job_creation_and_retrievalclient: TestClient, fresh_state, monkeypatch:
  - L77 expr _seed_template(fresh_state, template_id='tpl-create')
  - L79 annotated assign scheduled_ids: list[str] = []
  - L81 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L82 expr scheduled_ids.append(job_id)
  - L84 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L86 assign payload = {'template_id': 'tpl-create', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'}
  - L91 assign resp = client.post('/jobs/run-report', json=payload)
  - L92 assert resp.status_code == 200
  - L93 assign job_id = resp.json()['job_id']
  - L94 assert job_id
  - L95 assert job_id in scheduled_ids
  - L97 assign detail = client.get(f'/jobs/{job_id}')
  - L98 assert detail.status_code == 200
  - L99 assign job = detail.json()['job']
  - L100 assert job['status'] in {'queued', 'running'}
  - L101 assert job['createdAt'] is not None
  - L102 assert isinstance(job['steps'], list)
  - L104 assign active_resp = client.get('/jobs', params={'active_only': 'true'})
  - L105 assert active_resp.status_code == 200
  - L106 assign active_ids = [item['id'] for item in active_resp.json()['jobs']]
  - L107 assert job_id in active_ids
- L110 def test_job_run_success_records_stepsclient: TestClient, fresh_state, monkeypatch:
  - L111 expr _seed_template(fresh_state, template_id='tpl-success')
  - L113 def fake_run_report_with_emailpayload, *, kind, correlation_id=None, job_tracker=None:
    - L114 if job_tracker:
      - L115 expr job_tracker.step_running('dataLoad', label='Load DB')
      - L116 expr job_tracker.step_succeeded('dataLoad', progress=25)
      - L117 expr job_tracker.step_running('renderPdf', label='Render PDF artifacts')
      - L118 expr job_tracker.step_succeeded('renderPdf', progress=80)
    - L119 return {'ok': True, 'template_id': payload.template_id}
  - L121 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L122 expr api._run_report_job_sync(job_id, payload_data, kind, correlation_id, step_progress)
  - L124 expr monkeypatch.setattr(api, '_run_report_with_email', fake_run_report_with_email)
  - L125 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L127 assign payload = {'template_id': 'tpl-success', 'start_date': '2024-02-01 00:00:00', 'end_date': '2024-02-15 23:59:59'}
  - L132 assign resp = client.post('/jobs/run-report', json=payload)
  - L133 assign job_id = resp.json()['job_id']
  - L135 assign job = client.get(f'/jobs/{job_id}').json()['job']
  - L136 assert job['status'] == 'succeeded'
  - L137 assert job['progress'] == 100
  - L138 assign step_names = {step['name']: step['status'] for step in job['steps']}
  - L139 assert step_names.get('dataLoad') == 'succeeded'
  - L140 assert step_names.get('renderPdf') == 'succeeded'
- L143 def test_job_run_failure_captures_errorclient: TestClient, fresh_state, monkeypatch:
  - L144 expr _seed_template(fresh_state, template_id='tpl-fail')
  - L146 def fake_run_report_with_emailpayload, *, kind, correlation_id=None, job_tracker=None:
    - L147 if job_tracker:
      - L148 expr job_tracker.step_running('dataLoad', label='Load DB')
      - L149 expr job_tracker.step_failed('dataLoad', 'Database unreachable')
    - L150 raise HTTPException(status_code=400, detail={'message': 'Simulated failure'})
  - L152 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L153 expr api._run_report_job_sync(job_id, payload_data, kind, correlation_id, step_progress)
  - L155 expr monkeypatch.setattr(api, '_run_report_with_email', fake_run_report_with_email)
  - L156 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L158 assign payload = {'template_id': 'tpl-fail', 'start_date': '2024-03-01 00:00:00', 'end_date': '2024-03-05 23:59:59'}
  - L163 assign resp = client.post('/jobs/run-report', json=payload)
  - L164 assign job_id = resp.json()['job_id']
  - L166 assign job = client.get(f'/jobs/{job_id}').json()['job']
  - L167 assert job['status'] == 'failed'
  - L168 assert 'Simulated failure' in (job['error'] or '')
  - L169 assign step_names = {step['name']: step['status'] for step in job['steps']}
  - L170 assert step_names.get('dataLoad') == 'failed'
- L173 def test_job_listing_filters_respect_status_and_typeclient: TestClient, fresh_state:
  - L174 assign queued = fresh_state.create_job(job_type='run_report', template_id='tpl-queued', template_name='Queued Template', template_kind='pdf')
  - L180 assign running = fresh_state.create_job(job_type='run_report', template_id='tpl-running', template_name='Running Template', template_kind='pdf')
  - L186 expr fresh_state.record_job_start(running['id'])
  - L187 assign succeeded = fresh_state.create_job(job_type='run_report', template_id='tpl-done', template_name='Done Template', template_kind='pdf')
  - L193 expr fresh_state.record_job_start(succeeded['id'])
  - L194 expr fresh_state.record_job_completion(succeeded['id'], status='succeeded', result={'ok': True})
  - L195 assign verify_job = fresh_state.create_job(job_type='verify', template_id='tpl-verify', template_name='Verify Template', template_kind='pdf')
  - L201 expr fresh_state.record_job_completion(verify_job['id'], status='failed', error='verify failed')
  - L203 assign running_only = client.get('/jobs', params={'status': 'running'}).json()['jobs']
  - L204 assert {job['id'] for job in running_only} == {running['id']}
  - L206 assign type_filtered = client.get('/jobs', params={'type': 'verify'}).json()['jobs']
  - L207 assert {job['id'] for job in type_filtered} == {verify_job['id']}
  - L209 assign limited = client.get('/jobs', params={'limit': 1}).json()['jobs']
  - L210 assert len(limited) == 1
  - L212 assign active_only = client.get('/jobs', params={'active_only': 'true'}).json()['jobs']
  - L213 assert {job['id'] for job in active_only} == {queued['id'], running['id']}
- L216 def test_batch_job_creation_and_active_listingclient: TestClient, fresh_state, monkeypatch:
  - L217 expr _seed_template(fresh_state, template_id='tpl-batch-1', name='Batch 1')
  - L218 expr _seed_template(fresh_state, template_id='tpl-batch-2', name='Batch 2')
  - L220 annotated assign scheduled: list[dict] = []
  - L222 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L223 expr scheduled.append({'job_id': job_id, 'payload': payload_data, 'correlation_id': correlation_id, 'kind': kind, 'step_progress': step_progress})
  - L233 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L235 assign payloads = [{'template_id': 'tpl-batch-1', 'start_date': '2024-04-01 00:00:00', 'end_date': '2024-04-02 00:00:00'}, {'template_id': 'tpl-batch-2', 'start_date': '2024-04-03 00:00:00', 'end_date': '2024-04-04 00:00:00'}]
  - L239 assign resp = client.post('/jobs/run-report', json=payloads)
  - L240 assert resp.status_code == 200
  - L241 assign data = resp.json()
  - L242 assert set(data['job_ids']) == {entry['job_id'] for entry in scheduled}
  - L243 assert data['job_id'] == data['job_ids'][0]
  - L244 assert data['count'] == 2
  - L246 assign active_jobs = client.get('/jobs', params={'active_only': 'true'}).json()['jobs']
  - L247 assign active_ids = {job['id'] for job in active_jobs}
  - L248 for entry in scheduled:
    - L249 assert entry['job_id'] in active_ids
    - L250 assign job = fresh_state.get_job(entry['job_id'])
    - L251 assert job['templateId'] in {'tpl-batch-1', 'tpl-batch-2'}
    - L252 assert len(job['steps']) > 0
    - L253 assert job['status'] in {'queued', 'running'}
- L256 def test_batch_jobs_record_steps_per_templateclient: TestClient, fresh_state, monkeypatch:
  - L257 expr _seed_template(fresh_state, template_id='tpl-batch-a', name='Batch A')
  - L258 expr _seed_template(fresh_state, template_id='tpl-batch-b', name='Batch B')
  - L260 def fake_run_report_with_emailpayload, *, kind, correlation_id=None, job_tracker=None:
    - L261 if job_tracker:
      - L262 expr job_tracker.step_running('dataLoad', label=f'Load DB for {payload.template_id}')
    - L263 if payload.template_id.endswith('a'):
      - L264 if job_tracker:
        - L265 expr job_tracker.step_succeeded('dataLoad', progress=20)
        - L266 expr job_tracker.step_running('renderPdf', label='Render PDF artifacts')
        - L267 expr job_tracker.step_succeeded('renderPdf', progress=90)
      - L268 return {'ok': True, 'template_id': payload.template_id}
    - L269 if job_tracker:
      - L270 expr job_tracker.step_failed('dataLoad', f'DB missing for {payload.template_id}')
    - L271 raise HTTPException(status_code=400, detail={'message': f'DB missing {payload.template_id}'})
  - L273 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L274 expr api._run_report_job_sync(job_id, payload_data, kind, correlation_id, step_progress)
  - L276 expr monkeypatch.setattr(api, '_run_report_with_email', fake_run_report_with_email)
  - L277 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L279 assign payloads = [{'template_id': 'tpl-batch-a', 'start_date': '2024-05-01 00:00:00', 'end_date': '2024-05-02 00:00:00'}, {'template_id': 'tpl-batch-b', 'start_date': '2024-05-03 00:00:00', 'end_date': '2024-05-04 00:00:00'}]
  - L283 assign resp = client.post('/jobs/run-report', json=payloads)
  - L284 assert resp.status_code == 200
  - L285 assign job_ids = resp.json()['job_ids']
  - L286 assert len(job_ids) == 2
  - L288 assign jobs = {fresh_state.get_job(job_id)['templateId']: fresh_state.get_job(job_id) for job_id in job_ids}
  - L289 assign success_job = jobs['tpl-batch-a']
  - L290 assign failure_job = jobs['tpl-batch-b']
  - L292 assert success_job['status'] == 'succeeded'
  - L293 assign success_steps = {step['name']: step for step in success_job['steps']}
  - L294 assert success_steps['dataLoad']['status'] == 'succeeded'
  - L295 assert success_steps['renderPdf']['status'] == 'succeeded'
  - L296 assert success_job['progress'] == 100
  - L298 assert failure_job['status'] == 'failed'
  - L299 assign failure_steps = {step['name']: step for step in failure_job['steps']}
  - L300 assert failure_steps['dataLoad']['status'] == 'failed'
  - L301 assert 'DB missing' in (failure_job['error'] or '')
- L304 def test_cancel_job_force_invokes_force_cancelmonkeypatch, client: TestClient, fresh_state:
  - L305 expr _seed_template(fresh_state, template_id='tpl-cancel')
  - L306 assign job = fresh_state.create_job(job_type='run_report', template_id='tpl-cancel', template_name='Cancelable')
  - L307 assign called = {}
  - L309 class _FakeReportService:
    - L311 def force_cancel_jobjob_id, *, force=False:
      - L312 assign called['job_id'] = job_id
      - L313 assign called['force'] = force
      - L314 return True
  - L316 assign api.report_service = _FakeReportService()
  - L318 assign resp = client.post(f"/jobs/{job['id']}/cancel", params={'force': 'true'})
  - L319 assert resp.status_code == 200
  - L320 assert called == {'job_id': job['id'], 'force': True}
  - L321 assert fresh_state.get_job(job['id'])['status'] == 'cancelled'
- L324 def test_cancel_job_route_marks_cancelled_without_forcemonkeypatch, client: TestClient, fresh_state:
  - L325 expr _seed_template(fresh_state, template_id='tpl-cancel-soft')
  - L326 assign job = fresh_state.create_job(job_type='run_report', template_id='tpl-cancel-soft', template_name='Cancelable')
  - L327 assign called = {}
  - L329 class _FakeReportService:
    - L331 def force_cancel_jobjob_id, *, force=False:
      - L332 assign called['job_id'] = job_id
      - L333 assign called['force'] = force
      - L334 return True
  - L336 assign api.report_service = _FakeReportService()
  - L338 assign resp = client.post(f"/jobs/{job['id']}/cancel")
  - L339 assert resp.status_code == 200
  - L340 assert called == {'job_id': job['id'], 'force': False}
  - L341 assign job_record = fresh_state.get_job(job['id'])
  - L342 assert job_record['status'] == 'cancelled'
  - L343 assert (job_record.get('error') or '').lower().startswith('cancelled')
- L346 def test_report_scheduler_creates_job_for_schedulemonkeypatch, fresh_state, tmp_path:
  - L347 from backend.app.services.jobs import report_scheduler as scheduler_module
  - L349 assign scheduler_module.state_store = fresh_state
  - L350 expr _seed_template(fresh_state, template_id='tpl-schedule', name='Scheduled Template')
  - L351 expr fresh_state.upsert_connection(conn_id='conn-1', name='Connection 1', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload={'token': 'secret'})
  - L358 assign schedule = fresh_state.create_schedule(name='Nightly run', template_id='tpl-schedule', template_name='Scheduled Template', template_kind='pdf', connection_id='conn-1', connection_name='Connection 1', start_date='2024-01-01 00:00:00', end_date='2024-01-02 00:00:00', key_values=None, batch_ids=None, docx=True, xlsx=False, email_recipients=[], email_subject=None, email_message=None, frequency='daily', interval_minutes=60, next_run_at='2024-01-01T00:00:00Z', first_run_at='2024-01-01T00:00:00Z', active=True)
  - L381 annotated assign recorded: list[dict] = []
  - L383 def runnerpayload, kind, job_tracker=None:
    - L384 expr recorded.append({'payload': payload, 'kind': kind, 'job_id': getattr(job_tracker, 'job_id', None)})
    - L385 return {'html_url': '/reports/r.html'}
  - L387 assign scheduler = scheduler_module.ReportScheduler(runner, poll_seconds=1)
  - L388 expr asyncio.run(scheduler._run_schedule(schedule))
  - L390 assign jobs = fresh_state.list_jobs(limit=5)
  - L391 assert len(jobs) == 1
  - L392 assign job = jobs[0]
  - L393 assert job['scheduleId'] == schedule['id']
  - L394 assert job['status'] == 'succeeded'
  - L395 assert recorded and recorded[0]['job_id'] == job['id']
- L398 def test_job_retry_flow_allows_second_successmonkeypatch, client: TestClient, fresh_state:
  - L399 expr _seed_template(fresh_state, template_id='tpl-retry', name='Retry Template')
  - L401 annotated assign attempts: list[str] = []
  - L403 def fake_run_report_with_emailpayload, *, kind, correlation_id=None, job_tracker=None:
    - L404 expr attempts.append(payload.template_id)
    - L405 if len(attempts) == 1:
      - L406 if job_tracker:
        - L407 expr job_tracker.step_failed('renderPdf', 'boom')
      - L408 raise HTTPException(status_code=500, detail={'message': 'boom'})
    - L409 if job_tracker:
      - L410 expr job_tracker.step_succeeded('dataLoad')
      - L411 expr job_tracker.step_succeeded('renderPdf')
    - L412 return {'ok': True}
  - L414 def immediate_schedulejob_id, payload_data, kind, correlation_id, step_progress:
    - L415 expr api._run_report_job_sync(job_id, payload_data, kind, correlation_id, step_progress)
  - L417 expr monkeypatch.setattr(api, '_run_report_with_email', fake_run_report_with_email)
  - L418 expr monkeypatch.setattr(api, '_schedule_report_job', immediate_schedule)
  - L420 assign payload = {'template_id': 'tpl-retry', 'start_date': '2024-06-01 00:00:00', 'end_date': '2024-06-02 00:00:00'}
  - L426 assign first_resp = client.post('/jobs/run-report', json=payload)
  - L427 assign first_job_id = first_resp.json()['job_id']
  - L428 assign first_job = fresh_state.get_job(first_job_id)
  - L429 assert first_job['status'] == 'failed'
  - L430 assert attempts == ['tpl-retry']
  - L432 assign second_resp = client.post('/jobs/run-report', json=payload)
  - L433 assign second_job_id = second_resp.json()['job_id']
  - L434 assign second_job = fresh_state.get_job(second_job_id)
  - L435 assert second_job['status'] == 'succeeded'
  - L436 assert attempts == ['tpl-retry', 'tpl-retry']

## backend\tests\test_llm_call_3_5_invariants.py
- L1 import pytest
- L3 from backend.app.services.mapping.CorrectionsPreview import CorrectionsPreviewError, _ensure_invariants
- L8 assign ORIGINAL_HTML = '<!doctype html>\n<html>\n  <body>\n    <h1>{report_title}</h1>\n    <!--BEGIN:BLOCK_REPEAT line-->\n    <table data-region="rows">\n      <tbody>\n        <tr><td>{row_value}</td></tr>\n      </tbody>\n    </table>\n    <!--END:BLOCK_REPEAT line-->\n  </body>\n</html>'
- L23 def test_invariants_allow_constant_removal:
  - L24 assign final_html = ORIGINAL_HTML.replace('{report_title}', 'Consumption Report')
  - L25 expr _ensure_invariants(original_html=ORIGINAL_HTML, final_html=final_html, additional_constants=[{'token': 'report_title', 'value': 'Consumption Report'}], sample_values={'row_value': '42'})
- L33 def test_invariants_fail_token_rename:
  - L34 assign final_html = ORIGINAL_HTML.replace('{report_title}', '{report_title_v2}')
  - L35 with pytest.raises(CorrectionsPreviewError):
    - L36 expr _ensure_invariants(original_html=ORIGINAL_HTML, final_html=final_html, additional_constants=[], sample_values={})
- L44 def test_invariants_fail_sample_leak:
  - L45 assign final_html = ORIGINAL_HTML.replace('{report_title}', 'Consumption Report').replace('{row_value}', '42')
  - L46 with pytest.raises(CorrectionsPreviewError):
    - L47 expr _ensure_invariants(original_html=ORIGINAL_HTML, final_html=final_html, additional_constants=[{'token': 'report_title', 'value': 'Consumption Report'}], sample_values={'row_value': '42'})

## backend\tests\test_llm_call_3_5_schema.py
- L1 import pytest
- L3 from backend.app.services.utils.validation import SchemaValidationError, validate_llm_call_3_5
- L9 def _sample_payload:
  - L10 return {'final_template_html': '<html><body><h1>{report_title}</h1></body></html>', 'page_summary': 'Header titled {report_title} followed by a table of {row_value}.'}
- L16 def test_validate_llm_call_3_5_schema_happy_path:
  - L17 assign payload = _sample_payload()
  - L18 expr validate_llm_call_3_5(payload)
- L21 def test_validate_llm_call_3_5_schema_missing_required:
  - L22 assign payload = _sample_payload()
  - L23 expr payload.pop('final_template_html')
  - L24 with pytest.raises(SchemaValidationError):
    - L25 expr validate_llm_call_3_5(payload)

## backend\tests\test_llm_call_3_validation.py
- L1 syntax_error: invalid non-printable character U+FEFF

## backend\tests\test_locking.py
- L1 import os
- L2 import threading
- L3 import time
- L4 from pathlib import Path
- L6 expr os.environ.setdefault('OPENAI_API_KEY', 'test-key')
- L8 from backend.app.services.utils.lock import acquire_template_lock
- L11 def test_acquire_template_lock_non_blockingtmp_path: Path:
  - L12 assign tdir = tmp_path / 'template'
  - L13 expr tdir.mkdir(parents=True, exist_ok=True)
  - L15 with acquire_template_lock(tdir, 'mapping'):
    - L16 with acquire_template_lock(tdir, 'mapping'):
      - L17 pass
  - L18 assert not list(tdir.glob('.lock.*'))
- L21 def test_acquire_template_lock_allows_releasetmp_path: Path:
  - L22 assign tdir = tmp_path / 'template'
  - L23 expr tdir.mkdir(parents=True, exist_ok=True)
  - L25 assign first_ready = threading.Event()
  - L26 assign second_started = threading.Event()
  - L27 assign results = []
  - L29 def first:
    - L30 with acquire_template_lock(tdir, 'run'):
      - L31 expr first_ready.set()
      - L32 expr second_started.wait(timeout=1)
      - L33 expr time.sleep(0.1)
      - L34 expr results.append('first')
  - L36 def second:
    - L37 expr first_ready.wait(timeout=1)
    - L38 with acquire_template_lock(tdir, 'run'):
      - L39 expr results.append('second')
    - L40 expr second_started.set()
  - L42 assign t1 = threading.Thread(target=first)
  - L43 assign t2 = threading.Thread(target=second)
  - L44 expr t1.start()
  - L45 expr t2.start()
  - L46 expr t1.join()
  - L47 expr t2.join()
  - L49 assert 'first' in results
  - L50 assert 'second' in results
  - L51 assert not list(tdir.glob('.lock.*'))

## backend\tests\test_mailer.py
- L1 docstring: "Comprehensive tests for mailer service."
- L2 from __future__ import annotations
- L4 import os
- L5 import sys
- L6 import types
- L7 from pathlib import Path
- L8 from unittest.mock import MagicMock, patch, PropertyMock
- L10 import pytest
- L13 assign fernet_module = types.ModuleType('cryptography.fernet')
- L16 class _DummyFernet:
  - L17 def __init__self, key:
    - L18 assign self.key = key
  - L21 def generate_key:
    - L22 return b'A' * 44
  - L24 def encryptself, payload: bytes:
    - L25 return payload
  - L27 def decryptself, token: bytes:
    - L28 return token
- L31 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L32 expr setattr(fernet_module, 'InvalidToken', Exception)
- L33 assign crypto_module = types.ModuleType('cryptography')
- L34 expr setattr(crypto_module, 'fernet', fernet_module)
- L35 expr sys.modules.setdefault('cryptography', crypto_module)
- L36 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L38 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L46 class TestMailerConfig:
  - L47 docstring: "Tests for mailer configuration loading."
  - L49 def test_mailer_disabled_by_defaultself, monkeypatch:
    - L50 docstring: "Mailer should be disabled when host/sender not configured."
    - L52 expr monkeypatch.delenv('NEURA_MAIL_HOST', raising=False)
    - L53 expr monkeypatch.delenv('NEURA_MAIL_SENDER', raising=False)
    - L56 from backend.app.services.utils import mailer
    - L57 assign config = mailer._load_mailer_config()
    - L59 assert config.enabled is False
    - L60 assert config.host is None
    - L61 assert config.sender is None
  - L63 def test_mailer_enabled_with_host_and_senderself, monkeypatch:
    - L64 docstring: "Mailer should be enabled when both host and sender are set."
    - L65 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L66 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L68 from backend.app.services.utils import mailer
    - L69 assign config = mailer._load_mailer_config()
    - L71 assert config.enabled is True
    - L72 assert config.host == 'smtp.example.com'
    - L73 assert config.sender == 'noreply@example.com'
  - L75 def test_mailer_config_defaultsself, monkeypatch:
    - L76 docstring: "Mailer should use sensible defaults."
    - L77 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L78 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L79 expr monkeypatch.delenv('NEURA_MAIL_PORT', raising=False)
    - L80 expr monkeypatch.delenv('NEURA_MAIL_USE_TLS', raising=False)
    - L82 from backend.app.services.utils import mailer
    - L83 assign config = mailer._load_mailer_config()
    - L85 assert config.port == 587
    - L86 assert config.use_tls is True
  - L88 def test_mailer_config_custom_portself, monkeypatch:
    - L89 docstring: "Custom port should be respected."
    - L90 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L91 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L92 expr monkeypatch.setenv('NEURA_MAIL_PORT', '465')
    - L94 from backend.app.services.utils import mailer
    - L95 assign config = mailer._load_mailer_config()
    - L97 assert config.port == 465
  - L99 def test_mailer_config_invalid_port_uses_defaultself, monkeypatch:
    - L100 docstring: "Invalid port should fall back to default."
    - L101 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L102 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L103 expr monkeypatch.setenv('NEURA_MAIL_PORT', 'invalid')
    - L105 from backend.app.services.utils import mailer
    - L106 assign config = mailer._load_mailer_config()
    - L108 assert config.port == 587
  - L110 def test_mailer_tls_disabledself, monkeypatch:
    - L111 docstring: "TLS can be disabled."
    - L112 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L113 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L114 expr monkeypatch.setenv('NEURA_MAIL_USE_TLS', 'false')
    - L116 from backend.app.services.utils import mailer
    - L117 assign config = mailer._load_mailer_config()
    - L119 assert config.use_tls is False
  - L121 def test_refresh_mailer_configself, monkeypatch:
    - L122 docstring: "Config refresh should pick up new env vars."
    - L123 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L124 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L126 from backend.app.services.utils import mailer
    - L127 assign config1 = mailer.refresh_mailer_config()
    - L128 assert config1.host == 'smtp.example.com'
    - L131 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.newhost.com')
    - L132 assign config2 = mailer.refresh_mailer_config()
    - L133 assert config2.host == 'smtp.newhost.com'
- L141 class TestRecipientNormalization:
  - L142 docstring: "Tests for email recipient normalization."
  - L144 def test_normalize_empty_returns_emptyself:
    - L145 docstring: "Empty input should return empty list."
    - L146 from backend.app.services.utils.mailer import _normalize_recipients
    - L148 assert _normalize_recipients(None) == []
    - L149 assert _normalize_recipients([]) == []
    - L150 assert _normalize_recipients(['', '  ']) == []
  - L152 def test_normalize_removes_duplicatesself:
    - L153 docstring: "Duplicates should be removed."
    - L154 from backend.app.services.utils.mailer import _normalize_recipients
    - L156 assign result = _normalize_recipients(['user@example.com', 'user@example.com', 'other@example.com'])
    - L161 assert len(result) == 2
    - L162 assert 'user@example.com' in result
    - L163 assert 'other@example.com' in result
  - L165 def test_normalize_trims_whitespaceself:
    - L166 docstring: "Whitespace should be trimmed."
    - L167 from backend.app.services.utils.mailer import _normalize_recipients
    - L169 assign result = _normalize_recipients(['  user@example.com  ', '\tother@example.com\n'])
    - L173 assert result == ['user@example.com', 'other@example.com']
  - L175 def test_normalize_handles_none_valuesself:
    - L176 docstring: "None values in list should be handled."
    - L177 from backend.app.services.utils.mailer import _normalize_recipients
    - L179 assign result = _normalize_recipients(['user@example.com', None, 'other@example.com'])
    - L184 assert len(result) == 2
- L192 class TestSendReportEmail:
  - L193 docstring: "Tests for send_report_email function."
  - L196 def enabled_mailerself, monkeypatch:
    - L197 docstring: "Configure an enabled mailer."
    - L198 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L199 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L200 expr monkeypatch.setenv('NEURA_MAIL_USERNAME', 'testuser')
    - L201 expr monkeypatch.setenv('NEURA_MAIL_PASSWORD', 'testpass')
    - L203 from backend.app.services.utils import mailer
    - L204 expr mailer.refresh_mailer_config()
    - L205 return mailer
  - L207 def test_send_fails_without_recipientsself, enabled_mailer, monkeypatch:
    - L208 docstring: "Send should fail without recipients."
    - L209 assign result = enabled_mailer.send_report_email(to_addresses=[], subject='Test Subject', body='Test Body')
    - L214 assert result is False
  - L216 def test_send_fails_when_disabledself, monkeypatch:
    - L217 docstring: "Send should fail when mailer is disabled."
    - L218 expr monkeypatch.delenv('NEURA_MAIL_HOST', raising=False)
    - L219 expr monkeypatch.delenv('NEURA_MAIL_SENDER', raising=False)
    - L221 from backend.app.services.utils import mailer
    - L222 expr mailer.refresh_mailer_config()
    - L224 assign result = mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body')
    - L229 assert result is False
  - L231 def test_send_with_tlsself, enabled_mailer, monkeypatch:
    - L232 docstring: "Send with TLS should use STARTTLS."
    - L233 assign mock_smtp = MagicMock()
    - L234 assign mock_smtp.__enter__ = MagicMock(return_value=mock_smtp)
    - L235 assign mock_smtp.__exit__ = MagicMock(return_value=False)
    - L237 with patch('smtplib.SMTP', return_value=mock_smtp) as smtp_class:
      - L238 assign result = enabled_mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body')
      - L245 expr smtp_class.assert_called_once()
      - L247 expr mock_smtp.starttls.assert_called_once()
      - L249 expr mock_smtp.login.assert_called_once_with('testuser', 'testpass')
      - L251 expr mock_smtp.send_message.assert_called_once()
  - L253 def test_send_without_tlsself, monkeypatch:
    - L254 docstring: "Send without TLS should skip STARTTLS."
    - L255 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L256 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L257 expr monkeypatch.setenv('NEURA_MAIL_USE_TLS', 'false')
    - L258 expr monkeypatch.setenv('NEURA_MAIL_USERNAME', 'testuser')
    - L259 expr monkeypatch.setenv('NEURA_MAIL_PASSWORD', 'testpass')
    - L261 from backend.app.services.utils import mailer
    - L262 expr mailer.refresh_mailer_config()
    - L264 assign mock_smtp = MagicMock()
    - L265 assign mock_smtp.__enter__ = MagicMock(return_value=mock_smtp)
    - L266 assign mock_smtp.__exit__ = MagicMock(return_value=False)
    - L268 with patch('smtplib.SMTP', return_value=mock_smtp):
      - L269 assign result = mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body')
      - L276 expr mock_smtp.starttls.assert_not_called()
  - L278 def test_send_handles_smtp_errorself, enabled_mailer, monkeypatch:
    - L279 docstring: "Send should handle SMTP errors gracefully."
    - L280 with patch('smtplib.SMTP', side_effect=Exception('Connection refused')):
      - L281 assign result = enabled_mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body')
      - L286 assert result is False
  - L288 def test_send_with_attachmentsself, enabled_mailer, tmp_path, monkeypatch:
    - L289 docstring: "Send with attachments should include them."
    - L291 assign test_file = tmp_path / 'report.pdf'
    - L292 expr test_file.write_bytes(b'%PDF-1.4 test content')
    - L294 assign mock_smtp = MagicMock()
    - L295 assign mock_smtp.__enter__ = MagicMock(return_value=mock_smtp)
    - L296 assign mock_smtp.__exit__ = MagicMock(return_value=False)
    - L298 with patch('smtplib.SMTP', return_value=mock_smtp):
      - L299 assign result = enabled_mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body', attachments=[test_file])
      - L307 expr mock_smtp.send_message.assert_called_once()
      - L308 assign call_args = mock_smtp.send_message.call_args
      - L309 assign message = call_args[0][0]
      - L311 assert message.is_multipart() or message.get_content_type() == 'text/plain'
  - L313 def test_send_with_missing_attachment_continuesself, enabled_mailer, tmp_path, monkeypatch:
    - L314 docstring: "Send should continue if attachment file is missing."
    - L315 assign missing_file = tmp_path / 'nonexistent.pdf'
    - L317 assign mock_smtp = MagicMock()
    - L318 assign mock_smtp.__enter__ = MagicMock(return_value=mock_smtp)
    - L319 assign mock_smtp.__exit__ = MagicMock(return_value=False)
    - L321 with patch('smtplib.SMTP', return_value=mock_smtp):
      - L322 assign result = enabled_mailer.send_report_email(to_addresses=['user@example.com'], subject='Test Subject', body='Test Body', attachments=[missing_file])
      - L330 expr mock_smtp.send_message.assert_called_once()
- L338 class TestNotificationStrategy:
  - L339 docstring: "Tests for the notification strategy integration."
  - L341 def test_notification_strategy_uses_mailerself, monkeypatch:
    - L342 docstring: "NotificationStrategy should call send_report_email."
    - L343 expr monkeypatch.setenv('NEURA_MAIL_HOST', 'smtp.example.com')
    - L344 expr monkeypatch.setenv('NEURA_MAIL_SENDER', 'noreply@example.com')
    - L346 from backend.app.domain.reports.strategies import NotificationStrategy
    - L348 assign strategy = NotificationStrategy()
    - L350 with patch('backend.app.domain.reports.strategies.send_report_email', return_value=True) as mock_send:
      - L354 assign result = strategy.send(recipients=['user@example.com'], subject='Test Report', body='Report content', attachments=[])
      - L361 assert result is True
      - L362 expr mock_send.assert_called_once_with(to_addresses=['user@example.com'], subject='Test Report', body='Report content', attachments=[])
  - L369 def test_notification_strategy_registryself:
    - L370 docstring: "Notification strategy registry should have email strategy."
    - L371 from backend.app.domain.reports.strategies import build_notification_strategy_registry, NotificationStrategy
    - L376 assign registry = build_notification_strategy_registry()
    - L377 assign strategy = registry.resolve('email')
    - L379 assert isinstance(strategy, NotificationStrategy)
- L387 class TestReportServiceEmailIntegration:
  - L388 docstring: "Tests for email integration in report service."
  - L390 def test_maybe_send_email_skips_without_recipientsself, monkeypatch:
    - L391 docstring: "_maybe_send_email should skip without recipients."
    - L392 from src.services.report_service import _maybe_send_email
    - L393 from backend.app.features.generate.schemas.reports import RunPayload
    - L395 assign payload = RunPayload(template_id='test-tpl', start_date='2024-01-01', end_date='2024-01-31', email_recipients=[])
    - L403 expr _maybe_send_email(payload, artifact_paths={}, run_result={}, kind='pdf', correlation_id='test-123')
  - L412 def test_build_job_steps_includes_email_stepself:
    - L413 docstring: "Job steps should include email when recipients provided."
    - L414 from src.services.report_service import _build_job_steps
    - L415 from backend.app.features.generate.schemas.reports import RunPayload
    - L417 assign payload = RunPayload(template_id='test-tpl', start_date='2024-01-01', end_date='2024-01-31', email_recipients=['user@example.com'])
    - L424 assign steps = _build_job_steps(payload, kind='pdf')
    - L425 assign step_names = [s['name'] for s in steps]
    - L427 assert 'email' in step_names
  - L429 def test_build_job_steps_excludes_email_without_recipientsself:
    - L430 docstring: "Job steps should exclude email without recipients."
    - L431 from src.services.report_service import _build_job_steps
    - L432 from backend.app.features.generate.schemas.reports import RunPayload
    - L434 assign payload = RunPayload(template_id='test-tpl', start_date='2024-01-01', end_date='2024-01-31', email_recipients=[])
    - L441 assign steps = _build_job_steps(payload, kind='pdf')
    - L442 assign step_names = [s['name'] for s in steps]
    - L444 assert 'email' not in step_names
- L447 if __name__ == '__main__':
  - L448 expr pytest.main([__file__, '-v'])

## backend\tests\test_manifest_updates.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 import json
- L5 import os
- L6 from pathlib import Path
- L8 import pytest
- L9 from fastapi.testclient import TestClient
- L11 expr os.environ.setdefault('NEURA_ALLOW_MISSING_OPENAI', 'true')
- L13 assign api = importlib.import_module('backend.api')
- L14 assign utils_module = importlib.import_module('backend.app.services.utils')
- L15 assign write_artifact_manifest = utils_module.write_artifact_manifest
- L16 assign write_json_atomic = utils_module.write_json_atomic
- L17 assign contract_test_module = importlib.import_module('backend.tests.test_api_mapping_approve_contract_v2')
- L18 assign PNG_BYTES = contract_test_module.PNG_BYTES
- L19 assign _make_template_dir = contract_test_module._make_template_dir
- L23 def client:
  - L24 return TestClient(api.app)
- L27 def test_manifest_contains_contract_artifactsmonkeypatch, tmp_path, client:
  - L28 assign template_id = '00000000-0000-0000-0000-000000000002'
  - L29 assign uploads_root = tmp_path
  - L30 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', uploads_root)
  - L31 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', uploads_root)
  - L33 assign template_dir = _make_template_dir(uploads_root, template_id)
  - L35 assign db_path = tmp_path / 'db.sqlite'
  - L36 expr db_path.touch()
  - L38 expr monkeypatch.setattr(api, '_db_path_from_payload_or_default', lambda _: db_path)
  - L39 expr monkeypatch.setattr(api, 'get_parent_child_info', lambda _: {'parent table': 'batches', 'child table': 'lines', 'parent_columns': ['batch_id', 'batch_date'], 'child_columns': ['batch_id', 'line_date', 'material', 'qty']})
  - L49 expr monkeypatch.setattr(api, 'compute_db_signature', lambda _: 'sig')
  - L51 def fake_build_or_load_contract_v2*, template_dir: Path, catalog, final_template_html: str, schema, auto_mapping_proposal, mapping_override, user_instructions: str, dialect_hint: str | None, db_signature: str | None=None, key_tokens=None, prompt_builder=None, prompt_version=None:
    - L66 assign contract_path = template_dir / 'contract.json'
    - L67 assign overview_path = template_dir / 'overview.md'
    - L68 assign step5_path = template_dir / 'step5_requirements.json'
    - L69 assign meta_path = template_dir / 'contract_v2_meta.json'
    - L71 assign contract_payload = {'tokens': schema, 'mapping': auto_mapping_proposal.get('mapping', {}), 'join': {'parent_table': 'batches', 'parent_key': 'batch_id', 'child_table': 'lines', 'child_key': 'batch_id'}, 'date_columns': {'batches': 'batch_date', 'lines': 'line_date'}, 'reshape_rules': [], 'row_computed': {}, 'totals_math': {}, 'formatters': {}, 'order_by': {'rows': ['material_name ASC']}, 'filters': {}}
    - L88 expr write_json_atomic(contract_path, contract_payload, ensure_ascii=False, indent=2)
    - L89 expr overview_path.write_text('# Contract Overview', encoding='utf-8')
    - L90 expr write_json_atomic(step5_path, {'datasets': {}, 'parameters': {}, 'transformations': [], 'edge_cases': [], 'dialect_notes': [], 'artifact_expectations': {}}, ensure_ascii=False, indent=2)
    - L103 expr write_json_atomic(meta_path, {'assumptions': [], 'warnings': [], 'validation': {}}, ensure_ascii=False, indent=2)
    - L107 expr write_artifact_manifest(template_dir, step='contract_build_v2_test', files={'contract.json': contract_path, 'overview.md': overview_path, 'step5_requirements.json': step5_path, 'contract_v2_meta.json': meta_path}, inputs=[], correlation_id=None)
    - L120 return {'contract': contract_payload, 'overview_md': '# Contract Overview', 'step5_requirements': {}, 'assumptions': [], 'warnings': [], 'validation': {'unknown_columns': [], 'unknown_tokens': [], 'token_coverage': {}}, 'artifacts': {'contract': contract_path, 'overview': overview_path, 'step5_requirements': step5_path, 'meta': meta_path}, 'meta': {'assumptions': [], 'warnings': [], 'validation': {}}, 'cached': False}
  - L137 expr monkeypatch.setattr(api, 'build_or_load_contract_v2', fake_build_or_load_contract_v2)
  - L138 expr monkeypatch.setattr(api, 'render_html_to_png', lambda *_, **__: None)
  - L140 def fake_panelhtml_path: Path, dest_png: Path, **kwargs:
    - L141 expr dest_png.write_bytes(PNG_BYTES)
    - L142 return dest_png
  - L144 expr monkeypatch.setattr(api, 'render_panel_preview', fake_panel)
  - L146 assign payload = {'mapping': {'report_title': 'batches.title', 'material_name': 'lines.material', 'qty': 'lines.qty', 'total_qty': 'lines.qty'}, 'connection_id': 'test-connection', 'user_values_text': '', 'user_instructions': 'Manifest check instruction.'}
  - L158 assign response = client.post(f'/templates/{template_id}/mapping/approve', json=payload, headers={'x-correlation-id': 'manifest-test'})
  - L163 assert response.status_code == 200
  - L165 assign manifest_path = template_dir / 'artifact_manifest.json'
  - L166 assert manifest_path.exists()
  - L167 assign manifest = json.loads(manifest_path.read_text(encoding='utf-8'))
  - L168 assign files = manifest.get('files', {})
  - L169 assert 'overview.md' in files
  - L170 assert 'step5_requirements.json' in files

## backend\tests\test_mapping_utils.py
- L1 import os
- L2 import unittest
- L3 from unittest import mock
- L5 from backend.app.services.templates import TemplateVerify
- L6 from src.services.mapping import helpers as mapping_helpers
- L9 class NormalizeMappingTests(unittest.TestCase):
  - L10 def test_norm_placeholder_wraps_plain_labelsself:
    - L11 docstring: "Plain headers should be wrapped in curly braces for placeholders."
    - L12 expr self.assertEqual(mapping_helpers.norm_placeholder('Amount Due'), '{Amount Due}')
    - L13 expr self.assertEqual(mapping_helpers.norm_placeholder(' amount '), '{amount}')
  - L15 def test_norm_placeholder_preserves_existing_tokensself:
    - L16 docstring: "Existing {token} or {{token}} placeholders are left untouched."
    - L17 expr self.assertEqual(mapping_helpers.norm_placeholder('{Total}'), '{Total}')
    - L18 expr self.assertEqual(mapping_helpers.norm_placeholder('{{GrandTotal}}'), '{{GrandTotal}}')
  - L20 def test_normalize_mapping_for_autofill_builds_expected_listself:
    - L21 assign mapping = {'Amount': 'orders.total', '   {Name}  ': 'UNRESOLVED', 'Notes': 'INPUT_SAMPLE'}
    - L26 assign normalized = mapping_helpers.normalize_mapping_for_autofill(mapping)
    - L27 expr self.assertEqual(normalized, [{'header': 'Amount', 'placeholder': '{Amount}', 'mapping': 'orders.total'}, {'header': '   {Name}  ', 'placeholder': '{Name}', 'mapping': 'UNRESOLVED'}, {'header': 'Notes', 'placeholder': '{Notes}', 'mapping': 'INPUT_SAMPLE'}])
- L37 class TemplateVerifyClientTests(unittest.TestCase):
  - L38 def tearDownself:
    - L39 assign TemplateVerify._client = None
  - L41 def test_missing_api_key_raisesself:
    - L42 assign TemplateVerify._client = None
    - L43 assign fake_openai = mock.Mock()
    - L44 with mock.patch.object(TemplateVerify, 'OpenAI', fake_openai):
      - L45 with mock.patch.dict(os.environ, {}, clear=True):
        - L46 with self.assertRaisesRegex(RuntimeError, 'OPENAI_API_KEY'):
          - L47 expr TemplateVerify.get_openai_client()
  - L49 def test_client_cached_and_configured_onceself:
    - L50 assign TemplateVerify._client = None
    - L51 assign fake_client = object()
    - L52 assign fake_openai = mock.Mock(return_value=fake_client)
    - L53 with mock.patch.object(TemplateVerify, 'OpenAI', fake_openai):
      - L54 with mock.patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key'}, clear=True):
        - L55 assign first = TemplateVerify.get_openai_client()
        - L56 assign second = TemplateVerify.get_openai_client()
    - L57 expr self.assertIs(first, fake_client)
    - L58 expr self.assertIs(second, fake_client)
    - L59 expr fake_openai.assert_called_once_with(api_key='test-key')
- L62 if __name__ == '__main__':
  - L63 expr unittest.main()

## backend\tests\test_persistence.py
- L1 from __future__ import annotations
- L3 import sqlite3
- L4 import sys
- L5 import types
- L6 import uuid
- L7 from pathlib import Path
- L9 import pytest
- L10 from fastapi.testclient import TestClient
- L12 assign fernet_module = types.ModuleType('cryptography.fernet')
- L15 class _DummyFernet:
  - L16 def __init__self, key:
    - L17 assign self.key = key
  - L20 def generate_key:
    - L21 return b'A' * 44
  - L23 def encryptself, payload: bytes:
    - L24 return payload
  - L26 def decryptself, token: bytes:
    - L27 return token
- L30 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L31 expr setattr(fernet_module, 'InvalidToken', Exception)
- L32 assign crypto_module = types.ModuleType('cryptography')
- L33 expr setattr(crypto_module, 'fernet', fernet_module)
- L34 expr sys.modules.setdefault('cryptography', crypto_module)
- L35 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L37 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L39 from .. import api
- L40 from ..app.services.connections import db_connection as db_conn_module
- L41 from ..app.services.state import store as state_store_module
- L42 from src.services.report_service import _extract_excel_print_scale_from_html
- L46 def fresh_statetmp_path, monkeypatch:
  - L47 assign base_dir = tmp_path / 'state'
  - L48 assign store = state_store_module.StateStore(base_dir=base_dir)
  - L49 assign state_store_module.state_store = store
  - L50 assign api.state_store = store
  - L51 assign db_conn_module.state_store = store
  - L53 assign upload_root = tmp_path / 'uploads'
  - L54 expr upload_root.mkdir(parents=True, exist_ok=True)
  - L55 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', upload_root)
  - L56 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', upload_root.resolve())
  - L57 assign excel_root = tmp_path / 'excel-uploads'
  - L58 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L59 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L60 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L61 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (upload_root.resolve(), '/uploads'))
  - L62 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L63 return store
- L67 def clientfresh_state:
  - L68 return TestClient(api.app)
- L71 def test_bootstrap_returns_persistent_stateclient: TestClient, fresh_state:
  - L72 assign db_path = Path(client.app.root_path) / 'dummy.db'
  - L73 expr db_path.write_text('', encoding='utf-8')
  - L74 assign conn = fresh_state.upsert_connection(conn_id=None, name='sqlite@test.db', db_type='sqlite', database_path=str(db_path), secret_payload={'db_url': f'sqlite:///{db_path}'}, status='connected', latency_ms=12.5)
  - L83 expr fresh_state.record_connection_ping(conn['id'], status='connected', detail='OK', latency_ms=12.5)
  - L85 assign tpl = fresh_state.upsert_template('tpl-1', name='Sample Template', status='approved', artifacts={'template_html_url': '/uploads/tpl-1/template.html'}, connection_id=conn['id'])
  - L92 expr fresh_state.set_last_used(conn['id'], tpl['id'])
  - L94 assign resp = client.get('/state/bootstrap')
  - L95 assert resp.status_code == 200
  - L96 assign payload = resp.json()
  - L97 assert payload['status'] == 'ok'
  - L98 assert payload['connections'][0]['id'] == conn['id']
  - L99 assert payload['connections'][0]['summary'] == db_path.name
  - L100 assert payload['connections'][0]['hasCredentials'] is True
  - L101 assert payload['templates'][0]['id'] == tpl['id']
  - L102 assert payload['last_used']['connection_id'] == conn['id']
  - L103 assert payload['last_used']['template_id'] == tpl['id']
- L106 def test_last_used_endpoint_updates_storeclient: TestClient, fresh_state:
  - L107 assign resp = client.post('/state/last-used', json={'connection_id': 'conn-1', 'template_id': 'tpl-9'})
  - L108 assert resp.status_code == 200
  - L109 assign data = resp.json()['last_used']
  - L110 assert data['connection_id'] == 'conn-1'
  - L111 assert data['template_id'] == 'tpl-9'
  - L112 assign store_state = fresh_state.get_last_used()
  - L113 assert store_state['connection_id'] == 'conn-1'
  - L114 assert store_state['template_id'] == 'tpl-9'
- L117 def test_delete_template_removes_state_and_filesclient: TestClient, fresh_state:
  - L118 assign template_id = str(uuid.uuid4())
  - L119 expr fresh_state.upsert_template(template_id, name='To remove', status='approved', artifacts={'template_html_url': f'/uploads/{template_id}/template.html'})
  - L125 expr fresh_state.set_last_used(None, template_id)
  - L127 assign template_dir = api.UPLOAD_ROOT / template_id
  - L128 expr template_dir.mkdir(parents=True, exist_ok=True)
  - L129 expr (template_dir / 'template.html').write_text('<html/>', encoding='utf-8')
  - L131 assign resp = client.delete(f'/templates/{template_id}')
  - L132 assert resp.status_code == 200
  - L133 assign payload = resp.json()
  - L134 assert payload['status'] == 'ok'
  - L135 assert payload['template_id'] == template_id
  - L136 assert not template_dir.exists()
  - L137 assert fresh_state.get_template_record(template_id) is None
  - L138 assert fresh_state.get_last_used()['template_id'] is None
- L141 def test_delete_template_missing_returns_404client: TestClient:
  - L142 assign template_id = str(uuid.uuid4())
  - L143 assign resp = client.delete(f'/templates/{template_id}')
  - L144 assert resp.status_code == 404
  - L145 assign payload = resp.json()
  - L146 assert payload['status'] == 'error'
  - L147 assert payload['code'] == 'template_not_found'
- L150 def test_reports_run_uses_persisted_connectionclient: TestClient, fresh_state, tmp_path, monkeypatch:
  - L152 assign db_path = tmp_path / 'data.db'
  - L153 expr sqlite3.connect(db_path).close()
  - L155 assign conn = fresh_state.upsert_connection(conn_id=None, name='sqlite@data.db', db_type='sqlite', database_path=str(db_path), secret_payload={'db_url': f'sqlite:///{db_path}'}, status='connected', latency_ms=4.2)
  - L164 expr fresh_state.record_connection_ping(conn['id'], status='connected', detail='OK', latency_ms=4.2)
  - L166 assign template_id = '11111111-1111-1111-1111-111111111111'
  - L167 assign template_dir = api.UPLOAD_ROOT / template_id
  - L168 expr template_dir.mkdir(parents=True, exist_ok=True)
  - L169 expr (template_dir / 'report_final.html').write_text('<html></html>', encoding='utf-8')
  - L170 expr (template_dir / 'contract.json').write_text('{}', encoding='utf-8')
  - L172 expr fresh_state.upsert_template(template_id, name='Run Template', status='approved')
  - L174 expr monkeypatch.setattr(api, 'validate_contract_schema', lambda _: None)
  - L176 from backend.app.services.reports import ReportGenerate as report_generate
  - L178 def fake_fill_and_print**kwargs:
    - L179 expr kwargs['OUT_HTML'].write_text('<html><head><style id="excel-print-sizing">:root { --excel-print-scale: 0.67; }</style></head><body>filled</body></html>', encoding='utf-8')
    - L184 expr kwargs['OUT_PDF'].write_bytes(b'%PDF-1.4\n%fake\n')
  - L186 expr monkeypatch.setattr(report_generate, 'fill_and_print', fake_fill_and_print)
  - L188 def fake_html_to_docxhtml_path, output_path, **kwargs:
    - L189 expr output_path.write_bytes(b'DOCX')
    - L190 return output_path
  - L192 def fake_html_to_xlsxhtml_path, output_path, **kwargs:
    - L193 expr output_path.write_bytes(b'XLSX')
    - L194 return output_path
  - L196 expr monkeypatch.setattr(api, 'html_file_to_docx', fake_html_to_docx)
  - L197 expr monkeypatch.setattr(api, 'html_file_to_xlsx', fake_html_to_xlsx)
  - L199 assign payload = {'template_id': template_id, 'connection_id': conn['id'], 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59', 'docx': True, 'xlsx': True}
  - L207 assign resp = client.post('/reports/run', json=payload)
  - L208 assert resp.status_code == 200
  - L209 assign data = resp.json()
  - L210 assert data['ok'] is True
  - L211 assert Path(api.UPLOAD_ROOT / template_id / Path(data['html_url']).name).exists()
  - L212 assert Path(api.UPLOAD_ROOT / template_id / Path(data['pdf_url']).name).exists()
  - L213 assert data['docx_url'] is not None
  - L214 assert Path(api.UPLOAD_ROOT / template_id / Path(data['docx_url']).name).exists()
  - L215 assert data['xlsx_url'] is not None
  - L216 assert Path(api.UPLOAD_ROOT / template_id / Path(data['xlsx_url']).name).exists()
- L219 def test_extract_excel_print_scale_from_htmltmp_path:
  - L220 assign html_path = tmp_path / 'excel.html'
  - L221 expr html_path.write_text('<html><head><style id="excel-print-sizing">:root { --excel-print-scale: 0.63; }</style></head><body></body></html>', encoding='utf-8')
  - L225 assign value = _extract_excel_print_scale_from_html(html_path)
  - L226 assert value == pytest.approx(0.63, rel=0.01)
- L229 def test_verify_sqlite_materializes_dataframetmp_path:
  - L230 assign db_path = tmp_path / 'verify.db'
  - L231 with sqlite3.connect(db_path) as con:
    - L232 expr con.execute('CREATE TABLE sample (value TEXT)')
    - L233 expr con.execute('INSERT INTO sample (value) VALUES (?)', ('ok',))
  - L236 expr db_conn_module.verify_sqlite(db_path)
- L239 def test_verify_sqlite_missing_filetmp_path:
  - L240 with pytest.raises(FileNotFoundError):
    - L241 expr db_conn_module.verify_sqlite(tmp_path / 'missing.db')

## backend\tests\test_pipeline_integration.py
- L1 docstring: "Integration tests for the complete report generation pipeline.\n\nTests the full..."
- L6 from __future__ import annotations
- L8 import asyncio
- L9 import json
- L10 import sys
- L11 import types
- L12 from pathlib import Path
- L13 from unittest.mock import MagicMock, patch, AsyncMock
- L14 import time
- L16 import pytest
- L19 assign fernet_module = types.ModuleType('cryptography.fernet')
- L22 class _DummyFernet:
  - L23 def __init__self, key:
    - L24 assign self.key = key
  - L27 def generate_key:
    - L28 return b'A' * 44
  - L30 def encryptself, payload: bytes:
    - L31 return payload
  - L33 def decryptself, token: bytes:
    - L34 return token
- L37 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L38 expr setattr(fernet_module, 'InvalidToken', Exception)
- L39 assign crypto_module = types.ModuleType('cryptography')
- L40 expr setattr(crypto_module, 'fernet', fernet_module)
- L41 expr sys.modules.setdefault('cryptography', crypto_module)
- L42 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L44 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L46 from fastapi.testclient import TestClient
- L47 from .. import api
- L48 from ..app.services.state import store as state_store_module
- L52 def fresh_statetmp_path, monkeypatch:
  - L53 docstring: "Create a fresh state store and upload directories for each test."
  - L54 assign base_dir = tmp_path / 'state'
  - L55 assign store = state_store_module.StateStore(base_dir=base_dir)
  - L56 assign state_store_module.state_store = store
  - L57 assign api.state_store = store
  - L60 assign api.SCHEDULER_DISABLED = True
  - L61 assign api.SCHEDULER = None
  - L64 assign upload_root = tmp_path / 'uploads'
  - L65 expr upload_root.mkdir(parents=True, exist_ok=True)
  - L66 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', upload_root)
  - L67 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', upload_root.resolve())
  - L69 assign excel_root = tmp_path / 'excel-uploads'
  - L70 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L71 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L72 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L74 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (upload_root.resolve(), '/uploads'))
  - L75 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L77 return store
- L81 def clientfresh_state:
  - L82 docstring: "Create test client."
  - L83 return TestClient(api.app)
- L87 def sample_dbtmp_path:
  - L88 docstring: "Create a sample SQLite database for testing."
  - L89 import sqlite3
  - L91 assign db_path = tmp_path / 'sample.db'
  - L92 assign conn = sqlite3.connect(str(db_path))
  - L93 assign cursor = conn.cursor()
  - L96 expr cursor.execute('\n        CREATE TABLE reports (\n            id INTEGER PRIMARY KEY,\n            date TEXT,\n            value REAL,\n            category TEXT\n        )\n    ')
  - L106 assign sample_data = [(1, '2024-01-01', 100.5, 'A'), (2, '2024-01-02', 200.3, 'B'), (3, '2024-01-03', 150.7, 'A'), (4, '2024-01-04', 300.1, 'C'), (5, '2024-01-05', 175.9, 'B')]
  - L113 expr cursor.executemany('INSERT INTO reports (id, date, value, category) VALUES (?, ?, ?, ?)', sample_data)
  - L118 expr conn.commit()
  - L119 expr conn.close()
  - L121 return db_path
- L124 class TestConnectionPipeline:
  - L125 docstring: "Test database connection workflow."
  - L127 def test_connection_workflow_successself, client, fresh_state, sample_db:
    - L128 docstring: "Test complete connection workflow: test -> save -> use."
    - L130 assign test_resp = client.post('/connections/test', json={'db_url': str(sample_db), 'db_type': 'sqlite'})
    - L134 assert test_resp.status_code == 200
    - L135 assign test_data = test_resp.json()
    - L136 assert test_data.get('ok') is True or test_data.get('status') == 'ok'
    - L139 assign save_resp = client.post('/connections', json={'name': 'Sample DB', 'db_type': 'sqlite', 'db_url': str(sample_db)})
    - L144 assert save_resp.status_code == 200
    - L145 assign save_data = save_resp.json()
    - L146 assign conn_id = save_data['connection']['id']
    - L147 assert conn_id
    - L150 assign list_resp = client.get('/connections')
    - L151 assert list_resp.status_code == 200
    - L152 assign connections = list_resp.json()['connections']
    - L153 assert len(connections) == 1
    - L154 assert connections[0]['id'] == conn_id
    - L157 assign health_resp = client.post(f'/connections/{conn_id}/health')
    - L159 assert health_resp.status_code in (200, 400, 500)
  - L161 def test_connection_workflow_invalid_dbself, client, fresh_state:
    - L162 docstring: "Test connection workflow with invalid database."
    - L163 assign test_resp = client.post('/connections/test', json={'db_url': '/nonexistent/path/db.sqlite', 'db_type': 'sqlite'})
    - L169 assign data = test_resp.json()
    - L170 assert test_resp.status_code >= 400 or data.get('ok') is False
- L173 class TestTemplatePipeline:
  - L174 docstring: "Test template management pipeline."
  - L176 def test_template_lifecycleself, client, fresh_state, tmp_path:
    - L177 docstring: "Test template creation, listing, and deletion."
    - L179 expr fresh_state.upsert_template('tpl-lifecycle', name='Lifecycle Template', status='approved')
    - L186 assign tpl_dir = tmp_path / 'uploads' / 'tpl-lifecycle'
    - L187 expr tpl_dir.mkdir(parents=True, exist_ok=True)
    - L190 assign contract = {'template_id': 'tpl-lifecycle', 'batches': [], 'fields': {}}
    - L195 expr (tpl_dir / 'contract.json').write_text(json.dumps(contract))
    - L198 expr (tpl_dir / 'template_p1.html').write_text('<html><body>Test</body></html>')
    - L201 assign list_resp = client.get('/templates')
    - L202 assert list_resp.status_code == 200
    - L203 assign templates = list_resp.json()['templates']
    - L204 assert any((t['id'] == 'tpl-lifecycle' for t in templates))
    - L207 assign delete_resp = client.delete('/templates/tpl-lifecycle')
    - L208 assert delete_resp.status_code == 200
    - L211 assign list_resp2 = client.get('/templates')
    - L212 assign templates2 = list_resp2.json()['templates']
    - L213 assert not any((t['id'] == 'tpl-lifecycle' for t in templates2))
- L216 class TestJobPipeline:
  - L217 docstring: "Test job creation and execution pipeline."
  - L219 def test_job_complete_success_flowself, client, fresh_state, monkeypatch, tmp_path:
    - L220 docstring: "Test successful job execution from creation to completion."
    - L222 expr fresh_state.upsert_template('tpl-job-flow', name='Job Flow Template', status='approved')
    - L223 expr fresh_state.upsert_connection(conn_id='conn-job-flow', name='Job Flow Connection', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L232 assign scheduled_jobs = []
    - L234 def mock_schedulejob_id, payload_data, kind, correlation_id, step_progress:
      - L235 expr scheduled_jobs.append({'job_id': job_id, 'payload': payload_data, 'kind': kind})
      - L241 assign tracker = api.JobRunTracker(job_id, step_progress=step_progress)
      - L242 expr tracker.start()
      - L243 expr tracker.step_succeeded('dataLoad')
      - L244 expr tracker.step_succeeded('renderPdf')
      - L245 expr tracker.succeed({'ok': True})
    - L247 expr monkeypatch.setattr(api, '_schedule_report_job', mock_schedule)
    - L250 assign submit_resp = client.post('/jobs/run-report', json={'template_id': 'tpl-job-flow', 'connection_id': 'conn-job-flow', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'})
    - L257 assert submit_resp.status_code == 200
    - L258 assign job_id = submit_resp.json()['job_id']
    - L259 assert job_id in [j['job_id'] for j in scheduled_jobs]
    - L262 assign status_resp = client.get(f'/jobs/{job_id}')
    - L263 assert status_resp.status_code == 200
    - L264 assign job = status_resp.json()['job']
    - L265 assert job['status'] == 'succeeded'
    - L266 assert job['progress'] == 100
  - L268 def test_job_failure_handlingself, client, fresh_state, monkeypatch, tmp_path:
    - L269 docstring: "Test job failure is properly recorded."
    - L270 expr fresh_state.upsert_template('tpl-job-fail', name='Job Fail Template', status='approved')
    - L272 def mock_schedulejob_id, payload_data, kind, correlation_id, step_progress:
      - L273 assign tracker = api.JobRunTracker(job_id, step_progress=step_progress)
      - L274 expr tracker.start()
      - L275 expr tracker.step_running('dataLoad')
      - L276 expr tracker.step_failed('dataLoad', 'Database connection failed')
      - L277 expr tracker.fail('Job failed: Database connection failed')
    - L279 expr monkeypatch.setattr(api, '_schedule_report_job', mock_schedule)
    - L281 assign submit_resp = client.post('/jobs/run-report', json={'template_id': 'tpl-job-fail', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'})
    - L287 assign job_id = submit_resp.json()['job_id']
    - L289 assign status_resp = client.get(f'/jobs/{job_id}')
    - L290 assign job = status_resp.json()['job']
    - L291 assert job['status'] == 'failed'
    - L292 assert 'Database connection failed' in job['error']
  - L294 def test_job_cancellation_flowself, client, fresh_state, monkeypatch:
    - L295 docstring: "Test job cancellation workflow."
    - L296 expr fresh_state.upsert_template('tpl-cancel-flow', name='Cancel Flow Template', status='approved')
    - L299 assign scheduled_jobs = []
    - L301 def mock_schedulejob_id, payload_data, kind, correlation_id, step_progress:
      - L302 expr scheduled_jobs.append(job_id)
    - L305 expr monkeypatch.setattr(api, '_schedule_report_job', mock_schedule)
    - L308 class MockReportService:
      - L310 def force_cancel_jobjob_id, *, force=False:
        - L311 return True
    - L313 assign api.report_service = MockReportService()
    - L316 assign submit_resp = client.post('/jobs/run-report', json={'template_id': 'tpl-cancel-flow', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59'})
    - L321 assign job_id = submit_resp.json()['job_id']
    - L324 assign cancel_resp = client.post(f'/jobs/{job_id}/cancel')
    - L325 assert cancel_resp.status_code == 200
    - L328 assign status_resp = client.get(f'/jobs/{job_id}')
    - L329 assign job = status_resp.json()['job']
    - L330 assert job['status'] == 'cancelled'
- L333 class TestBatchJobPipeline:
  - L334 docstring: "Test batch job submission and tracking."
  - L336 def test_batch_job_submissionself, client, fresh_state, monkeypatch:
    - L337 docstring: "Test submitting multiple jobs in one request."
    - L338 expr fresh_state.upsert_template('tpl-batch-1', name='Batch 1', status='approved')
    - L339 expr fresh_state.upsert_template('tpl-batch-2', name='Batch 2', status='approved')
    - L340 expr fresh_state.upsert_template('tpl-batch-3', name='Batch 3', status='approved')
    - L342 assign scheduled = []
    - L344 def mock_schedulejob_id, payload_data, kind, correlation_id, step_progress:
      - L345 expr scheduled.append({'job_id': job_id, 'template_id': payload_data.get('template_id')})
    - L350 expr monkeypatch.setattr(api, '_schedule_report_job', mock_schedule)
    - L352 assign payloads = [{'template_id': 'tpl-batch-1', 'start_date': '2024-01-01', 'end_date': '2024-01-31'}, {'template_id': 'tpl-batch-2', 'start_date': '2024-02-01', 'end_date': '2024-02-28'}, {'template_id': 'tpl-batch-3', 'start_date': '2024-03-01', 'end_date': '2024-03-31'}]
    - L358 assign resp = client.post('/jobs/run-report', json=payloads)
    - L359 assert resp.status_code == 200
    - L361 assign data = resp.json()
    - L362 assert data['count'] == 3
    - L363 assert len(data['job_ids']) == 3
    - L364 assert len(scheduled) == 3
    - L367 assign scheduled_templates = {s['template_id'] for s in scheduled}
    - L368 assert scheduled_templates == {'tpl-batch-1', 'tpl-batch-2', 'tpl-batch-3'}
- L371 class TestSchedulePipeline:
  - L372 docstring: "Test scheduled report execution."
  - L374 def test_schedule_creation_and_listingself, client, fresh_state, tmp_path:
    - L375 docstring: "Test schedule creation and listing."
    - L376 expr fresh_state.upsert_template('tpl-schedule', name='Schedule Template', status='approved')
    - L377 expr fresh_state.upsert_connection(conn_id='conn-schedule', name='Schedule Connection', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L386 assign create_resp = client.post('/reports/schedules', json={'template_id': 'tpl-schedule', 'connection_id': 'conn-schedule', 'start_date': '2024-01-01 00:00:00', 'end_date': '2024-01-31 23:59:59', 'frequency': 'daily', 'name': 'Daily Report Schedule'})
    - L395 assert create_resp.status_code == 200
    - L396 assign schedule_data = create_resp.json()
    - L397 assign schedule_id = schedule_data.get('schedule', {}).get('id') or schedule_data.get('id')
    - L398 assert schedule_id
    - L401 assign list_resp = client.get('/reports/schedules')
    - L402 assert list_resp.status_code == 200
    - L403 assign schedules = list_resp.json()['schedules']
    - L404 assert any((s['id'] == schedule_id for s in schedules))
    - L407 assign delete_resp = client.delete(f'/reports/schedules/{schedule_id}')
    - L408 assert delete_resp.status_code == 200
    - L411 assign list_resp2 = client.get('/reports/schedules')
    - L412 assign schedules2 = list_resp2.json()['schedules']
    - L413 assert not any((s['id'] == schedule_id for s in schedules2))
- L416 class TestBootstrapPipeline:
  - L417 docstring: "Test application state bootstrap."
  - L419 def test_bootstrap_returns_complete_stateself, client, fresh_state, tmp_path:
    - L420 docstring: "Test bootstrap endpoint returns all required state."
    - L422 expr fresh_state.upsert_connection(conn_id='conn-boot', name='Bootstrap Connection', db_type='sqlite', database_path=str(tmp_path / 'db.sqlite'), secret_payload=None)
    - L429 expr fresh_state.upsert_template('tpl-boot', name='Bootstrap Template', status='approved')
    - L430 expr fresh_state.set_last_used('conn-boot', 'tpl-boot')
    - L433 assign resp = client.get('/state/bootstrap')
    - L434 assert resp.status_code == 200
    - L436 assign data = resp.json()
    - L437 assert 'connections' in data
    - L438 assert 'templates' in data
    - L439 assert 'last_used' in data
    - L442 assert len(data['connections']) == 1
    - L443 assert data['connections'][0]['id'] == 'conn-boot'
    - L445 assert len(data['templates']) == 1
    - L446 assert data['templates'][0]['id'] == 'tpl-boot'
    - L448 assert data['last_used']['connection_id'] == 'conn-boot'
    - L449 assert data['last_used']['template_id'] == 'tpl-boot'
- L452 class TestSavedChartsPipeline:
  - L453 docstring: "Test saved charts workflow."
  - L455 def test_chart_crud_operationsself, client, fresh_state:
    - L456 docstring: "Test create, read, update, delete for saved charts."
    - L457 expr fresh_state.upsert_template('tpl-charts', name='Charts Template', status='approved')
    - L460 assign chart_spec = {'type': 'line', 'xField': 'date', 'yFields': ['value']}
    - L466 assign created = fresh_state.create_saved_chart(template_id='tpl-charts', name='Test Chart', spec=chart_spec)
    - L471 assert created['id']
    - L472 assign chart_id = created['id']
    - L475 assign charts = fresh_state.list_saved_charts('tpl-charts')
    - L476 assert len(charts) == 1
    - L477 assert charts[0]['name'] == 'Test Chart'
    - L480 assign updated = fresh_state.update_saved_chart(chart_id, name='Updated Chart')
    - L481 assert updated['name'] == 'Updated Chart'
    - L484 assign deleted = fresh_state.delete_saved_chart(chart_id)
    - L485 assert deleted is True
    - L488 assign charts_after = fresh_state.list_saved_charts('tpl-charts')
    - L489 assert len(charts_after) == 0
- L492 class TestErrorRecovery:
  - L493 docstring: "Test error recovery scenarios."
  - L495 def test_state_recovery_after_invalid_jsonself, fresh_state, tmp_path:
    - L496 docstring: "Test state store handles corrupted state file."
    - L498 assign state_path = fresh_state._state_path
    - L499 expr state_path.write_text('{ invalid json }', encoding='utf-8')
    - L502 assign state = fresh_state._read_state()
    - L503 assert 'connections' in state
    - L504 assert 'templates' in state
  - L506 def test_concurrent_state_writesself, fresh_state:
    - L507 docstring: "Test concurrent state modifications don't corrupt data."
    - L508 import threading
    - L509 import time
    - L511 assign errors = []
    - L513 def workeri:
      - L514 try:
        - L515 for j in range(5):
          - L516 expr fresh_state.upsert_template(f'tpl-concurrent-{i}-{j}', name=f'Template {i}-{j}', status='approved')
          - L521 expr time.sleep(0.001)
        - L522 except Exception as e:
          - L523 expr errors.append(e)
    - L525 assign threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]
    - L527 for t in threads:
      - L528 expr t.start()
    - L529 for t in threads:
      - L530 expr t.join()
    - L532 assert not errors
    - L535 assign templates = fresh_state.list_templates()
    - L537 assert len(templates) == 25

## backend\tests\test_pipeline_stage_3_5_integration.py
- L1 import json
- L2 from pathlib import Path
- L3 from types import SimpleNamespace
- L5 import pytest
- L7 from backend.app.services.mapping import CorrectionsPreview as cp_module
- L8 from backend.app.services.mapping.CorrectionsPreview import run_corrections_preview
- L10 assign TEMPLATE_HTML = '<!doctype html>\n<html>\n  <body>\n    <h1>{report_title}</h1>\n    <!--BEGIN:BLOCK_REPEAT rows-->\n    <table data-region="rows">\n      <tbody>\n        <tr><td>{row_value}</td></tr>\n      </tbody>\n    </table>\n    <!--END:BLOCK_REPEAT rows-->\n  </body>\n</html>'
- L25 assign FAKE_RESPONSE = {'final_template_html': TEMPLATE_HTML, 'page_summary': 'Full-width header titled {report_title}, followed by a single-column table listing {row_value}.'}
- L31 def _fake_response:
  - L32 assign content = json.dumps(FAKE_RESPONSE)
  - L33 return SimpleNamespace(choices=[SimpleNamespace(message=SimpleNamespace(content=content))])
- L36 def write_fixture_filesbase_dir: Path:
  - L37 expr base_dir.mkdir(parents=True, exist_ok=True)
  - L38 expr (base_dir / 'template_p1.html').write_text(TEMPLATE_HTML, encoding='utf-8')
  - L39 expr (base_dir / 'reference_p1.png').write_bytes(b'\x89PNG\r\n\x1a\n')
  - L40 assign mapping_payload = {'mapping': {'row_value': 'reports.value'}, 'meta': {'unresolved': ['row_value']}}
  - L44 expr (base_dir / 'mapping_step3.json').write_text(json.dumps(mapping_payload), encoding='utf-8')
  - L45 assign schema_payload = {'scalars': ['report_title'], 'row_tokens': ['row_value'], 'totals': [], 'notes': ''}
  - L51 expr (base_dir / 'schema_ext.json').write_text(json.dumps(schema_payload), encoding='utf-8')
- L54 def read_jsonpath: Path:
  - L55 return json.loads(path.read_text(encoding='utf-8'))
- L59 def monkeypatched_llmmonkeypatch:
  - L60 expr monkeypatch.setattr(cp_module, 'get_openai_client', lambda: object())
  - L61 expr monkeypatch.setattr(cp_module, 'call_chat_completion', lambda *args, **kwargs: _fake_response())
- L64 def test_corrections_preview_integrationtmp_path, monkeypatched_llm:
  - L65 assign upload_dir = tmp_path / 'tmpl'
  - L66 expr write_fixture_files(upload_dir)
  - L68 assign result = run_corrections_preview(upload_dir=upload_dir, template_html_path=upload_dir / 'template_p1.html', mapping_step3_path=upload_dir / 'mapping_step3.json', schema_ext_path=upload_dir / 'schema_ext.json', user_input='Fix spelling mistakes', page_png_path=upload_dir / 'reference_p1.png', model_selector='test-model')
  - L78 assert result['summary']['constants_inlined'] == 0
  - L79 assert 'page_summary' in result['processed']
  - L80 assert result['processed']['page_summary']
  - L81 assert not result['cache_hit']
  - L83 assign template_html_after = (upload_dir / 'template_p1.html').read_text(encoding='utf-8')
  - L84 assign page_summary_text = (upload_dir / 'page_summary.txt').read_text(encoding='utf-8')
  - L85 assign stage_payload = read_json(upload_dir / 'stage_3_5.json')
  - L87 assert template_html_after == TEMPLATE_HTML
  - L88 assert 'header' in page_summary_text.lower()
  - L89 assert stage_payload['cache_key'] == result['cache_key']
  - L91 assign manifest = read_json(upload_dir / 'artifact_manifest.json')
  - L92 assign files = manifest['files']
  - L93 assert 'page_summary.txt' in files.values()
  - L95 assign mapping_labels = read_json(upload_dir / 'mapping_pdf_labels.json')
  - L96 assert mapping_labels == [{'header': 'row_value', 'placeholder': '{row_value}', 'mapping': 'reports.value'}]
  - L99 assign cached = run_corrections_preview(upload_dir=upload_dir, template_html_path=upload_dir / 'template_p1.html', mapping_step3_path=upload_dir / 'mapping_step3.json', schema_ext_path=upload_dir / 'schema_ext.json', user_input='Fix spelling mistakes', page_png_path=upload_dir / 'reference_p1.png', model_selector='test-model')
  - L108 assert cached['cache_hit']
  - L109 assert cached['cache_key'] == result['cache_key']
- L112 def test_corrections_preview_updates_mapping_overridetmp_path, monkeypatched_llm:
  - L113 assign upload_dir = tmp_path / 'tmpl'
  - L114 expr write_fixture_files(upload_dir)
  - L116 expr run_corrections_preview(upload_dir=upload_dir, template_html_path=upload_dir / 'template_p1.html', mapping_step3_path=upload_dir / 'mapping_step3.json', schema_ext_path=upload_dir / 'schema_ext.json', user_input='', page_png_path=upload_dir / 'reference_p1.png', model_selector='test-model', mapping_override={'row_value': 'INPUT_SAMPLE', 'extra_token': 'params.foo'})
  - L127 assign mapping_labels = read_json(upload_dir / 'mapping_pdf_labels.json')
  - L128 assert mapping_labels[0]['mapping'] == 'INPUT_SAMPLE'
  - L130 assign headers = {entry['header'] for entry in mapping_labels}
  - L131 assert {'row_value', 'extra_token'} <= headers
- L134 def test_mapping_labels_display_alias_for_report_selectedtmp_path, monkeypatched_llm:
  - L135 assign upload_dir = tmp_path / 'tmpl'
  - L136 expr write_fixture_files(upload_dir)
  - L138 assign mapping_path = upload_dir / 'mapping_step3.json'
  - L139 assign mapping_doc = read_json(mapping_path)
  - L140 assign mapping_doc['mapping']['from_date'] = 'INPUT_SAMPLE'
  - L141 assign mapping_doc['mapping']['page_info'] = 'INPUT_SAMPLE'
  - L142 expr mapping_path.write_text(json.dumps(mapping_doc), encoding='utf-8')
  - L144 assign schema_path = upload_dir / 'schema_ext.json'
  - L145 assign schema_doc = read_json(schema_path)
  - L146 expr schema_doc['scalars'].extend(['from_date', 'page_info'])
  - L147 expr schema_path.write_text(json.dumps(schema_doc), encoding='utf-8')
  - L149 expr run_corrections_preview(upload_dir=upload_dir, template_html_path=upload_dir / 'template_p1.html', mapping_step3_path=mapping_path, schema_ext_path=schema_path, user_input='', page_png_path=upload_dir / 'reference_p1.png', model_selector='test-model')
  - L159 assign mapping_labels = read_json(upload_dir / 'mapping_pdf_labels.json')
  - L160 assign alias_entries = {entry['header']: entry['mapping'] for entry in mapping_labels if entry['header'] in {'from_date', 'page_info'}}
  - L163 assert alias_entries == {'from_date': 'To Be Selected in report generator', 'page_info': 'To Be Selected in report generator'}

## backend\tests\test_pipeline_verification.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 import json
- L5 import os
- L6 import sys
- L7 import time
- L8 from pathlib import Path
- L9 from typing import Any
- L11 import pytest
- L13 expr os.environ.setdefault('OPENAI_API_KEY', 'test-key')
- L14 expr os.environ.setdefault('NEURA_ALLOW_MISSING_OPENAI', 'true')
- L16 assign REPO_ROOT = Path(__file__).resolve().parents[2]
- L17 if str(REPO_ROOT) not in sys.path:
  - L18 expr sys.path.insert(0, str(REPO_ROOT))
- L20 assign artifacts_module = importlib.import_module('backend.app.services.utils.artifacts')
- L21 assign write_artifact_manifest = artifacts_module.write_artifact_manifest
- L23 assign lock_module = importlib.import_module('backend.app.services.utils.lock')
- L24 assign acquire_template_lock = lock_module.acquire_template_lock
- L26 assign validation_module = importlib.import_module('backend.app.services.utils.validation')
- L27 assign SchemaValidationError = validation_module.SchemaValidationError
- L28 assign validate_mapping_schema = validation_module.validate_mapping_schema
- L30 assign pipeline_module = importlib.import_module('scripts.verify_pipeline')
- L31 assign verify_pipeline = pipeline_module.verify_pipeline
- L33 assign TEMPLATE_ID = 'ad6a0b1f-d98a-41c2-8ffe-8b651de9100f'
- L34 assign UPLOADS_ROOT = REPO_ROOT / 'backend' / 'uploads'
- L37 def _lookup_checkchecks: list[Any], name: str:
  - L38 for check in checks:
    - L39 if check.name == name:
      - L40 return check
  - L41 raise AssertionError(f'Check {name} not found in {checks}')
- L44 def _clear_fail_hook:
  - L45 expr os.environ.pop('NEURA_FAIL_AFTER_STEP', None)
- L49 def test_verify_pipeline_success:
  - L50 expr _clear_fail_hook()
  - L51 assign (success, checks) = verify_pipeline(TEMPLATE_ID, UPLOADS_ROOT)
  - L52 assert success
  - L54 assign contract_check = _lookup_check(checks, 'contract_schema')
  - L55 assert contract_check.ok
  - L57 assign manifest_check = _lookup_check(checks, 'artifact_manifest')
  - L58 assert manifest_check.ok
  - L60 assign staleness_check = _lookup_check(checks, 'artifact_staleness')
  - L61 assert staleness_check.ok
  - L63 assign images_check = _lookup_check(checks, 'report_final_images')
  - L64 assert images_check.ok
- L68 def test_verify_pipeline_simulate_flag:
  - L69 expr _clear_fail_hook()
  - L70 assign (success, checks) = verify_pipeline(TEMPLATE_ID, UPLOADS_ROOT, simulate=['mapping_save'])
  - L71 assert success
  - L72 assign sim_check = _lookup_check(checks, 'simulate_mapping_save')
  - L73 assert sim_check.ok
- L83 def test_verify_pipeline_failure_casestemplate_id: str, expected_check: str:
  - L84 assign (success, checks) = verify_pipeline(template_id, UPLOADS_ROOT)
  - L85 assert not success
  - L86 assert checks[0].name in {expected_check, 'template_dir_exists'}
  - L87 assert not checks[0].ok
- L90 def test_template_lock_non_blockingtmp_path: Path:
  - L91 assign lock_dir = tmp_path / 'tmpl'
  - L92 expr lock_dir.mkdir()
  - L94 with acquire_template_lock(lock_dir, 'mapping', 'corr-1'):
    - L95 with acquire_template_lock(lock_dir, 'mapping', 'corr-2'):
      - L96 pass
- L99 def test_validate_mapping_schema_rejects_invalid:
  - L100 with pytest.raises(SchemaValidationError):
    - L101 expr validate_mapping_schema([{'header': 'A'}])
- L104 def test_staleness_detectiontmp_path: Path:
  - L105 assign uploads_root = tmp_path
  - L106 assign tid = '00000000-0000-0000-0000-000000000123'
  - L107 assign tdir = uploads_root / tid
  - L108 expr tdir.mkdir(parents=True, exist_ok=True)
  - L110 expr (tdir / 'source.pdf').write_bytes(b'%PDF')
  - L111 expr (tdir / 'reference_p1.png').write_bytes(b'\x89PNG\r\n')
  - L112 expr (tdir / 'template_p1.html').write_text('<html><body>Template</body></html>', encoding='utf-8')
  - L113 assign report_html = tdir / 'report_final.html'
  - L114 expr report_html.write_text('<html><body><img src="reference_p1.png" /></body></html>', encoding='utf-8')
  - L116 assign mapping_path = tdir / 'mapping_pdf_labels.json'
  - L117 assign mapping_data = [{'header': 'H', 'placeholder': '{H}', 'mapping': 'table.col'}]
  - L120 expr mapping_path.write_text(json.dumps(mapping_data), encoding='utf-8')
  - L122 assign contract_path = tdir / 'contract.json'
  - L123 assign contract_data = {'mapping': {'H': 'table.col'}, 'join': {'parent_table': 'table', 'parent_key': 'id', 'child_table': 'table', 'child_key': 'id'}, 'date_columns': {'table': 'created_at'}, 'header_tokens': ['H'], 'row_tokens': ['H'], 'totals': {}, 'row_order': ['id'], 'literals': {}}
  - L138 expr contract_path.write_text(json.dumps(contract_data), encoding='utf-8')
  - L141 expr write_artifact_manifest(tdir, step='test_setup', files={'report_final.html': report_html, 'template_p1.html': tdir / 'template_p1.html', 'mapping_pdf_labels.json': mapping_path, 'contract.json': contract_path}, inputs=[], correlation_id='test')
  - L154 assign filled_html = tdir / 'filled_1.html'
  - L155 assign filled_pdf = tdir / 'filled_1.pdf'
  - L156 expr filled_html.write_text('<html></html>', encoding='utf-8')
  - L157 expr filled_pdf.write_bytes(b'%PDF')
  - L160 assign now = time.time()
  - L161 expr os.utime(mapping_path, (now, now))
  - L162 expr os.utime(contract_path, (now - 60, now - 60))
  - L163 expr os.utime(report_html, (now - 120, now - 120))
  - L165 assign (success, checks) = verify_pipeline(tid, uploads_root)
  - L166 assert not success
  - L167 assign stale_check = _lookup_check(checks, 'artifact_staleness')
  - L168 assert not stale_check.ok

## backend\tests\test_report_generate_autoload.py
- L1 from __future__ import annotations
- L3 import json
- L4 import sqlite3
- L5 from pathlib import Path
- L6 from typing import Any, cast
- L8 from backend.app.services.reports.ReportGenerate import fill_and_print
- L11 def _write_generator_bundletemplate_dir: Path, use_page_total: bool=False, include_page_fields: bool=True:
  - L16 assign generator_dir = template_dir / 'generator'
  - L17 expr generator_dir.mkdir(parents=True, exist_ok=True)
  - L19 assign page_total_col = 'page_total' if use_page_total else 'page_count'
  - L21 assign header_select_parts = ["'PlantX' AS plant_name", "'LocY' AS location", "date('2024-01-01') AS print_date", ':from_date AS from_date', ':to_date AS to_date', "COALESCE(:recipe_code, '') AS recipe_code"]
  - L29 if include_page_fields:
    - L30 expr header_select_parts.extend(['1 AS page_no', f'1 AS {page_total_col}'])
  - L37 assign entrypoints = {'header': 'SELECT ' + ', '.join(header_select_parts) + ';', 'rows': 'SELECT ROW_NUMBER() OVER (ORDER BY material_name) AS sl_no, material_name, set_wt, ach_wt, (ach_wt - set_wt) AS error_kg, CASE WHEN set_wt = 0 THEN NULL ELSE 100.0 * (ach_wt - set_wt) / set_wt END AS error_percent FROM recipes WHERE date(start_time) BETWEEN date(:from_date) AND date(:to_date) ORDER BY material_name;', 'totals': 'SELECT SUM(set_wt) AS total_set_wt, SUM(ach_wt) AS total_ach_wt, SUM(ach_wt - set_wt) AS total_error_kg, CASE WHEN SUM(set_wt) = 0 THEN NULL ELSE 100.0 * SUM(ach_wt - set_wt) / SUM(set_wt) END AS total_error_percent FROM recipes WHERE date(start_time) BETWEEN date(:from_date) AND date(:to_date);'}
  - L62 assign meta = {'entrypoints': entrypoints, 'params': {'required': ['from_date', 'to_date'], 'optional': ['recipe_code']}, 'dialect': 'duckdb', 'needs_user_fix': [], 'invalid': False, 'summary': {}}
  - L70 expr (generator_dir / 'generator_assets.json').write_text(json.dumps(meta), encoding='utf-8')
  - L71 assign header_fields = ['plant_name', 'location', 'print_date', 'from_date', 'to_date', 'recipe_code']
  - L79 if include_page_fields:
    - L80 expr header_fields.extend(['page_no', page_total_col])
  - L82 expr (generator_dir / 'output_schemas.json').write_text(json.dumps({'header': header_fields, 'rows': ['sl_no', 'material_name', 'set_wt', 'ach_wt', 'error_kg', 'error_percent'], 'totals': ['total_set_wt', 'total_ach_wt', 'total_error_kg', 'total_error_percent']}), encoding='utf-8')
  - L92 expr (generator_dir / 'sql_pack.sql').write_text('-- generated sql', encoding='utf-8')
- L95 def _write_contracttemplate_dir: Path, use_page_total: bool=False, include_page_fields: bool=True:
  - L100 assign page_total_col = 'page_total' if use_page_total else 'page_count'
  - L102 assign contract = {'mapping': {'plant_name': 'PARAM:plant_name', 'location': 'PARAM:location', 'print_date': 'PARAM:print_date', 'from_date': 'PARAM:from_date', 'to_date': 'PARAM:to_date', 'recipe_code': 'PARAM:recipe_code', 'sl_no': 'rows.sl_no', 'material_name': 'rows.material_name', 'set_wt': 'rows.set_wt', 'ach_wt': 'rows.ach_wt', 'error_kg': 'rows.error_kg', 'error_percent': 'rows.error_percent', 'total_set_wt': 'totals.total_set_wt', 'total_ach_wt': 'totals.total_ach_wt', 'total_error_kg': 'totals.total_error_kg', 'total_error_percent': 'totals.total_error_percent'}, 'join': {'parent_table': 'recipes', 'parent_key': 'id', 'child_table': '', 'child_key': ''}, 'date_columns': {'recipes': 'start_time'}, 'header_tokens': ['plant_name', 'location', 'print_date', 'from_date', 'to_date', 'recipe_code'], 'row_tokens': ['sl_no', 'material_name', 'set_wt', 'ach_wt', 'error_kg', 'error_percent'], 'totals': {'total_set_wt': 'totals.total_set_wt', 'total_ach_wt': 'totals.total_ach_wt', 'total_error_kg': 'totals.total_error_kg', 'total_error_percent': 'totals.total_error_percent'}, 'row_order': ['ROWID'], 'literals': {}, 'filters': {'required': {}, 'optional': {}}, 'row_computed': {}, 'totals_math': {}, 'formatters': {}, 'order_by': {'rows': ['material_name ASC']}, 'unresolved': [], 'tokens': {'scalars': ['plant_name', 'location', 'print_date', 'from_date', 'to_date', 'recipe_code'], 'row_tokens': ['sl_no', 'material_name', 'set_wt', 'ach_wt', 'error_kg', 'error_percent'], 'totals': ['total_set_wt', 'total_ach_wt', 'total_error_kg', 'total_error_percent']}}
  - L164 if include_page_fields:
    - L165 assign mapping = cast(dict[str, Any], contract['mapping'])
    - L166 assign mapping['page_no'] = 'PARAM:page_no'
    - L167 assign mapping[page_total_col] = f'PARAM:{page_total_col}'
    - L169 assign header_tokens = cast(list[str], contract['header_tokens'])
    - L170 expr header_tokens.extend(['page_no', page_total_col])
    - L172 assign tokens = cast(dict[str, Any], contract['tokens'])
    - L173 assign scalars = cast(list[str], tokens.get('scalars') or [])
    - L174 expr scalars.extend(['page_no', page_total_col])
    - L175 assign tokens['scalars'] = scalars
  - L176 return contract
- L179 def _write_templatetemplate_path: Path:
  - L180 expr template_path.write_text('<html><body>\n<section class="batch-block">\n  <div>{{plant_name}} - {{location}}</div>\n  <div>From {{from_date}} to {{to_date}}</div>\n  <table>\n    <tbody>\n      <tr>\n        <td>{{sl_no}}</td>\n        <td>{{material_name}}</td>\n        <td>{{set_wt}}</td>\n        <td>{{ach_wt}}</td>\n        <td>{{error_kg}}</td>\n        <td>{{error_percent}}</td>\n      </tr>\n    </tbody>\n  </table>\n  <footer>\n    Totals: {{total_set_wt}} / {{total_ach_wt}} / {{total_error_kg}} / {{total_error_percent}}\n    Page {{page_no}} of {{page_count}}\n  </footer>\n</section>\n</body></html>', encoding='utf-8')
- L207 def _write_dbdb_path: Path:
  - L208 assign con = sqlite3.connect(str(db_path))
  - L209 try:
    - L210 expr con.execute('CREATE TABLE recipes (id INTEGER PRIMARY KEY, start_time TEXT, material_name TEXT, set_wt REAL, ach_wt REAL)')
    - L213 expr con.executemany('INSERT INTO recipes (id, start_time, material_name, set_wt, ach_wt) VALUES (?, ?, ?, ?, ?)', [(1, '2024-05-01', 'MaterialA', 10.0, 11.5), (2, '2024-05-02', 'MaterialB', 20.0, 19.0)])
    - L220 expr con.commit()
    - L222 finally:
      - L222 expr con.close()
- L225 def test_fill_and_print_autoloads_generator_bundletmp_path:
  - L226 assign template_dir = tmp_path / 'template'
  - L227 expr template_dir.mkdir()
  - L228 assign template_path = template_dir / 'report_final.html'
  - L229 expr _write_template(template_path)
  - L230 expr _write_generator_bundle(template_dir, include_page_fields=False)
  - L232 assign contract = _write_contract(template_dir, include_page_fields=False)
  - L233 assign db_path = tmp_path / 'recipes.db'
  - L234 expr _write_db(db_path)
  - L236 assign out_html = tmp_path / 'out.html'
  - L237 assign out_pdf = tmp_path / 'out.pdf'
  - L239 expr fill_and_print(OBJ=contract, TEMPLATE_PATH=template_path, DB_PATH=db_path, OUT_HTML=out_html, OUT_PDF=out_pdf, START_DATE='2024-01-01', END_DATE='2024-12-31', batch_ids=None)
  - L250 assign output = out_html.read_text(encoding='utf-8')
  - L251 assert 'MaterialA' in output
  - L252 assert 'MaterialB' in output
  - L253 assert 'Totals' in output
  - L254 assert 'class="nr-page-number"' in output
  - L255 assert 'class="nr-page-count"' in output
- L258 def test_fill_and_print_supports_page_total_tokentmp_path:
  - L259 assign template_dir = tmp_path / 'template_total'
  - L260 expr template_dir.mkdir()
  - L261 assign template_path = template_dir / 'report_final.html'
  - L262 expr _write_template(template_path)
  - L264 assign template_html = template_path.read_text(encoding='utf-8')
  - L265 assign template_html = template_html.replace('{{page_count}}', '{{page_total}}')
  - L266 expr template_path.write_text(template_html, encoding='utf-8')
  - L268 expr _write_generator_bundle(template_dir, use_page_total=True, include_page_fields=False)
  - L269 assign contract = _write_contract(template_dir, use_page_total=True, include_page_fields=False)
  - L271 assign db_path = tmp_path / 'recipes_total.db'
  - L272 expr _write_db(db_path)
  - L274 assign out_html = tmp_path / 'out_total.html'
  - L275 assign out_pdf = tmp_path / 'out_total.pdf'
  - L277 expr fill_and_print(OBJ=contract, TEMPLATE_PATH=template_path, DB_PATH=db_path, OUT_HTML=out_html, OUT_PDF=out_pdf, START_DATE='2024-01-01', END_DATE='2024-12-31', batch_ids=None)
  - L288 assign output = out_html.read_text(encoding='utf-8')
  - L289 assert 'class="nr-page-number"' in output
  - L290 assert 'class="nr-page-count"' in output
  - L291 assert 'Page <span class="nr-page-number"' in output
  - L292 assert 'of <span class="nr-page-count"' in output

## backend\tests\test_saved_charts.py
- L1 from __future__ import annotations
- L3 import sys
- L4 import types
- L5 import uuid
- L6 from pathlib import Path
- L8 import pytest
- L9 from fastapi.testclient import TestClient
- L11 assign fernet_module = types.ModuleType('cryptography.fernet')
- L14 class _DummyFernet:
  - L15 def __init__self, key:
    - L16 assign self.key = key
  - L19 def generate_key:
    - L20 return b'A' * 44
  - L22 def encryptself, payload: bytes:
    - L23 return payload
  - L25 def decryptself, token: bytes:
    - L26 return token
- L29 expr setattr(fernet_module, 'Fernet', _DummyFernet)
- L30 expr setattr(fernet_module, 'InvalidToken', Exception)
- L31 assign crypto_module = types.ModuleType('cryptography')
- L32 expr setattr(crypto_module, 'fernet', fernet_module)
- L33 expr sys.modules.setdefault('cryptography', crypto_module)
- L34 expr sys.modules.setdefault('cryptography.fernet', fernet_module)
- L36 expr sys.path.insert(0, str(Path(__file__).resolve().parents[2]))
- L38 from .. import api
- L39 from ..app.services.connections import db_connection as db_conn_module
- L40 from ..app.services.state import store as state_store_module
- L44 def fresh_statetmp_path, monkeypatch:
  - L45 assign base_dir = tmp_path / 'state'
  - L46 assign store = state_store_module.StateStore(base_dir=base_dir)
  - L47 assign state_store_module.state_store = store
  - L48 assign api.state_store = store
  - L49 assign db_conn_module.state_store = store
  - L51 assign upload_root = tmp_path / 'uploads'
  - L52 expr upload_root.mkdir(parents=True, exist_ok=True)
  - L53 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', upload_root)
  - L54 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', upload_root.resolve())
  - L55 assign excel_root = tmp_path / 'excel-uploads'
  - L56 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L57 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L58 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L59 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (upload_root.resolve(), '/uploads'))
  - L60 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L61 return store
- L65 def clientfresh_state:
  - L66 return TestClient(api.app)
- L69 def _create_templatestore, template_id: str:
  - L70 return store.upsert_template(template_id, name='Saved Chart Template', status='approved', artifacts={}, connection_id=None)
- L79 def _sample_spec:
  - L80 return {'type': 'bar', 'xField': 'batch_index', 'yFields': ['rows'], 'chartTemplateId': 'time_series_basic', 'title': 'Rows Trend', 'description': 'Rows over time'}
- L90 def test_saved_charts_crud_flowclient: TestClient, fresh_state:
  - L91 assign template_id = str(uuid.uuid4())
  - L92 expr _create_template(fresh_state, template_id)
  - L94 assign list_resp = client.get(f'/templates/{template_id}/charts/saved')
  - L95 assert list_resp.status_code == 200
  - L96 assert list_resp.json()['charts'] == []
  - L98 assign create_resp = client.post(f'/templates/{template_id}/charts/saved', json={'template_id': template_id, 'name': 'My chart', 'spec': _sample_spec()})
  - L106 assert create_resp.status_code == 200
  - L107 assign created = create_resp.json()
  - L108 assign chart_id = created['id']
  - L109 assert created['name'] == 'My chart'
  - L110 assert created['spec']['type'] == 'bar'
  - L112 assign list_resp = client.get(f'/templates/{template_id}/charts/saved')
  - L113 assert list_resp.status_code == 200
  - L114 assert len(list_resp.json()['charts']) == 1
  - L116 assign update_resp = client.put(f'/templates/{template_id}/charts/saved/{chart_id}', json={'name': 'Updated name'})
  - L120 assert update_resp.status_code == 200
  - L121 assert update_resp.json()['name'] == 'Updated name'
  - L123 assign delete_resp = client.delete(f'/templates/{template_id}/charts/saved/{chart_id}')
  - L124 assert delete_resp.status_code == 200
  - L125 assert delete_resp.json()['status'] == 'ok'
  - L127 assign list_resp = client.get(f'/templates/{template_id}/charts/saved')
  - L128 assert list_resp.status_code == 200
  - L129 assert list_resp.json()['charts'] == []
- L132 def test_saved_chart_validation_errorsclient: TestClient, fresh_state:
  - L133 assign template_id = str(uuid.uuid4())
  - L134 assign other_id = str(uuid.uuid4())
  - L135 expr _create_template(fresh_state, template_id)
  - L137 assign resp = client.post(f'/templates/{template_id}/charts/saved', json={'template_id': other_id, 'name': 'Mismatch', 'spec': _sample_spec()})
  - L141 assert resp.status_code == 400
  - L143 assign resp = client.post(f'/templates/{other_id}/charts/saved', json={'template_id': other_id, 'name': 'Missing template', 'spec': _sample_spec()})
  - L147 assert resp.status_code == 404
  - L149 assign resp = client.put(f'/templates/{template_id}/charts/saved/not-found', json={'name': 'New name'})
  - L153 assert resp.status_code == 404
  - L155 assign resp = client.delete(f'/templates/{template_id}/charts/saved/not-found')
  - L156 assert resp.status_code == 404

## backend\tests\test_scheduler_date_range.py
- L1 docstring: "Tests for Scheduler date range enforcement."
- L2 import pytest
- L3 import asyncio
- L4 from datetime import datetime, timezone, timedelta
- L5 from unittest.mock import MagicMock, patch, AsyncMock
- L7 from backend.app.services.jobs.report_scheduler import ReportScheduler, _parse_iso, _now_utc
- L14 class TestSchedulerDateRangeEnforcement:
  - L15 docstring: "Test scheduler date range enforcement functionality."
  - L18 def mock_runnerself:
    - L19 docstring: "Create a mock runner function."
    - L20 async def runnerpayload, kind, job_tracker=None:
      - L21 return {'html_url': 'test.html', 'pdf_url': 'test.pdf'}
    - L22 return runner
  - L25 def mock_state_storeself:
    - L26 docstring: "Create a mock state store."
    - L27 assign mock = MagicMock()
    - L28 assign mock.create_job = MagicMock(return_value={'id': 'job-123'})
    - L29 assign mock.record_schedule_run = MagicMock()
    - L30 return mock
  - L32 def test_parse_iso_with_timezoneself:
    - L33 docstring: "Test parsing ISO timestamp with timezone."
    - L34 assign result = _parse_iso('2024-01-15T10:00:00+00:00')
    - L35 assert result is not None
    - L36 assert result.tzinfo is not None
  - L38 def test_parse_iso_without_timezoneself:
    - L39 docstring: "Test parsing ISO timestamp without timezone assumes UTC."
    - L40 assign result = _parse_iso('2024-01-15T10:00:00')
    - L41 assert result is not None
    - L42 assert result.tzinfo == timezone.utc
  - L44 def test_parse_iso_invalidself:
    - L45 docstring: "Test parsing invalid ISO timestamp returns None."
    - L46 assert _parse_iso(None) is None
    - L47 assert _parse_iso('') is None
    - L48 assert _parse_iso('not-a-date') is None
  - L51 async def test_scheduler_skips_past_end_dateself, mock_runner:
    - L52 docstring: "Test that scheduler skips schedules past their end_date."
    - L53 assign past_end = (datetime.now(timezone.utc) - timedelta(days=1)).isoformat()
    - L54 assign past_start = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
    - L56 assign schedules = [{'id': 'schedule-past', 'active': True, 'start_date': past_start, 'end_date': past_end, 'next_run_at': None, 'interval_minutes': 1440, 'template_id': 'template-1', 'connection_id': 'conn-1'}]
    - L69 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L70 assign mock_store.list_schedules.return_value = schedules
      - L72 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
      - L75 expr await scheduler._dispatch_due_jobs()
      - L78 assert len(scheduler._inflight) == 0
  - L81 async def test_scheduler_skips_future_start_dateself, mock_runner:
    - L82 docstring: "Test that scheduler skips schedules before their start_date."
    - L83 assign future_start = (datetime.now(timezone.utc) + timedelta(days=7)).isoformat()
    - L84 assign future_end = (datetime.now(timezone.utc) + timedelta(days=30)).isoformat()
    - L86 assign schedules = [{'id': 'schedule-future', 'active': True, 'start_date': future_start, 'end_date': future_end, 'next_run_at': None, 'interval_minutes': 1440, 'template_id': 'template-1', 'connection_id': 'conn-1'}]
    - L99 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L100 assign mock_store.list_schedules.return_value = schedules
      - L102 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
      - L105 expr await scheduler._dispatch_due_jobs()
      - L108 assert len(scheduler._inflight) == 0
  - L111 async def test_scheduler_runs_within_date_rangeself, mock_runner:
    - L112 docstring: "Test that scheduler runs schedules within their date range."
    - L113 assign past_start = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()
    - L114 assign future_end = (datetime.now(timezone.utc) + timedelta(days=7)).isoformat()
    - L115 assign past_next_run = (datetime.now(timezone.utc) - timedelta(hours=1)).isoformat()
    - L117 assign schedules = [{'id': 'schedule-active', 'active': True, 'start_date': past_start, 'end_date': future_end, 'next_run_at': past_next_run, 'interval_minutes': 1440, 'template_id': 'template-1', 'connection_id': 'conn-1', 'template_kind': 'pdf'}]
    - L131 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L132 assign mock_store.list_schedules.return_value = schedules
      - L133 assign mock_store.create_job.return_value = {'id': 'job-123'}
      - L135 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
      - L138 expr await scheduler._dispatch_due_jobs()
      - L141 assert 'schedule-active' in scheduler._inflight
  - L144 async def test_scheduler_handles_null_datesself, mock_runner:
    - L145 docstring: "Test that scheduler handles null start/end dates gracefully."
    - L146 assign past_next_run = (datetime.now(timezone.utc) - timedelta(hours=1)).isoformat()
    - L148 assign schedules = [{'id': 'schedule-no-dates', 'active': True, 'start_date': None, 'end_date': None, 'next_run_at': past_next_run, 'interval_minutes': 1440, 'template_id': 'template-1', 'connection_id': 'conn-1', 'template_kind': 'pdf'}]
    - L162 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L163 assign mock_store.list_schedules.return_value = schedules
      - L164 assign mock_store.create_job.return_value = {'id': 'job-123'}
      - L166 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
      - L169 expr await scheduler._dispatch_due_jobs()
      - L172 assert 'schedule-no-dates' in scheduler._inflight
  - L175 async def test_scheduler_skips_inactive_schedulesself, mock_runner:
    - L176 docstring: "Test that scheduler skips inactive schedules."
    - L177 assign schedules = [{'id': 'schedule-inactive', 'active': False, 'start_date': None, 'end_date': None, 'next_run_at': None, 'interval_minutes': 1440, 'template_id': 'template-1', 'connection_id': 'conn-1'}]
    - L190 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L191 assign mock_store.list_schedules.return_value = schedules
      - L193 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
      - L195 expr await scheduler._dispatch_due_jobs()
      - L198 assert len(scheduler._inflight) == 0
- L201 class TestSchedulerStartStop:
  - L202 docstring: "Test scheduler start/stop functionality."
  - L205 def mock_runnerself:
    - L206 docstring: "Create a mock runner function."
    - L207 async def runnerpayload, kind, job_tracker=None:
      - L208 return {}
    - L209 return runner
  - L212 async def test_scheduler_start_creates_taskself, mock_runner:
    - L213 docstring: "Test that starting scheduler creates a task."
    - L214 assign scheduler = ReportScheduler(mock_runner, poll_seconds=1)
    - L216 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L217 assign mock_store.list_schedules.return_value = []
      - L219 expr await scheduler.start()
      - L221 assert scheduler._task is not None
      - L222 assert not scheduler._task.done()
      - L224 expr await scheduler.stop()
  - L227 async def test_scheduler_stop_cancels_taskself, mock_runner:
    - L228 docstring: "Test that stopping scheduler cancels the task."
    - L229 assign scheduler = ReportScheduler(mock_runner, poll_seconds=1)
    - L231 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L232 assign mock_store.list_schedules.return_value = []
      - L234 expr await scheduler.start()
      - L235 expr await scheduler.stop()
      - L237 assert scheduler._task is None
- L240 if __name__ == '__main__':
  - L241 expr pytest.main([__file__, '-v'])

## backend\tests\test_scheduler_email.py
- L1 docstring: "Comprehensive tests for scheduler and email functionality.\n\nThis module tests:..."
- L10 from __future__ import annotations
- L12 import asyncio
- L13 import os
- L14 import time
- L15 from datetime import datetime, timedelta, timezone
- L16 from pathlib import Path
- L17 from unittest.mock import MagicMock, patch, AsyncMock
- L18 import pytest
- L25 class TestMailerConfig:
  - L26 docstring: "Tests for mailer configuration and environment variable handling."
  - L28 def test_mailer_config_disabled_when_host_missingself:
    - L29 docstring: "Mailer should be disabled when NEURA_MAIL_HOST is not set."
    - L30 with patch.dict(os.environ, {}, clear=True):
      - L32 for key in list(os.environ.keys()):
        - L33 if key.startswith('NEURA_MAIL'):
          - L34 delete os.environ[key]
      - L36 from backend.app.services.utils.mailer import _load_mailer_config
      - L37 assign config = _load_mailer_config()
      - L38 assert config.enabled is False
      - L39 assert config.host is None
  - L41 def test_mailer_config_disabled_when_sender_missingself:
    - L42 docstring: "Mailer should be disabled when NEURA_MAIL_SENDER is not set."
    - L43 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com'}, clear=False):
      - L45 expr os.environ.pop('NEURA_MAIL_SENDER', None)
      - L47 from backend.app.services.utils.mailer import _load_mailer_config
      - L48 assign config = _load_mailer_config()
      - L49 assert config.enabled is False
  - L51 def test_mailer_config_enabled_with_host_and_senderself:
    - L52 docstring: "Mailer should be enabled when both host and sender are set."
    - L53 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com', 'NEURA_MAIL_SENDER': 'test@example.com'}, clear=False):
      - L57 from backend.app.services.utils.mailer import _load_mailer_config
      - L58 assign config = _load_mailer_config()
      - L59 assert config.enabled is True
      - L60 assert config.host == 'smtp.example.com'
      - L61 assert config.sender == 'test@example.com'
  - L63 def test_mailer_config_default_portself:
    - L64 docstring: "Default SMTP port should be 587."
    - L65 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com', 'NEURA_MAIL_SENDER': 'test@example.com'}, clear=False):
      - L69 expr os.environ.pop('NEURA_MAIL_PORT', None)
      - L70 from backend.app.services.utils.mailer import _load_mailer_config
      - L71 assign config = _load_mailer_config()
      - L72 assert config.port == 587
  - L74 def test_mailer_config_custom_portself:
    - L75 docstring: "Custom SMTP port should be respected."
    - L76 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com', 'NEURA_MAIL_SENDER': 'test@example.com', 'NEURA_MAIL_PORT': '465'}, clear=False):
      - L81 from backend.app.services.utils.mailer import _load_mailer_config
      - L82 assign config = _load_mailer_config()
      - L83 assert config.port == 465
  - L85 def test_mailer_config_tls_default_enabledself:
    - L86 docstring: "TLS should be enabled by default."
    - L87 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com', 'NEURA_MAIL_SENDER': 'test@example.com'}, clear=False):
      - L91 expr os.environ.pop('NEURA_MAIL_USE_TLS', None)
      - L92 from backend.app.services.utils.mailer import _load_mailer_config
      - L93 assign config = _load_mailer_config()
      - L94 assert config.use_tls is True
  - L96 def test_mailer_config_tls_disabledself:
    - L97 docstring: "TLS can be disabled via environment variable."
    - L98 with patch.dict(os.environ, {'NEURA_MAIL_HOST': 'smtp.example.com', 'NEURA_MAIL_SENDER': 'test@example.com', 'NEURA_MAIL_USE_TLS': 'false'}, clear=False):
      - L103 from backend.app.services.utils.mailer import _load_mailer_config
      - L104 assign config = _load_mailer_config()
      - L105 assert config.use_tls is False
  - L107 def test_normalize_recipients_deduplicationself:
    - L108 docstring: "Recipient normalization should deduplicate emails."
    - L109 from backend.app.services.utils.mailer import _normalize_recipients
    - L111 assign recipients = ['test@example.com', 'TEST@example.com', 'test@example.com', 'other@example.com']
    - L112 assign normalized = _normalize_recipients(recipients)
    - L115 assert len(normalized) == 3
    - L116 assert 'test@example.com' in normalized
    - L117 assert 'TEST@example.com' in normalized
    - L118 assert 'other@example.com' in normalized
  - L120 def test_normalize_recipients_strips_whitespaceself:
    - L121 docstring: "Recipient normalization should strip whitespace."
    - L122 from backend.app.services.utils.mailer import _normalize_recipients
    - L124 assign recipients = ['  test@example.com  ', '\nother@example.com\t']
    - L125 assign normalized = _normalize_recipients(recipients)
    - L127 assert normalized == ['test@example.com', 'other@example.com']
  - L129 def test_normalize_recipients_empty_inputself:
    - L130 docstring: "Recipient normalization should handle empty input."
    - L131 from backend.app.services.utils.mailer import _normalize_recipients
    - L133 assert _normalize_recipients(None) == []
    - L134 assert _normalize_recipients([]) == []
    - L135 assert _normalize_recipients(['']) == []
- L138 class TestSendReportEmail:
  - L139 docstring: "Tests for the send_report_email function."
  - L141 def test_send_email_returns_false_when_disabledself:
    - L142 docstring: "send_report_email should return False when mailer is disabled."
    - L143 from backend.app.services.utils.mailer import send_report_email
    - L145 with patch('backend.app.services.utils.mailer.MAILER_CONFIG') as mock_config:
      - L146 assign mock_config.enabled = False
      - L147 assign mock_config.host = None
      - L148 assign mock_config.sender = None
      - L150 assign result = send_report_email(to_addresses=['test@example.com'], subject='Test', body='Test body')
      - L155 assert result is False
  - L157 def test_send_email_returns_false_with_no_recipientsself:
    - L158 docstring: "send_report_email should return False with no recipients."
    - L159 from backend.app.services.utils.mailer import send_report_email
    - L161 assign result = send_report_email(to_addresses=[], subject='Test', body='Test body')
    - L166 assert result is False
  - L169 def test_send_email_successself, mock_smtp:
    - L170 docstring: "send_report_email should return True on successful send."
    - L171 from backend.app.services.utils.mailer import send_report_email, MailerConfig
    - L173 assign mock_instance = MagicMock()
    - L174 assign mock_smtp.return_value.__enter__ = MagicMock(return_value=mock_instance)
    - L175 assign mock_smtp.return_value.__exit__ = MagicMock(return_value=False)
    - L177 with patch('backend.app.services.utils.mailer.MAILER_CONFIG', MailerConfig(host='smtp.example.com', port=587, username=None, password=None, sender='sender@example.com', use_tls=True, enabled=True)):
      - L186 assign result = send_report_email(to_addresses=['test@example.com'], subject='Test Subject', body='Test body content')
      - L192 assert result is True
      - L193 expr mock_instance.starttls.assert_called_once()
      - L194 expr mock_instance.send_message.assert_called_once()
  - L197 def test_send_email_with_attachmentsself, mock_smtp, tmp_path:
    - L198 docstring: "send_report_email should attach files correctly."
    - L199 from backend.app.services.utils.mailer import send_report_email, MailerConfig
    - L202 assign test_file = tmp_path / 'report.pdf'
    - L203 expr test_file.write_bytes(b'%PDF-1.4 test content')
    - L205 assign mock_instance = MagicMock()
    - L206 assign mock_smtp.return_value.__enter__ = MagicMock(return_value=mock_instance)
    - L207 assign mock_smtp.return_value.__exit__ = MagicMock(return_value=False)
    - L209 with patch('backend.app.services.utils.mailer.MAILER_CONFIG', MailerConfig(host='smtp.example.com', port=587, username=None, password=None, sender='sender@example.com', use_tls=True, enabled=True)):
      - L218 assign result = send_report_email(to_addresses=['test@example.com'], subject='Test with Attachment', body='See attached', attachments=[test_file])
      - L225 assert result is True
      - L227 expr mock_instance.send_message.assert_called_once()
      - L229 assign sent_message = mock_instance.send_message.call_args[0][0]
      - L231 assert sent_message.is_multipart() or len(list(sent_message.iter_attachments())) >= 0
  - L234 def test_send_email_handles_missing_attachmentself, mock_smtp, tmp_path:
    - L235 docstring: "send_report_email should handle missing attachment files gracefully."
    - L236 from backend.app.services.utils.mailer import send_report_email, MailerConfig
    - L238 assign nonexistent_file = tmp_path / 'nonexistent.pdf'
    - L240 assign mock_instance = MagicMock()
    - L241 assign mock_smtp.return_value.__enter__ = MagicMock(return_value=mock_instance)
    - L242 assign mock_smtp.return_value.__exit__ = MagicMock(return_value=False)
    - L244 with patch('backend.app.services.utils.mailer.MAILER_CONFIG', MailerConfig(host='smtp.example.com', port=587, username=None, password=None, sender='sender@example.com', use_tls=True, enabled=True)):
      - L254 assign result = send_report_email(to_addresses=['test@example.com'], subject='Test', body='Test body', attachments=[nonexistent_file])
      - L261 assert result is True
- L268 class TestReportScheduler:
  - L269 docstring: "Tests for the ReportScheduler class."
  - L272 def mock_runnerself:
    - L273 docstring: "Create a mock runner function."
    - L274 return MagicMock(return_value={'html_url': '/uploads/test.html', 'pdf_url': '/uploads/test.pdf'})
  - L277 def schedulerself, mock_runner:
    - L278 docstring: "Create a ReportScheduler instance."
    - L279 from backend.app.services.jobs.report_scheduler import ReportScheduler
    - L280 return ReportScheduler(mock_runner, poll_seconds=5)
  - L283 async def test_scheduler_start_stopself, scheduler:
    - L284 docstring: "Scheduler should start and stop cleanly."
    - L285 expr await scheduler.start()
    - L286 assert scheduler._task is not None
    - L287 assert not scheduler._task.done()
    - L289 expr await scheduler.stop()
    - L290 assert scheduler._task is None
  - L293 async def test_scheduler_prevents_duplicate_startself, scheduler:
    - L294 docstring: "Starting scheduler twice should be idempotent."
    - L295 expr await scheduler.start()
    - L296 assign first_task = scheduler._task
    - L298 expr await scheduler.start()
    - L299 assert scheduler._task is first_task
    - L301 expr await scheduler.stop()
  - L304 async def test_scheduler_stop_without_startself, scheduler:
    - L305 docstring: "Stopping scheduler without starting should not raise."
    - L306 expr await scheduler.stop()
  - L308 def test_parse_iso_validself:
    - L309 docstring: "_parse_iso should parse valid ISO timestamps."
    - L310 from backend.app.services.jobs.report_scheduler import _parse_iso
    - L312 assign result = _parse_iso('2024-01-15T10:30:00Z')
    - L313 assert result is not None
    - L314 assert result.year == 2024
    - L315 assert result.month == 1
    - L316 assert result.day == 15
  - L318 def test_parse_iso_invalidself:
    - L319 docstring: "_parse_iso should return None for invalid timestamps."
    - L320 from backend.app.services.jobs.report_scheduler import _parse_iso
    - L322 assert _parse_iso(None) is None
    - L323 assert _parse_iso('') is None
    - L324 assert _parse_iso('not-a-date') is None
  - L326 def test_next_run_datetime_calculationself:
    - L327 docstring: "_next_run_datetime should calculate correct next run time."
    - L328 from backend.app.services.jobs.report_scheduler import _next_run_datetime
    - L330 assign baseline = datetime(2024, 1, 15, 10, 0, 0, tzinfo=timezone.utc)
    - L331 assign schedule = {'interval_minutes': 60}
    - L333 assign result = _next_run_datetime(schedule, baseline)
    - L334 assign expected = datetime(2024, 1, 15, 11, 0, 0, tzinfo=timezone.utc)
    - L336 assert result == expected
  - L338 def test_next_run_datetime_minimum_intervalself:
    - L339 docstring: "_next_run_datetime should enforce minimum 1-minute interval."
    - L340 from backend.app.services.jobs.report_scheduler import _next_run_datetime
    - L342 assign baseline = datetime(2024, 1, 15, 10, 0, 0, tzinfo=timezone.utc)
    - L343 assign schedule = {'interval_minutes': 0}
    - L345 assign result = _next_run_datetime(schedule, baseline)
    - L346 assign expected = datetime(2024, 1, 15, 10, 1, 0, tzinfo=timezone.utc)
    - L348 assert result == expected
- L351 class TestSchedulerDispatch:
  - L352 docstring: "Tests for scheduler dispatch logic."
  - L355 async def test_dispatch_skips_inactive_schedulesself:
    - L356 docstring: "Dispatcher should skip inactive schedules."
    - L357 from backend.app.services.jobs.report_scheduler import ReportScheduler
    - L359 assign mock_runner = MagicMock()
    - L360 assign scheduler = ReportScheduler(mock_runner, poll_seconds=60)
    - L362 with patch.object(scheduler, '_run_schedule', new_callable=AsyncMock) as mock_run:
      - L363 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
        - L364 assign mock_store.list_schedules.return_value = [{'id': 'sched-1', 'active': False, 'next_run_at': '2024-01-01T00:00:00Z'}]
        - L368 expr await scheduler._dispatch_due_jobs()
        - L370 expr mock_run.assert_not_called()
  - L373 async def test_dispatch_skips_future_schedulesself:
    - L374 docstring: "Dispatcher should skip schedules not yet due."
    - L375 from backend.app.services.jobs.report_scheduler import ReportScheduler
    - L377 assign mock_runner = MagicMock()
    - L378 assign scheduler = ReportScheduler(mock_runner, poll_seconds=60)
    - L380 assign future_time = (datetime.now(timezone.utc) + timedelta(hours=1)).isoformat()
    - L382 with patch.object(scheduler, '_run_schedule', new_callable=AsyncMock) as mock_run:
      - L383 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
        - L384 assign mock_store.list_schedules.return_value = [{'id': 'sched-1', 'active': True, 'next_run_at': future_time}]
        - L388 expr await scheduler._dispatch_due_jobs()
        - L390 expr mock_run.assert_not_called()
  - L393 async def test_dispatch_prevents_duplicate_executionself:
    - L394 docstring: "Dispatcher should prevent duplicate concurrent executions."
    - L395 from backend.app.services.jobs.report_scheduler import ReportScheduler
    - L397 assign mock_runner = MagicMock()
    - L398 assign scheduler = ReportScheduler(mock_runner, poll_seconds=60)
    - L399 expr scheduler._inflight.add('sched-1')
    - L401 assign past_time = (datetime.now(timezone.utc) - timedelta(hours=1)).isoformat()
    - L403 with patch.object(scheduler, '_run_schedule', new_callable=AsyncMock) as mock_run:
      - L404 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
        - L405 assign mock_store.list_schedules.return_value = [{'id': 'sched-1', 'active': True, 'next_run_at': past_time}]
        - L409 expr await scheduler._dispatch_due_jobs()
        - L411 expr mock_run.assert_not_called()
- L418 class TestHealthEndpoints:
  - L419 docstring: "Tests for health check endpoints."
  - L422 def clientself:
    - L423 docstring: "Create a test client using the API app."
    - L424 import sys
    - L425 import os
    - L428 assign backend_dir = Path(__file__).parent.parent.parent
    - L429 if str(backend_dir) not in sys.path:
      - L430 expr sys.path.insert(0, str(backend_dir))
    - L432 from fastapi.testclient import TestClient
    - L435 try:
      - L436 from backend.api import app
      - L437 except ImportError:
        - L438 try:
          - L440 from fastapi import FastAPI
          - L441 from backend.app.api.routes.health import router as health_router
          - L443 assign app = FastAPI()
          - L444 expr app.include_router(health_router)
          - L445 except ImportError:
            - L446 expr pytest.skip('Cannot import app for testing')
            - L447 return None
    - L449 return TestClient(app)
  - L451 def test_basic_health_endpointself, client:
    - L452 docstring: "Basic health endpoint should return ok."
    - L453 if client is None:
      - L454 expr pytest.skip('Client not available')
    - L455 assign response = client.get('/health')
    - L456 assert response.status_code == 200
    - L457 assign data = response.json()
    - L458 assert data['status'] == 'ok'
  - L460 def test_healthz_endpointself, client:
    - L461 docstring: "Kubernetes liveness probe should return ok."
    - L462 if client is None:
      - L463 expr pytest.skip('Client not available')
    - L464 assign response = client.get('/healthz')
    - L465 assert response.status_code == 200
    - L466 assign data = response.json()
    - L467 assert data['status'] == 'ok'
  - L469 def test_email_health_endpointself, client:
    - L470 docstring: "Email health endpoint should return configuration status."
    - L471 if client is None:
      - L472 expr pytest.skip('Client not available')
    - L473 assign response = client.get('/health/email')
    - L474 assert response.status_code == 200
    - L475 assign data = response.json()
    - L476 assert 'email' in data
    - L477 assert 'enabled' in data['email']
  - L479 def test_scheduler_health_endpointself, client:
    - L480 docstring: "Scheduler health endpoint should return status."
    - L481 if client is None:
      - L482 expr pytest.skip('Client not available')
    - L483 assign response = client.get('/health/scheduler')
    - L484 assert response.status_code == 200
    - L485 assign data = response.json()
    - L486 assert 'scheduler' in data
    - L487 assert 'enabled' in data['scheduler']
- L494 class TestScheduleEndpoints:
  - L495 docstring: "Tests for schedule management API endpoints."
  - L498 def clientself:
    - L499 docstring: "Create a test client with schedule routes."
    - L500 import sys
    - L503 assign backend_dir = Path(__file__).parent.parent.parent
    - L504 if str(backend_dir) not in sys.path:
      - L505 expr sys.path.insert(0, str(backend_dir))
    - L507 from fastapi.testclient import TestClient
    - L509 try:
      - L510 from fastapi import FastAPI
      - L511 from backend.app.api.routes.schedules import router as schedules_router
      - L513 assign app = FastAPI()
      - L514 expr app.include_router(schedules_router, prefix='/reports/schedules')
      - L515 return TestClient(app)
      - L516 except ImportError as e:
        - L517 expr pytest.skip(f'Cannot import schedule routes: {e}')
        - L518 return None
  - L520 def test_list_schedulesself, client:
    - L521 docstring: "List schedules endpoint should return schedule list."
    - L522 if client is None:
      - L523 expr pytest.skip('Client not available')
    - L524 with patch('backend.app.api.routes.schedules.list_schedules', return_value=[]):
      - L525 assign response = client.get('/reports/schedules')
      - L526 assert response.status_code == 200
      - L527 assign data = response.json()
      - L528 assert 'schedules' in data
  - L530 def test_get_schedule_not_foundself, client:
    - L531 docstring: "Get non-existent schedule should return 404."
    - L532 if client is None:
      - L533 expr pytest.skip('Client not available')
    - L534 with patch('backend.app.api.routes.schedules.get_schedule', return_value=None):
      - L535 assign response = client.get('/reports/schedules/nonexistent-id')
      - L536 assert response.status_code == 404
  - L538 def test_trigger_schedule_not_foundself, client:
    - L539 docstring: "Trigger non-existent schedule should return 404."
    - L540 if client is None:
      - L541 expr pytest.skip('Client not available')
    - L542 with patch('backend.app.api.routes.schedules.get_schedule', return_value=None):
      - L543 assign response = client.post('/reports/schedules/nonexistent-id/trigger')
      - L544 assert response.status_code == 404
  - L546 def test_pause_scheduleself, client:
    - L547 docstring: "Pause schedule endpoint should set active to false."
    - L548 if client is None:
      - L549 expr pytest.skip('Client not available')
    - L550 assign mock_schedule = {'id': 'test-schedule', 'name': 'Test Schedule', 'active': True, 'template_id': 'template-1'}
    - L556 assign updated_schedule = {**mock_schedule, 'active': False}
    - L558 with patch('backend.app.api.routes.schedules.get_schedule', return_value=mock_schedule):
      - L559 with patch('backend.app.api.routes.schedules.update_schedule', return_value=updated_schedule):
        - L560 assign response = client.post('/reports/schedules/test-schedule/pause')
        - L561 assert response.status_code == 200
        - L562 assign data = response.json()
        - L563 assert data['status'] == 'ok'
        - L564 assert data['message'] == 'Schedule paused'
  - L566 def test_resume_scheduleself, client:
    - L567 docstring: "Resume schedule endpoint should set active to true."
    - L568 if client is None:
      - L569 expr pytest.skip('Client not available')
    - L570 assign mock_schedule = {'id': 'test-schedule', 'name': 'Test Schedule', 'active': False, 'template_id': 'template-1'}
    - L576 assign updated_schedule = {**mock_schedule, 'active': True}
    - L578 with patch('backend.app.api.routes.schedules.get_schedule', return_value=mock_schedule):
      - L579 with patch('backend.app.api.routes.schedules.update_schedule', return_value=updated_schedule):
        - L580 assign response = client.post('/reports/schedules/test-schedule/resume')
        - L581 assert response.status_code == 200
        - L582 assign data = response.json()
        - L583 assert data['status'] == 'ok'
        - L584 assert data['message'] == 'Schedule resumed'
- L591 class TestSchedulerEmailIntegration:
  - L592 docstring: "Integration tests for scheduler and email working together."
  - L595 async def test_scheduled_report_sends_emailself:
    - L596 docstring: "Scheduled report execution should trigger email if recipients configured."
    - L597 from backend.app.services.jobs.report_scheduler import ReportScheduler
    - L599 assign email_sent = []
    - L601 def mock_runnerpayload, kind, **kwargs:
      - L602 if payload.get('email_recipients'):
        - L603 expr email_sent.append(payload['email_recipients'])
      - L604 return {'html_url': '/uploads/test.html', 'pdf_url': '/uploads/test.pdf'}
    - L609 assign scheduler = ReportScheduler(mock_runner, poll_seconds=5)
    - L611 assign mock_schedule = {'id': 'sched-1', 'template_id': 'template-1', 'connection_id': 'conn-1', 'start_date': '2024-01-01', 'end_date': '2024-01-31', 'email_recipients': ['test@example.com'], 'email_subject': 'Monthly Report', 'name': 'Monthly Report Schedule', 'template_kind': 'pdf'}
    - L623 with patch('backend.app.services.jobs.report_scheduler.state_store') as mock_store:
      - L624 assign mock_store.create_job.return_value = {'id': 'job-1'}
      - L625 assign mock_store.record_schedule_run.return_value = None
      - L627 with patch('backend.app.services.jobs.report_scheduler._build_job_steps', return_value=[]):
        - L628 with patch('backend.app.services.jobs.report_scheduler._step_progress_from_steps', return_value={}):
          - L629 expr await scheduler._run_schedule(mock_schedule)
    - L632 assert len(email_sent) == 1
    - L633 assert 'test@example.com' in email_sent[0]
  - L635 def test_notification_strategy_calls_mailerself:
    - L636 docstring: "NotificationStrategy should call send_report_email."
    - L637 from backend.app.domain.reports.strategies import NotificationStrategy
    - L639 assign strategy = NotificationStrategy()
    - L641 with patch('backend.app.domain.reports.strategies.send_report_email', return_value=True) as mock_send:
      - L642 assign result = strategy.send(recipients=['test@example.com'], subject='Test', body='Test body', attachments=[])
      - L649 assert result is True
      - L650 expr mock_send.assert_called_once_with(to_addresses=['test@example.com'], subject='Test', body='Test body', attachments=[])
- L662 class TestMainAppLifespan:
  - L663 docstring: "Tests for backend.api lifespan management."
  - L666 async def test_lifespan_starts_schedulerself:
    - L667 docstring: "Lifespan should start scheduler on app startup."
    - L668 import sys
    - L671 assign backend_dir = Path(__file__).parent.parent.parent
    - L672 if str(backend_dir) not in sys.path:
      - L673 expr sys.path.insert(0, str(backend_dir))
    - L675 try:
      - L676 with patch.dict(os.environ, {'NEURA_SCHEDULER_DISABLED': 'false'}):
        - L677 from backend.api import lifespan
        - L678 from fastapi import FastAPI
        - L680 assign test_app = FastAPI()
        - L682 with patch('backend.api.ReportScheduler') as MockScheduler:
          - L683 assign mock_scheduler = AsyncMock()
          - L684 assign mock_scheduler.start = AsyncMock()
          - L685 assign mock_scheduler.stop = AsyncMock()
          - L686 assign MockScheduler.return_value = mock_scheduler
          - L689 import backend.api as api_module
          - L690 assign original_scheduler = api_module.SCHEDULER
          - L691 assign api_module.SCHEDULER = None
          - L693 try:
            - L694 async with lifespan(test_app):
              - L696 expr mock_scheduler.start.assert_called_once()
            - L699 expr mock_scheduler.stop.assert_called_once()
            - L701 finally:
              - L701 assign api_module.SCHEDULER = original_scheduler
      - L702 except Exception as e:
        - L703 expr pytest.skip(f'Cannot test lifespan: {e}')
  - L706 async def test_lifespan_respects_disabled_flagself:
    - L707 docstring: "Lifespan should not start scheduler when disabled."
    - L708 import sys
    - L711 assign backend_dir = Path(__file__).parent.parent.parent
    - L712 if str(backend_dir) not in sys.path:
      - L713 expr sys.path.insert(0, str(backend_dir))
    - L715 try:
      - L716 with patch.dict(os.environ, {'NEURA_SCHEDULER_DISABLED': 'true'}):
        - L717 from backend.api import lifespan, SCHEDULER_DISABLED
        - L718 from fastapi import FastAPI
        - L720 assign test_app = FastAPI()
        - L723 import backend.api as api_module
        - L724 assign original_scheduler = api_module.SCHEDULER
        - L725 assign api_module.SCHEDULER = None
        - L727 try:
          - L728 with patch('backend.api.ReportScheduler') as MockScheduler:
            - L729 assign mock_scheduler = AsyncMock()
            - L730 assign MockScheduler.return_value = mock_scheduler
            - L732 async with lifespan(test_app):
              - L733 pass
          - L738 finally:
            - L738 assign api_module.SCHEDULER = original_scheduler
      - L739 except Exception as e:
        - L740 expr pytest.skip(f'Cannot test lifespan: {e}')

## backend\tests\test_schema_normalize.py
- L1 from backend.app.services.templates.TemplateVerify import normalize_schema_for_initial_html
- L6 def test_normalize_preserves_legacy_shape:
  - L7 assign legacy = {'scalars': {'name': 'Name', 'date': 'Date'}, 'blocks': {'rows': ['row_token'], 'headers': ['Row Token']}, 'notes': 'legacy schema', 'page_tokens_protect': ['page_no']}
  - L14 assign normalized = normalize_schema_for_initial_html(legacy)
  - L16 assert normalized == legacy
- L19 def test_normalize_handles_enhanced_schema_metadata:
  - L20 assign enhanced = {'scalars': {'name': {'label': 'Name Label', 'token': 'name', 'bbox_mm': [0, 0, 10, 5]}, 'code': {'label': 'Code Label', 'type': 'id'}}, 'blocks': {'rows': [{'token': 'item_no'}, {'name': 'qty'}], 'headers': ['Item No', 'Qty'], 'repeat_regions': [{'kind': 'table', 'selector': 'batch-block'}]}, 'notes': 'enhanced schema', 'page_tokens_protect': ['page_no', 'page_total']}
  - L37 assign normalized = normalize_schema_for_initial_html(enhanced)
  - L39 assert normalized == {'scalars': {'name': 'Name Label', 'code': 'Code Label'}, 'blocks': {'rows': ['item_no', 'qty'], 'headers': ['Item No', 'Qty'], 'repeat_regions': [{'kind': 'table', 'selector': 'batch-block'}]}, 'notes': 'enhanced schema', 'page_tokens_protect': ['page_no', 'page_total']}

## backend\tests\test_template_edit.py
- L1 from __future__ import annotations
- L3 import json
- L4 from pathlib import Path
- L6 import pytest
- L7 from fastapi.testclient import TestClient
- L9 from backend import api
- L10 from backend.app.services.state import StateStore
- L13 def _write_final_htmlroot: Path, template_id: str, html: str:
  - L14 assign tdir = root / template_id
  - L15 expr tdir.mkdir(parents=True, exist_ok=True)
  - L16 expr (tdir / 'report_final.html').write_text(html, encoding='utf-8')
  - L17 return tdir
- L20 def _register_templatestore: StateStore, template_id: str:
  - L21 expr store.upsert_template(template_id, name=f'Template {template_id}', status='approved')
- L25 def edit_envtmp_path, monkeypatch:
  - L26 assign uploads_root = tmp_path / 'uploads'
  - L27 expr uploads_root.mkdir(parents=True, exist_ok=True)
  - L28 assign excel_root = tmp_path / 'excel-uploads'
  - L29 expr excel_root.mkdir(parents=True, exist_ok=True)
  - L31 expr monkeypatch.setattr(api, 'UPLOAD_ROOT', uploads_root)
  - L32 expr monkeypatch.setattr(api, 'UPLOAD_ROOT_BASE', uploads_root.resolve())
  - L33 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT', excel_root)
  - L34 expr monkeypatch.setattr(api, 'EXCEL_UPLOAD_ROOT_BASE', excel_root.resolve())
  - L35 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'pdf', (uploads_root.resolve(), '/uploads'))
  - L36 expr monkeypatch.setitem(api._UPLOAD_KIND_BASES, 'excel', (excel_root.resolve(), '/excel-uploads'))
  - L37 expr monkeypatch.setattr('src.core.config.UPLOAD_ROOT', uploads_root)
  - L38 expr monkeypatch.setattr('src.core.config.EXCEL_UPLOAD_ROOT', excel_root)
  - L40 assign store = StateStore(tmp_path / 'state')
  - L41 expr monkeypatch.setattr(api, 'state_store', store, raising=False)
  - L42 expr monkeypatch.setattr('backend.app.services.state.state_store', store)
  - L43 expr monkeypatch.setattr('backend.app.services.state.store.state_store', store)
  - L44 expr monkeypatch.setattr('src.services.file_service.helpers.state_store', store)
  - L46 assign client = TestClient(api.app)
  - L47 return {'client': client, 'uploads_root': uploads_root, 'state_store': store}
- L50 def test_manual_edit_snapshots_and_diffedit_env:
  - L51 annotated assign client: TestClient = edit_env['client']
  - L52 annotated assign uploads_root: Path = edit_env['uploads_root']
  - L53 annotated assign state_store: StateStore = edit_env['state_store']
  - L54 assign template_id = 'tpl-manual'
  - L55 expr _register_template(state_store, template_id)
  - L56 assign tdir = _write_final_html(uploads_root, template_id, '<html>old</html>')
  - L58 assign response = client.post(f'/templates/{template_id}/edit-manual', json={'html': '<html>new</html>'})
  - L59 assert response.status_code == 200
  - L60 assign data = response.json()
  - L62 assert data['diff_summary'] == '+1 line, -1 line'
  - L63 assert data['metadata']['historyCount'] == 1
  - L64 assert (tdir / 'report_final_prev.html').read_text(encoding='utf-8') == '<html>old</html>'
  - L65 assert (tdir / 'report_final.html').read_text(encoding='utf-8') == '<html>new</html>'
- L68 def test_ai_edit_records_diff_and_prev_copyedit_env, monkeypatch:
  - L69 annotated assign client: TestClient = edit_env['client']
  - L70 annotated assign uploads_root: Path = edit_env['uploads_root']
  - L71 annotated assign state_store: StateStore = edit_env['state_store']
  - L72 assign template_id = 'tpl-ai'
  - L73 expr _register_template(state_store, template_id)
  - L74 assign tdir = _write_final_html(uploads_root, template_id, '<html>start</html>')
  - L76 def fake_llmhtml: str, instructions: str:
    - L77 return ('<html>ai</html>', [f'applied:{instructions}'])
  - L79 expr monkeypatch.setattr('src.services.file_service.edit._run_template_edit_llm', fake_llm)
  - L81 assign response = client.post(f'/templates/{template_id}/edit-ai', json={'instructions': 'make it better'})
  - L85 assert response.status_code == 200
  - L86 assign data = response.json()
  - L88 assert data['diff_summary'] == '+1 line, -1 line'
  - L89 assert data['summary'] == ['applied:make it better']
  - L90 assert (tdir / 'report_final_prev.html').read_text(encoding='utf-8') == '<html>start</html>'
  - L91 assert (tdir / 'report_final.html').read_text(encoding='utf-8') == '<html>ai</html>'
- L94 def test_undo_swaps_versions_and_reports_diffedit_env:
  - L95 annotated assign client: TestClient = edit_env['client']
  - L96 annotated assign uploads_root: Path = edit_env['uploads_root']
  - L97 annotated assign state_store: StateStore = edit_env['state_store']
  - L98 assign template_id = 'tpl-undo'
  - L99 expr _register_template(state_store, template_id)
  - L100 assign tdir = _write_final_html(uploads_root, template_id, '<html>current</html>')
  - L101 expr (tdir / 'report_final_prev.html').write_text('<html>previous</html>', encoding='utf-8')
  - L103 assign response = client.post(f'/templates/{template_id}/undo-last-edit')
  - L104 assert response.status_code == 200
  - L105 assign data = response.json()
  - L107 assert data['metadata']['lastEditType'] == 'undo'
  - L108 assert data['diff_summary'] == '+1 line, -1 line'
  - L109 assert (tdir / 'report_final.html').read_text(encoding='utf-8') == '<html>previous</html>'
  - L110 assert (tdir / 'report_final_prev.html').read_text(encoding='utf-8') == '<html>current</html>'
- L113 def test_template_history_is_capped_at_two_entriesedit_env:
  - L114 annotated assign client: TestClient = edit_env['client']
  - L115 annotated assign uploads_root: Path = edit_env['uploads_root']
  - L116 annotated assign state_store: StateStore = edit_env['state_store']
  - L117 assign template_id = 'tpl-history'
  - L118 expr _register_template(state_store, template_id)
  - L119 assign tdir = _write_final_html(uploads_root, template_id, '<html>v0</html>')
  - L121 annotated assign timestamps: list[str | None] = []
  - L122 for idx in range(3):
    - L123 assign html = f'<html>v{idx + 1}</html>'
    - L124 assign response = client.post(f'/templates/{template_id}/edit-manual', json={'html': html})
    - L125 assert response.status_code == 200
    - L126 expr timestamps.append(response.json()['metadata']['lastEditAt'])
  - L128 assign history_path = tdir / 'template_history.json'
  - L129 assign history = json.loads(history_path.read_text(encoding='utf-8'))
  - L130 assert len(history) == 2
  - L131 assert history[0].get('timestamp') == timestamps[1]
  - L132 assert history[1].get('timestamp') == timestamps[2]

## backend\tests\test_template_initial_html.py
- L1 from __future__ import annotations
- L3 import json
- L4 from pathlib import Path
- L6 import pytest
- L8 from backend.app.services.templates import TemplateVerify as tv
- L9 from backend.app.services.utils import write_json_atomic
- L10 from backend.app.services.utils.artifacts import MANIFEST_NAME, write_artifact_manifest
- L13 class _DummyMessage:
  - L14 def __init__self, content: str:
    - L15 assign self.content = content
- L18 class _DummyChoice:
  - L19 def __init__self, content: str:
    - L20 assign self.message = _DummyMessage(content)
- L23 class _DummyResponse:
  - L24 def __init__self, content: str:
    - L25 assign self.choices = [_DummyChoice(content)]
- L28 def _make_pngpath: Path:
  - L29 expr path.write_bytes(b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x02\x00\x00\x00\x90wS\xde\x00\x00\x00\x00IEND\xaeB`\x82')
- L38 def stubbed_llmmonkeypatch:
  - L39 def factorycontent: str:
    - L40 def _fake_callclient, **kwargs:
      - L41 return _DummyResponse(content)
    - L43 expr monkeypatch.setattr(tv, 'call_chat_completion', _fake_call)
    - L44 expr monkeypatch.setattr(tv, 'get_openai_client', lambda: object())
  - L46 return factory
- L49 def test_request_initial_html_with_schematmp_path: Path, stubbed_llm:
  - L50 assign html_section = '<!DOCTYPE html>\n<html><body><span class="title">{{ report_title }}</span><table><tr><td>{line_amount}</td></tr></table><footer>{grand_total}</footer></body></html>'
  - L52 assign schema_section = json.dumps({'scalars': ['report_title'], 'row_tokens': ['line_amount'], 'totals': ['grand_total'], 'notes': ''})
  - L60 assign content = f'<!--BEGIN_HTML-->\n{html_section}\n<!--END_HTML-->\n<!--BEGIN_SCHEMA_JSON-->\n{schema_section}\n<!--END_SCHEMA_JSON-->'
  - L67 expr stubbed_llm(content)
  - L69 assign png_path = tmp_path / 'reference_p1.png'
  - L70 expr _make_png(png_path)
  - L71 assign pdf_path = tmp_path / 'source.pdf'
  - L72 expr pdf_path.write_bytes(b'%PDF')
  - L74 assign result = tv.request_initial_html(png_path, schema_json={}, layout_hints=None)
  - L75 assert result.schema == {'scalars': ['report_title'], 'row_tokens': ['line_amount'], 'totals': ['grand_total'], 'notes': ''}
  - L81 assert '{report_title}' in result.html
  - L82 assert '{{' not in result.html
  - L84 assign html_path = tmp_path / 'template_p1.html'
  - L85 expr tv.save_html(html_path, result.html)
  - L86 assert html_path.exists()
  - L88 assign schema_path = tmp_path / 'schema_ext.json'
  - L89 expr write_json_atomic(schema_path, result.schema, indent=2, ensure_ascii=False, sort_keys=True, step='test_schema_ext')
  - L97 assign payload = json.loads(schema_path.read_text(encoding='utf-8'))
  - L98 assert payload['scalars'] == ['report_title']
  - L100 assign manifest_path = write_artifact_manifest(tmp_path, step='unit_test', files={'source.pdf': pdf_path, 'reference_p1.png': png_path, 'template_p1.html': html_path, 'schema_ext.json': schema_path}, inputs=[str(pdf_path)], correlation_id='unit')
  - L112 assign manifest = json.loads(manifest_path.read_text(encoding='utf-8'))
  - L113 assert MANIFEST_NAME == manifest_path.name
  - L114 assert 'schema_ext.json' in manifest['files']
- L117 def test_request_initial_html_without_schematmp_path: Path, stubbed_llm:
  - L118 assign html_section = '<html><body><p>{{missing_schema}}</p></body></html>'
  - L119 assign content = f'<!--BEGIN_HTML-->\n{html_section}\n<!--END_HTML-->'
  - L123 expr stubbed_llm(content)
  - L125 assign png_path = tmp_path / 'reference_p1.png'
  - L126 expr _make_png(png_path)
  - L128 assign result = tv.request_initial_html(png_path, schema_json=None, layout_hints=None)
  - L129 assert result.schema is None
  - L130 assert '{missing_schema}' in result.html
  - L131 assert '{{' not in result.html
  - L133 assign html_path = tmp_path / 'template_p1.html'
  - L134 expr tv.save_html(html_path, result.html)
  - L135 assert html_path.exists()
  - L136 assign saved_html = html_path.read_text(encoding='utf-8')
  - L137 assert '{missing_schema}' in saved_html
  - L140 assign schema_path = tmp_path / 'schema_ext.json'
  - L141 assert not schema_path.exists()

## backend\tests\test_template_recommend_api.py
- L1 from __future__ import annotations
- L3 import os
- L5 import pytest
- L6 from fastapi.testclient import TestClient
- L8 expr os.environ.setdefault('NEURA_ALLOW_MISSING_OPENAI', 'true')
- L10 from backend import api
- L11 from backend.app.services.state import StateStore
- L12 import src.services.template_service as template_service
- L16 def clienttmp_path, monkeypatch:
  - L17 assign state_dir = tmp_path / 'state'
  - L18 expr state_dir.mkdir()
  - L19 assign store = StateStore(base_dir=state_dir)
  - L20 expr monkeypatch.setattr(api, 'state_store', store, raising=False)
  - L21 expr monkeypatch.setattr('backend.app.services.state.state_store', store)
  - L22 expr monkeypatch.setattr('backend.app.services.state.store.state_store', store)
  - L23 return TestClient(api.app)
- L26 def test_templates_recommend_threads_hints_into_promptclient: TestClient, monkeypatch:
  - L27 assign catalog = [{'id': 'tpl-1', 'name': 'Starter', 'kind': 'pdf', 'domain': 'finance', 'tags': ['kpi'], 'useCases': ['dashboard'], 'primaryMetrics': ['revenue'], 'source': 'starter'}]
  - L39 expr monkeypatch.setattr(template_service, 'build_unified_template_catalog', lambda: catalog)
  - L41 annotated assign captured: dict = {}
  - L43 def fake_recommendcatalog_arg, *, requirement, hints, max_results:
    - L44 assign captured['catalog'] = catalog_arg
    - L45 assign captured['requirement'] = requirement
    - L46 assign captured['hints'] = hints
    - L47 assign captured['max_results'] = max_results
    - L48 return [{'id': 'tpl-1', 'explanation': 'match', 'score': 0.9}]
  - L50 expr monkeypatch.setattr(template_service, 'recommend_templates_from_catalog', fake_recommend)
  - L52 assign payload = {'requirement': 'Revenue dashboard', 'kind': 'pdf', 'kinds': ['excel', 'pdf'], 'domain': 'growth', 'domains': ['finance', 'growth'], 'schema_snapshot': {'tables': ['revenues']}, 'tables': ['revenues', 'users', 'revenues']}
  - L61 assign resp = client.post('/templates/recommend', json=payload)
  - L62 assert resp.status_code == 200
  - L64 assign body = resp.json()
  - L65 assert body['recommendations'][0]['template']['id'] == 'tpl-1'
  - L66 assert body['recommendations'][0]['score'] == pytest.approx(0.9)
  - L67 assert body['recommendations'][0]['explanation']
  - L69 assert captured['catalog'] == catalog
  - L70 assert captured['requirement'] == 'Revenue dashboard'
  - L71 assign hints = captured['hints']
  - L72 assert hints['kind'] == 'pdf'
  - L73 assert hints['kinds'] == ['pdf', 'excel']
  - L74 assert hints['domain'] == 'growth'
  - L75 assert hints['domains'] == ['growth', 'finance']
  - L76 assert hints['schema_snapshot'] == {'tables': ['revenues']}
  - L77 assert hints['tables'] == ['revenues', 'users']
  - L78 assert captured['max_results'] == 6
- L81 def test_templates_recommend_empty_catalog_returns_empty_listclient: TestClient, monkeypatch:
  - L82 expr monkeypatch.setattr(template_service, 'build_unified_template_catalog', lambda: [])
  - L84 assign called = {}
  - L86 def fake_recommendcatalog_arg, *, requirement, hints, max_results:
    - L87 assign called['called'] = True
    - L88 assert catalog_arg == []
    - L89 assert hints == {}
    - L90 return []
  - L92 expr monkeypatch.setattr(template_service, 'recommend_templates_from_catalog', fake_recommend)
  - L94 assign resp = client.post('/templates/recommend', json={'requirement': 'Anything goes'})
  - L95 assert resp.status_code == 200
  - L96 assert resp.json()['recommendations'] == []
  - L97 assert called.get('called') is True

## backend\tests\test_v4_connections.py
- L1 from __future__ import annotations
- L3 import pytest
- L5 from backend.app.core.errors import AppError
- L6 from backend.app.domain.connections.schemas import ConnectionTestRequest, ConnectionUpsertRequest
- L7 from backend.app.domain.connections.service import ConnectionService
- L10 class _FakeRepo:
  - L11 def __init__self:
    - L12 assign self.saved = {}
    - L13 assign self.deleted = set()
    - L14 assign self.pings = []
  - L16 def resolve_pathself, *, connection_id, db_url, db_path:
    - L17 if db_url == 'bad':
      - L18 raise RuntimeError('boom')
    - L19 return 'db.sqlite'
  - L21 def verifyself, path:
    - L22 return None
  - L24 def saveself, payload:
    - L25 assign self.saved['save'] = payload
    - L26 return 'conn-1'
  - L28 def upsertself, **kwargs:
    - L29 assign self.saved['upsert'] = kwargs
    - L30 return {'id': kwargs.get('conn_id') or 'conn-upsert', 'name': kwargs.get('name') or 'n', 'db_type': kwargs.get('db_type') or 'sqlite', 'database_path': kwargs.get('database_path') or '', 'status': kwargs.get('status') or 'connected', 'latency_ms': kwargs.get('latency_ms')}
  - L39 def listself:
    - L40 return []
  - L42 def deleteself, connection_id: str:
    - L43 if connection_id == 'missing':
      - L44 return False
    - L45 expr self.deleted.add(connection_id)
    - L46 return True
  - L48 def record_pingself, connection_id, status, detail, latency_ms:
    - L49 expr self.pings.append((connection_id, status, detail, latency_ms))
- L52 def test_connection_test_records_ping:
  - L53 assign repo = _FakeRepo()
  - L54 assign svc = ConnectionService(repo)
  - L55 assign payload = ConnectionTestRequest(db_url='sqlite:///tmp.db')
  - L56 assign result = svc.test(payload, correlation_id='cid-1')
  - L57 assert result['ok'] is True
  - L58 assert result['connection_id'] == 'conn-1'
  - L59 assert repo.pings[-1][0] == 'conn-1'
- L62 def test_connection_upsert_success:
  - L63 assign repo = _FakeRepo()
  - L64 assign svc = ConnectionService(repo)
  - L65 assign payload = ConnectionUpsertRequest(id='abc', database='file.db', name='Friendly')
  - L66 assign out = svc.upsert(payload)
  - L67 assert out.id == 'abc'
  - L68 assert out.name == 'Friendly'
  - L69 assert repo.saved['upsert']['database_path']
- L72 def test_connection_delete_missing_raises:
  - L73 assign repo = _FakeRepo()
  - L74 assign svc = ConnectionService(repo)
  - L75 with pytest.raises(AppError) as exc:
    - L76 expr svc.delete('missing')
  - L77 assert exc.value.code == 'connection_not_found'
- L80 def test_connection_invalid_db_raises:
  - L81 assign repo = _FakeRepo()
  - L82 assign svc = ConnectionService(repo)
  - L83 assign payload = ConnectionTestRequest(db_url='bad')
  - L84 with pytest.raises(AppError) as exc:
    - L85 expr svc.test(payload)
  - L86 assert exc.value.code == 'invalid_database'

## backend\tests\test_v4_templates.py
- L1 from __future__ import annotations
- L3 import io
- L4 import os
- L5 import zipfile
- L6 from pathlib import Path
- L8 import pytest
- L9 from starlette.datastructures import UploadFile
- L11 from backend.app.core.errors import AppError
- L12 from backend.app.domain.templates.service import TemplateService
- L13 from backend.app.services.state import StateStore
- L16 def _zip_bytescontents: dict[str, bytes]:
  - L17 assign buf = io.BytesIO()
  - L18 with zipfile.ZipFile(buf, mode='w') as zf:
    - L19 for (name, data) in contents.items():
      - L20 expr zf.writestr(name, data)
  - L21 return buf.getvalue()
- L25 def temp_statetmp_path, monkeypatch:
  - L26 assign state_dir = tmp_path / 'state'
  - L27 expr state_dir.mkdir()
  - L28 assign os.environ['NEURA_STATE_DIR'] = str(state_dir)
  - L29 assign store = StateStore(base_dir=state_dir)
  - L31 import backend.app.domain.templates.service as tsvc
  - L33 assign tsvc.state_store = store
  - L34 return store
- L38 async def test_template_import_writes_statetemp_state, tmp_path, monkeypatch:
  - L39 assign uploads = tmp_path / 'uploads'
  - L40 expr uploads.mkdir()
  - L41 assign excel_uploads = tmp_path / 'excel'
  - L42 expr excel_uploads.mkdir()
  - L44 assign manifest_payload = b'{"artifacts":{"template_html_url":"/uploads/x.html"}}'
  - L45 assign zip_bytes = _zip_bytes({'artifact_manifest.json': manifest_payload, 'template_p1.html': b'<html></html>'})
  - L51 assign upload = UploadFile(filename='tpl.zip', file=io.BytesIO(zip_bytes))
  - L53 assign service = TemplateService(uploads_root=uploads, excel_uploads_root=excel_uploads, max_bytes=1024 * 1024)
  - L54 assign result = await service.import_zip(upload, display_name='My Template', correlation_id='cid-123')
  - L56 assert result['template_id']
  - L57 assert temp_state.list_templates()
  - L58 assign rec = temp_state.list_templates()[0]
  - L59 assert rec['name'] == 'My Template'
  - L60 assert rec['artifacts'].get('template_html_url')
- L64 async def test_template_import_rejects_oversizetmp_path, monkeypatch:
  - L65 assign uploads = tmp_path / 'uploads'
  - L66 assign excel_uploads = tmp_path / 'excel'
  - L67 expr uploads.mkdir()
  - L68 expr excel_uploads.mkdir()
  - L69 assign big_bytes = b'a' * 2048
  - L70 assign upload = UploadFile(filename='big.zip', file=io.BytesIO(big_bytes))
  - L71 assign service = TemplateService(uploads_root=uploads, excel_uploads_root=excel_uploads, max_bytes=64)
  - L72 with pytest.raises(AppError) as exc:
    - L73 expr await service.import_zip(upload, display_name=None, correlation_id=None)
  - L74 assert exc.value.code == 'upload_too_large'

## backend\tests\test_validate_excel_bundle.py
- L1 from __future__ import annotations
- L3 import json
- L4 from pathlib import Path
- L6 from scripts.validate_excel_bundle import validate_excel_upload
- L9 def _write_jsonpath: Path, data: dict:
  - L10 expr path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
- L13 def _sample_contract:
  - L14 return {'tokens': {'scalars': [], 'row_tokens': ['row_value'], 'totals': []}, 'mapping': {'row_value': 'rows.row_value'}, 'join': {'parent_table': 'batches', 'parent_key': 'id', 'child_table': 'rows', 'child_key': 'batch_id'}, 'date_columns': {}, 'filters': {}, 'reshape_rules': [{'purpose': 'rows dataset', 'strategy': 'select', 'alias': 'rows', 'from': 'rows', 'columns': [{'as': 'row_value', 'from': ['rows.row_value']}]}], 'row_computed': {}, 'totals_math': {}, 'formatters': {}, 'order_by': {'rows': ['rows.row_value ASC']}, 'header_tokens': [], 'row_tokens': ['row_value'], 'totals': {}, 'row_order': ['rows.row_value ASC']}
- L45 def _sample_step5:
  - L46 return {'parameters': {'required': [], 'optional': []}, 'datasets': [{'alias': 'rows', 'source_table': 'rows', 'description': '', 'columns': [{'as': 'row_value', 'from': 'rows.row_value'}]}], 'reshape_rules': [], 'order_by': ['rows.row_value ASC'], 'row_order': ['rows.row_value ASC']}
- L62 def _sample_generator_meta:
  - L63 return {'dialect': 'duckdb', 'entrypoints': {'header': 'SELECT 1', 'rows': 'SELECT 1 AS row_value', 'totals': 'SELECT 1'}, 'params': {'required': [], 'optional': []}, 'needs_user_fix': [], 'invalid': False, 'summary': {}, 'cached': False, 'key_tokens': []}
- L79 def test_validate_excel_bundle_flags_missing_filestmp_path:
  - L80 assign bundle_dir = tmp_path / 'bundle'
  - L81 expr bundle_dir.mkdir()
  - L82 assign issues = validate_excel_upload(bundle_dir)
  - L83 assert issues and 'Missing required files' in issues[0]
- L86 def test_validate_excel_bundle_passes_with_valid_payloadtmp_path:
  - L87 assign bundle_dir = tmp_path / 'bundle'
  - L88 assign generator_dir = bundle_dir / 'generator'
  - L89 expr generator_dir.mkdir(parents=True)
  - L91 expr _write_json(bundle_dir / 'contract.json', _sample_contract())
  - L92 expr _write_json(bundle_dir / 'step5_requirements.json', _sample_step5())
  - L93 expr _write_json(generator_dir / 'generator_assets.json', _sample_generator_meta())
  - L94 expr _write_json(generator_dir / 'output_schemas.json', {'header': [], 'rows': ['row_value'], 'totals': []})
  - L95 expr (generator_dir / 'sql_pack.sql').write_text('-- HEADER SELECT --\nSELECT 1;\n-- ROWS SELECT --\nSELECT 1 AS row_value;\n-- TOTALS SELECT --\nSELECT 1;\n', encoding='utf-8')
  - L100 assign issues = validate_excel_upload(bundle_dir)
  - L101 assert issues == []

## backend\tests\test_zip_tools_limits.py
- L1 from __future__ import annotations
- L3 import zipfile
- L5 import pytest
- L7 from backend.app.services.utils.zip_tools import extract_zip_to_dir
- L10 def _create_zipzip_path, files:
  - L11 with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
    - L12 for (name, data) in files.items():
      - L13 expr zf.writestr(name, data)
- L16 def test_extract_zip_to_dir_enforces_entry_limittmp_path:
  - L17 assign zip_path = tmp_path / 'test.zip'
  - L18 assign files = {f'file_{i}.txt': b'data' for i in range(3)}
  - L19 expr _create_zip(zip_path, files)
  - L21 with pytest.raises(ValueError):
    - L22 expr extract_zip_to_dir(zip_path, tmp_path / 'out', max_entries=2)
- L25 def test_extract_zip_to_dir_enforces_uncompressed_limittmp_path:
  - L26 assign zip_path = tmp_path / 'test.zip'
  - L27 assign files = {'big.txt': b'a' * 1024}
  - L28 expr _create_zip(zip_path, files)
  - L30 with pytest.raises(ValueError):
    - L31 expr extract_zip_to_dir(zip_path, tmp_path / 'out', max_uncompressed_bytes=512)

## src\__init__.py
- L1 docstring: "V4 refactored modules."

## src\core\__init__.py
- L1 docstring: "Core configuration and app wiring."

## src\core\config.py
- L1 from __future__ import annotations
- L3 from pathlib import Path
- L5 from backend.app.core.config import get_settings
- L7 assign SETTINGS = get_settings()
- L8 assign APP_VERSION = SETTINGS.version
- L9 assign APP_COMMIT = SETTINGS.commit
- L11 annotated assign UPLOAD_ROOT: Path = SETTINGS.uploads_root
- L12 annotated assign EXCEL_UPLOAD_ROOT: Path = SETTINGS.excel_uploads_root
- L15 def get_settings:
  - L16 return SETTINGS

## src\endpoints\__init__.py
- L1 docstring: "HTTP route modules."

## src\endpoints\artifacts.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Request
- L5 from src.services.file_service import artifact_head_response, artifact_manifest_response
- L7 assign router = APIRouter()
- L10 def _correlationrequest: Request:
  - L11 return getattr(request.state, 'correlation_id', None)
- L14 def _wrappayload: dict, correlation_id: str | None:
  - L15 assign payload = dict(payload)
  - L16 if correlation_id is not None:
    - L17 assign payload['correlation_id'] = correlation_id
  - L18 return payload
- L22 def get_artifact_manifesttemplate_id: str, request: Request:
  - L23 assign data = artifact_manifest_response(template_id, kind='pdf')
  - L24 return _wrap(data, _correlation(request))
- L28 def get_artifact_manifest_exceltemplate_id: str, request: Request:
  - L29 assign data = artifact_manifest_response(template_id, kind='excel')
  - L30 return _wrap(data, _correlation(request))
- L34 def get_artifact_headtemplate_id: str, request: Request, name: str:
  - L35 assign data = artifact_head_response(template_id, name, kind='pdf')
  - L36 return _wrap(data, _correlation(request))
- L40 def get_artifact_head_exceltemplate_id: str, request: Request, name: str:
  - L41 assign data = artifact_head_response(template_id, name, kind='excel')
  - L42 return _wrap(data, _correlation(request))

## src\endpoints\connections.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, Depends, HTTPException, Request
- L5 from src.core.config import get_settings
- L6 from src.schemas.connection_schema import ConnectionUpsertPayload, TestPayload
- L7 from src.services.connection_service import delete_connection, healthcheck_connection, list_connections, test_connection, upsert_connection
- L14 from src.services.connection_inspector import get_connection_schema, get_connection_table_preview
- L16 assign router = APIRouter()
- L19 def _correlationrequest: Request:
  - L20 return getattr(request.state, 'correlation_id', None)
- L24 def test_connection_routepayload: TestPayload, request: Request, settings=Depends(get_settings):
  - L25 assign result = test_connection(payload)
  - L26 assign result['correlation_id'] = _correlation(request)
  - L27 return result
- L31 def list_connections_routerequest: Request:
  - L32 return {'status': 'ok', 'connections': list_connections(), 'correlation_id': _correlation(request)}
- L36 def upsert_connection_routepayload: ConnectionUpsertPayload, request: Request:
  - L37 assign record = upsert_connection(payload)
  - L38 return {'status': 'ok', 'connection': record, 'correlation_id': _correlation(request)}
- L42 def delete_connection_routeconnection_id: str, request: Request:
  - L43 assign removed = delete_connection(connection_id)
  - L44 if not removed:
    - L45 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'connection_not_found', 'message': 'Connection not found.'})
  - L46 return {'status': 'ok', 'connection_id': connection_id, 'correlation_id': _correlation(request)}
- L50 def healthcheck_connection_routeconnection_id: str, request: Request:
  - L51 assign result = healthcheck_connection(connection_id)
  - L52 assign result['correlation_id'] = _correlation(request)
  - L53 return result
- L57 def connection_schema_routeconnection_id: str, request: Request, include_row_counts: bool=True, include_foreign_keys: bool=True, sample_rows: int=0:
  - L64 assign result = get_connection_schema(connection_id, include_row_counts=include_row_counts, include_foreign_keys=include_foreign_keys, sample_rows=sample_rows)
  - L70 assign result['correlation_id'] = _correlation(request)
  - L71 return result
- L75 def connection_preview_routeconnection_id: str, request: Request, table: str, limit: int=10, offset: int=0:
  - L82 assign result = get_connection_table_preview(connection_id, table=table, limit=limit, offset=offset)
  - L88 assign result['correlation_id'] = _correlation(request)
  - L89 return result

## src\endpoints\feature_routes.py
- L1 from __future__ import annotations
- L3 import logging
- L4 import os
- L6 from fastapi import HTTPException
- L8 from backend.app.features.generate.routes.chart_suggest_routes import build_chart_suggest_router
- L9 from backend.app.features.generate.routes.discover_routes import build_discover_router
- L10 from backend.app.features.generate.routes.saved_charts_routes import build_saved_charts_router
- L11 from backend.app.features.generate.schemas.reports import DiscoverPayload
- L12 from backend.app.features.generate.services.chart_suggestions_service import suggest_charts as suggest_charts_service
- L13 from backend.app.features.generate.services.discovery_service import discover_reports as discover_reports_service
- L14 from backend.app.services.prompts.llm_prompts_charts import CHART_SUGGEST_PROMPT_VERSION, build_chart_suggestions_prompt
- L18 from backend.app.services.prompts.llm_prompts import PROMPT_VERSION, PROMPT_VERSION_3_5, PROMPT_VERSION_4
- L19 from backend.app.services.contract.ContractBuilderV2 import load_contract_v2
- L20 from backend.app.services.state import store as state_store_module
- L21 from backend.app.services.utils import call_chat_completion, get_correlation_id, strip_code_fences
- L22 from backend.app.services.utils.artifacts import load_manifest
- L23 from backend.app.services.reports.discovery import discover_batches_and_counts
- L24 from backend.app.services.reports.discovery_excel import discover_batches_and_counts as discover_batches_and_counts_excel
- L25 from backend.app.services.reports.discovery_metrics import build_batch_field_catalog_and_stats, build_batch_metrics
- L26 from backend.app.services.templates.TemplateVerify import get_openai_client
- L27 from src.utils.connection_utils import db_path_from_payload_or_default
- L28 from src.utils.schedule_utils import clean_key_values
- L29 from src.utils.template_utils import manifest_endpoint, normalize_template_id, template_dir
- L31 assign _build_sample_data_rows = lambda batches, metadata=None, limit=100: build_batch_metrics(batches, metadata or {}, limit=limit)
- L37 assign DEFAULT_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
- L40 def _state_store:
  - L41 return state_store_module.state_store
- L44 def _ensure_template_existstemplate_id: str:
  - L45 assign normalized = normalize_template_id(template_id)
  - L46 assign record = _state_store().get_template_record(normalized)
  - L47 if not record:
    - L48 raise HTTPException(status_code=404, detail='template_not_found')
  - L49 return (normalized, record)
- L52 def build_feature_routers:
  - L53 assign logger = logging.getLogger('neura.api')
  - L54 assign saved_charts_router = build_saved_charts_router(_ensure_template_exists, normalize_template_id)
  - L56 assign chart_suggest_router = build_chart_suggest_router(template_dir_fn=template_dir, db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_pdf_fn=discover_batches_and_counts, discover_excel_fn=discover_batches_and_counts_excel, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_metrics_fn=build_batch_metrics, build_prompt_fn=build_chart_suggestions_prompt, call_chat_completion_fn=lambda **kwargs: call_chat_completion(get_openai_client(), **kwargs, description=CHART_SUGGEST_PROMPT_VERSION), model=DEFAULT_MODEL, strip_code_fences_fn=strip_code_fences, get_correlation_id_fn=get_correlation_id, logger=logger)
  - L75 assign discover_router = build_discover_router(template_dir_fn=template_dir, db_path_fn=db_path_from_payload_or_default, load_contract_fn=load_contract_v2, clean_key_values_fn=clean_key_values, discover_pdf_fn=discover_batches_and_counts, discover_excel_fn=discover_batches_and_counts_excel, build_field_catalog_fn=build_batch_field_catalog_and_stats, build_batch_metrics_fn=build_batch_metrics, load_manifest_fn=load_manifest, manifest_endpoint_fn_pdf=lambda tpl_id, kind='pdf': manifest_endpoint(tpl_id, kind=kind), manifest_endpoint_fn_excel=lambda tpl_id, kind='excel': manifest_endpoint(tpl_id, kind=kind), logger=logger)
  - L90 return (saved_charts_router, chart_suggest_router, discover_router)
- L93 assign __all__ = ['build_feature_routers', 'DiscoverPayload', 'discover_reports_service', 'PROMPT_VERSION', 'PROMPT_VERSION_3_5', 'PROMPT_VERSION_4', 'suggest_charts_service']

## src\endpoints\health.py
- L1 from __future__ import annotations
- L3 import os
- L5 from fastapi import APIRouter, Request
- L7 from src.core.config import SETTINGS, UPLOAD_ROOT
- L8 from src.utils.health_utils import check_clock, check_external_head, check_fs_writable, health_response
- L10 assign router = APIRouter()
- L14 def health:
  - L15 return {'status': 'ok'}
- L19 def healthzrequest: Request:
  - L20 annotated assign checks: dict[str, tuple[bool, str]] = {}
  - L21 assign checks['fs_write'] = check_fs_writable(UPLOAD_ROOT.resolve())
  - L22 assign checks['clock'] = check_clock()
  - L23 assign external_url = os.getenv('NEURA_HEALTH_EXTERNAL_HEAD')
  - L24 if external_url:
    - L25 assign checks['external'] = check_external_head(external_url, SETTINGS.openai_api_key or None)
  - L26 return health_response(request, checks)
- L30 def readyzrequest: Request:
  - L31 annotated assign checks: dict[str, tuple[bool, str]] = {}
  - L32 assign checks['fs_write'] = check_fs_writable(UPLOAD_ROOT.resolve())
  - L33 assign checks['clock'] = check_clock()
  - L34 assign checks['openai_key'] = (bool(SETTINGS.openai_api_key), 'configured' if SETTINGS.openai_api_key else 'missing')
  - L38 assign external_url = os.getenv('NEURA_HEALTH_EXTERNAL_HEAD') or 'https://api.openai.com/v1/models'
  - L39 assign checks['external'] = check_external_head(external_url, SETTINGS.openai_api_key or None)
  - L40 return health_response(request, checks)

## src\endpoints\jobs.py
- L1 from __future__ import annotations
- L3 from typing import Any, Dict, List, Optional
- L5 from fastapi import APIRouter, Query, Request
- L7 from src.services.scheduler_service import get_job, list_active_jobs, list_jobs, cancel_job
- L9 assign router = APIRouter()
- L12 def _normalize_job_statusstatus: Optional[str]:
  - L13 docstring: "Normalize job status to consistent UI-friendly values.\n\n    Maps backend statu..."
  - L18 assign value = (status or '').strip().lower()
  - L19 if value in {'succeeded', 'success', 'done'}:
    - L20 return 'completed'
  - L21 if value in {'queued'}:
    - L22 return 'pending'
  - L23 if value in {'in_progress', 'started'}:
    - L24 return 'running'
  - L25 if value in {'error'}:
    - L26 return 'failed'
  - L27 if value in {'canceled'}:
    - L28 return 'cancelled'
  - L30 if value in {'pending', 'running', 'completed', 'failed', 'cancelled', 'cancelling'}:
    - L31 return value
  - L32 return value or 'pending'
- L35 def _normalize_jobjob: Optional[Dict[str, Any]]:
  - L36 docstring: "Normalize a job record for consistent API responses."
  - L37 if not job:
    - L38 return job
  - L39 assign normalized = dict(job)
  - L40 if 'status' in normalized:
    - L41 assign normalized['status'] = _normalize_job_status(normalized['status'])
  - L42 if 'state' in normalized and 'status' not in normalized:
    - L43 assign normalized['status'] = _normalize_job_status(normalized['state'])
  - L44 return normalized
- L47 def _correlationrequest: Request:
  - L48 return getattr(request.state, 'correlation_id', None)
- L52 def list_jobs_routerequest: Request, status: Optional[List[str]]=Query(None), job_type: Optional[List[str]]=Query(None, alias='type'), limit: int=Query(50, ge=1, le=200), active_only: bool=Query(False):
  - L59 assign jobs = list_jobs(status, job_type, limit, active_only)
  - L60 assign normalized_jobs = [_normalize_job(job) for job in jobs] if jobs else []
  - L61 return {'jobs': normalized_jobs, 'correlation_id': _correlation(request)}
- L65 def list_active_jobs_routerequest: Request, limit: int=Query(20, ge=1, le=200):
  - L66 assign jobs = list_active_jobs(limit)
  - L67 assign normalized_jobs = [_normalize_job(job) for job in jobs] if jobs else []
  - L68 return {'jobs': normalized_jobs, 'correlation_id': _correlation(request)}
- L72 def get_job_routejob_id: str, request: Request:
  - L73 assign job = get_job(job_id)
  - L74 return {'job': _normalize_job(job), 'correlation_id': _correlation(request)}
- L78 def cancel_job_routejob_id: str, request: Request, force: bool=Query(False):
  - L79 assign job = cancel_job(job_id, force=force)
  - L80 return {'job': _normalize_job(job), 'correlation_id': _correlation(request)}

## src\endpoints\reports.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, HTTPException, Request
- L5 from backend.app.features.generate.schemas.reports import RunPayload
- L6 from src.services.report_service import queue_report_job, run_report as run_report_service, list_report_runs as list_report_runs_service, get_report_run as get_report_run_service
- L13 assign router = APIRouter()
- L17 def run_reportpayload: RunPayload, request: Request:
  - L18 return run_report_service(payload, request, kind='pdf')
- L22 def run_report_excelpayload: RunPayload, request: Request:
  - L23 return run_report_service(payload, request, kind='excel')
- L27 async def enqueue_report_jobpayload: RunPayload | list[RunPayload], request: Request:
  - L28 return await queue_report_job(payload, request, kind='pdf')
- L32 async def enqueue_report_job_excelpayload: RunPayload | list[RunPayload], request: Request:
  - L33 return await queue_report_job(payload, request, kind='excel')
- L37 def list_report_runs_routerequest: Request, template_id: str | None=None, connection_id: str | None=None, schedule_id: str | None=None, limit: int=50:
  - L44 assign runs = list_report_runs_service(template_id=template_id, connection_id=connection_id, schedule_id=schedule_id, limit=limit)
  - L50 return {'runs': runs, 'correlation_id': getattr(request.state, 'correlation_id', None)}
- L54 def get_report_run_routerun_id: str, request: Request:
  - L55 assign run = get_report_run_service(run_id)
  - L56 if not run:
    - L57 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'run_not_found', 'message': 'Run not found.'})
  - L58 return {'run': run, 'correlation_id': getattr(request.state, 'correlation_id', None)}

## src\endpoints\schedules.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter, HTTPException, Request
- L5 from src.schemas.report_schema import ScheduleCreatePayload, ScheduleUpdatePayload
- L6 from src.services.scheduler_service import create_schedule, delete_schedule, list_schedules, update_schedule
- L8 assign router = APIRouter()
- L11 def _correlationrequest: Request:
  - L12 return getattr(request.state, 'correlation_id', None)
- L16 def list_report_schedulesrequest: Request:
  - L17 return {'schedules': list_schedules(), 'correlation_id': _correlation(request)}
- L21 def create_report_schedulepayload: ScheduleCreatePayload, request: Request:
  - L22 assign schedule = create_schedule(payload)
  - L23 return {'schedule': schedule, 'correlation_id': _correlation(request)}
- L27 def update_report_scheduleschedule_id: str, payload: ScheduleUpdatePayload, request: Request:
  - L28 assign schedule = update_schedule(schedule_id, payload)
  - L29 return {'schedule': schedule, 'correlation_id': _correlation(request)}
- L33 def delete_report_scheduleschedule_id: str, request: Request:
  - L34 assign removed = delete_schedule(schedule_id)
  - L35 if not removed:
    - L36 raise HTTPException(status_code=404, detail={'status': 'error', 'code': 'schedule_not_found', 'message': 'Schedule not found.'})
  - L37 return {'status': 'ok', 'schedule_id': schedule_id, 'correlation_id': _correlation(request)}

## src\endpoints\templates.py
- L1 from __future__ import annotations
- L3 from typing import Optional
- L5 import contextlib
- L6 import tempfile
- L7 from pathlib import Path
- L8 from types import SimpleNamespace
- L10 from fastapi import APIRouter, File, Form, Query, Request, UploadFile
- L12 from src.schemas.template_schema import CorrectionsPreviewPayload, GeneratorAssetsPayload, LastUsedPayload, MappingPayload, TemplateAiEditPayload, TemplateChatPayload, TemplateManualEditPayload, TemplateRecommendPayload, TemplateRecommendResponse, TemplateUpdatePayload
- L24 from backend.app.services.state import store as state_store_module
- L25 from src.services.mapping.approve import run_mapping_approve
- L26 from src.services.mapping.corrections import run_corrections_preview
- L27 from src.services.mapping.key_options import mapping_key_options as mapping_key_options_service
- L28 from src.services.mapping.preview import mapping_preview_internal, run_mapping_preview
- L29 from src.services.template_service import get_template_html, edit_template_ai, edit_template_manual, chat_template_edit, apply_chat_template_edit, export_template_zip as export_template_zip_service, import_template_zip as import_template_zip_service, undo_last_template_edit, verify_excel, verify_template, list_templates, templates_catalog, recommend_templates, bootstrap_state, delete_template, update_template_metadata, generator_assets
- L48 from backend.app.services.background_tasks import enqueue_background_job, iter_ndjson_events_async, run_event_stream_async
- L54 assign router = APIRouter()
- L57 def _correlationrequest: Request:
  - L58 return getattr(request.state, 'correlation_id', None)
- L61 def _state_store:
  - L62 return state_store_module.state_store
- L65 def _request_with_correlationcorrelation_id: str | None:
  - L66 return SimpleNamespace(state=SimpleNamespace(correlation_id=correlation_id))
- L69 async def _persist_uploadfile: UploadFile, suffix: str:
  - L70 assign filename = Path(file.filename or f'upload{suffix}').name
  - L71 assign tmp = tempfile.NamedTemporaryFile(prefix='nr-upload-', suffix=suffix, delete=False)
  - L72 try:
    - L73 with tmp:
      - L74 expr file.file.seek(0)
      - L75 while True:
        - L76 assign chunk = file.file.read(1024 * 1024)
        - L77 if not chunk:
          - L78 break
        - L79 expr tmp.write(chunk)
    - L81 finally:
      - L81 with contextlib.suppress(Exception):
        - L82 expr await file.close()
  - L83 return (Path(tmp.name), filename)
- L87 async def mapping_previewtemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False:
  - L88 return await run_mapping_preview(template_id, connection_id, request, force_refresh, kind='pdf')
- L92 async def mapping_preview_exceltemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False:
  - L93 return await run_mapping_preview(template_id, connection_id, request, force_refresh, kind='excel')
- L97 async def mapping_approvetemplate_id: str, payload: MappingPayload, request: Request:
  - L98 return await run_mapping_approve(template_id, payload, request, kind='pdf')
- L102 async def mapping_approve_exceltemplate_id: str, payload: MappingPayload, request: Request:
  - L103 return await run_mapping_approve(template_id, payload, request, kind='excel')
- L107 def mapping_corrections_previewtemplate_id: str, payload: CorrectionsPreviewPayload, request: Request:
  - L108 return run_corrections_preview(template_id, payload, request, kind='pdf')
- L112 def mapping_corrections_preview_exceltemplate_id: str, payload: CorrectionsPreviewPayload, request: Request:
  - L113 return run_corrections_preview(template_id, payload, request, kind='excel')
- L117 def export_template_zip_routetemplate_id: str, request: Request:
  - L118 return export_template_zip_service(template_id, request)
- L122 assign export_template_zip = export_template_zip_route
- L126 async def import_template_zip_routerequest: Request, file: UploadFile=File(...), name: str | None=Form(None):
  - L131 return await import_template_zip_service(file=file, name=name, request=request)
- L135 async def verify_template_routerequest: Request, file: UploadFile=File(...), connection_id: str=Form(...), refine_iters: int=Form(0), background: bool=Query(False):
  - L142 if not background:
    - L143 return verify_template(file=file, connection_id=connection_id, refine_iters=refine_iters, request=request)
  - L145 assign (upload_path, filename) = await _persist_upload(file, suffix='.pdf')
  - L146 assign correlation_id = _correlation(request)
  - L147 assign template_name = Path(filename).stem or filename
  - L149 async def runnerjob_id: str:
    - L150 assign upload = UploadFile(filename=filename, file=upload_path.open('rb'))
    - L151 try:
      - L152 assign response = verify_template(file=upload, connection_id=connection_id, refine_iters=refine_iters, request=_request_with_correlation(correlation_id))
      - L158 expr await run_event_stream_async(job_id, iter_ndjson_events_async(response.body_iterator))
      - L163 finally:
        - L163 with contextlib.suppress(Exception):
          - L164 expr await upload.close()
        - L165 with contextlib.suppress(FileNotFoundError):
          - L166 expr upload_path.unlink(missing_ok=True)
  - L168 assign job = await enqueue_background_job(job_type='verify_template', connection_id=connection_id, template_name=template_name, template_kind='pdf', meta={'filename': filename, 'background': True, 'refine_iters': refine_iters}, runner=runner)
  - L181 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L185 async def verify_excel_routerequest: Request, file: UploadFile=File(...), connection_id: str | None=Form(None), background: bool=Query(False):
  - L191 if not background:
    - L192 return verify_excel(file=file, request=request, connection_id=connection_id)
  - L194 assign (upload_path, filename) = await _persist_upload(file, suffix='.xlsx')
  - L195 assign correlation_id = _correlation(request)
  - L196 assign template_name = Path(filename).stem or filename
  - L198 async def runnerjob_id: str:
    - L199 assign upload = UploadFile(filename=filename, file=upload_path.open('rb'))
    - L200 try:
      - L201 assign response = verify_excel(file=upload, request=_request_with_correlation(correlation_id), connection_id=connection_id)
      - L206 expr await run_event_stream_async(job_id, iter_ndjson_events_async(response.body_iterator))
      - L211 finally:
        - L211 with contextlib.suppress(Exception):
          - L212 expr await upload.close()
        - L213 with contextlib.suppress(FileNotFoundError):
          - L214 expr upload_path.unlink(missing_ok=True)
  - L216 assign job = await enqueue_background_job(job_type='verify_excel', connection_id=connection_id, template_name=template_name, template_kind='excel', meta={'filename': filename, 'background': True}, runner=runner)
  - L228 return {'status': 'queued', 'job_id': job['id'], 'correlation_id': correlation_id}
- L232 def get_template_html_routetemplate_id: str, request: Request:
  - L233 return get_template_html(template_id, request)
- L237 def edit_template_manual_routetemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L238 return edit_template_manual(template_id, payload, request)
- L242 def edit_template_ai_routetemplate_id: str, payload: TemplateAiEditPayload, request: Request:
  - L243 return edit_template_ai(template_id, payload, request)
- L247 def undo_last_edit_routetemplate_id: str, request: Request:
  - L248 return undo_last_template_edit(template_id, request)
- L252 def chat_template_edit_routetemplate_id: str, payload: TemplateChatPayload, request: Request:
  - L253 docstring: "\n    Conversational template editing endpoint.\n\n    Send a conversation histo..."
  - L260 return chat_template_edit(template_id, payload, request)
- L264 def apply_chat_template_edit_routetemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L265 docstring: "\n    Apply the HTML changes from a chat conversation.\n\n    Call this endpoint..."
  - L271 return apply_chat_template_edit(template_id, payload.html, request)
- L275 def bootstrap_state_routerequest: Request:
  - L276 return bootstrap_state(request)
- L280 def set_last_used_routepayload: LastUsedPayload, request: Request:
  - L281 docstring: "Record the last-used connection and template IDs for session persistence."
  - L282 assign last_used = _state_store().set_last_used(connection_id=payload.connection_id, template_id=payload.template_id)
  - L286 return {'status': 'ok', 'last_used': last_used, 'correlation_id': _correlation(request)}
- L294 def templates_catalog_routerequest: Request:
  - L295 return templates_catalog(request)
- L299 def list_templates_routerequest: Request, status: Optional[str]=None:
  - L300 return list_templates(status, request)
- L304 def recommend_templates_routepayload: TemplateRecommendPayload, request: Request:
  - L305 return recommend_templates(payload, request)
- L309 def delete_template_routetemplate_id: str, request: Request:
  - L310 return delete_template(template_id, request)
- L314 def update_template_metadata_routetemplate_id: str, payload: TemplateUpdatePayload, request: Request:
  - L315 return update_template_metadata(template_id, payload, request)
- L319 def generator_assets_routetemplate_id: str, payload: GeneratorAssetsPayload, request: Request:
  - L320 return generator_assets(template_id, payload, request, kind='pdf')
- L324 def generator_assets_excel_routetemplate_id: str, payload: GeneratorAssetsPayload, request: Request:
  - L325 return generator_assets(template_id, payload, request, kind='excel')
- L329 def mapping_key_optionstemplate_id: str, request: Request, connection_id: str | None=None, tokens: str | None=None, limit: int=500, start_date: str | None=None, end_date: str | None=None, debug: bool=False:
  - L339 return mapping_key_options_service(template_id=template_id, request=request, connection_id=connection_id, tokens=tokens, limit=limit, start_date=start_date, end_date=end_date, kind='pdf', debug=debug)
- L353 def mapping_key_options_exceltemplate_id: str, request: Request, connection_id: str | None=None, tokens: str | None=None, limit: int=500, start_date: str | None=None, end_date: str | None=None, debug: bool=False:
  - L363 return mapping_key_options_service(template_id=template_id, request=request, connection_id=connection_id, tokens=tokens, limit=limit, start_date=start_date, end_date=end_date, kind='excel', debug=debug)

## src\routes.py
- L1 from __future__ import annotations
- L3 from fastapi import APIRouter
- L5 from src.endpoints import artifacts, connections, health, jobs, reports, schedules, templates
- L14 from src.endpoints.feature_routes import build_feature_routers
- L16 assign router = APIRouter()
- L19 for r in (health.router, connections.router, jobs.router, schedules.router, artifacts.router, reports.router, templates.router):
  - L28 expr router.include_router(r)
- L31 assign (saved_charts_router, chart_suggest_router, discover_router) = build_feature_routers()
- L32 for r in (saved_charts_router, chart_suggest_router, discover_router):
  - L33 expr router.include_router(r)
- L36 assign __all__ = ['router']

## src\schemas\__init__.py
- L1 docstring: "Pydantic schemas for API I/O."

## src\schemas\connection_schema.py
- L1 from __future__ import annotations
- L3 from typing import Optional
- L5 from pydantic import BaseModel
- L8 class TestPayload(BaseModel):
  - L9 annotated assign db_url: Optional[str] = None
  - L10 annotated assign db_type: Optional[str] = None
  - L11 annotated assign database: Optional[str] = None
- L14 class ConnectionUpsertPayload(BaseModel):
  - L15 annotated assign id: Optional[str] = None
  - L16 annotated assign name: str
  - L17 annotated assign db_type: str
  - L18 annotated assign db_url: Optional[str] = None
  - L19 annotated assign database: Optional[str] = None
  - L20 annotated assign status: Optional[str] = None
  - L21 annotated assign latency_ms: Optional[float] = None
  - L22 annotated assign tags: Optional[list[str]] = None

## src\schemas\report_schema.py
- L1 from __future__ import annotations
- L3 from typing import Any, Optional
- L5 from pydantic import BaseModel
- L8 class ScheduleCreatePayload(BaseModel):
  - L9 annotated assign template_id: str
  - L10 annotated assign connection_id: str
  - L11 annotated assign start_date: str
  - L12 annotated assign end_date: str
  - L13 annotated assign key_values: Optional[dict[str, Any]] = None
  - L14 annotated assign batch_ids: Optional[list[str]] = None
  - L15 annotated assign docx: bool = False
  - L16 annotated assign xlsx: bool = False
  - L17 annotated assign email_recipients: Optional[list[str]] = None
  - L18 annotated assign email_subject: Optional[str] = None
  - L19 annotated assign email_message: Optional[str] = None
  - L20 annotated assign frequency: str = 'daily'
  - L21 annotated assign interval_minutes: Optional[int] = None
  - L22 annotated assign name: Optional[str] = None
  - L23 annotated assign active: bool = True
- L26 class ScheduleUpdatePayload(BaseModel):
  - L27 docstring: "All fields optional for partial updates."
  - L28 annotated assign name: Optional[str] = None
  - L29 annotated assign start_date: Optional[str] = None
  - L30 annotated assign end_date: Optional[str] = None
  - L31 annotated assign key_values: Optional[dict[str, Any]] = None
  - L32 annotated assign batch_ids: Optional[list[str]] = None
  - L33 annotated assign docx: Optional[bool] = None
  - L34 annotated assign xlsx: Optional[bool] = None
  - L35 annotated assign email_recipients: Optional[list[str]] = None
  - L36 annotated assign email_subject: Optional[str] = None
  - L37 annotated assign email_message: Optional[str] = None
  - L38 annotated assign frequency: Optional[str] = None
  - L39 annotated assign interval_minutes: Optional[int] = None
  - L40 annotated assign active: Optional[bool] = None

## src\schemas\template_schema.py
- L1 from __future__ import annotations
- L3 from typing import Any, Optional
- L5 from pydantic import BaseModel
- L8 class TemplateManualEditPayload(BaseModel):
  - L9 annotated assign html: str
- L12 class TemplateAiEditPayload(BaseModel):
  - L13 annotated assign instructions: str
  - L14 annotated assign html: Optional[str] = None
- L17 class MappingPayload(BaseModel):
  - L18 annotated assign mapping: dict[str, str]
  - L19 annotated assign connection_id: Optional[str] = None
  - L20 annotated assign user_values_text: Optional[str] = None
  - L21 annotated assign user_instructions: Optional[str] = None
  - L22 annotated assign dialect_hint: Optional[str] = None
  - L23 annotated assign catalog_allowlist: Optional[list[str]] = None
  - L24 annotated assign params_spec: Optional[list[str]] = None
  - L25 annotated assign sample_params: Optional[dict[str, Any]] = None
  - L26 annotated assign generator_dialect: Optional[str] = None
  - L27 annotated assign force_generator_rebuild: bool = False
  - L28 annotated assign keys: Optional[list[str]] = None
  - L30 class Config:
    - L31 assign extra = 'allow'
- L34 class GeneratorAssetsPayload(BaseModel):
  - L35 annotated assign step4_output: Optional[dict[str, Any]] = None
  - L36 annotated assign contract: Optional[dict[str, Any]] = None
  - L37 annotated assign overview_md: Optional[str] = None
  - L38 annotated assign final_template_html: Optional[str] = None
  - L39 annotated assign reference_pdf_image: Optional[str] = None
  - L40 annotated assign catalog: Optional[list[str]] = None
  - L41 annotated assign dialect: Optional[str] = 'duckdb'
  - L42 annotated assign params: Optional[list[str]] = None
  - L43 annotated assign sample_params: Optional[dict[str, Any]] = None
  - L44 annotated assign force_rebuild: bool = False
  - L45 annotated assign key_tokens: Optional[list[str]] = None
  - L47 class Config:
    - L48 assign extra = 'allow'
- L51 class CorrectionsPreviewPayload(BaseModel):
  - L52 annotated assign user_input: Optional[str] = ''
  - L53 annotated assign page: int = 1
  - L54 annotated assign mapping_override: Optional[dict[str, Any]] = None
  - L55 annotated assign sample_tokens: Optional[list[str]] = None
  - L56 annotated assign model_selector: Optional[str] = None
  - L58 class Config:
    - L59 assign extra = 'allow'
- L62 class TemplateRecommendPayload(BaseModel):
  - L63 annotated assign requirement: str
  - L64 annotated assign kind: Optional[str] = None
  - L65 annotated assign domain: Optional[str] = None
  - L66 annotated assign kinds: Optional[list[str]] = None
  - L67 annotated assign domains: Optional[list[str]] = None
  - L68 annotated assign schema_snapshot: Optional[dict[str, Any]] = None
  - L69 annotated assign tables: Optional[list[str]] = None
  - L71 class Config:
    - L72 assign extra = 'allow'
- L75 class TemplateRecommendation(BaseModel):
  - L76 annotated assign template: dict[str, Any]
  - L77 annotated assign explanation: str
  - L78 annotated assign score: float
- L81 class TemplateRecommendResponse(BaseModel):
  - L82 annotated assign recommendations: list[TemplateRecommendation]
- L85 class LastUsedPayload(BaseModel):
  - L86 annotated assign connection_id: Optional[str] = None
  - L87 annotated assign template_id: Optional[str] = None
- L90 class TemplateUpdatePayload(BaseModel):
  - L91 annotated assign name: Optional[str] = None
  - L92 annotated assign description: Optional[str] = None
  - L93 annotated assign tags: Optional[list[str]] = None
  - L94 annotated assign status: Optional[str] = None
  - L96 class Config:
    - L97 assign extra = 'allow'
- L100 class TemplateChatMessage(BaseModel):
  - L101 docstring: "A single message in the template editing chat conversation."
  - L102 annotated assign role: str
  - L103 annotated assign content: str
- L106 class TemplateChatPayload(BaseModel):
  - L107 docstring: "Payload for conversational template editing."
  - L108 annotated assign messages: list[TemplateChatMessage]
  - L109 annotated assign html: Optional[str] = None
  - L111 class Config:
    - L112 assign extra = 'allow'
- L115 class TemplateChatResponse(BaseModel):
  - L116 docstring: "Response from conversational template editing."
  - L117 annotated assign message: str
  - L118 annotated assign ready_to_apply: bool
  - L119 annotated assign proposed_changes: Optional[list[str]] = None
  - L120 annotated assign updated_html: Optional[str] = None
  - L121 annotated assign follow_up_questions: Optional[list[str]] = None

## src\services\__init__.py
- L1 docstring: "Service layer modules."

## src\services\connection_inspector.py
- L1 from __future__ import annotations
- L3 import os
- L4 import threading
- L5 import time
- L6 from pathlib import Path
- L7 from typing import Any
- L9 from fastapi import HTTPException
- L11 from backend.app.services.connections.db_connection import resolve_db_path, verify_sqlite
- L12 from backend.app.services.dataframes.sqlite_loader import get_loader
- L13 from backend.app.services.dataframes import sqlite_shim
- L14 from backend.app.services.state import store as state_store_module
- L16 annotated assign _SCHEMA_CACHE: dict[tuple[str, bool, bool, int], dict] = {}
- L17 assign _SCHEMA_CACHE_LOCK = threading.Lock()
- L18 assign _SCHEMA_CACHE_TTL_SECONDS = max(int(os.getenv('NR_SCHEMA_CACHE_TTL_SECONDS', '30') or '30'), 0)
- L19 assign _SCHEMA_CACHE_MAX_ENTRIES = max(int(os.getenv('NR_SCHEMA_CACHE_MAX_ENTRIES', '32') or '32'), 5)
- L22 def _http_errorstatus_code: int, code: str, message: str:
  - L23 return HTTPException(status_code=status_code, detail={'status': 'error', 'code': code, 'message': message})
- L26 def _state_store:
  - L27 return state_store_module.state_store
- L30 def _quote_identifiername: str:
  - L31 return name.replace('"', '""')
- L34 def _coerce_valuevalue: Any:
  - L35 if isinstance(value, (bytes, bytearray, memoryview)):
    - L36 return bytes(value).hex()
  - L37 return value
- L40 def _count_rowsdb_path: Path, table: str:
  - L41 docstring: "Get row count from DataFrame (no direct database access)."
  - L42 assign loader = get_loader(db_path)
  - L43 try:
    - L44 assign frame = loader.frame(table)
    - L45 return len(frame)
    - L46 except Exception:
      - L47 return 0
- L50 def _sample_rowsdb_path: Path, table: str, limit: int, offset: int=0:
  - L51 docstring: "Get sample rows from DataFrame (no direct database access)."
  - L52 assign loader = get_loader(db_path)
  - L53 try:
    - L54 assign frame = loader.frame(table)
    - L56 assign sample = frame.iloc[offset:offset + limit]
    - L57 assign rows = []
    - L58 for (_, row) in sample.iterrows():
      - L59 expr rows.append({key: _coerce_value(value) for key, value in row.to_dict().items()})
    - L60 return rows
    - L61 except Exception:
      - L62 return []
- L65 def get_connection_schemaconnection_id: str, *, include_row_counts: bool=True, include_foreign_keys: bool=True, sample_rows: int=0:
  - L72 if not connection_id:
    - L73 raise _http_error(400, 'connection_missing', 'connection_id is required')
  - L74 try:
    - L75 assign db_path = resolve_db_path(connection_id=connection_id, db_url=None, db_path=None)
    - L76 expr verify_sqlite(db_path)
    - L77 except Exception as exc:
      - L78 raise _http_error(400, 'connection_invalid', str(exc))
  - L80 assign cache_key = (connection_id, include_row_counts, include_foreign_keys, int(sample_rows or 0))
  - L81 assign cache_enabled = _SCHEMA_CACHE_TTL_SECONDS > 0
  - L82 assign cache_mtime = 0.0
  - L83 if cache_enabled:
    - L84 try:
      - L85 assign cache_mtime = db_path.stat().st_mtime
      - L86 except OSError:
        - L87 assign cache_mtime = 0.0
    - L88 assign now = time.time()
    - L89 with _SCHEMA_CACHE_LOCK:
      - L90 assign entry = _SCHEMA_CACHE.get(cache_key)
    - L91 if entry:
      - L92 assign cached_age = now - float(entry.get('ts') or 0.0)
      - L93 if entry.get('mtime') == cache_mtime and cached_age <= _SCHEMA_CACHE_TTL_SECONDS:
        - L94 return entry.get('data') or {}
  - L96 assign loader = get_loader(db_path)
  - L97 assign tables = []
  - L98 for table_name in loader.table_names():
    - L99 assign columns = [{'name': col.get('name'), 'type': col.get('type'), 'notnull': bool(col.get('notnull')), 'pk': bool(col.get('pk')), 'default': col.get('dflt_value')} for col in loader.pragma_table_info(table_name)]
    - L109 assign table_record = {'name': table_name, 'columns': columns}
    - L113 if include_foreign_keys:
      - L114 assign table_record['foreign_keys'] = loader.foreign_keys(table_name)
    - L115 if include_row_counts:
      - L116 assign table_record['row_count'] = _count_rows(db_path, table_name)
    - L117 if sample_rows and sample_rows > 0:
      - L118 assign table_record['sample_rows'] = _sample_rows(db_path, table_name, min(sample_rows, 25))
    - L119 expr tables.append(table_record)
  - L121 assign connection_record = _state_store().get_connection_record(connection_id) or {}
  - L122 assign result = {'connection_id': connection_id, 'connection_name': connection_record.get('name'), 'database': str(db_path), 'table_count': len(tables), 'tables': tables}
  - L129 if cache_enabled:
    - L130 with _SCHEMA_CACHE_LOCK:
      - L131 assign _SCHEMA_CACHE[cache_key] = {'mtime': cache_mtime, 'ts': time.time(), 'data': result}
      - L132 if len(_SCHEMA_CACHE) > _SCHEMA_CACHE_MAX_ENTRIES:
        - L133 assign oldest = sorted(_SCHEMA_CACHE.items(), key=lambda item: item[1].get('ts') or 0.0)
        - L134 for (key, _) in oldest[:max(len(_SCHEMA_CACHE) - _SCHEMA_CACHE_MAX_ENTRIES, 0)]:
          - L135 expr _SCHEMA_CACHE.pop(key, None)
  - L136 return result
- L139 def get_connection_table_previewconnection_id: str, *, table: str, limit: int=10, offset: int=0:
  - L146 if not connection_id:
    - L147 raise _http_error(400, 'connection_missing', 'connection_id is required')
  - L148 if not table:
    - L149 raise _http_error(400, 'table_missing', 'table name is required')
  - L150 try:
    - L151 assign db_path = resolve_db_path(connection_id=connection_id, db_url=None, db_path=None)
    - L152 expr verify_sqlite(db_path)
    - L153 except Exception as exc:
      - L154 raise _http_error(400, 'connection_invalid', str(exc))
  - L156 assign loader = get_loader(db_path)
  - L157 assign tables = loader.table_names()
  - L158 if table not in tables:
    - L159 raise _http_error(404, 'table_not_found', f"Table '{table}' not found")
  - L161 assign safe_limit = max(1, min(int(limit or 10), 200))
  - L162 assign safe_offset = max(0, int(offset or 0))
  - L163 assign columns = [col.get('name') for col in loader.pragma_table_info(table)]
  - L164 assign rows = _sample_rows(db_path, table, safe_limit, safe_offset)
  - L165 return {'connection_id': connection_id, 'table': table, 'columns': columns, 'rows': rows, 'row_count': _count_rows(db_path, table), 'limit': safe_limit, 'offset': safe_offset}

## src\services\connection_service.py
- L1 from __future__ import annotations
- L3 import time
- L4 from pathlib import Path
- L5 from typing import Any, Optional
- L7 from fastapi import HTTPException
- L9 from backend.app.services.connections.db_connection import resolve_db_path, save_connection, verify_sqlite
- L10 from backend.app.services.state import store as state_store_module
- L11 from src.schemas.connection_schema import ConnectionUpsertPayload, TestPayload
- L12 from src.utils.connection_utils import display_name_for_path
- L15 def _http_errorstatus_code: int, code: str, message: str:
  - L16 return HTTPException(status_code=status_code, detail={'status': 'error', 'code': code, 'message': message})
- L19 def _state_store:
  - L20 return state_store_module.state_store
- L23 def test_connectionpayload: TestPayload:
  - L24 assign t0 = time.time()
  - L25 try:
    - L26 annotated assign db_path: Path = resolve_db_path(connection_id=None, db_url=payload.db_url, db_path=payload.database if (payload.db_type or '').lower() == 'sqlite' else None)
    - L31 expr verify_sqlite(db_path)
    - L32 except Exception as exc:
      - L33 raise _http_error(400, 'connection_invalid', str(exc))
  - L35 assign latency_ms = int((time.time() - t0) * 1000)
  - L36 assign resolved = Path(db_path).resolve()
  - L37 assign display_name = display_name_for_path(resolved, 'sqlite')
  - L38 assign cfg = {'db_type': 'sqlite', 'database': str(resolved), 'db_url': payload.db_url, 'name': display_name, 'status': 'connected', 'latency_ms': latency_ms}
  - L46 assign cid = save_connection(cfg)
  - L47 expr _state_store().record_connection_ping(cid, status='connected', detail=f'Connected ({display_name})', latency_ms=latency_ms)
  - L54 return {'ok': True, 'details': f'Connected ({display_name})', 'latency_ms': latency_ms, 'connection_id': cid, 'normalized': {'db_type': 'sqlite', 'database': str(resolved)}}
- L66 def list_connections:
  - L67 return _state_store().list_connections()
- L70 def upsert_connectionpayload: ConnectionUpsertPayload:
  - L71 if not payload.db_url and (not payload.database) and (not payload.id):
    - L72 raise _http_error(400, 'invalid_payload', 'Provide db_url or database when creating a connection.')
  - L74 assign existing = _state_store().get_connection_record(payload.id) if payload.id else None
  - L75 try:
    - L76 if payload.db_url:
      - L77 assign db_path = resolve_db_path(connection_id=None, db_url=payload.db_url, db_path=None)
      - L78 else:
        - L78 if payload.database:
          - L79 assign db_path = Path(payload.database)
          - L80 else:
            - L80 if existing and existing.get('database_path'):
              - L81 assign db_path = Path(existing['database_path'])
              - L83 else:
                - L83 raise RuntimeError('No database information supplied.')
    - L84 except Exception as exc:
      - L85 raise _http_error(400, 'invalid_database', f'Invalid database reference: {exc}')
  - L87 assign db_type = (payload.db_type or (existing or {}).get('db_type') or 'sqlite').lower()
  - L88 if db_type != 'sqlite':
    - L89 raise _http_error(400, 'unsupported_db', 'Only sqlite connections are supported in this build.')
  - L91 annotated assign secret_payload: Optional[dict[str, Any]] = None
  - L92 if payload.db_url or payload.database:
    - L93 assign secret_payload = {'db_url': payload.db_url, 'database': str(db_path)}
  - L98 assign record = _state_store().upsert_connection(conn_id=payload.id, name=payload.name or display_name_for_path(Path(db_path), db_type), db_type=db_type, database_path=str(db_path), secret_payload=secret_payload, status=payload.status, latency_ms=payload.latency_ms, tags=payload.tags)
  - L109 if payload.status:
    - L110 expr _state_store().record_connection_ping(record['id'], status=payload.status, detail=None, latency_ms=payload.latency_ms)
  - L116 return record
- L119 def delete_connectionconnection_id: str:
  - L120 return _state_store().delete_connection(connection_id)
- L123 def healthcheck_connectionconnection_id: str:
  - L124 assign t0 = time.time()
  - L125 try:
    - L126 assign db_path = resolve_db_path(connection_id=connection_id, db_url=None, db_path=None)
    - L127 expr verify_sqlite(db_path)
    - L128 except Exception as exc:
      - L129 expr _state_store().record_connection_ping(connection_id, status='failed', detail=str(exc), latency_ms=None)
      - L135 raise _http_error(400, 'connection_unhealthy', str(exc))
  - L137 assign latency_ms = int((time.time() - t0) * 1000)
  - L138 expr _state_store().record_connection_ping(connection_id, status='connected', detail='Healthcheck succeeded', latency_ms=latency_ms)
  - L144 return {'status': 'ok', 'connection_id': connection_id, 'latency_ms': latency_ms}

## src\services\file_service\__init__.py
- L1 from .edit import apply_chat_template_edit, chat_template_edit, edit_template_ai, edit_template_manual, get_template_html, undo_last_template_edit
- L9 from .verify import verify_excel, verify_template
- L10 from .generator import generator_assets
- L11 from .artifacts import artifact_head_response, artifact_manifest_response
- L12 from .helpers import load_template_generator_summary, update_template_generator_summary_for_edit, resolve_template_kind
- L18 assign __all__ = ['apply_chat_template_edit', 'artifact_head_response', 'artifact_manifest_response', 'chat_template_edit', 'edit_template_ai', 'edit_template_manual', 'get_template_html', 'undo_last_template_edit', 'verify_excel', 'verify_template', 'generator_assets', 'load_template_generator_summary', 'update_template_generator_summary_for_edit', 'resolve_template_kind']

## src\services\file_service\artifacts.py
- L1 from __future__ import annotations
- L3 from pathlib import Path
- L4 from typing import Callable
- L6 from fastapi import HTTPException
- L8 from backend.app.services.utils.artifacts import load_manifest
- L9 from src.utils.template_utils import artifact_url, template_dir
- L12 def artifact_manifest_responsetemplate_id: str, *, kind: str='pdf', template_dir_fn: Callable[..., Path]=template_dir:
  - L18 assign tdir = template_dir_fn(template_id, kind=kind, must_exist=True, create=False)
  - L19 assign manifest = load_manifest(tdir)
  - L20 if not manifest:
    - L21 raise HTTPException(status_code=404, detail='manifest_not_found')
  - L22 assign manifest = dict(manifest)
  - L23 expr manifest.setdefault('template_id', template_id)
  - L24 expr manifest.setdefault('kind', kind)
  - L25 return manifest
- L28 def artifact_head_responsetemplate_id: str, name: str, *, kind: str='pdf', template_dir_fn: Callable[..., Path]=template_dir:
  - L35 assign tdir = template_dir_fn(template_id, kind=kind, must_exist=True, create=False)
  - L36 assign target = tdir / name
  - L37 if not target.exists():
    - L38 raise HTTPException(status_code=404, detail='artifact_not_found')
  - L39 assign stat = target.stat()
  - L40 assign url = artifact_url(target)
  - L41 return {'template_id': template_id, 'kind': kind, 'name': name, 'url': url, 'size': stat.st_size, 'modified': int(stat.st_mtime)}

## src\services\file_service\edit.py
- L1 from __future__ import annotations
- L3 import contextlib
- L4 import difflib
- L5 import json
- L6 from pathlib import Path
- L7 from typing import Any, Mapping, Optional
- L9 from fastapi import Request
- L11 from backend.app.services.templates.TemplateVerify import MODEL, get_openai_client
- L12 from backend.app.services.utils import TemplateLockError, acquire_template_lock, get_correlation_id, write_text_atomic, call_chat_completion, strip_code_fences
- L20 from src.schemas.template_schema import TemplateAiEditPayload, TemplateManualEditPayload, TemplateChatPayload, TemplateChatResponse
- L26 from src.utils.template_utils import template_dir
- L28 from .helpers import append_template_history_entry, http_error, load_template_generator_summary, read_template_history, resolve_template_kind, update_template_generator_summary_for_edit
- L38 def _summarize_html_diffbefore: str, after: str:
  - L39 assign before_lines = (before or '').splitlines()
  - L40 assign after_lines = (after or '').splitlines()
  - L41 assign matcher = difflib.SequenceMatcher(None, before_lines, after_lines, autojunk=False)
  - L42 assign added = 0
  - L43 assign removed = 0
  - L44 for (tag, i1, i2, j1, j2) in matcher.get_opcodes():
    - L45 if tag in ('replace', 'delete'):
      - L46 aug assign removed Add i2 - i1
    - L47 if tag in ('replace', 'insert'):
      - L48 aug assign added Add j2 - j1
  - L49 annotated assign parts: list[str] = []
  - L50 if added:
    - L51 expr parts.append(f"+{added} line{('s' if added != 1 else '')}")
  - L52 if removed:
    - L53 expr parts.append(f"-{removed} line{('s' if removed != 1 else '')}")
  - L54 if not parts:
    - L55 return 'no line changes'
  - L56 return ', '.join(parts)
- L59 def _snapshot_final_htmltemplate_dir_path: Path, final_path: Path, base_path: Path:
  - L60 if final_path.exists():
    - L61 assign source_path = final_path
    - L62 else:
      - L62 if base_path.exists():
        - L63 assign source_path = base_path
        - L65 else:
          - L65 raise http_error(404, 'template_html_missing', 'Template HTML not found (report_final.html or template_p1.html).')
  - L71 assign current_html = source_path.read_text(encoding='utf-8', errors='ignore')
  - L73 if source_path is base_path and (not final_path.exists()):
    - L74 expr write_text_atomic(final_path, current_html, encoding='utf-8', step='template_edit_seed_final')
  - L76 assign prev_path = template_dir_path / 'report_final_prev.html'
  - L77 expr write_text_atomic(prev_path, current_html, encoding='utf-8', step='template_edit_prev')
  - L78 return current_html
- L81 def _build_template_html_response*, template_id: str, kind: str, html: str, source: str, template_dir_path: Path, history: Optional[list[dict]]=None, summary: Optional[Mapping[str, Any]]=None, ai_summary: Optional[list[str]]=None, correlation_id: str | None=None, diff_summary: str | None=None:
  - L94 assign prev_path = template_dir_path / 'report_final_prev.html'
  - L95 assign effective_history = history if history is not None else read_template_history(template_dir_path)
  - L96 assign summary_payload = dict(summary or {})
  - L97 assign metadata = {'lastEditType': summary_payload.get('lastEditType'), 'lastEditAt': summary_payload.get('lastEditAt'), 'lastEditNotes': summary_payload.get('lastEditNotes'), 'historyCount': len(effective_history)}
  - L103 annotated assign result: dict[str, Any] = {'status': 'ok', 'template_id': template_id, 'kind': kind, 'html': html, 'source': source, 'can_undo': prev_path.exists(), 'metadata': metadata, 'history': effective_history}
  - L113 if diff_summary is not None:
    - L114 assign result['diff_summary'] = diff_summary
  - L115 if ai_summary:
    - L116 assign result['summary'] = ai_summary
  - L117 if correlation_id:
    - L118 assign result['correlation_id'] = correlation_id
  - L119 return result
- L122 def _resolve_template_html_pathstemplate_id: str, *, kind: str:
  - L123 assign template_dir_path = template_dir(template_id, kind=kind)
  - L124 assign final_path = template_dir_path / 'report_final.html'
  - L125 assign base_path = template_dir_path / 'template_p1.html'
  - L126 if final_path.exists():
    - L127 return (template_dir_path, final_path, base_path, 'report_final')
  - L128 if base_path.exists():
    - L129 return (template_dir_path, final_path, base_path, 'template_p1')
  - L130 raise http_error(404, 'template_html_missing', 'Template HTML not found (report_final.html or template_p1.html). Run template verification first.')
- L137 def _run_template_edit_llmtemplate_html: str, instructions: str:
  - L138 if not instructions or not str(instructions).strip():
    - L139 raise http_error(400, 'missing_instructions', 'instructions is required for AI template edit.')
  - L140 from backend.app.services.prompts.llm_prompts_template_edit import TEMPLATE_EDIT_PROMPT_VERSION, build_template_edit_prompt
  - L145 assign prompt_payload = build_template_edit_prompt(template_html, instructions)
  - L146 assign messages = prompt_payload.get('messages') or []
  - L147 if not messages:
    - L148 raise http_error(500, 'prompt_build_failed', 'Failed to build template edit prompt.')
  - L149 try:
    - L150 assign client = get_openai_client()
    - L151 except Exception as exc:
      - L152 raise http_error(503, 'llm_unavailable', f'LLM client is unavailable: {exc}')
  - L154 try:
    - L155 assign response = call_chat_completion(client, model=MODEL, messages=messages, description=TEMPLATE_EDIT_PROMPT_VERSION)
    - L156 except Exception as exc:
      - L157 raise http_error(502, 'llm_call_failed', f'Template edit LLM call failed: {exc}')
  - L159 assign raw_text = (response.choices[0].message.content or '').strip()
  - L160 assign parsed_text = strip_code_fences(raw_text)
  - L161 try:
    - L162 assign payload = json.loads(parsed_text)
    - L163 except Exception as exc:
      - L164 raise http_error(502, 'llm_invalid_response', f'LLM did not return valid JSON: {exc}')
  - L166 if not isinstance(payload, dict):
    - L167 raise http_error(502, 'llm_invalid_response', 'LLM response was not a JSON object.')
  - L169 assign updated_html = payload.get('updated_html')
  - L170 if not isinstance(updated_html, str) or not updated_html.strip():
    - L171 raise http_error(502, 'llm_invalid_response', "LLM response missing 'updated_html' string.")
  - L173 assign summary_raw = payload.get('summary')
  - L174 annotated assign summary: list[str] = []
  - L175 if isinstance(summary_raw, list):
    - L176 for item in summary_raw:
      - L177 assign text = str(item).strip()
      - L178 if text:
        - L179 expr summary.append(text)
    - L180 else:
      - L180 if isinstance(summary_raw, str):
        - L181 assign text = summary_raw.strip()
        - L182 if text:
          - L183 expr summary.append(text)
  - L185 return (updated_html, summary)
- L188 def get_template_htmltemplate_id: str, request: Request:
  - L189 assign template_kind = resolve_template_kind(template_id)
  - L190 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L191 assign (template_dir_path, final_path, base_path, source) = _resolve_template_html_paths(template_id, kind=template_kind)
  - L192 assign active_path = final_path if source == 'report_final' else base_path
  - L193 assign html_text = active_path.read_text(encoding='utf-8', errors='ignore')
  - L194 assign history = read_template_history(template_dir_path)
  - L195 assign summary = load_template_generator_summary(template_id)
  - L196 return _build_template_html_response(template_id=template_id, kind=template_kind, html=html_text, source=source, template_dir_path=template_dir_path, history=history, summary=summary, correlation_id=correlation_id)
- L208 def edit_template_manualtemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L209 assign template_kind = resolve_template_kind(template_id)
  - L210 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L211 assign (template_dir_path, final_path, base_path, _) = _resolve_template_html_paths(template_id, kind=template_kind)
  - L212 try:
    - L213 assign lock_ctx = acquire_template_lock(template_dir_path, 'template_edit_manual', correlation_id)
    - L214 except TemplateLockError:
      - L215 raise http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L217 with lock_ctx:
    - L218 assign current_html = _snapshot_final_html(template_dir_path, final_path, base_path)
    - L220 assign new_html = payload.html or ''
    - L221 expr write_text_atomic(final_path, new_html, encoding='utf-8', step='template_edit_manual')
    - L222 assign diff_summary = _summarize_html_diff(current_html, new_html)
    - L224 assign notes = 'Manual HTML edit via template editor'
    - L225 assign summary = update_template_generator_summary_for_edit(template_id, edit_type='manual', notes=notes)
    - L226 assign history_entry = {'timestamp': summary.get('lastEditAt') or None, 'type': 'manual', 'notes': notes}
    - L227 assign history = append_template_history_entry(template_dir_path, history_entry)
  - L229 return _build_template_html_response(template_id=template_id, kind=template_kind, html=new_html, source='report_final', template_dir_path=template_dir_path, history=history, summary=summary, correlation_id=correlation_id, diff_summary=diff_summary)
- L242 def edit_template_aitemplate_id: str, payload: TemplateAiEditPayload, request: Request:
  - L243 assign template_kind = resolve_template_kind(template_id)
  - L244 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L245 assign (template_dir_path, final_path, base_path, _) = _resolve_template_html_paths(template_id, kind=template_kind)
  - L246 try:
    - L247 assign lock_ctx = acquire_template_lock(template_dir_path, 'template_edit_ai', correlation_id)
    - L248 except TemplateLockError:
      - L249 raise http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L251 with lock_ctx:
    - L252 assign current_html = _snapshot_final_html(template_dir_path, final_path, base_path)
    - L254 assign llm_input_html = payload.html.strip() if isinstance(payload.html, str) and payload.html.strip() else current_html
    - L255 assign (updated_html, change_summary) = _run_template_edit_llm(llm_input_html, payload.instructions or '')
    - L256 expr write_text_atomic(final_path, updated_html, encoding='utf-8', step='template_edit_ai')
    - L257 assign diff_summary = _summarize_html_diff(current_html, updated_html)
    - L259 assign notes = 'AI-assisted HTML edit via template editor'
    - L260 assign summary = update_template_generator_summary_for_edit(template_id, edit_type='ai', notes=notes)
    - L261 assign history_entry = {'timestamp': summary.get('lastEditAt') or None, 'type': 'ai', 'notes': notes, 'instructions': payload.instructions or '', 'summary': change_summary}
    - L268 assign history = append_template_history_entry(template_dir_path, history_entry)
  - L270 return _build_template_html_response(template_id=template_id, kind=template_kind, html=updated_html, source='report_final', template_dir_path=template_dir_path, history=history, summary=summary, ai_summary=change_summary, correlation_id=correlation_id, diff_summary=diff_summary)
- L284 def undo_last_template_edittemplate_id: str, request: Request:
  - L285 assign template_kind = resolve_template_kind(template_id)
  - L286 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L287 assign template_dir_path = template_dir(template_id, kind=template_kind)
  - L288 assign final_path = template_dir_path / 'report_final.html'
  - L289 assign prev_path = template_dir_path / 'report_final_prev.html'
  - L291 if not prev_path.exists() or not prev_path.is_file():
    - L292 raise http_error(400, 'no_previous_version', 'No previous template version found to undo.')
  - L293 if not final_path.exists() or not final_path.is_file():
    - L294 raise http_error(404, 'template_html_missing', 'Current template HTML not found for undo.')
  - L296 try:
    - L297 assign lock_ctx = acquire_template_lock(template_dir_path, 'template_edit_undo', correlation_id)
    - L298 except TemplateLockError:
      - L299 raise http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L301 with lock_ctx:
    - L302 assign tmp_path = template_dir_path / 'report_final_undo_tmp.html'
    - L303 expr tmp_path.unlink(missing_ok=True)
    - L305 assign current_html = final_path.read_text(encoding='utf-8', errors='ignore')
    - L306 try:
      - L307 expr final_path.rename(tmp_path)
      - L308 expr prev_path.rename(final_path)
      - L309 expr tmp_path.rename(prev_path)
      - L310 except Exception as exc:
        - L311 with contextlib.suppress(Exception):
          - L312 if tmp_path.exists() and (not final_path.exists()):
            - L313 expr tmp_path.rename(final_path)
        - L314 raise http_error(500, 'undo_failed', f'Failed to restore previous template version: {exc}')
    - L316 assign restored_html = final_path.read_text(encoding='utf-8', errors='ignore')
    - L317 assign diff_summary = _summarize_html_diff(current_html, restored_html)
    - L319 assign notes = 'Undo last template HTML edit'
    - L320 assign summary = update_template_generator_summary_for_edit(template_id, edit_type='undo', notes=notes)
    - L321 assign history_entry = {'timestamp': summary.get('lastEditAt') or None, 'type': 'undo', 'notes': notes}
    - L322 assign history = append_template_history_entry(template_dir_path, history_entry)
  - L324 return _build_template_html_response(template_id=template_id, kind=template_kind, html=restored_html, source='report_final', template_dir_path=template_dir_path, history=history, summary=summary, correlation_id=correlation_id, diff_summary=diff_summary)
- L337 def _run_template_chat_llmtemplate_html: str, conversation_history: list[dict]:
  - L338 docstring: "\n    Run the conversational template editing LLM.\n\n    Returns a dict with:\n..."
  - L348 from backend.app.services.prompts.llm_prompts_template_chat import TEMPLATE_CHAT_PROMPT_VERSION, build_template_chat_prompt
  - L353 assign prompt_payload = build_template_chat_prompt(template_html, conversation_history)
  - L354 assign messages = prompt_payload.get('messages') or []
  - L355 if not messages:
    - L356 raise http_error(500, 'prompt_build_failed', 'Failed to build template chat prompt.')
  - L358 try:
    - L359 assign client = get_openai_client()
    - L360 except Exception as exc:
      - L361 raise http_error(503, 'llm_unavailable', f'LLM client is unavailable: {exc}')
  - L363 try:
    - L364 assign response = call_chat_completion(client, model=MODEL, messages=messages, description=TEMPLATE_CHAT_PROMPT_VERSION)
    - L367 except Exception as exc:
      - L368 raise http_error(502, 'llm_call_failed', f'Template chat LLM call failed: {exc}')
  - L370 assign raw_text = (response.choices[0].message.content or '').strip()
  - L371 assign parsed_text = strip_code_fences(raw_text)
  - L373 try:
    - L374 assign payload = json.loads(parsed_text)
    - L375 except Exception as exc:
      - L377 return {'message': 'I apologize, but I encountered an issue processing your request. Could you please rephrase or try again?', 'ready_to_apply': False, 'proposed_changes': None, 'follow_up_questions': ["Could you describe what changes you'd like to make to the template?"], 'updated_html': None}
  - L385 if not isinstance(payload, dict):
    - L386 return {'message': 'I apologize, but I encountered an issue. Could you please try again?', 'ready_to_apply': False, 'proposed_changes': None, 'follow_up_questions': None, 'updated_html': None}
  - L394 return {'message': payload.get('message', ''), 'ready_to_apply': bool(payload.get('ready_to_apply', False)), 'proposed_changes': payload.get('proposed_changes'), 'follow_up_questions': payload.get('follow_up_questions'), 'updated_html': payload.get('updated_html')}
- L403 def chat_template_edittemplate_id: str, payload: TemplateChatPayload, request: Request:
  - L404 docstring: "\n    Handle a conversational template editing request.\n\n    This endpoint mai..."
  - L411 assign template_kind = resolve_template_kind(template_id)
  - L412 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L413 assign (template_dir_path, final_path, base_path, _) = _resolve_template_html_paths(template_id, kind=template_kind)
  - L416 if payload.html and payload.html.strip():
    - L417 assign current_html = payload.html.strip()
    - L419 else:
      - L419 assign active_path = final_path if final_path.exists() else base_path
      - L420 assign current_html = active_path.read_text(encoding='utf-8', errors='ignore')
  - L423 assign conversation_history = [{'role': msg.role, 'content': msg.content} for msg in payload.messages]
  - L429 assign llm_response = _run_template_chat_llm(current_html, conversation_history)
  - L431 assign result = {'status': 'ok', 'template_id': template_id, 'message': llm_response['message'], 'ready_to_apply': llm_response['ready_to_apply'], 'proposed_changes': llm_response.get('proposed_changes'), 'follow_up_questions': llm_response.get('follow_up_questions'), 'correlation_id': correlation_id}
  - L442 if llm_response['ready_to_apply'] and llm_response.get('updated_html'):
    - L443 assign result['updated_html'] = llm_response['updated_html']
  - L445 return result
- L448 def apply_chat_template_edittemplate_id: str, html: str, request: Request:
  - L449 docstring: "\n    Apply the HTML changes from a chat conversation.\n\n    This is called aft..."
  - L454 assign template_kind = resolve_template_kind(template_id)
  - L455 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L456 assign (template_dir_path, final_path, base_path, _) = _resolve_template_html_paths(template_id, kind=template_kind)
  - L458 try:
    - L459 assign lock_ctx = acquire_template_lock(template_dir_path, 'template_edit_chat_apply', correlation_id)
    - L460 except TemplateLockError:
      - L461 raise http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L463 with lock_ctx:
    - L464 assign current_html = _snapshot_final_html(template_dir_path, final_path, base_path)
    - L466 assign new_html = html or ''
    - L467 expr write_text_atomic(final_path, new_html, encoding='utf-8', step='template_edit_chat_apply')
    - L468 assign diff_summary = _summarize_html_diff(current_html, new_html)
    - L470 assign notes = 'AI chat-assisted HTML edit via template editor'
    - L471 assign summary = update_template_generator_summary_for_edit(template_id, edit_type='chat', notes=notes)
    - L472 assign history_entry = {'timestamp': summary.get('lastEditAt') or None, 'type': 'chat', 'notes': notes}
    - L477 assign history = append_template_history_entry(template_dir_path, history_entry)
  - L479 return _build_template_html_response(template_id=template_id, kind=template_kind, html=new_html, source='report_final', template_dir_path=template_dir_path, history=history, summary=summary, correlation_id=correlation_id, diff_summary=diff_summary)

## src\services\file_service\generator.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 import json
- L5 import time
- L6 from pathlib import Path
- L7 from typing import Any
- L9 from fastapi import HTTPException, Request
- L10 from fastapi.responses import StreamingResponse
- L12 from backend.app.services.generator.GeneratorAssetsV1 import GeneratorAssetsError, build_generator_assets_from_payload
- L13 from backend.app.services.state import store as state_store_module
- L14 from backend.app.services.utils import TemplateLockError, acquire_template_lock, get_correlation_id
- L15 from backend.app.services.utils.artifacts import load_manifest
- L16 from src.schemas.template_schema import GeneratorAssetsPayload
- L17 from src.utils.template_utils import manifest_endpoint, template_dir
- L19 from .helpers import http_error, normalize_artifact_map, resolve_template_kind
- L21 import logging
- L23 assign logger = logging.getLogger(__name__)
- L26 def _state_store:
  - L27 return state_store_module.state_store
- L30 def generator_assetstemplate_id: str, payload: GeneratorAssetsPayload, request: Request, *, kind: str='pdf':
  - L31 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L32 expr logger.info('generator_assets_v1_start', extra={'event': 'generator_assets_v1_start', 'template_id': template_id, 'correlation_id': correlation_id, 'force_rebuild': bool(payload.force_rebuild), 'template_kind': kind})
  - L43 assign resolved_kind = resolve_template_kind(template_id) if kind == 'pdf' else kind
  - L44 assign template_dir_path = template_dir(template_id, kind=resolved_kind)
  - L45 assign require_contract_join = (resolved_kind or 'pdf').lower() != 'excel'
  - L46 assign base_template_path = template_dir_path / 'template_p1.html'
  - L47 assign final_template_path = template_dir_path / 'report_final.html'
  - L48 assign contract_path = template_dir_path / 'contract.json'
  - L49 assign overview_path = template_dir_path / 'overview.md'
  - L50 assign step5_path = template_dir_path / 'step5_requirements.json'
  - L52 def _load_step4_payload:
    - L53 assign contract_payload = payload.step4_output.get('contract') if payload.step4_output else None
    - L54 assign overview_md = payload.step4_output.get('overview_md') if payload.step4_output else None
    - L55 assign step5_requirements = payload.step4_output.get('step5_requirements') if payload.step4_output else None
    - L57 if contract_payload is None:
      - L58 if payload.contract is not None:
        - L59 assign contract_payload = payload.contract
        - L60 else:
          - L60 if contract_path.exists():
            - L61 assign contract_payload = json.loads(contract_path.read_text(encoding='utf-8'))
    - L62 if contract_payload is None:
      - L63 raise HTTPException(status_code=422, detail='Contract payload is required to build generator assets.')
    - L65 if overview_md is None:
      - L66 if payload.overview_md is not None:
        - L67 assign overview_md = payload.overview_md
        - L68 else:
          - L68 if overview_path.exists():
            - L69 assign overview_md = overview_path.read_text(encoding='utf-8')
    - L71 if step5_requirements is None:
      - L72 if step5_path.exists():
        - L73 try:
          - L74 assign step5_requirements = json.loads(step5_path.read_text(encoding='utf-8'))
          - L75 except Exception:
            - L76 assign step5_requirements = {}
        - L78 else:
          - L78 assign step5_requirements = {}
    - L80 return {'contract': contract_payload, 'overview_md': overview_md, 'step5_requirements': step5_requirements or {}}
  - L86 assign step4_output = payload.step4_output or _load_step4_payload()
  - L88 if payload.final_template_html is not None:
    - L89 assign final_template_html = payload.final_template_html
    - L91 else:
      - L91 assign source_path = final_template_path if final_template_path.exists() else base_template_path
      - L92 if not source_path.exists():
        - L93 raise HTTPException(status_code=422, detail='Template HTML not found. Run mapping approval first.')
      - L94 assign final_template_html = source_path.read_text(encoding='utf-8', errors='ignore')
  - L96 assign catalog_allowlist = payload.catalog or None
  - L97 assign params_spec = payload.params or None
  - L98 assign sample_params = payload.sample_params or None
  - L99 assign dialect = payload.dialect or payload.dialect_hint or 'duckdb'
  - L100 assign incoming_key_tokens = payload.key_tokens
  - L102 try:
    - L103 assign lock_ctx = acquire_template_lock(template_dir_path, 'generator_assets_v1', correlation_id)
    - L104 except TemplateLockError:
      - L105 raise http_error(status_code=409, code='template_locked', message='Template is currently processing another request.')
  - L107 try:
    - L108 assign api_mod = importlib.import_module('backend.api')
    - L109 except Exception:
      - L110 assign api_mod = None
  - L111 assign builder = getattr(api_mod, 'build_generator_assets_from_payload', build_generator_assets_from_payload)
  - L113 def event_stream:
    - L114 assign started = time.time()
    - L116 def emitevent: str, **data: Any:
      - L117 return (json.dumps({'event': event, **data}, ensure_ascii=False) + '\n').encode('utf-8')
    - L119 with lock_ctx:
      - L120 expr (yield emit('stage', stage='generator_assets_v1', status='start', progress=10, template_id=template_id, correlation_id=correlation_id))
      - L128 try:
        - L129 assign result = builder(template_dir=template_dir_path, step4_output=step4_output, final_template_html=final_template_html, reference_pdf_image=payload.reference_pdf_image, catalog_allowlist=catalog_allowlist, dialect=dialect, params_spec=params_spec, sample_params=sample_params, force_rebuild=payload.force_rebuild, key_tokens=incoming_key_tokens, require_contract_join=require_contract_join)
        - L142 except GeneratorAssetsError as exc:
          - L143 expr logger.warning('generator_assets_v1_failed', extra={'event': 'generator_assets_v1_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L147 expr (yield emit('error', stage='generator_assets_v1', detail=str(exc), template_id=template_id))
          - L148 return None
        - L149 except Exception as exc:
          - L150 expr logger.exception('generator_assets_v1_unexpected', extra={'event': 'generator_assets_v1_unexpected', 'template_id': template_id, 'correlation_id': correlation_id})
          - L154 expr (yield emit('error', stage='generator_assets_v1', detail=str(exc), template_id=template_id))
          - L155 return None
      - L157 assign artifacts_urls = normalize_artifact_map(result.get('artifacts'))
      - L158 expr (yield emit('stage', stage='generator_assets_v1', status='done', progress=90, template_id=template_id, correlation_id=correlation_id, invalid=result.get('invalid'), needs_user_fix=result.get('needs_user_fix') or [], dialect=result.get('dialect'), params=result.get('params'), summary=result.get('summary'), dry_run=result.get('dry_run'), cached=result.get('cached'), artifacts=artifacts_urls))
      - L175 assign manifest = load_manifest(template_dir_path) or {}
      - L176 assign manifest_url = manifest_endpoint(template_id, kind=resolved_kind)
      - L178 assign existing_tpl = _state_store().get_template_record(template_id) or {}
      - L179 assign artifacts_payload = {'contract_url': artifacts_urls.get('contract'), 'generator_sql_pack_url': artifacts_urls.get('sql_pack'), 'generator_output_schemas_url': artifacts_urls.get('output_schemas'), 'generator_assets_url': artifacts_urls.get('generator_assets'), 'manifest_url': manifest_url}
      - L186 expr _state_store().upsert_template(template_id, name=existing_tpl.get('name') or f'Template {template_id[:8]}', status=existing_tpl.get('status') or 'approved', artifacts={k: v for k, v in artifacts_payload.items() if v}, connection_id=existing_tpl.get('last_connection_id'), template_type=resolved_kind)
      - L194 expr _state_store().update_template_generator(template_id, dialect=result.get('dialect'), params=result.get('params'), invalid=bool(result.get('invalid')), needs_user_fix=result.get('needs_user_fix') or [], summary=result.get('summary'), dry_run=result.get('dry_run'))
      - L204 expr (yield emit('result', template_id=template_id, invalid=result.get('invalid'), needs_user_fix=result.get('needs_user_fix') or [], dialect=result.get('dialect'), params=result.get('params'), summary=result.get('summary'), dry_run=result.get('dry_run'), cached=result.get('cached'), artifacts=artifacts_urls, manifest=manifest, manifest_url=manifest_url))
      - L219 expr logger.info('generator_assets_v1_complete', extra={'event': 'generator_assets_v1_complete', 'template_id': template_id, 'invalid': result.get('invalid'), 'needs_user_fix': len(result.get('needs_user_fix') or []), 'correlation_id': correlation_id, 'elapsed_ms': int((time.time() - started) * 1000)})
  - L231 assign headers = {'Content-Type': 'application/x-ndjson'}
  - L232 return StreamingResponse(event_stream(), headers=headers, media_type='application/x-ndjson')

## src\services\file_service\helpers.py
- L1 from __future__ import annotations
- L3 import json
- L4 import logging
- L5 import os
- L6 import re
- L7 import uuid
- L8 from pathlib import Path
- L9 from typing import Any, Mapping, Optional, Callable
- L11 from fastapi import HTTPException
- L13 from backend.app.services.state import state_store
- L14 from backend.app.services.utils import write_json_atomic
- L15 from src.utils.schedule_utils import utcnow_iso
- L16 from src.utils.template_utils import artifact_url, template_dir, normalize_template_id
- L18 assign logger = logging.getLogger(__name__)
- L20 annotated assign _DEFAULT_VERIFY_PDF_BYTES: int | None = None
- L21 assign _TEMPLATE_ID_SAFE_RE = re.compile('^[a-z0-9][a-z0-9_-]{2,180}$')
- L24 def format_bytesnum_bytes: int:
  - L25 if num_bytes < 1024:
    - L26 return f'{num_bytes} bytes'
  - L27 assign value = num_bytes / 1024
  - L28 assign unit = 'KiB'
  - L29 if value >= 1024:
    - L30 aug assign value Div 1024
    - L31 assign unit = 'MiB'
    - L32 if value >= 1024:
      - L33 aug assign value Div 1024
      - L34 assign unit = 'GiB'
  - L35 assign human = f'{value:.1f}'.rstrip('0').rstrip('.')
  - L36 return f'{human} {unit}'
- L39 def resolve_pdf_upload_limitdefault: int | None=_DEFAULT_VERIFY_PDF_BYTES:
  - L40 assign raw = os.getenv('NEURA_MAX_VERIFY_PDF_BYTES')
  - L41 if raw is None or raw.strip() == '':
    - L42 return default
  - L43 try:
    - L44 assign value = int(raw)
    - L45 except ValueError:
      - L46 expr logger.warning('invalid_pdf_upload_limit', extra={'event': 'invalid_pdf_upload_limit', 'value': raw})
      - L47 return default
  - L48 if value <= 0:
    - L49 return None
  - L50 return value
- L53 assign MAX_VERIFY_PDF_BYTES = resolve_pdf_upload_limit()
- L56 def slugify_template_namevalue: str | None:
  - L57 assign raw = str(value or '').lower()
  - L58 assign slug = re.sub('[^a-z0-9]+', '-', raw).strip('-')
  - L59 return slug[:60].strip('-')
- L62 def template_id_existstemplate_id: str, *, kind: str='pdf':
  - L63 try:
    - L64 expr template_dir(template_id, must_exist=True, create=False, kind=kind)
    - L65 return True
    - L66 except HTTPException:
      - L67 return False
- L70 def generate_template_idbase_name: str | None=None, *, kind: str='pdf':
  - L71 assign slug = slugify_template_name(base_name)
  - L72 if not slug:
    - L73 assign slug = 'template'
  - L74 for _ in range(10):
    - L75 assign suffix = uuid.uuid4().hex[:6]
    - L76 assign candidate = f'{slug}-{suffix}'
    - L77 if _TEMPLATE_ID_SAFE_RE.fullmatch(candidate) and (not template_id_exists(candidate, kind=kind)):
      - L78 return candidate
  - L79 assign fallback = f'{slug}-{uuid.uuid4().hex[:10]}'
  - L80 if _TEMPLATE_ID_SAFE_RE.fullmatch(fallback) and (not template_id_exists(fallback, kind=kind)):
    - L81 return fallback
  - L82 return uuid.uuid4().hex
- L85 def http_errorstatus_code: int, code: str, message: str, details: str | None=None:
  - L86 assign payload = {'status': 'error', 'code': code, 'message': message}
  - L87 if details:
    - L88 assign payload['details'] = details
  - L89 return HTTPException(status_code=status_code, detail=payload)
- L92 def template_history_pathtemplate_dir_path: Path:
  - L93 return template_dir_path / 'template_history.json'
- L96 def _truncate_historyentries: list[dict], limit: int=2:
  - L97 if limit <= 0:
    - L98 return []
  - L99 if len(entries) <= limit:
    - L100 return entries
  - L101 return entries[-limit:]
- L104 def read_template_historytemplate_dir_path: Path:
  - L105 assign path = template_history_path(template_dir_path)
  - L106 if not path.exists():
    - L107 return []
  - L108 try:
    - L109 assign raw = json.loads(path.read_text(encoding='utf-8'))
    - L110 except Exception:
      - L111 return []
  - L112 if not isinstance(raw, list):
    - L113 return []
  - L114 assign cleaned = [entry for entry in raw if isinstance(entry, dict)]
  - L115 return _truncate_history(cleaned)
- L118 def append_template_history_entrytemplate_dir_path: Path, entry: dict:
  - L119 assign history = read_template_history(template_dir_path)
  - L120 expr history.append(entry)
  - L121 assign history = _truncate_history(history)
  - L122 expr write_json_atomic(template_history_path(template_dir_path), history, ensure_ascii=False, indent=2, step='template_history')
  - L129 return history
- L132 def load_template_generator_summarytemplate_id: str:
  - L133 assign record = state_store.get_template_record(template_id) or {}
  - L134 assign generator = record.get('generator') or {}
  - L135 assign raw_summary = generator.get('summary') or {}
  - L136 if isinstance(raw_summary, dict):
    - L137 return dict(raw_summary)
  - L138 return {}
- L141 def update_template_generator_summary_for_edittemplate_id: str, *, edit_type: str, notes: str | None=None:
  - L147 assign summary = load_template_generator_summary(template_id)
  - L148 assign now_iso = utcnow_iso()
  - L149 assign summary['lastEditType'] = edit_type
  - L150 assign summary['lastEditAt'] = now_iso
  - L151 if notes is not None:
    - L152 assign summary['lastEditNotes'] = notes
  - L153 expr state_store.update_template_generator(template_id, summary=summary)
  - L154 return summary
- L157 def normalize_artifact_mapartifacts: Mapping[str, Any] | None, artifact_url_fn: Callable[[Path | str | None], Optional[str]]=artifact_url:
  - L158 annotated assign normalized: dict[str, str] = {}
  - L159 if not artifacts:
    - L160 return normalized
  - L161 for (name, raw) in artifacts.items():
    - L162 assign url = None
    - L163 if isinstance(raw, Path):
      - L164 assign url = artifact_url_fn(raw)
      - L165 else:
        - L165 if isinstance(raw, str):
          - L166 assign url = raw if raw.startswith('/') else artifact_url_fn(Path(raw))
    - L167 if url:
      - L168 assign normalized[str(name)] = url
  - L169 return normalized
- L172 def resolve_template_kindtemplate_id: str:
  - L173 from src.utils.template_utils import UPLOAD_KIND_PREFIXES
  - L175 assign record = state_store.get_template_record(template_id) or {}
  - L176 assign kind = str(record.get('kind') or '').lower()
  - L177 if kind in UPLOAD_KIND_PREFIXES:
    - L178 return kind
  - L179 assign normalized = normalize_template_id(template_id)
  - L180 assign tdir = template_dir(normalized, kind='excel', must_exist=False, create=False)
  - L181 return 'excel' if tdir.exists() else 'pdf'
- L184 def ensure_template_existstemplate_id: str, *, kind: str='pdf':
  - L185 return template_dir(template_id, must_exist=True, create=False, kind=kind)

## src\services\file_service\verify.py
- L1 from __future__ import annotations
- L3 import contextlib
- L4 import importlib
- L5 import os
- L6 import tempfile
- L7 import time
- L8 import uuid
- L9 import json
- L10 from pathlib import Path
- L11 from typing import Any, Optional
- L13 from fastapi import HTTPException, Request, UploadFile
- L14 from fastapi.responses import StreamingResponse
- L16 from backend.app.services.excel.ExcelVerify import xlsx_to_html_preview
- L17 from backend.app.services.templates.TemplateVerify import pdf_to_pngs, render_html_to_png, render_panel_preview, request_fix_html, request_initial_html, save_html
- L25 from backend.app.services.templates.layout_hints import get_layout_hints
- L26 from backend.app.services.utils import TemplateLockError, acquire_template_lock, get_correlation_id, write_artifact_manifest, write_json_atomic
- L33 from backend.app.services.utils.artifacts import load_manifest
- L34 from backend.app.services.state import state_store
- L35 from src.schemas.template_schema import GeneratorAssetsPayload
- L36 from src.utils.template_utils import artifact_url, manifest_endpoint, template_dir
- L38 from .helpers import MAX_VERIFY_PDF_BYTES, format_bytes, generate_template_id, http_error
- L46 def verify_templatefile: UploadFile, connection_id: str | None, request: Request, refine_iters: int=0:
  - L47 assign original_filename = getattr(file, 'filename', '') or ''
  - L48 assign template_name_hint = Path(original_filename).stem if original_filename else ''
  - L49 assign tid = generate_template_id(template_name_hint, kind='pdf')
  - L50 assign tdir = template_dir(tid, must_exist=False, create=True)
  - L51 assign pdf_path = tdir / 'source.pdf'
  - L52 assign html_path = tdir / 'template_p1.html'
  - L54 assign request_state = getattr(request, 'state', None)
  - L55 assign correlation_id = getattr(request_state, 'correlation_id', None) or get_correlation_id()
  - L57 try:
    - L58 assign api_mod = importlib.import_module('backend.api')
    - L59 except Exception:
      - L60 assign api_mod = None
  - L61 assign pdf_to_pngs_fn = getattr(api_mod, 'pdf_to_pngs', pdf_to_pngs)
  - L62 assign request_initial_html_fn = getattr(api_mod, 'request_initial_html', request_initial_html)
  - L63 assign save_html_fn = getattr(api_mod, 'save_html', save_html)
  - L64 assign render_html_to_png_fn = getattr(api_mod, 'render_html_to_png', render_html_to_png)
  - L65 assign render_panel_preview_fn = getattr(api_mod, 'render_panel_preview', render_panel_preview)
  - L66 assign request_fix_html_fn = getattr(api_mod, 'request_fix_html', request_fix_html)
  - L67 assign write_artifact_manifest_fn = getattr(api_mod, 'write_artifact_manifest', write_artifact_manifest)
  - L68 assign get_layout_hints_fn = getattr(api_mod, 'get_layout_hints', get_layout_hints)
  - L69 assign state_store_ref = getattr(api_mod, 'state_store', state_store)
  - L71 def event_stream:
    - L72 assign pipeline_started = time.time()
    - L74 def emitevent: str, **payload:
      - L75 assign data = {'event': event, **payload}
      - L76 return (json.dumps(data, ensure_ascii=False) + '\n').encode('utf-8')
    - L78 annotated assign stage_timings: dict[str, float] = {}
    - L80 def start_stagestage_key: str, label: str, progress: int | float, **payload: Any:
      - L81 assign stage_timings[stage_key] = time.time()
      - L82 annotated assign event_payload: dict[str, Any] = {'stage': stage_key, 'label': label, 'status': 'started', 'progress': progress, 'template_id': tid}
      - L89 if payload:
        - L90 expr event_payload.update(payload)
      - L91 return emit('stage', **event_payload)
    - L93 def finish_stagestage_key: str, label: str, *, progress: int | float | None=None, status: str='complete', **payload: Any:
      - L101 assign started = stage_timings.pop(stage_key, None)
      - L102 assign elapsed_ms = int((time.time() - started) * 1000) if started else None
      - L103 annotated assign event_payload: dict[str, Any] = {'stage': stage_key, 'label': label, 'status': status, 'template_id': tid}
      - L109 if progress is not None:
        - L110 assign event_payload['progress'] = progress
      - L111 if elapsed_ms is not None:
        - L112 assign event_payload['elapsed_ms'] = elapsed_ms
      - L113 if payload:
        - L114 expr event_payload.update(payload)
      - L115 return emit('stage', **event_payload)
    - L117 try:
      - L118 assign stage_key = 'verify.upload_pdf'
      - L119 assign stage_label = 'Uploading your PDF'
      - L120 expr (yield start_stage(stage_key, stage_label, progress=5))
      - L121 assign total_bytes = 0
      - L122 try:
        - L123 assign tmp = tempfile.NamedTemporaryFile(dir=str(tdir), prefix='source.', suffix='.pdf.tmp', delete=False)
        - L129 try:
          - L130 with tmp:
            - L131 assign limit_bytes = MAX_VERIFY_PDF_BYTES
            - L132 while True:
              - L133 assign chunk = file.file.read(1024 * 1024)
              - L134 if not chunk:
                - L135 break
              - L136 aug assign total_bytes Add len(chunk)
              - L137 if limit_bytes is not None and total_bytes > limit_bytes:
                - L138 raise RuntimeError(f'Uploaded PDF exceeds {format_bytes(limit_bytes)} limit.')
              - L139 expr tmp.write(chunk)
            - L140 expr tmp.flush()
            - L141 with contextlib.suppress(OSError):
              - L142 expr os.fsync(tmp.fileno())
          - L143 expr Path(tmp.name).replace(pdf_path)
          - L145 finally:
            - L145 with contextlib.suppress(FileNotFoundError):
              - L146 expr Path(tmp.name).unlink(missing_ok=True)
        - L147 except Exception as exc:
          - L148 expr (yield finish_stage(stage_key, stage_label, progress=5, status='error', detail=str(exc), size_bytes=total_bytes or None))
          - L156 raise
        - L158 else:
          - L158 expr (yield finish_stage(stage_key, stage_label, progress=20, size_bytes=total_bytes))
      - L160 assign stage_key = 'verify.render_reference_preview'
      - L161 assign stage_label = 'Rendering a preview image'
      - L162 expr (yield start_stage(stage_key, stage_label, progress=25))
      - L163 annotated assign png_path: Path | None = None
      - L164 annotated assign layout_hints: dict[str, Any] | None = None
      - L165 try:
        - L166 assign ref_pngs = pdf_to_pngs_fn(pdf_path, tdir, dpi=int(os.getenv('PDF_DPI', '400')))
        - L167 if not ref_pngs:
          - L168 raise RuntimeError('No pages rendered from PDF')
        - L169 assign png_path = ref_pngs[0]
        - L170 assign layout_hints = get_layout_hints_fn(pdf_path, 0)
        - L171 except Exception as exc:
          - L172 expr (yield finish_stage(stage_key, stage_label, progress=25, status='error', detail=str(exc)))
          - L173 raise
        - L175 else:
          - L175 expr (yield finish_stage(stage_key, stage_label, progress=60))
      - L177 assign stage_key = 'verify.generate_html'
      - L178 assign stage_label = 'Converting preview to HTML'
      - L179 expr (yield start_stage(stage_key, stage_label, progress=70))
      - L180 try:
        - L181 assign initial_result = request_initial_html_fn(png_path, None, layout_hints=layout_hints)
        - L182 assign html_text = initial_result.html
        - L183 assign schema_payload = initial_result.schema or {}
        - L184 expr save_html_fn(html_path, html_text)
        - L185 except Exception as exc:
          - L186 expr (yield finish_stage(stage_key, stage_label, progress=70, status='error', detail=str(exc)))
          - L187 raise
      - L189 assign schema_path = tdir / 'schema_ext.json'
      - L190 if schema_payload:
        - L191 try:
          - L192 expr write_json_atomic(schema_path, schema_payload, indent=2, ensure_ascii=False, step='verify_schema_ext')
          - L199 except Exception:
            - L200 pass
        - L202 else:
          - L202 with contextlib.suppress(FileNotFoundError):
            - L203 expr schema_path.unlink()
      - L205 expr (yield finish_stage(stage_key, stage_label, progress=78))
      - L207 assign render_png_path = tdir / 'render_p1.png'
      - L208 assign tight_render_png_path = render_png_path
      - L209 assign stage_key = 'verify.render_html_preview'
      - L210 assign stage_label = 'Rendering the HTML preview'
      - L211 expr (yield start_stage(stage_key, stage_label, progress=80))
      - L212 try:
        - L213 expr render_html_to_png_fn(html_path, render_png_path)
        - L214 assign panel_png_path = render_png_path.with_name('render_p1_llm.png')
        - L215 expr render_panel_preview_fn(html_path, panel_png_path, fallback_png=render_png_path)
        - L216 assign tight_render_png_path = panel_png_path if panel_png_path.exists() else render_png_path
        - L217 expr (yield finish_stage(stage_key, stage_label, progress=88))
        - L218 except Exception as exc:
          - L219 expr (yield finish_stage(stage_key, stage_label, progress=80, status='error', detail=str(exc)))
          - L220 raise
      - L222 assign stage_key = 'verify.refine_html_layout'
      - L223 assign stage_label = 'Refining HTML layout fidelity...'
      - L224 assign max_fix_passes = int(os.getenv('MAX_FIX_PASSES', '1'))
      - L225 assign fix_enabled = os.getenv('VERIFY_FIX_HTML_ENABLED', 'true').lower() not in {'false', '0'}
      - L230 expr (yield start_stage(stage_key, stage_label, progress=90, max_fix_passes=max_fix_passes, fix_enabled=fix_enabled))
      - L238 annotated assign fix_result: Optional[dict[str, Any]] = None
      - L239 annotated assign render_after_path: Optional[Path] = None
      - L240 annotated assign render_after_full_path: Optional[Path] = None
      - L241 annotated assign metrics_path: Optional[Path] = None
      - L242 assign fix_attempted = fix_enabled and max_fix_passes > 0
      - L244 if fix_attempted:
        - L245 try:
          - L246 assign fix_result = request_fix_html_fn(tdir, html_path, schema_path if schema_payload else None, png_path, tight_render_png_path, 0.0)
          - L254 except Exception:
            - L255 pass
          - L257 else:
            - L257 assign render_after_path = fix_result.get('render_after_path')
            - L258 assign render_after_full_path = fix_result.get('render_after_full_path')
            - L259 assign metrics_path = fix_result.get('metrics_path')
      - L261 expr (yield finish_stage(stage_key, stage_label, progress=96, skipped=not fix_attempted, fix_attempted=fix_attempted, fix_accepted=bool(fix_result and fix_result.get('accepted')), render_after=artifact_url(render_after_path) if render_after_path else None, render_after_full=artifact_url(render_after_full_path) if render_after_full_path else None, metrics=artifact_url(metrics_path) if metrics_path else None))
      - L273 assign schema_url = artifact_url(schema_path) if schema_payload else None
      - L274 assign render_url = artifact_url(tight_render_png_path)
      - L275 assign render_after_url = artifact_url(render_after_path) if render_after_path else None
      - L276 assign render_after_full_url = artifact_url(render_after_full_path) if render_after_full_path else None
      - L277 assign metrics_url = artifact_url(metrics_path) if metrics_path else None
      - L279 annotated assign manifest_files: dict[str, Path] = {'source.pdf': pdf_path, 'reference_p1.png': png_path, 'template_p1.html': html_path, 'render_p1.png': render_png_path}
      - L285 if tight_render_png_path and tight_render_png_path.exists():
        - L286 assign manifest_files['render_p1_llm.png'] = tight_render_png_path
      - L287 if schema_payload:
        - L288 assign manifest_files['schema_ext.json'] = schema_path
      - L289 if render_after_path:
        - L290 assign manifest_files['render_p1_after.png'] = render_after_path
      - L291 if render_after_full_path:
        - L292 assign manifest_files['render_p1_after_full.png'] = render_after_full_path
      - L293 if metrics_path:
        - L294 assign manifest_files['fix_metrics.json'] = metrics_path
      - L296 assign stage_key = 'verify.save_artifacts'
      - L297 assign stage_label = 'Saving verification artifacts'
      - L298 expr (yield start_stage(stage_key, stage_label, progress=97))
      - L299 try:
        - L300 expr write_artifact_manifest_fn(tdir, step='templates_verify', files=manifest_files, inputs=[str(pdf_path)], correlation_id=correlation_id)
        - L307 except Exception as exc:
          - L308 expr (yield finish_stage(stage_key, stage_label, progress=97, status='error', detail=str(exc)))
        - L310 else:
          - L310 expr (yield finish_stage(stage_key, stage_label, progress=99, manifest_files=len(manifest_files), schema_url=schema_url, render_url=render_url, render_after_url=render_after_url, render_after_full_url=render_after_full_url, metrics_url=metrics_url))
      - L322 assign template_name = template_name_hint or f'Template {tid[:8]}'
      - L323 assign artifacts_for_state = {'template_html_url': artifact_url(html_path), 'thumbnail_url': artifact_url(png_path), 'pdf_url': artifact_url(pdf_path), 'manifest_url': manifest_endpoint(tid, kind='pdf')}
      - L329 if schema_url:
        - L330 assign artifacts_for_state['schema_ext_url'] = schema_url
      - L331 if render_url:
        - L332 assign artifacts_for_state['render_png_url'] = render_url
      - L333 if render_after_url:
        - L334 assign artifacts_for_state['render_after_png_url'] = render_after_url
      - L335 if render_after_full_url:
        - L336 assign artifacts_for_state['render_after_full_png_url'] = render_after_full_url
      - L337 if metrics_url:
        - L338 assign artifacts_for_state['fix_metrics_url'] = metrics_url
      - L340 expr state_store_ref.upsert_template(tid, name=template_name, status='draft', artifacts=artifacts_for_state, connection_id=connection_id or None, template_type='pdf')
      - L348 expr state_store_ref.set_last_used(connection_id or None, tid)
      - L350 assign total_elapsed_ms = int((time.time() - pipeline_started) * 1000)
      - L351 expr (yield emit('result', stage='Verification complete.', progress=100, template_id=tid, schema=schema_payload, elapsed_ms=total_elapsed_ms, artifacts=artifacts_for_state))
      - L360 except Exception as e:
        - L361 expr (yield emit('error', stage='Verification failed.', detail=str(e), template_id=tid))
      - L368 finally:
        - L368 with contextlib.suppress(Exception):
          - L369 expr file.file.close()
  - L371 assign headers = {'Content-Type': 'application/x-ndjson'}
  - L372 return StreamingResponse(event_stream(), headers=headers, media_type='application/x-ndjson')
- L375 def verify_excelfile: UploadFile, request: Request, connection_id: str | None=None:
  - L376 assign template_kind = 'excel'
  - L377 assign original_filename = getattr(file, 'filename', '') or ''
  - L378 assign template_name_hint = Path(original_filename).stem if original_filename else ''
  - L379 assign tid = generate_template_id(template_name_hint or 'Workbook', kind=template_kind)
  - L380 assign tdir = template_dir(tid, must_exist=False, create=True, kind=template_kind)
  - L381 assign xlsx_path = tdir / 'source.xlsx'
  - L383 assign request_state = getattr(request, 'state', None)
  - L384 assign correlation_id = getattr(request_state, 'correlation_id', None) or get_correlation_id()
  - L386 try:
    - L387 assign api_mod = importlib.import_module('backend.api')
    - L388 except Exception:
      - L389 assign api_mod = None
  - L390 assign write_artifact_manifest_fn = getattr(api_mod, 'write_artifact_manifest', write_artifact_manifest)
  - L391 assign state_store_ref = getattr(api_mod, 'state_store', state_store)
  - L393 def event_stream:
    - L394 assign pipeline_started = time.time()
    - L396 def emitevent: str, **payload:
      - L397 assign data = {'event': event, **payload}
      - L398 return (json.dumps(data, ensure_ascii=False) + '\n').encode('utf-8')
    - L400 annotated assign stage_timings: dict[str, float] = {}
    - L402 def start_stagestage_key: str, label: str, progress: int | float, **payload: Any:
      - L403 assign stage_timings[stage_key] = time.time()
      - L404 assign event_payload = {'stage': stage_key, 'label': label, 'status': 'started', 'progress': progress, 'template_id': tid, 'kind': template_kind}
      - L412 if payload:
        - L413 expr event_payload.update(payload)
      - L414 return emit('stage', **event_payload)
    - L416 def finish_stagestage_key: str, label: str, *, progress: int | float | None=None, status: str='complete', **payload: Any:
      - L424 assign started = stage_timings.pop(stage_key, None)
      - L425 assign elapsed_ms = int((time.time() - started) * 1000) if started else None
      - L426 assign event_payload = {'stage': stage_key, 'label': label, 'status': status, 'template_id': tid, 'kind': template_kind}
      - L433 if progress is not None:
        - L434 assign event_payload['progress'] = progress
      - L435 if elapsed_ms is not None:
        - L436 assign event_payload['elapsed_ms'] = elapsed_ms
      - L437 if payload:
        - L438 expr event_payload.update(payload)
      - L439 return emit('stage', **event_payload)
    - L441 try:
      - L442 assign stage_key = 'excel.upload_file'
      - L443 assign stage_label = 'Uploading your workbook'
      - L444 expr (yield start_stage(stage_key, stage_label, progress=5))
      - L445 assign total_bytes = 0
      - L446 try:
        - L447 assign tmp = tempfile.NamedTemporaryFile(dir=str(tdir), prefix='source.', suffix='.xlsx.tmp', delete=False)
        - L453 try:
          - L454 with tmp:
            - L455 while True:
              - L456 assign chunk = file.file.read(1024 * 1024)
              - L457 if not chunk:
                - L458 break
              - L459 aug assign total_bytes Add len(chunk)
              - L460 expr tmp.write(chunk)
            - L461 expr tmp.flush()
            - L462 with contextlib.suppress(OSError):
              - L463 expr os.fsync(tmp.fileno())
          - L464 expr Path(tmp.name).replace(xlsx_path)
          - L466 finally:
            - L466 with contextlib.suppress(FileNotFoundError):
              - L467 expr Path(tmp.name).unlink(missing_ok=True)
        - L468 except Exception as exc:
          - L469 expr (yield finish_stage(stage_key, stage_label, progress=5, status='error', detail=str(exc)))
          - L470 raise
        - L472 else:
          - L472 expr (yield finish_stage(stage_key, stage_label, progress=25, size_bytes=total_bytes))
      - L474 assign stage_key = 'excel.generate_html'
      - L475 assign stage_label = 'Building preview HTML'
      - L476 expr (yield start_stage(stage_key, stage_label, progress=45))
      - L477 try:
        - L478 assign preview = xlsx_to_html_preview(xlsx_path, tdir)
        - L479 assign html_path = preview.html_path
        - L480 assign png_path = preview.png_path
        - L481 except Exception as exc:
          - L482 expr (yield finish_stage(stage_key, stage_label, progress=45, status='error', detail=str(exc)))
          - L483 raise
        - L485 else:
          - L485 expr (yield finish_stage(stage_key, stage_label, progress=80))
      - L487 assign schema_path = tdir / 'schema_ext.json'
      - L488 assign sample_rows_path = tdir / 'sample_rows.json'
      - L489 assign reference_html_path = tdir / 'reference_p1.html'
      - L490 assign reference_png_path = tdir / 'reference_p1.png'
      - L491 annotated assign manifest_files: dict[str, Path] = {'source.xlsx': xlsx_path, 'template_p1.html': html_path}
      - L492 if png_path and png_path.exists():
        - L493 assign manifest_files[png_path.name] = png_path
      - L494 if reference_png_path.exists():
        - L495 assign manifest_files[reference_png_path.name] = reference_png_path
      - L496 if sample_rows_path.exists():
        - L497 assign manifest_files[sample_rows_path.name] = sample_rows_path
      - L498 if reference_html_path.exists():
        - L499 assign manifest_files[reference_html_path.name] = reference_html_path
      - L500 if schema_path.exists():
        - L501 assign manifest_files[schema_path.name] = schema_path
      - L503 assign stage_key = 'excel.save_artifacts'
      - L504 assign stage_label = 'Saving verification artifacts'
      - L505 expr (yield start_stage(stage_key, stage_label, progress=90))
      - L506 try:
        - L507 expr write_artifact_manifest_fn(tdir, step='excel_verify', files=manifest_files, inputs=[str(xlsx_path)], correlation_id=correlation_id)
        - L514 except Exception as exc:
          - L515 expr (yield finish_stage(stage_key, stage_label, progress=90, status='error', detail=str(exc)))
          - L516 raise
        - L518 else:
          - L518 expr (yield finish_stage(stage_key, stage_label, progress=96, manifest_files=len(manifest_files)))
      - L520 assign manifest_url = manifest_endpoint(tid, kind=template_kind)
      - L521 assign html_url = artifact_url(html_path)
      - L522 assign png_url = artifact_url(png_path)
      - L523 assign xlsx_url = artifact_url(xlsx_path)
      - L524 assign sample_rows_url = artifact_url(sample_rows_path) if sample_rows_path.exists() else None
      - L525 assign reference_html_url = artifact_url(reference_html_path) if reference_html_path.exists() else None
      - L526 assign reference_png_url = artifact_url(reference_png_path) if reference_png_path.exists() else None
      - L527 assign schema_url = artifact_url(schema_path) if schema_path.exists() else None
      - L529 assign template_display_name = template_name_hint or 'Workbook'
      - L530 expr state_store_ref.upsert_template(tid, name=template_display_name, status='draft', artifacts={'template_html_url': html_url, 'thumbnail_url': png_url, 'xlsx_url': xlsx_url, 'manifest_url': manifest_url, **({'sample_rows_url': sample_rows_url} if sample_rows_url else {}), **({'reference_html_url': reference_html_url} if reference_html_url else {}), **({'reference_png_url': reference_png_url} if reference_png_url else {}), **({'schema_ext_url': schema_url} if schema_url else {})}, connection_id=connection_id or None, template_type=template_kind)
      - L547 expr state_store_ref.set_last_used(connection_id or None, tid)
      - L549 assign total_elapsed_ms = int((time.time() - pipeline_started) * 1000)
      - L550 expr (yield emit('result', stage='Excel verification complete.', progress=100, template_id=tid, kind=template_kind, schema=None, elapsed_ms=total_elapsed_ms, artifacts={'xlsx_url': xlsx_url, 'png_url': png_url, 'html_url': html_url, 'manifest_url': manifest_url, **({'sample_rows_url': sample_rows_url} if sample_rows_url else {}), **({'reference_html_url': reference_html_url} if reference_html_url else {}), **({'reference_png_url': reference_png_url} if reference_png_url else {}), **({'schema_ext_url': schema_url} if schema_url else {})}))
      - L569 except Exception as exc:
        - L570 expr (yield emit('error', stage='Excel verification failed.', detail=str(exc), template_id=tid, kind=template_kind))
      - L578 finally:
        - L578 expr file.file.close()
  - L580 assign headers = {'Content-Type': 'application/x-ndjson'}
  - L581 return StreamingResponse(event_stream(), headers=headers, media_type='application/x-ndjson')

## src\services\llm_service.py
- L1 from __future__ import annotations
- L3 from typing import Any
- L5 from backend.app.services.mapping.AutoMapInline import run_llm_call_3
- L6 from backend.app.services.mapping.CorrectionsPreview import run_corrections_preview
- L7 from backend.app.services.utils import call_chat_completion
- L10 def call_llm_chat**kwargs: Any:
  - L11 docstring: "Centralized OpenAI chat entrypoint."
  - L12 return call_chat_completion(**kwargs)
- L15 def run_llm_mapping*args, **kwargs:
  - L16 docstring: "Run mapping prompt round-trip."
  - L17 return run_llm_call_3(*args, **kwargs)
- L20 def run_llm_corrections*args, **kwargs:
  - L21 docstring: "Run mapping corrections prompt round-trip."
  - L22 return run_corrections_preview(*args, **kwargs)

## src\services\mapping\__init__.py
- L1 from src.services.mapping.preview import mapping_preview_internal, run_mapping_preview
- L2 from src.services.mapping.approve import run_mapping_approve
- L3 from src.services.mapping.corrections import run_corrections_preview
- L4 from src.services.mapping.key_options import mapping_key_options
- L6 assign __all__ = ['mapping_preview_internal', 'run_mapping_preview', 'run_mapping_approve', 'run_corrections_preview', 'mapping_key_options']

## src\services\mapping\approve.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import importlib
- L5 import json
- L6 import logging
- L7 import time
- L8 from typing import Any
- L10 from fastapi import HTTPException, Request
- L11 from fastapi.responses import StreamingResponse
- L13 from backend.app.services.connections.db_connection import verify_sqlite
- L14 from backend.app.services.contract.ContractBuilderV2 import ContractBuilderError, build_or_load_contract_v2
- L15 from backend.app.services.generator.GeneratorAssetsV1 import GeneratorAssetsError, build_generator_assets_from_payload
- L16 from backend.app.services.prompts.llm_prompts import PROMPT_VERSION, PROMPT_VERSION_3_5, PROMPT_VERSION_4
- L17 from backend.app.services.state import state_store
- L18 from backend.app.services.templates.TemplateVerify import render_html_to_png, render_panel_preview
- L19 from backend.app.services.utils import TemplateLockError, acquire_template_lock, validate_mapping_schema, write_artifact_manifest, write_json_atomic, write_text_atomic
- L27 from backend.app.services.utils.artifacts import load_manifest
- L28 from src.utils.connection_utils import db_path_from_payload_or_default
- L29 from src.utils.template_utils import artifact_url, manifest_endpoint, template_dir
- L30 from src.utils.mapping_utils import mapping_keys_path, normalize_key_tokens, write_mapping_keys
- L31 from src.services.mapping.helpers import build_catalog_from_db as _build_catalog_from_db, compute_db_signature, http_error as _http_error, load_mapping_step3 as _load_mapping_step3, load_schema_ext as _load_schema_ext, normalize_artifact_map as _normalize_artifact_map, normalize_mapping_for_autofill as _normalize_mapping_for_autofill
- L41 assign logger = logging.getLogger(__name__)
- L43 async def run_mapping_approvetemplate_id: str, payload: Any, request: Request, *, kind: str='pdf':
  - L50 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L51 try:
    - L52 assign api_mod = importlib.import_module('backend.api')
    - L53 except Exception:
      - L54 assign api_mod = None
  - L55 assign contract_builder = getattr(api_mod, 'build_or_load_contract_v2', build_or_load_contract_v2)
  - L56 assign generator_builder = getattr(api_mod, 'build_generator_assets_from_payload', build_generator_assets_from_payload)
  - L57 assign render_html_fn = getattr(api_mod, 'render_html_to_png', render_html_to_png)
  - L58 assign render_panel_fn = getattr(api_mod, 'render_panel_preview', render_panel_preview)
  - L59 expr logger.info('mapping_approve_start', extra={'event': 'mapping_approve_start', 'template_id': template_id, 'connection_id': payload.connection_id, 'mapping_size': len(payload.mapping or {}), 'template_kind': kind, 'correlation_id': correlation_id})
  - L71 assign template_dir_path = template_dir(template_id, kind=kind)
  - L72 assign require_contract_join = (kind or 'pdf').lower() != 'excel'
  - L73 assign base_template_path = template_dir_path / 'template_p1.html'
  - L74 assign final_html_path = template_dir_path / 'report_final.html'
  - L75 assign mapping_path = template_dir_path / 'mapping_pdf_labels.json'
  - L76 assign mapping_keys_file = mapping_keys_path(template_dir_path)
  - L77 assign incoming_keys = normalize_key_tokens(payload.keys)
  - L78 assign mapping_dict = payload.mapping or {}
  - L79 assign keys_clean = [key for key in incoming_keys if key in mapping_dict]
  - L81 try:
    - L82 assign db_path = db_path_from_payload_or_default(payload.connection_id)
    - L83 expr verify_sqlite(db_path)
    - L84 except HTTPException:
      - L85 raise
    - L86 except Exception as exc:
      - L87 raise _http_error(400, 'db_invalid', f'Invalid database reference: {exc}')
  - L89 assign schema_ext = _load_schema_ext(template_dir_path) or {}
  - L90 assign (auto_mapping_doc, _) = _load_mapping_step3(template_dir_path)
  - L91 assign auto_mapping_proposal = auto_mapping_doc or {}
  - L92 assign catalog = list(dict.fromkeys(_build_catalog_from_db(db_path)))
  - L93 assign db_sig = compute_db_signature(db_path)
  - L95 try:
    - L96 assign lock_ctx = acquire_template_lock(template_dir_path, 'mapping_approve', correlation_id)
    - L97 except TemplateLockError:
      - L98 raise _http_error(status_code=409, code='template_locked', message='Template is currently processing another request.')
  - L104 def event_stream:
    - L105 assign pipeline_started = time.time()
    - L106 Nonlocal
    - L108 def log_stagestage_name: str, status: str, started: float:
      - L109 expr logger.info('mapping_approve_stage', extra={'event': 'mapping_approve_stage', 'template_id': template_id, 'stage': stage_name, 'status': status, 'elapsed_ms': int((time.time() - started) * 1000), 'correlation_id': correlation_id})
    - L121 def emitevent: str, **payload_data: Any:
      - L122 assign data = {'event': event, **payload_data}
      - L123 return (json.dumps(data, ensure_ascii=False) + '\n').encode('utf-8')
    - L125 annotated assign stage_timings: dict[str, float] = {}
    - L127 def start_stagestage_key: str, label: str, progress: int | float, **payload_data: Any:
      - L128 assign stage_timings[stage_key] = time.time()
      - L129 assign payload = {'stage': stage_key, 'label': label, 'status': 'started', 'progress': progress, 'template_id': template_id}
      - L130 expr payload.update(payload_data)
      - L131 return emit('stage', **payload)
    - L133 def finish_stagestage_key: str, label: str, *, progress: int | float | None=None, status: str='complete', **payload_data: Any:
      - L141 assign started = stage_timings.pop(stage_key, None)
      - L142 assign elapsed_ms = int((time.time() - started) * 1000) if started else None
      - L143 annotated assign payload: dict[str, Any] = {'stage': stage_key, 'label': label, 'status': status, 'template_id': template_id}
      - L144 if progress is not None:
        - L145 assign payload['progress'] = progress
      - L146 if elapsed_ms is not None:
        - L147 assign payload['elapsed_ms'] = elapsed_ms
      - L148 expr payload.update(payload_data)
      - L149 return emit('stage', **payload)
    - L151 assign contract_ready = False
    - L152 annotated assign contract_stage_summary: dict[str, Any] | None = None
    - L153 annotated assign generator_stage_summary: dict[str, Any] | None = None
    - L154 annotated assign contract_result: dict[str, Any] = {}
    - L155 annotated assign generator_result: dict[str, Any] | None = None
    - L156 annotated assign generator_artifacts_urls: dict[str, str] = {}
    - L158 with lock_ctx:
      - L159 assign stage_key = 'mapping.save'
      - L160 assign stage_label = 'Saving mapping changes'
      - L161 assign stage_started = time.time()
      - L162 try:
        - L163 expr (yield start_stage(stage_key, stage_label, progress=5))
        - L164 assign normalized_list = _normalize_mapping_for_autofill(payload.mapping)
        - L165 assign normalized_headers = {entry['header'] for entry in normalized_list}
        - L166 assign keys_clean = [key for key in keys_clean if key in normalized_headers]
        - L167 expr validate_mapping_schema(normalized_list)
        - L168 expr write_json_atomic(mapping_path, normalized_list, indent=2, ensure_ascii=False, step='mapping_save')
        - L169 assign keys_clean = write_mapping_keys(template_dir_path, keys_clean)
        - L170 assign manifest_files = {mapping_path.name: mapping_path}
        - L171 if mapping_keys_file.exists():
          - L172 assign manifest_files[mapping_keys_file.name] = mapping_keys_file
        - L173 expr write_artifact_manifest(template_dir_path, step='mapping_save', files=manifest_files, inputs=[f'mapping_tokens={len(normalized_list)}', f'mapping_keys={len(keys_clean)}'], correlation_id=correlation_id)
        - L180 expr log_stage(stage_label, 'ok', stage_started)
        - L181 expr (yield finish_stage(stage_key, stage_label, progress=20, mapping_tokens=len(normalized_list)))
        - L182 except Exception as exc:
          - L183 expr log_stage(stage_label, 'error', stage_started)
          - L184 expr logger.exception('mapping_save_failed', extra={'event': 'mapping_save_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L188 expr (yield finish_stage(stage_key, stage_label, progress=5, status='error', detail=str(exc)))
          - L189 expr (yield emit('error', stage=stage_key, label=stage_label, detail=str(exc), template_id=template_id))
          - L190 return None
      - L192 assign stage_key = 'mapping.prepare_template'
      - L193 assign stage_label = 'Preparing template shell'
      - L194 assign stage_started = time.time()
      - L195 try:
        - L196 expr (yield start_stage(stage_key, stage_label, progress=25))
        - L197 if not base_template_path.exists():
          - L198 raise FileNotFoundError('template_p1.html not found. Run /templates/verify first.')
        - L199 if not final_html_path.exists():
          - L200 expr final_html_path.write_text(base_template_path.read_text(encoding='utf-8', errors='ignore'), encoding='utf-8')
        - L204 expr log_stage(stage_label, 'ok', stage_started)
        - L205 expr (yield finish_stage(stage_key, stage_label, progress=50))
        - L206 except Exception as exc:
          - L207 expr log_stage(stage_label, 'error', stage_started)
          - L208 expr logger.exception('mapping_prepare_final_html_failed', extra={'event': 'mapping_prepare_final_html_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L216 expr (yield finish_stage(stage_key, stage_label, progress=25, status='error', detail=str(exc)))
          - L217 expr (yield emit('error', stage=stage_key, label=stage_label, detail=str(exc), template_id=template_id))
          - L218 return None
      - L220 assign final_html_url = artifact_url(final_html_path)
      - L221 assign template_html_url = final_html_url or artifact_url(base_template_path)
      - L222 assign tokens_mapped = len(payload.mapping or {})
      - L224 assign stage_key = 'contract_build_v2'
      - L225 assign stage_label = 'Drafting contract package'
      - L226 assign stage_started = time.time()
      - L227 expr (yield start_stage(stage_key, stage_label, progress=55, contract_ready=False, blueprint_ready=bool(auto_mapping_proposal), overview_md=None, cached=False, warnings=[], assumptions=[], validation={}, prompt_version=PROMPT_VERSION_4))
      - L240 try:
        - L241 assign final_html_text = final_html_path.read_text(encoding='utf-8', errors='ignore')
        - L242 assign contract_result = contract_builder(template_dir=template_dir_path, catalog=catalog, final_template_html=final_html_text, schema=schema_ext, auto_mapping_proposal=auto_mapping_proposal, mapping_override=payload.mapping, user_instructions=payload.user_instructions or '', dialect_hint=payload.dialect_hint, db_signature=db_sig, key_tokens=keys_clean)
        - L254 assign contract_ready = True
        - L255 assign contract_artifacts_urls = _normalize_artifact_map(contract_result.get('artifacts'))
        - L256 assign contract_stage_summary = {'stage': stage_key, 'status': 'done', 'contract_ready': True, 'overview_md': contract_result.get('overview_md'), 'cached': contract_result.get('cached'), 'warnings': contract_result.get('warnings'), 'assumptions': contract_result.get('assumptions'), 'validation': contract_result.get('validation'), 'artifacts': contract_artifacts_urls, 'prompt_version': PROMPT_VERSION_4}
        - L268 expr log_stage(stage_label, 'ok', stage_started)
        - L269 expr (yield finish_stage(stage_key, stage_label, progress=75, contract_ready=True, overview_md=contract_result.get('overview_md'), cached=contract_result.get('cached'), warnings=contract_result.get('warnings'), assumptions=contract_result.get('assumptions'), validation=contract_result.get('validation'), artifacts=contract_artifacts_urls, prompt_version=PROMPT_VERSION_4))
        - L282 except ContractBuilderError as exc:
          - L283 expr log_stage(stage_label, 'error', stage_started)
          - L284 expr logger.exception('contract_build_failed', extra={'event': 'contract_build_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L288 expr (yield finish_stage(stage_key, stage_label, progress=55, status='error', detail=str(exc), prompt_version=PROMPT_VERSION_4))
          - L296 expr (yield emit('error', stage=stage_key, label=stage_label, detail=str(exc), template_id=template_id, prompt_version=PROMPT_VERSION_4))
          - L304 return None
        - L305 except Exception as exc:
          - L306 expr log_stage(stage_label, 'error', stage_started)
          - L307 expr logger.exception('contract_build_failed', extra={'event': 'contract_build_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L311 expr (yield finish_stage(stage_key, stage_label, progress=55, status='error', detail=str(exc), prompt_version=PROMPT_VERSION_4))
          - L319 expr (yield emit('error', stage=stage_key, label=stage_label, detail=str(exc), template_id=template_id, prompt_version=PROMPT_VERSION_4))
          - L327 return None
      - L329 assign stage_key = 'generator_assets_v1'
      - L330 assign stage_label = 'Creating generator assets'
      - L331 assign stage_started = time.time()
      - L332 assign generator_dialect = payload.generator_dialect or payload.dialect_hint or 'duckdb'
      - L333 expr (yield start_stage(stage_key, stage_label, progress=80, dialect=generator_dialect))
      - L334 try:
        - L335 assign generator_result = generator_builder(template_dir=template_dir_path, step4_output=contract_result, final_template_html=final_html_path.read_text(encoding='utf-8', errors='ignore'), reference_pdf_image=None, catalog_allowlist=payload.catalog_allowlist or catalog, dialect=generator_dialect, params_spec=payload.params_spec, sample_params=payload.sample_params, force_rebuild=payload.force_generator_rebuild, key_tokens=keys_clean, require_contract_join=require_contract_join)
        - L348 assign generator_artifacts_urls = _normalize_artifact_map(generator_result.get('artifacts'))
        - L349 assign generator_stage_summary = {'stage': stage_key, 'status': 'done', 'invalid': generator_result.get('invalid'), 'needs_user_fix': list(generator_result.get('needs_user_fix') or []), 'dialect': generator_result.get('dialect'), 'params': generator_result.get('params'), 'summary': generator_result.get('summary'), 'dry_run': generator_result.get('dry_run'), 'cached': generator_result.get('cached'), 'artifacts': generator_artifacts_urls}
        - L361 expr log_stage(stage_label, 'ok', stage_started)
        - L362 expr (yield finish_stage(stage_key, stage_label, progress=92, invalid=generator_result.get('invalid'), needs_user_fix=list(generator_result.get('needs_user_fix') or []), dialect=generator_result.get('dialect'), params=generator_result.get('params'), summary=generator_result.get('summary'), dry_run=generator_result.get('dry_run'), cached=generator_result.get('cached'), artifacts=generator_artifacts_urls))
        - L375 except GeneratorAssetsError as exc:
          - L376 expr log_stage(stage_label, 'error', stage_started)
          - L377 expr logger.exception('generator_assets_failed', extra={'event': 'generator_assets_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L385 assign generator_stage_summary = {'stage': stage_key, 'status': 'error', 'detail': str(exc)}
          - L386 assign generator_artifacts_urls = {}
          - L387 expr (yield finish_stage(stage_key, stage_label, progress=90, status='error', detail=str(exc)))
        - L388 except Exception as exc:
          - L389 expr log_stage(stage_label, 'error', stage_started)
          - L390 expr logger.exception('generator_assets_failed', extra={'event': 'generator_assets_failed', 'template_id': template_id, 'correlation_id': correlation_id})
          - L398 assign generator_stage_summary = {'stage': stage_key, 'status': 'error', 'detail': str(exc)}
          - L399 assign generator_artifacts_urls = {}
          - L400 expr (yield finish_stage(stage_key, stage_label, progress=90, status='error', detail=str(exc)))
      - L402 assign stage_key = 'mapping.thumbnail'
      - L403 assign stage_label = 'Capturing template thumbnail'
      - L404 assign stage_started = time.time()
      - L405 assign thumbnail_url = None
      - L406 try:
        - L407 expr (yield start_stage(stage_key, stage_label, progress=95))
        - L408 assign thumb_path = final_html_path.parent / 'report_final.png'
        - L409 expr asyncio.run(render_html_fn(final_html_path, thumb_path))
        - L410 assign thumbnail_url = artifact_url(thumb_path)
        - L411 expr write_artifact_manifest(template_dir_path, step='mapping_thumbnail', files={'report_final.html': final_html_path, 'template_p1.html': base_template_path, 'report_final.png': thumb_path}, inputs=[str(mapping_path)], correlation_id=correlation_id)
        - L422 expr log_stage(stage_label, 'ok', stage_started)
        - L423 expr (yield finish_stage(stage_key, stage_label, progress=98, thumbnail_url=thumbnail_url))
        - L424 except Exception:
          - L425 expr log_stage(stage_label, 'error', stage_started)
          - L426 expr (yield finish_stage(stage_key, stage_label, progress=95, status='error'))
      - L428 assign manifest_data = load_manifest(template_dir_path) or {}
      - L429 assign manifest_url = manifest_endpoint(template_id, kind=kind)
      - L430 assign page_summary_path = template_dir_path / 'page_summary.txt'
      - L431 assign page_summary_url = artifact_url(page_summary_path)
      - L433 assign contract_artifacts = contract_stage_summary.get('artifacts') if isinstance(contract_stage_summary, dict) else {}
      - L436 assign generator_artifacts = generator_stage_summary.get('artifacts') if isinstance(generator_stage_summary, dict) else {}
      - L439 if not isinstance(generator_artifacts, dict):
        - L440 assign generator_artifacts = {}
      - L442 assign generator_contract_url = generator_artifacts.get('contract') or generator_artifacts.get('contract.json')
      - L443 assign contract_url = generator_contract_url or contract_artifacts.get('contract') or contract_artifacts.get('contract.json')
      - L445 assign artifacts_payload = {'template_html_url': template_html_url, 'final_html_url': final_html_url, 'thumbnail_url': thumbnail_url, 'manifest_url': manifest_url, 'page_summary_url': page_summary_url, 'contract_url': contract_url, 'overview_url': contract_artifacts.get('overview'), 'step5_requirements_url': contract_artifacts.get('step5_requirements'), 'generator_sql_pack_url': generator_artifacts.get('sql_pack'), 'generator_output_schemas_url': generator_artifacts.get('output_schemas'), 'generator_assets_url': generator_artifacts.get('generator_assets'), 'mapping_keys_url': artifact_url(mapping_keys_file) if mapping_keys_file.exists() else None}
      - L460 assign final_contract_ready = bool(generator_contract_url)
      - L462 assign existing_tpl = state_store.get_template_record(template_id) or {}
      - L463 expr state_store.upsert_template(template_id, name=existing_tpl.get('name') or f'Template {template_id[:8]}', status='approved' if final_contract_ready else 'pending', artifacts={k: v for k, v in artifacts_payload.items() if v}, connection_id=payload.connection_id or existing_tpl.get('last_connection_id'), mapping_keys=keys_clean, template_type=kind)
      - L473 if generator_result:
        - L474 expr state_store.update_template_generator(template_id, dialect=generator_result.get('dialect'), params=generator_result.get('params'), invalid=bool(generator_result.get('invalid')), needs_user_fix=generator_result.get('needs_user_fix') or [], summary=generator_result.get('summary'), dry_run=generator_result.get('dry_run'))
      - L484 expr state_store.set_last_used(payload.connection_id or existing_tpl.get('last_connection_id'), template_id)
      - L486 assign total_elapsed_ms = int((time.time() - pipeline_started) * 1000)
      - L487 assign contract_ready = final_contract_ready
      - L488 assign result_payload = {'stage': 'Approval complete.', 'progress': 100, 'template_id': template_id, 'saved': artifact_url(mapping_path), 'final_html_path': str(final_html_path), 'final_html_url': final_html_url, 'template_html_url': template_html_url, 'thumbnail_url': thumbnail_url, 'contract_ready': contract_ready, 'token_map_size': tokens_mapped, 'user_values_supplied': bool((payload.user_values_text or '').strip()), 'manifest': manifest_data, 'manifest_url': manifest_url, 'artifacts': {k: v for k, v in artifacts_payload.items() if v}, 'contract_stage': contract_stage_summary, 'generator_stage': generator_stage_summary, 'prompt_versions': {'mapping': PROMPT_VERSION, 'corrections': PROMPT_VERSION_3_5, 'contract': PROMPT_VERSION_4}, 'elapsed_ms': total_elapsed_ms, 'keys': keys_clean, 'keys_count': len(keys_clean)}
      - L514 expr (yield emit('result', **result_payload))
      - L516 expr logger.info('mapping_approve_complete', extra={'event': 'mapping_approve_complete', 'template_id': template_id, 'contract_ready': contract_ready, 'thumbnail_url': thumbnail_url, 'correlation_id': correlation_id, 'elapsed_ms': total_elapsed_ms})
  - L528 assign headers = {'Content-Type': 'application/x-ndjson'}
  - L529 return StreamingResponse(event_stream(), headers=headers, media_type='application/x-ndjson')

## src\services\mapping\corrections.py
- L1 from __future__ import annotations
- L3 import json
- L4 import logging
- L5 import time
- L6 from pathlib import Path
- L7 from typing import Any, Optional
- L9 from fastapi import Request
- L10 from fastapi.responses import StreamingResponse
- L12 from backend.app.services.mapping.CorrectionsPreview import CorrectionsPreviewError, run_corrections_preview as corrections_preview_fn
- L13 from backend.app.services.prompts.llm_prompts import PROMPT_VERSION_3_5
- L14 from backend.app.services.state import state_store
- L15 from src.services.mapping.helpers import http_error as _http_error
- L16 from src.utils.template_utils import artifact_url, template_dir
- L18 assign logger = logging.getLogger(__name__)
- L20 def run_corrections_previewtemplate_id: str, payload: Any, request: Request, *, kind: str='pdf':
  - L27 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L28 expr logger.info('corrections_preview_start', extra={'event': 'corrections_preview_start', 'template_id': template_id, 'correlation_id': correlation_id, 'template_kind': kind})
  - L38 assign template_dir_path = template_dir(template_id, kind=kind)
  - L39 assign template_html_path = template_dir_path / 'template_p1.html'
  - L40 assign mapping_step3_path = template_dir_path / 'mapping_step3.json'
  - L41 assign schema_ext_path = template_dir_path / 'schema_ext.json'
  - L43 assign page_index = max(1, int(payload.page or 1))
  - L44 assign reference_png = template_dir_path / f'reference_p{page_index}.png'
  - L45 assign page_png_path = reference_png if reference_png.exists() else None
  - L47 def event_stream:
    - L48 assign started = time.time()
    - L50 def emitevent: str, **data: Any:
      - L51 return (json.dumps({'event': event, **data}, ensure_ascii=False) + '\n').encode('utf-8')
    - L53 expr (yield emit('stage', stage='corrections_preview', status='start', progress=10, template_id=template_id, correlation_id=correlation_id, prompt_version=PROMPT_VERSION_3_5))
    - L62 try:
      - L63 assign result = corrections_preview_fn(upload_dir=template_dir_path, template_html_path=template_html_path, mapping_step3_path=mapping_step3_path, schema_ext_path=schema_ext_path, user_input=payload.user_input or '', page_png_path=page_png_path, model_selector=payload.model_selector, mapping_override=payload.mapping_override, sample_tokens=payload.sample_tokens)
      - L74 except CorrectionsPreviewError as exc:
        - L75 expr logger.warning('corrections_preview_failed', extra={'event': 'corrections_preview_failed', 'template_id': template_id, 'correlation_id': correlation_id})
        - L79 expr (yield emit('error', stage='corrections_preview', detail=str(exc), template_id=template_id))
        - L80 return None
      - L81 except Exception as exc:
        - L82 expr logger.exception('corrections_preview_unexpected', extra={'event': 'corrections_preview_unexpected', 'template_id': template_id, 'correlation_id': correlation_id})
        - L90 expr (yield emit('error', stage='corrections_preview', detail=str(exc), template_id=template_id))
        - L91 return None
    - L93 assign artifacts_raw = result.get('artifacts') or {}
    - L94 annotated assign artifacts: dict[str, str] = {}
    - L95 for (name, value) in artifacts_raw.items():
      - L96 annotated assign resolved: Optional[Path]
      - L97 if isinstance(value, Path):
        - L98 assign resolved = value
        - L100 else:
          - L100 try:
            - L101 assign resolved = Path(value)
            - L102 except Exception:
              - L103 assign resolved = None
      - L104 assign url = artifact_url(resolved)
      - L105 if url:
        - L106 assign artifacts[str(name)] = url
    - L108 assign template_html_url = artifacts.get('template_html')
    - L109 assign page_summary_url = artifacts.get('page_summary')
    - L110 if template_html_url or page_summary_url:
      - L111 assign existing_tpl = state_store.get_template_record(template_id) or {}
      - L112 annotated assign artifacts_for_state: dict[str, str] = {}
      - L113 if template_html_url:
        - L114 assign artifacts_for_state['template_html_url'] = template_html_url
      - L115 if page_summary_url:
        - L116 assign artifacts_for_state['page_summary_url'] = page_summary_url
      - L117 if artifacts_for_state:
        - L118 assign existing_status = (existing_tpl.get('status') or '').lower()
        - L119 assign next_status = existing_tpl.get('status') or 'mapping_corrections_previewed'
        - L120 if existing_status != 'approved':
          - L121 assign next_status = 'mapping_corrections_previewed'
        - L122 expr state_store.upsert_template(template_id, name=existing_tpl.get('name') or f'Template {template_id[:8]}', status=next_status, artifacts=artifacts_for_state, connection_id=existing_tpl.get('last_connection_id'), template_type=kind)
    - L131 expr (yield emit('stage', stage='corrections_preview', status='done', progress=90, template_id=template_id, correlation_id=correlation_id, cache_hit=bool(result.get('cache_hit')), prompt_version=PROMPT_VERSION_3_5))
    - L142 expr (yield emit('result', template_id=template_id, summary=result.get('summary') or {}, processed=result.get('processed') or {}, artifacts=artifacts, cache_key=result.get('cache_key'), cache_hit=bool(result.get('cache_hit')), prompt_version=PROMPT_VERSION_3_5))
    - L153 expr logger.info('corrections_preview_complete', extra={'event': 'corrections_preview_complete', 'template_id': template_id, 'elapsed_ms': int((time.time() - started) * 1000), 'correlation_id': correlation_id})
  - L163 assign headers = {'Content-Type': 'application/x-ndjson'}
  - L164 return StreamingResponse(event_stream(), headers=headers, media_type='application/x-ndjson')

## src\services\mapping\helpers.py
- L1 from __future__ import annotations
- L3 import hashlib
- L4 import json
- L5 import logging
- L6 import re
- L7 import time
- L8 import uuid
- L9 from pathlib import Path
- L10 from typing import Any, Iterable, Mapping, Optional
- L12 from fastapi import HTTPException
- L14 from backend.app.services.dataframes.sqlite_loader import get_loader
- L15 from backend.app.services.mapping.HeaderMapping import REPORT_SELECTED_VALUE
- L16 from backend.app.services.mapping.auto_fill import _compute_db_signature as _compute_db_signature_impl
- L17 from backend.app.services.utils import write_json_atomic
- L18 from src.utils.mapping_utils import load_mapping_keys, mapping_keys_path, normalize_key_tokens, write_mapping_keys
- L19 from src.utils.template_utils import artifact_url, find_reference_pdf, find_reference_png, template_dir
- L21 assign logger = logging.getLogger(__name__)
- L23 assign _TOKEN_RE = re.compile('^\\s*\\{\\{?.+?\\}?\\}\\s*$')
- L24 assign _PARAM_REF_RE = re.compile('^params\\.[A-Za-z_][\\w]*$')
- L25 assign _DIRECT_COLUMN_RE = re.compile('\n    ["`\\[]?\n    (?P<table>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    \\.\n    ["`\\[]?\n    (?P<column>[A-Za-z_][\\w]*)\n    ["`\\]]?\n    ', re.VERBOSE)
- L37 assign _REPORT_DATE_PREFIXES = {'from', 'to', 'start', 'end', 'begin', 'finish', 'through', 'thru'}
- L38 assign _REPORT_DATE_KEYWORDS = {'date', 'dt', 'day', 'period', 'range', 'time', 'timestamp', 'window', 'month', 'year'}
- L41 def http_errorstatus_code: int, code: str, message: str, details: str | None=None:
  - L42 annotated assign payload: dict[str, Any] = {'status': 'error', 'code': code, 'message': message}
  - L43 if details:
    - L44 assign payload['details'] = details
  - L45 return HTTPException(status_code=status_code, detail=payload)
- L48 def load_json_filepath: Path:
  - L49 if not path.exists():
    - L50 return None
  - L51 try:
    - L52 return json.loads(path.read_text(encoding='utf-8'))
    - L53 except Exception:
      - L54 return None
- L57 def load_mapping_step3template_dir_path: Path:
  - L58 assign mapping_path = template_dir_path / 'mapping_step3.json'
  - L59 return (load_json_file(mapping_path), mapping_path)
- L62 def sha256_pathpath: Path | None:
  - L63 if path is None or not path.exists():
    - L64 return None
  - L65 assign h = hashlib.sha256()
  - L66 with path.open('rb') as handle:
    - L67 for chunk in iter(lambda: handle.read(65536), b''):
      - L68 if not chunk:
        - L69 break
      - L70 expr h.update(chunk)
  - L71 return h.hexdigest()
- L74 def sha256_texttext: str:
  - L75 return hashlib.sha256(text.encode('utf-8')).hexdigest()
- L78 def load_schema_exttemplate_dir_path: Path:
  - L79 assign schema_path = template_dir_path / 'schema_ext.json'
  - L80 if not schema_path.exists():
    - L81 return None
  - L82 try:
    - L83 return json.loads(schema_path.read_text(encoding='utf-8'))
    - L84 except Exception:
      - L85 return None
- L88 def build_catalog_from_dbdb_path: Path:
  - L89 annotated assign catalog: list[str] = []
  - L90 try:
    - L91 assign loader = get_loader(db_path)
    - L92 for table in loader.table_names():
      - L93 assign frame = loader.frame(table)
      - L94 for col in frame.columns:
        - L95 assign col_name = str(col or '').strip()
        - L96 if col_name:
          - L97 expr catalog.append(f'{table}.{col_name}')
    - L98 except Exception as exc:
      - L99 expr logger.exception('catalog_build_failed', extra={'event': 'catalog_build_failed', 'db_path': str(db_path)}, exc_info=exc)
      - L104 return []
  - L105 return catalog
- L108 def compute_db_signaturedb_path: Path:
  - L109 try:
    - L110 return _compute_db_signature_impl(db_path)
    - L111 except Exception:
      - L112 expr logger.exception('db_signature_failed', extra={'event': 'db_signature_failed', 'db_path': str(db_path)})
      - L113 return None
- L116 def normalize_artifact_mapartifacts: Mapping[str, Any] | None:
  - L117 annotated assign normalized: dict[str, str] = {}
  - L118 if not artifacts:
    - L119 return normalized
  - L120 for (name, raw) in artifacts.items():
    - L121 annotated assign url: Optional[str] = None
    - L122 if isinstance(raw, Path):
      - L123 assign url = artifact_url(raw)
      - L124 else:
        - L124 if isinstance(raw, str):
          - L125 assign url = raw if raw.startswith('/') else artifact_url(Path(raw))
          - L127 else:
            - L127 continue
    - L128 if url:
      - L129 assign normalized[str(name)] = url
  - L130 return normalized
- L133 def token_parts_for_report_filterstoken: str:
  - L134 assign normalized = re.sub('[^a-z0-9]+', '_', str(token or '').lower())
  - L135 return [part for part in normalized.split('_') if part]
- L138 def is_report_generator_date_token_labeltoken: str:
  - L139 assign parts = token_parts_for_report_filters(token)
  - L140 if not parts:
    - L141 return False
  - L142 assign has_prefix = any((part in _REPORT_DATE_PREFIXES for part in parts))
  - L143 assign has_keyword = any((part in _REPORT_DATE_KEYWORDS for part in parts))
  - L144 if has_prefix and has_keyword:
    - L145 return True
  - L146 if parts[0] in _REPORT_DATE_KEYWORDS and any((part in _REPORT_DATE_PREFIXES for part in parts[1:])):
    - L147 return True
  - L148 if parts[-1] in _REPORT_DATE_KEYWORDS and any((part in _REPORT_DATE_PREFIXES for part in parts[:-1])):
    - L149 return True
  - L150 return False
- L153 def norm_placeholdername: str:
  - L154 if _TOKEN_RE.match(name):
    - L155 return name.strip()
  - L156 assign core = name.strip().strip('{} ')
  - L157 return '{' + core + '}'
- L160 def normalize_mapping_for_autofillmapping: dict[str, str]:
  - L161 annotated assign out: list[dict] = []
  - L162 for (k, v) in mapping.items():
    - L163 assign mapping_value = v
    - L164 if isinstance(mapping_value, str) and is_report_generator_date_token_label(k):
      - L165 assign normalized_value = mapping_value.strip()
      - L166 assign lowered = normalized_value.lower()
      - L167 if not normalized_value:
        - L168 assign mapping_value = ''
        - L169 else:
          - L169 if _PARAM_REF_RE.match(normalized_value) or lowered.startswith('to be selected'):
            - L170 assign mapping_value = REPORT_SELECTED_VALUE
            - L171 else:
              - L171 if lowered == 'input_sample':
                - L172 assign mapping_value = 'INPUT_SAMPLE'
    - L173 expr out.append({'header': k, 'placeholder': norm_placeholder(k), 'mapping': mapping_value})
  - L174 return out
- L177 def normalize_tokens_requesttokens: str | None, keys_available: list[str]:
  - L178 if not tokens:
    - L179 return list(keys_available)
  - L180 assign requested = [token.strip() for token in str(tokens).split(',') if token.strip()]
  - L181 return [token for token in requested if token in keys_available]
- L184 def build_mapping_lookupmapping_doc: list[dict[str, Any]]:
  - L185 annotated assign lookup: dict[str, str] = {}
  - L186 for entry in mapping_doc:
    - L187 if not isinstance(entry, dict):
      - L188 continue
    - L189 assign header = entry.get('header')
    - L190 assign mapping_value = entry.get('mapping')
    - L191 if isinstance(header, str) and isinstance(mapping_value, str):
      - L192 assign lookup[header] = mapping_value.strip()
  - L193 return lookup
- L196 def extract_contract_metadatacontract_data: dict[str, Any]:
  - L197 annotated assign required: dict[str, str] = {}
  - L198 annotated assign optional: dict[str, str] = {}
  - L199 annotated assign date_columns: dict[str, str] = {}
  - L200 assign filters_section = contract_data.get('filters') or {}
  - L201 if isinstance(filters_section, dict):
    - L202 assign required_map = filters_section.get('required') or {}
    - L203 assign optional_map = filters_section.get('optional') or {}
    - L204 if isinstance(required_map, dict):
      - L205 for (key, expr) in required_map.items():
        - L206 if isinstance(key, str) and isinstance(expr, str):
          - L207 assign required[key] = expr.strip()
    - L208 if isinstance(optional_map, dict):
      - L209 for (key, expr) in optional_map.items():
        - L210 if isinstance(key, str) and isinstance(expr, str):
          - L211 assign optional[key] = expr.strip()
  - L212 assign date_columns_section = contract_data.get('date_columns') or {}
  - L213 if isinstance(date_columns_section, dict):
    - L214 for (table_name, column_name) in date_columns_section.items():
      - L215 if not isinstance(table_name, str) or not isinstance(column_name, str):
        - L216 continue
      - L217 assign table_clean = table_name.strip(' "`[]').lower()
      - L218 assign column_clean = column_name.strip(' "`[]')
      - L219 if table_clean and column_clean:
        - L220 assign date_columns[table_clean] = column_clean
  - L221 return (required, optional, date_columns)
- L224 def resolve_token_bindingtoken: str, mapping_lookup: Mapping[str, str], contract_filters_required: Mapping[str, str], contract_filters_optional: Mapping[str, str]:
  - L230 assign expr = mapping_lookup.get(token, '')
  - L231 assign match = _DIRECT_COLUMN_RE.match(expr)
  - L232 if match:
    - L233 assign table_raw = match.group('table')
    - L234 assign column_raw = match.group('column')
    - L235 assign table_clean = table_raw.strip(' "`[]') if isinstance(table_raw, str) else ''
    - L236 assign column_clean = column_raw.strip(' "`[]') if isinstance(column_raw, str) else ''
    - L237 if table_clean and column_clean:
      - L238 return (table_clean, column_clean, 'mapping')
  - L239 assign filter_expr = contract_filters_required.get(token) or contract_filters_optional.get(token)
  - L240 if isinstance(filter_expr, str):
    - L241 assign match_filter = _DIRECT_COLUMN_RE.match(filter_expr)
    - L242 if match_filter:
      - L243 assign table_raw = match_filter.group('table')
      - L244 assign column_raw = match_filter.group('column')
      - L245 assign table_clean = table_raw.strip(' "`[]') if isinstance(table_raw, str) else ''
      - L246 assign column_clean = column_raw.strip(' "`[]') if isinstance(column_raw, str) else ''
      - L247 if table_clean and column_clean:
        - L248 return (table_clean, column_clean, 'contract_filter')
  - L249 return (None, None, None)
- L252 def execute_token_querycon, *, token: str, table_clean: str, column_clean: str, date_column_name: Optional[str], start_date: Optional[str], end_date: Optional[str], limit_value: int:
  - L263 assign quoted_table = f'"{table_clean}"'
  - L264 assign quoted_column = f'"{column_clean}"'
  - L265 assign base_conditions = [f'{quoted_column} IS NOT NULL', f"TRIM(CAST({quoted_column} AS TEXT)) <> ''"]
  - L266 assign conditions = list(base_conditions)
  - L267 annotated assign params: list[str] = []
  - L268 assign ident_re = re.compile('^[A-Za-z_][\\w]*$')
  - L269 if date_column_name and ident_re.match(date_column_name):
    - L270 assign quoted_date_column = f'"{date_column_name}"'
    - L271 if start_date and end_date:
      - L272 expr conditions.append(f'{quoted_date_column} BETWEEN ? AND ?')
      - L273 expr params.extend([start_date, end_date])
      - L274 else:
        - L274 if start_date:
          - L275 expr conditions.append(f'{quoted_date_column} >= ?')
          - L276 expr params.append(start_date)
          - L277 else:
            - L277 if end_date:
              - L278 expr conditions.append(f'{quoted_date_column} <= ?')
              - L279 expr params.append(end_date)
  - L281 annotated assign debug_info: dict[str, Any] = {'table': table_clean, 'column': column_clean, 'date_column': date_column_name, 'applied_date_filters': len(params) > 0, 'sql': None, 'params': None, 'fallback_used': False, 'error': None, 'row_count': 0}
  - L293 def run_querywhere_clause: str, query_params: list[str]:
    - L294 assign sql = f'SELECT DISTINCT {quoted_column} AS value FROM {quoted_table} WHERE {where_clause} ORDER BY {quoted_column} ASC LIMIT ?'
    - L298 assign params_with_limit = tuple(list(query_params) + [limit_value])
    - L299 try:
      - L300 assign rows = [str(row['value']) for row in con.execute(sql, params_with_limit) if row and row['value'] is not None]
      - L303 return (rows, None)
      - L304 except Exception as exc:
        - L305 return ([], str(exc))
  - L307 assign where_clause = ' AND '.join(conditions) if conditions else '1=1'
  - L308 assign (rows, error) = run_query(where_clause, params)
  - L309 expr debug_info.update({'sql': where_clause, 'params': params, 'row_count': len(rows)})
  - L310 if error:
    - L311 assign debug_info['error'] = error
  - L312 if not rows and params:
    - L313 assign fallback_clause = ' AND '.join(base_conditions)
    - L314 assign (fallback_rows, fallback_error) = run_query(fallback_clause, [])
    - L315 assign debug_info['fallback_used'] = True
    - L316 assign debug_info['fallback_sql'] = fallback_clause
    - L317 assign debug_info['fallback_error'] = fallback_error
    - L318 if fallback_rows:
      - L319 assign rows = fallback_rows
      - L320 assign debug_info['row_count'] = len(rows)
      - L321 assign debug_info['error'] = fallback_error
  - L322 return (rows, debug_info)
- L325 def write_debug_logtemplate_id: str, *, kind: str, event: str, payload: Mapping[str, Any]:
  - L326 try:
    - L327 assign tdir = template_dir(template_id, kind=kind, must_exist=False, create=True)
    - L328 assign debug_dir = tdir / '_debug'
    - L329 expr debug_dir.mkdir(parents=True, exist_ok=True)
    - L330 assign timestamp = time.strftime('%Y%m%d-%H%M%S')
    - L331 assign filename = debug_dir / f'{event}-{timestamp}-{uuid.uuid4().hex[:6]}.json'
    - L332 expr write_json_atomic(filename, {'event': event, 'timestamp': timestamp, 'template_id': template_id, 'template_kind': kind, **{k: v for k, v in payload.items()}}, ensure_ascii=False, indent=2, step='debug_log')
    - L345 except Exception:
      - L346 expr logger.exception('debug_log_write_failed', extra={'event': 'debug_log_write_failed', 'template_id': template_id, 'template_kind': kind})
- L352 assign __all__ = ['http_error', 'load_json_file', 'load_mapping_step3', 'sha256_path', 'sha256_text', 'load_schema_ext', 'build_catalog_from_db', 'compute_db_signature', 'normalize_artifact_map', 'normalize_mapping_for_autofill', 'normalize_tokens_request', 'build_mapping_lookup', 'extract_contract_metadata', 'resolve_token_binding', 'execute_token_query', 'write_debug_log', 'load_mapping_keys', 'mapping_keys_path', 'normalize_key_tokens', 'write_mapping_keys', 'template_dir', 'artifact_url', 'find_reference_pdf', 'find_reference_png']

## src\services\mapping\key_options.py
- L1 from __future__ import annotations
- L3 import json
- L4 import logging
- L5 from typing import Any
- L7 from fastapi import Request
- L9 from backend.app.services.connections.db_connection import verify_sqlite
- L10 from backend.app.services.dataframes import sqlite_shim as sqlite3
- L11 from backend.app.services.mapping.HeaderMapping import approval_errors
- L12 from backend.app.services.state import state_store
- L13 from src.utils.connection_utils import db_path_from_payload_or_default
- L14 from src.utils.mapping_utils import load_mapping_keys
- L15 from src.utils.template_utils import template_dir
- L16 from src.services.mapping.helpers import build_mapping_lookup, execute_token_query, extract_contract_metadata, http_error, normalize_tokens_request, resolve_token_binding, write_debug_log
- L26 assign logger = logging.getLogger(__name__)
- L29 def mapping_key_optionstemplate_id: str, request: Request, connection_id: str | None=None, tokens: str | None=None, limit: int=50, start_date: str | None=None, end_date: str | None=None, *, kind: str='pdf', debug: bool=False:
  - L41 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L42 expr logger.info('mapping_key_options_start', extra={'event': 'mapping_key_options_start', 'template_id': template_id, 'connection_id': connection_id, 'tokens': tokens, 'limit': limit, 'start_date': start_date, 'end_date': end_date, 'template_kind': kind, 'correlation_id': correlation_id})
  - L57 def _resolve_connection_idexplicit_id: str | None:
    - L58 if explicit_id:
      - L59 assign explicit_id = str(explicit_id).strip()
      - L60 if explicit_id:
        - L61 return explicit_id
    - L62 try:
      - L63 assign record = state_store.get_template_record(template_id) or {}
      - L64 except Exception:
        - L65 assign record = {}
    - L66 assign last_conn = record.get('last_connection_id')
    - L67 if last_conn:
      - L68 return str(last_conn)
    - L69 assign last_used = state_store.get_last_used() or {}
    - L70 assign fallback_conn = last_used.get('connection_id')
    - L71 return str(fallback_conn) if fallback_conn else None
  - L73 assign effective_connection_id = _resolve_connection_id(connection_id)
  - L75 assign template_dir_path = template_dir(template_id, kind=kind)
  - L76 assign keys_available = load_mapping_keys(template_dir_path)
  - L77 if not keys_available:
    - L78 return {'keys': {}}
  - L80 assign token_list = normalize_tokens_request(tokens, keys_available)
  - L81 if not token_list:
    - L82 return {'keys': {}}
  - L84 try:
    - L85 assign limit_value = int(limit)
    - L86 except (TypeError, ValueError):
      - L87 assign limit_value = 50
  - L88 assign limit_value = max(1, min(limit_value, 500))
  - L90 assign mapping_path = template_dir_path / 'mapping_pdf_labels.json'
  - L91 if not mapping_path.exists():
    - L92 raise http_error(404, 'mapping_not_found', 'Approved mapping not found for template.')
  - L93 try:
    - L94 assign mapping_doc = json.loads(mapping_path.read_text(encoding='utf-8'))
    - L95 except Exception as exc:
      - L96 raise http_error(500, 'mapping_load_failed', f'Failed to read mapping file: {exc}')
  - L98 if not isinstance(mapping_doc, list):
    - L99 raise http_error(500, 'mapping_invalid', 'Approved mapping is not in the expected format.')
  - L100 assign mapping_lookup = build_mapping_lookup(mapping_doc)
  - L102 annotated assign contract_filters_required: dict[str, str] = {}
  - L103 annotated assign contract_filters_optional: dict[str, str] = {}
  - L104 annotated assign contract_date_columns: dict[str, str] = {}
  - L105 annotated assign contract_join: dict[str, Any] = {}
  - L106 assign contract_path = template_dir_path / 'contract.json'
  - L107 if contract_path.exists():
    - L108 try:
      - L109 assign contract_data = json.loads(contract_path.read_text(encoding='utf-8'))
      - L110 except Exception:
        - L111 assign contract_data = {}
    - L112 assign (contract_filters_required, contract_filters_optional, contract_date_columns) = extract_contract_metadata(contract_data)
    - L117 if isinstance(contract_data, dict):
      - L118 assign join_section = contract_data.get('join')
      - L119 if isinstance(join_section, dict):
        - L120 assign contract_join = join_section
  - L122 assign db_path = db_path_from_payload_or_default(effective_connection_id)
  - L123 expr verify_sqlite(db_path)
  - L125 annotated assign options: dict[str, list[str]] = {}
  - L126 annotated assign debug_payload: dict[str, Any] = {'template_id': template_id, 'connection_id': effective_connection_id, 'db_path': str(db_path), 'tokens_available': keys_available, 'token_details': {}}
  - L136 assign fallback_db_path = template_dir_path / 'runtime_machine_keys.db'
  - L138 with sqlite3.connect(str(db_path)) as con:
    - L139 assign con.row_factory = sqlite3.Row
    - L140 for token in token_list:
      - L141 assign (table_clean, column_clean, binding_source) = resolve_token_binding(token, mapping_lookup, contract_filters_required, contract_filters_optional)
      - L147 if not table_clean or not column_clean:
        - L148 assign options[token] = []
        - L149 continue
      - L150 assign date_column_name = contract_date_columns.get(table_clean.lower())
      - L152 def _schema_machine_columnsconnection:
        - L153 assign parent_table = str(contract_join.get('parent_table') or '').strip() or 'neuract__RUNHOURS'
        - L154 try:
          - L155 assign safe_table = parent_table.replace("'", "''")
          - L156 assign pragma_rows = list(connection.execute(f"PRAGMA table_info('{safe_table}')"))
          - L159 assign columns = [row[1] for row in pragma_rows if len(row) > 1]
          - L160 except Exception as exc:
            - L161 return ([], {'error': str(exc), 'table': parent_table})
        - L163 assign filtered = [col for col in columns if 'hrs' in str(col or '').lower()]
        - L164 expr filtered.sort()
        - L165 assign limited = filtered[:limit_value]
        - L166 return (limited, {'table': parent_table, 'column_source': 'schema_columns', 'row_count': len(limited)})
      - L173 if token == 'machine_name':
        - L174 assign (rows, token_debug) = _schema_machine_columns(con)
        - L175 if binding_source:
          - L176 assign token_debug['binding_source'] = binding_source
        - L177 assign options[token] = rows
        - L178 assign debug_payload['token_details'][token] = token_debug
        - L179 continue
      - L181 def _run_queryconnection, *, mark_fallback: bool=False:
        - L182 assign (rows_inner, debug_inner) = execute_token_query(connection, token=token, table_clean=table_clean, column_clean=column_clean, date_column_name=date_column_name, start_date=start_date, end_date=end_date, limit_value=limit_value)
        - L192 if mark_fallback:
          - L193 assign debug_inner['fallback_db'] = str(fallback_db_path)
        - L194 return (rows_inner, debug_inner)
      - L196 assign (rows, token_debug) = _run_query(con)
      - L198 def _fallback_schema_columnscon_ref:
        - L199 assign fallback_table = str(contract_join.get('parent_table') or '').strip()
        - L200 if not fallback_table:
          - L201 assign fallback_table = 'neuract__RUNHOURS'
        - L202 try:
          - L203 assign safe_fallback_table = fallback_table.replace("'", "''")
          - L204 assign pragma_rows = list(con_ref.execute(f"PRAGMA table_info('{safe_fallback_table}')"))
          - L207 assign columns = [row[1] for row in pragma_rows if len(row) > 1]
          - L208 except Exception as exc:
            - L209 return ([], {'fallback_error': str(exc), 'fallback_table': fallback_table})
        - L211 assign filtered = [col for col in columns if 'hrs' in str(col or '').lower()]
        - L212 expr filtered.sort()
        - L213 return (filtered[:limit_value], {'fallback_table': fallback_table})
      - L215 assign needs_fallback = not rows and fallback_db_path.exists() and isinstance(token_debug.get('error'), str) and ('no such table' in token_debug['error'].lower())
      - L221 if needs_fallback:
        - L222 with sqlite3.connect(str(fallback_db_path)) as fallback_con:
          - L223 assign fallback_con.row_factory = sqlite3.Row
          - L224 assign (rows, token_debug) = _run_query(fallback_con, mark_fallback=True)
        - L225 else:
          - L225 if not rows and isinstance(token_debug.get('error'), str):
            - L226 assign err_text = token_debug.get('error', '').lower()
            - L227 if 'no such table' in err_text or 'no such column' in err_text:
              - L228 assign (fallback_rows, fallback_meta) = _fallback_schema_columns(con)
              - L229 if fallback_rows:
                - L230 assign rows = fallback_rows
                - L231 assign token_debug['fallback_used'] = True
                - L232 assign token_debug['fallback_source'] = 'schema_columns'
                - L233 assign token_debug['row_count'] = len(rows)
              - L234 expr token_debug.update(fallback_meta)
      - L236 if binding_source:
        - L237 assign token_debug['binding_source'] = binding_source
      - L238 assign options[token] = rows
      - L239 if token_debug.get('error'):
        - L240 expr logger.warning('mapping_key_query_failed', extra={'event': 'mapping_key_query_failed', 'template_id': template_id, 'token': token, 'table': table_clean, 'column': column_clean, 'db_path': str(db_path), 'error': token_debug['error'], 'correlation_id': correlation_id})
      - L253 assign debug_payload['token_details'][token] = token_debug
  - L255 expr logger.info('mapping_key_options_complete', extra={'event': 'mapping_key_options_complete', 'template_id': template_id, 'tokens': token_list, 'template_kind': kind, 'correlation_id': correlation_id})
  - L265 annotated assign response: dict[str, Any] = {'keys': options}
  - L266 expr write_debug_log(template_id, kind=kind, event='mapping_key_options', payload=debug_payload)
  - L267 if debug:
    - L268 assign response['debug'] = debug_payload
  - L269 return response

## src\services\mapping\preview.py
- L1 from __future__ import annotations
- L3 import hashlib
- L4 import importlib
- L5 import json
- L6 import logging
- L7 from typing import Any, Iterator, Optional
- L9 from fastapi import HTTPException, Request
- L11 from backend.app.services.connections.db_connection import verify_sqlite
- L12 from backend.app.services.mapping.AutoMapInline import MappingInlineValidationError, run_llm_call_3
- L13 from backend.app.services.mapping.CorrectionsPreview import run_corrections_preview as corrections_preview_fn
- L14 from backend.app.services.mapping.HeaderMapping import approval_errors, get_parent_child_info
- L15 from backend.app.services.prompts.llm_prompts import PROMPT_VERSION
- L16 from backend.app.services.state import store as state_store_module
- L17 from backend.app.services.utils import TemplateLockError, acquire_template_lock, write_artifact_manifest, write_json_atomic, write_text_atomic
- L18 from src.utils.connection_utils import db_path_from_payload_or_default
- L19 from src.utils.mapping_utils import load_mapping_keys, mapping_keys_path
- L20 from src.utils.template_utils import artifact_url, find_reference_pdf, find_reference_png, manifest_endpoint, template_dir
- L22 from .helpers import build_catalog_from_db, compute_db_signature, http_error, load_mapping_step3, load_schema_ext, sha256_path, sha256_text
- L32 assign logger = logging.getLogger(__name__)
- L35 def _mapping_preview_pipelinetemplate_id: str, connection_id: Optional[str], request: Optional[Request], *, correlation_id: Optional[str]=None, force_refresh: bool=False, kind: str='pdf':
  - L44 try:
    - L45 assign api_mod = importlib.import_module('backend.api')
    - L46 except Exception:
      - L47 assign api_mod = None
  - L48 assign verify_sqlite_fn = getattr(api_mod, 'verify_sqlite', verify_sqlite)
  - L49 assign run_llm_call_3_fn = getattr(api_mod, 'run_llm_call_3', run_llm_call_3)
  - L50 assign build_catalog_fn = getattr(api_mod, '_build_catalog_from_db', build_catalog_from_db)
  - L51 assign get_parent_child_info_fn = getattr(api_mod, 'get_parent_child_info', get_parent_child_info)
  - L52 assign state_store_ref = getattr(api_mod, 'state_store', state_store_module.state_store)
  - L54 assign correlation_id = correlation_id or (getattr(request.state, 'correlation_id', None) if request else None)
  - L55 expr (yield {'event': 'stage', 'stage': 'mapping_preview', 'status': 'start', 'template_id': template_id, 'correlation_id': correlation_id, 'prompt_version': PROMPT_VERSION})
  - L64 assign template_dir_path = template_dir(template_id, kind=kind)
  - L65 assign mapping_keys_file = mapping_keys_path(template_dir_path)
  - L66 assign html_path = template_dir_path / 'template_p1.html'
  - L67 if not html_path.exists():
    - L68 raise http_error(404, 'template_not_ready', 'Run /templates/verify first')
  - L69 assign template_html = html_path.read_text(encoding='utf-8', errors='ignore')
  - L71 assign schema_ext = load_schema_ext(template_dir_path) or {}
  - L72 assign db_path = db_path_from_payload_or_default(connection_id)
  - L73 expr verify_sqlite_fn(db_path)
  - L75 try:
    - L76 assign schema_info = get_parent_child_info_fn(db_path)
    - L77 except Exception as exc:
      - L78 expr logger.exception('mapping_preview_schema_probe_failed', extra={'event': 'mapping_preview_schema_probe_failed', 'template_id': template_id})
      - L82 raise http_error(500, 'db_introspection_failed', f'DB introspection failed: {exc}')
  - L84 assign catalog = list(dict.fromkeys(build_catalog_fn(db_path)))
  - L85 assign pdf_sha = sha256_path(find_reference_pdf(template_dir_path)) or ''
  - L86 assign png_path = find_reference_png(template_dir_path)
  - L87 assign db_sig = compute_db_signature(db_path) or ''
  - L88 assign html_pre_sha = sha256_text(template_html)
  - L89 assign catalog_sha = hashlib.sha256(json.dumps(catalog, sort_keys=True).encode('utf-8')).hexdigest()
  - L90 assign schema_sha = hashlib.sha256(json.dumps(schema_ext, sort_keys=True).encode('utf-8')).hexdigest() if schema_ext else ''
  - L91 assign saved_keys = load_mapping_keys(template_dir_path)
  - L93 assign cache_payload = {'pdf_sha': pdf_sha, 'db_signature': db_sig, 'html_sha': html_pre_sha, 'prompt_version': PROMPT_VERSION, 'catalog_sha': catalog_sha, 'schema_sha': schema_sha}
  - L101 assign cache_key = hashlib.sha256(json.dumps(cache_payload, sort_keys=True).encode('utf-8')).hexdigest()
  - L103 assign (cached_doc, mapping_path) = load_mapping_step3(template_dir_path)
  - L104 assign constants_path = template_dir_path / 'constant_replacements.json'
  - L105 if not force_refresh and cached_doc:
    - L106 assign prompt_meta = cached_doc.get('prompt_meta') or {}
    - L107 assign post_sha = prompt_meta.get('post_html_sha256')
    - L108 assign pre_sha_cached = prompt_meta.get('pre_html_sha256')
    - L109 assign cache_key_stored = prompt_meta.get('cache_key')
    - L110 assign html_matches_pre = pre_sha_cached == html_pre_sha
    - L111 assign html_matches_post = bool(post_sha and post_sha == html_pre_sha)
    - L112 assign cache_key_matches = cache_key_stored == cache_key
    - L113 assign cache_match = cache_key_matches and (html_matches_pre or html_matches_post) or (html_matches_post and cache_key_stored and (not cache_key_matches))
    - L116 if cache_match:
      - L117 assign effective_cache_key = cache_key if cache_key_matches else cache_key_stored or cache_key
      - L118 assign mapping = cached_doc.get('mapping') or {}
      - L119 assign constant_replacements = cached_doc.get('constant_replacements') or {}
      - L120 if not constant_replacements and isinstance(cached_doc.get('raw_payload'), dict):
        - L121 assign constant_replacements = cached_doc['raw_payload'].get('constant_replacements') or {}
      - L122 assign errors = approval_errors(mapping)
      - L123 assign cached_prompt_version = prompt_meta.get('prompt_version') or PROMPT_VERSION
      - L124 expr (yield {'event': 'stage', 'stage': 'mapping_preview', 'status': 'cached', 'template_id': template_id, 'cache_key': effective_cache_key, 'correlation_id': correlation_id, 'prompt_version': cached_prompt_version})
      - L133 return {'mapping': mapping, 'errors': errors, 'schema_info': schema_info, 'catalog': catalog, 'cache_key': effective_cache_key, 'cached': True, 'constant_replacements': constant_replacements, 'constant_replacements_count': len(constant_replacements), 'prompt_version': cached_prompt_version, 'keys': saved_keys}
  - L146 try:
    - L147 assign lock_ctx = acquire_template_lock(template_dir_path, 'mapping_preview', correlation_id)
    - L148 except TemplateLockError:
      - L149 raise http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L151 with lock_ctx:
    - L152 try:
      - L153 assign result = run_llm_call_3_fn(template_html, catalog, schema_ext, PROMPT_VERSION, str(png_path) if png_path else '', cache_key)
      - L161 except MappingInlineValidationError as exc:
        - L162 raise http_error(422, 'mapping_llm_invalid', str(exc))
      - L163 except Exception as exc:
        - L164 expr logger.exception('mapping_preview_llm_failed', extra={'event': 'mapping_preview_llm_failed', 'template_id': template_id})
        - L168 raise http_error(500, 'mapping_llm_failed', str(exc))
    - L170 assign html_applied = result.html_constants_applied
    - L171 expr write_text_atomic(html_path, html_applied, encoding='utf-8', step='mapping_preview_html')
    - L172 assign html_post_sha = sha256_text(html_applied)
    - L174 assign mapping_doc = {'mapping': result.mapping, 'meta': result.meta, 'prompt_meta': {**(result.prompt_meta or {}), 'cache_key': cache_key, 'pre_html_sha256': html_pre_sha, 'post_html_sha256': html_post_sha, 'prompt_version': PROMPT_VERSION, 'catalog_sha256': cache_payload.get('catalog_sha'), 'schema_sha256': cache_payload.get('schema_sha'), 'pdf_sha256': pdf_sha, 'db_signature': db_sig}, 'raw_payload': result.raw_payload, 'constant_replacements': result.constant_replacements, 'token_samples': result.token_samples}
    - L192 expr write_json_atomic(mapping_path, mapping_doc, ensure_ascii=False, indent=2, step='mapping_preview_mapping')
    - L193 expr write_json_atomic(constants_path, result.constant_replacements, ensure_ascii=False, indent=2, step='mapping_preview_constants')
    - L200 assign files_payload = {html_path.name: html_path, mapping_path.name: mapping_path, constants_path.name: constants_path}
    - L201 if mapping_keys_file.exists():
      - L202 assign files_payload[mapping_keys_file.name] = mapping_keys_file
    - L203 expr write_artifact_manifest(template_dir_path, step='mapping_inline_llm_call_3', files=files_payload, inputs=[f'cache_key={cache_key}', f"catalog_sha256={cache_payload.get('catalog_sha')}", f"schema_sha256={cache_payload.get('schema_sha')}", f'html_pre_sha256={html_pre_sha}', f'html_post_sha256={html_post_sha}'], correlation_id=correlation_id)
  - L217 assign errors = approval_errors(result.mapping)
  - L218 assign constant_replacements = result.constant_replacements
  - L220 assign record = state_store_ref.get_template_record(template_id) or {}
  - L221 assign template_name = record.get('name') or f'Template {template_id[:8]}'
  - L222 assign artifacts = {'template_html_url': artifact_url(html_path), 'mapping_step3_url': artifact_url(mapping_path)}
  - L226 assign constants_url = artifact_url(constants_path)
  - L227 if constants_url:
    - L228 assign artifacts['constants_inlined_url'] = constants_url
  - L229 if mapping_keys_file.exists():
    - L230 assign artifacts['mapping_keys_url'] = artifact_url(mapping_keys_file)
  - L231 assign schema_path = template_dir_path / 'schema_ext.json'
  - L232 assign schema_url = artifact_url(schema_path) if schema_path.exists() else None
  - L233 if schema_url:
    - L234 assign artifacts['schema_ext_url'] = schema_url
  - L235 expr state_store_ref.upsert_template(template_id, name=template_name, status='mapping_previewed', artifacts={k: v for k, v in artifacts.items() if v}, connection_id=connection_id or record.get('last_connection_id'), mapping_keys=saved_keys, template_type=kind)
  - L245 expr (yield {'event': 'stage', 'stage': 'mapping_preview', 'status': 'ok', 'template_id': template_id, 'cache_key': cache_key, 'correlation_id': correlation_id, 'prompt_version': PROMPT_VERSION})
  - L255 return {'mapping': result.mapping, 'errors': errors, 'schema_info': schema_info, 'catalog': catalog, 'cache_key': cache_key, 'cached': False, 'constant_replacements': constant_replacements, 'constant_replacements_count': len(constant_replacements), 'prompt_version': PROMPT_VERSION, 'keys': saved_keys}
- L269 async def run_mapping_previewtemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False, *, kind: str='pdf':
  - L277 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L278 expr logger.info('mapping_preview_start', extra={'event': 'mapping_preview_start', 'template_id': template_id, 'connection_id': connection_id, 'force_refresh': force_refresh, 'template_kind': kind, 'correlation_id': correlation_id})
  - L289 assign pipeline = _mapping_preview_pipeline(template_id, connection_id, request, correlation_id=correlation_id, force_refresh=force_refresh, kind=kind)
  - L297 try:
    - L298 while True:
      - L299 expr next(pipeline)
    - L300 except StopIteration as stop:
      - L301 assign payload = stop.value or {}
    - L302 except HTTPException:
      - L303 raise
    - L304 except Exception as exc:
      - L305 expr logger.exception('mapping_preview_failed', extra={'event': 'mapping_preview_failed', 'template_id': template_id, 'correlation_id': correlation_id})
      - L309 raise http_error(500, 'mapping_preview_failed', str(exc))
  - L311 expr logger.info('mapping_preview_complete', extra={'event': 'mapping_preview_complete', 'template_id': template_id, 'connection_id': connection_id, 'cache_key': payload.get('cache_key'), 'cached': payload.get('cached', False), 'template_kind': kind, 'correlation_id': correlation_id})
  - L323 return payload
- L326 def mapping_preview_internaltemplate_id: str, connection_id: str, request: Request, force_refresh: bool=False, *, kind: str='pdf':
  - L334 return asyncio.get_event_loop().run_until_complete(run_mapping_preview(template_id, connection_id, request, force_refresh, kind=kind))

## src\services\report_service.py
- L1 from __future__ import annotations
- L3 import asyncio
- L4 import concurrent.futures
- L5 import ctypes
- L6 import contextlib
- L7 import importlib
- L8 import json
- L9 import logging
- L10 import os
- L11 import re
- L12 import signal
- L13 import subprocess
- L14 import time
- L15 import threading
- L16 import uuid
- L17 from pathlib import Path
- L18 from typing import Any, Iterable, Mapping, Optional, Sequence
- L20 from fastapi import HTTPException, Request
- L22 from backend.app.core.event_bus import Event, EventBus, logging_middleware, metrics_middleware
- L23 from backend.app.domain.reports.strategies import build_notification_strategy_registry, build_render_strategy_registry
- L24 from backend.app.features.generate.schemas.reports import RunPayload
- L25 from backend.app.services.utils import TemplateLockError, acquire_template_lock, validate_contract_schema, write_artifact_manifest
- L31 from backend.app.services.utils.artifacts import load_manifest
- L32 from backend.app.services.state import state_store
- L33 from src.core.config import EXCEL_UPLOAD_ROOT, UPLOAD_ROOT
- L34 from src.utils.connection_utils import db_path_from_payload_or_default
- L35 from src.utils.email_utils import normalize_email_targets
- L36 from src.utils.schedule_utils import clean_key_values
- L38 assign logger = logging.getLogger(__name__)
- L39 assign EVENT_BUS = EventBus(middlewares=[logging_middleware(logger), metrics_middleware(logger)])
- L40 assign RENDER_STRATEGIES = build_render_strategy_registry()
- L41 assign NOTIFICATION_STRATEGIES = build_notification_strategy_registry()
- L43 annotated assign _UPLOAD_KIND_PREFIXES: dict[str, str] = {'pdf': 'uploads', 'excel': 'excel-uploads'}
- L44 assign UPLOAD_ROOT_BASE = UPLOAD_ROOT.resolve()
- L45 assign EXCEL_UPLOAD_ROOT_BASE = EXCEL_UPLOAD_ROOT.resolve()
- L47 assign _DEFAULT_JOB_WORKERS = os.cpu_count() or 4
- L48 assign _JOB_MAX_WORKERS = max(int(os.getenv('NEURA_JOB_MAX_WORKERS', str(_DEFAULT_JOB_WORKERS)) or _DEFAULT_JOB_WORKERS), 1)
- L49 assign REPORT_JOB_EXECUTOR = concurrent.futures.ThreadPoolExecutor(max_workers=_JOB_MAX_WORKERS, thread_name_prefix='nr-job')
- L53 annotated assign _JOB_TASKS: set[asyncio.Task] = set()
- L54 annotated assign _JOB_FUTURES: dict[str, concurrent.futures.Future] = {}
- L55 annotated assign _JOB_THREADS: dict[str, int] = {}
- L56 annotated assign _JOB_PROCESSES: dict[str, set[int]] = {}
- L57 assign _JOB_PROCESS_LOCK = threading.RLock()
- L58 assign _SUBPROCESS_POPEN = subprocess.Popen
- L61 def _state_store:
  - L62 try:
    - L63 assign api_mod = importlib.import_module('backend.api')
    - L64 return getattr(api_mod, 'state_store', state_store)
    - L65 except Exception:
      - L66 return state_store
- L69 def _is_job_cancelledjob_id: str | None:
  - L70 if not job_id:
    - L71 return False
  - L72 try:
    - L73 assign record = _state_store().get_job(job_id) or {}
    - L74 except Exception:
      - L75 expr logger.exception('job_status_check_failed', extra={'event': 'job_status_check_failed', 'job_id': job_id})
      - L76 return False
  - L77 assign status = str(record.get('status') or '').lower()
  - L78 return status == 'cancelled'
- L81 def _raise_if_cancelledjob_tracker: 'JobRunTracker' | None:
  - L82 if _is_job_cancelled(job_tracker.job_id if job_tracker else None):
    - L83 raise _http_error(409, 'job_cancelled', 'Job was cancelled.')
- L86 def _http_errorstatus_code: int, code: str, message: str, details: str | None=None:
  - L87 annotated assign payload: dict[str, Any] = {'status': 'error', 'code': code, 'message': message}
  - L88 if details:
    - L89 assign payload['detail'] = details
  - L90 return HTTPException(status_code=status_code, detail=payload)
- L93 def _track_background_tasktask: asyncio.Task:
  - L94 expr _JOB_TASKS.add(task)
  - L96 def _cleanupt: asyncio.Task:
    - L97 expr _JOB_TASKS.discard(t)
  - L99 expr task.add_done_callback(_cleanup)
- L102 def _track_job_futurejob_id: str, future: concurrent.futures.Future:
  - L103 if not job_id or future is None:
    - L104 return None
  - L105 assign _JOB_FUTURES[job_id] = future
  - L107 def _cleanup_: concurrent.futures.Future:
    - L108 expr _JOB_FUTURES.pop(job_id, None)
  - L110 expr future.add_done_callback(_cleanup)
- L113 def _register_job_threadjob_id: str:
  - L114 if not job_id:
    - L115 return None
  - L116 try:
    - L117 assign _JOB_THREADS[job_id] = threading.get_ident()
    - L118 except Exception:
      - L119 expr logger.exception('job_thread_register_failed', extra={'event': 'job_thread_register_failed', 'job_id': job_id})
- L122 def _clear_job_threadjob_id: str:
  - L123 if not job_id:
    - L124 return None
  - L125 expr _JOB_THREADS.pop(job_id, None)
- L128 def _register_job_processjob_id: str, pid: int:
  - L129 if not job_id or not pid:
    - L130 return None
  - L131 with _JOB_PROCESS_LOCK:
    - L132 expr _JOB_PROCESSES.setdefault(job_id, set()).add(pid)
- L135 def _clear_job_processesjob_id: str:
  - L136 if not job_id:
    - L137 return None
  - L138 with _JOB_PROCESS_LOCK:
    - L139 expr _JOB_PROCESSES.pop(job_id, None)
- L142 def _terminate_pidpid: int, *, kill_tree: bool=True:
  - L143 if not pid:
    - L144 return False
  - L145 try:
    - L146 if os.name == 'nt' and kill_tree:
      - L148 expr _SUBPROCESS_POPEN(['taskkill', '/PID', str(pid), '/T', '/F'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
      - L149 return True
    - L150 expr os.kill(pid, signal.SIGTERM)
    - L151 return True
    - L152 except Exception:
      - L153 return False
- L156 def _kill_job_processesjob_id: str, *, kill_tree: bool=True:
  - L157 if not job_id:
    - L158 return None
  - L159 with _JOB_PROCESS_LOCK:
    - L160 assign pids = list(_JOB_PROCESSES.get(job_id) or [])
  - L161 for pid in pids:
    - L162 expr _terminate_pid(pid, kill_tree=kill_tree)
  - L163 expr _clear_job_processes(job_id)
- L166 def _inject_thread_cancelthread_id: int:
  - L167 docstring: "\n    Best-effort cancellation for a running thread by injecting CancelledError...."
  - L170 if not thread_id:
    - L171 return False
  - L172 try:
    - L173 assign res = ctypes.pythonapi.PyThreadState_SetAsyncExc(ctypes.c_long(thread_id), ctypes.py_object(asyncio.CancelledError))
    - L176 if res > 1:
      - L177 expr ctypes.pythonapi.PyThreadState_SetAsyncExc(ctypes.c_long(thread_id), 0)
      - L178 return False
    - L179 return res == 1
    - L180 except Exception:
      - L181 expr logger.exception('job_force_cancel_injection_failed', extra={'event': 'job_force_cancel_injection_failed', 'thread_id': thread_id})
      - L185 return False
- L188 def force_cancel_jobjob_id: str, *, force: bool=False:
  - L189 docstring: "\n    Attempt to cancel a running or queued job. When force=True, injects a Canc..."
  - L193 if not job_id:
    - L194 return False
  - L195 assign future = _JOB_FUTURES.get(job_id)
  - L196 assign cancelled = False
  - L197 if future and (not future.done()):
    - L198 assign cancelled = future.cancel()
  - L199 if force and (not cancelled):
    - L200 assign thread_id = _JOB_THREADS.get(job_id)
    - L201 if thread_id:
      - L202 assign cancelled = _inject_thread_cancel(thread_id)
    - L203 expr _kill_job_processes(job_id, kill_tree=True)
  - L204 return cancelled
- L207 def _publish_event_safeevent: Event:
  - L208 try:
    - L209 assign loop = asyncio.get_event_loop()
    - L210 if loop.is_running():
      - L211 expr asyncio.run_coroutine_threadsafe(EVENT_BUS.publish(event), loop)
      - L213 else:
        - L213 expr loop.run_until_complete(EVENT_BUS.publish(event))
    - L214 except RuntimeError:
      - L215 try:
        - L216 expr asyncio.run(EVENT_BUS.publish(event))
        - L217 except Exception:
          - L218 expr logger.exception('event_bus_publish_failed', extra={'event': event.name, 'correlation_id': event.correlation_id})
    - L222 except Exception:
      - L223 expr logger.exception('event_bus_publish_failed', extra={'event': event.name, 'correlation_id': event.correlation_id})
- L229 assign DEFAULT_JOB_STEP_PROGRESS = {'dataLoad': 5.0, 'contractCheck': 15.0, 'renderPdf': 60.0, 'renderDocx': 75.0, 'renderXlsx': 85.0, 'finalize': 95.0, 'email': 100.0}
- L240 def _job_error_messagedetail: Any:
  - L241 if isinstance(detail, Mapping):
    - L242 assign message = detail.get('message') or detail.get('detail')
    - L243 if message:
      - L244 return str(message)
    - L245 return json.dumps(detail, ensure_ascii=False)
  - L246 return str(detail)
- L249 def _build_job_stepspayload: RunPayload, *, kind: str:
  - L250 annotated assign steps: list[dict[str, str]] = [{'name': 'dataLoad', 'label': 'Load database'}, {'name': 'contractCheck', 'label': 'Prepare contract'}, {'name': 'renderPdf', 'label': 'Render PDF'}]
  - L255 expr steps.append({'name': 'renderDocx', 'label': 'Render DOCX'})
  - L256 if kind == 'excel' or bool(payload.xlsx):
    - L257 expr steps.append({'name': 'renderXlsx', 'label': 'Render XLSX'})
  - L258 expr steps.append({'name': 'finalize', 'label': 'Finalize artifacts'})
  - L259 if normalize_email_targets(payload.email_recipients):
    - L260 expr steps.append({'name': 'email', 'label': 'Send email'})
  - L261 return steps
- L264 def _step_progress_from_stepssteps: Iterable[Mapping[str, Any]]:
  - L265 annotated assign progress: dict[str, float] = {}
  - L266 for step in steps:
    - L267 assign name = str(step.get('name') or '').strip()
    - L268 if not name:
      - L269 continue
    - L270 assign progress[name] = DEFAULT_JOB_STEP_PROGRESS.get(name, 0.0)
  - L271 return progress
- L274 class JobRunTracker:
  - L275 def __init__self, job_id: str | None, *, correlation_id: str | None=None, step_progress: Optional[Mapping[str, float]]=None:
    - L282 assign self.job_id = job_id
    - L283 assign self.correlation_id = correlation_id
    - L284 assign self.step_progress = {k: float(v) for k, v in (step_progress or {}).items()}
    - L285 assign self._step_names = set(self.step_progress.keys()) if self.step_progress else None
  - L287 def _should_trackself, name: str:
    - L288 if not name:
      - L289 return False
    - L290 if self._step_names is None:
      - L291 return True
    - L292 return name in self._step_names
  - L294 def has_stepself, name: str:
    - L295 return self._should_track(name)
  - L297 def startself:
    - L298 if not self.job_id:
      - L299 return None
    - L300 try:
      - L301 expr _state_store().record_job_start(self.job_id)
      - L302 except Exception:
        - L303 expr logger.exception('job_start_record_failed', extra={'event': 'job_start_record_failed', 'job_id': self.job_id, 'correlation_id': self.correlation_id})
  - L312 def progressself, value: float:
    - L313 if not self.job_id:
      - L314 return None
    - L315 try:
      - L316 expr _state_store().record_job_progress(self.job_id, value)
      - L317 except Exception:
        - L318 expr logger.exception('job_progress_record_failed', extra={'event': 'job_progress_record_failed', 'job_id': self.job_id, 'correlation_id': self.correlation_id})
  - L327 def _record_stepself, name: str, status: str, *, error: Optional[str]=None, progress: Optional[float]=None, label: Optional[str]=None:
    - L336 if not self.job_id or not self._should_track(name):
      - L337 return None
    - L338 try:
      - L339 expr _state_store().record_job_step(self.job_id, name, status=status, error=error, progress=progress, label=label)
      - L347 except Exception:
        - L348 expr logger.exception('job_step_record_failed', extra={'event': 'job_step_record_failed', 'job_id': self.job_id, 'step': name, 'correlation_id': self.correlation_id})
  - L358 def step_runningself, name: str, *, label: Optional[str]=None:
    - L359 expr self._record_step(name, 'running', label=label)
  - L361 def step_succeededself, name: str, *, progress: Optional[float]=None:
    - L362 assign progress_value = progress if progress is not None else self.step_progress.get(name)
    - L363 expr self._record_step(name, 'succeeded')
    - L364 if progress_value is not None:
      - L365 expr self.progress(progress_value)
  - L367 def step_failedself, name: str, error: str:
    - L368 expr self._record_step(name, 'failed', error=str(error))
  - L370 def succeedself, result: Optional[Mapping[str, Any]]:
    - L371 if not self.job_id:
      - L372 return None
    - L373 expr self.progress(100.0)
    - L374 try:
      - L375 expr _state_store().record_job_completion(self.job_id, status='succeeded', error=None, result=result)
      - L376 except Exception:
        - L377 expr logger.exception('job_completion_record_failed', extra={'event': 'job_completion_record_failed', 'job_id': self.job_id, 'correlation_id': self.correlation_id})
  - L386 def failself, error: str, *, status: str='failed':
    - L387 if not self.job_id:
      - L388 return None
    - L389 try:
      - L390 expr _state_store().record_job_completion(self.job_id, status=status, error=str(error), result=None)
      - L391 except Exception:
        - L392 expr logger.exception('job_completion_record_failed', extra={'event': 'job_completion_record_failed', 'job_id': self.job_id, 'correlation_id': self.correlation_id})
- L402 assign _TEMPLATE_ID_SAFE_RE = re.compile('^[a-z0-9][a-z0-9_-]{2,180}$')
- L405 def _normalize_template_idtemplate_id: str:
  - L406 assign raw = str(template_id or '').strip()
  - L407 assign candidate = raw.replace('\\', '/').split('/')[-1].strip()
  - L408 if not candidate or candidate in {'.', '..'}:
    - L409 raise _http_error(400, 'invalid_template_id', 'Invalid template_id format')
  - L410 assign normalized = candidate.lower()
  - L411 if _TEMPLATE_ID_SAFE_RE.fullmatch(normalized):
    - L412 return normalized
  - L413 try:
    - L414 return str(uuid.UUID(candidate))
    - L415 except (ValueError, TypeError):
      - L416 raise _http_error(400, 'invalid_template_id', 'Invalid template_id format')
- L419 def _template_dirtemplate_id: str, *, must_exist: bool=True, create: bool=False, kind: str='pdf':
  - L426 assign normalized_kind = (kind or 'pdf').lower()
  - L427 if normalized_kind not in _UPLOAD_KIND_PREFIXES:
    - L428 raise _http_error(400, 'invalid_template_kind', f'Unsupported template kind: {kind}')
  - L430 try:
    - L431 assign api_mod = importlib.import_module('backend.api')
    - L432 assign base_dir = getattr(api_mod, 'UPLOAD_ROOT_BASE' if normalized_kind == 'pdf' else 'EXCEL_UPLOAD_ROOT_BASE')
    - L433 except Exception:
      - L434 assign base_dir = UPLOAD_ROOT_BASE if normalized_kind == 'pdf' else EXCEL_UPLOAD_ROOT_BASE
  - L436 assign tid = _normalize_template_id(template_id)
  - L437 assign tdir = (base_dir / tid).resolve()
  - L438 if base_dir not in tdir.parents:
    - L439 raise _http_error(400, 'invalid_template_path', 'Invalid template_id path')
  - L440 if must_exist and (not tdir.exists()):
    - L441 raise _http_error(404, 'template_not_found', 'template_id not found')
  - L442 if create:
    - L443 expr tdir.mkdir(parents=True, exist_ok=True)
  - L444 return tdir
- L447 def _artifact_urlpath: Path | None:
  - L448 if path is None:
    - L449 return None
  - L450 assign path = Path(path)
  - L451 assign resolved = path.resolve()
  - L452 try:
    - L453 assign api_mod = importlib.import_module('backend.api')
    - L454 assign upload_root_base = getattr(api_mod, 'UPLOAD_ROOT_BASE', UPLOAD_ROOT_BASE)
    - L455 assign excel_root_base = getattr(api_mod, 'EXCEL_UPLOAD_ROOT_BASE', EXCEL_UPLOAD_ROOT_BASE)
    - L456 except Exception:
      - L457 assign upload_root_base = UPLOAD_ROOT_BASE
      - L458 assign excel_root_base = EXCEL_UPLOAD_ROOT_BASE
  - L459 annotated assign mapping: dict[Path, str] = {upload_root_base: f"/{_UPLOAD_KIND_PREFIXES['pdf']}", excel_root_base: f"/{_UPLOAD_KIND_PREFIXES['excel']}"}
  - L463 for (base, prefix) in mapping.items():
    - L464 try:
      - L465 assign relative = resolved.relative_to(base)
      - L466 except ValueError:
        - L467 continue
    - L468 assign safe = relative.as_posix()
    - L469 return f'{prefix}/{safe}'
  - L470 return None
- L473 def _manifest_endpointtemplate_id: str, kind: str='pdf':
  - L474 if (kind or 'pdf').lower() == 'excel':
    - L475 return f'/excel/{template_id}/artifacts/manifest'
  - L476 return f'/templates/{template_id}/artifacts/manifest'
- L479 assign _EXCEL_SCALE_RE = re.compile('--excel-print-scale:\\s*([0-9]*\\.?[0-9]+)')
- L482 def _extract_excel_print_scale_from_htmlhtml_path: Path:
  - L483 try:
    - L484 assign html_text = html_path.read_text(encoding='utf-8', errors='ignore')
    - L485 except Exception:
      - L486 return None
  - L487 assign match = _EXCEL_SCALE_RE.search(html_text)
  - L488 if not match:
    - L489 return None
  - L490 try:
    - L491 assign value = float(match.group(1))
    - L492 except (TypeError, ValueError):
      - L493 return None
  - L494 if value <= 0 or value > 1.0:
    - L495 return None
  - L496 return value
- L499 def _ensure_contract_filestemplate_id: str, *, kind: str='pdf':
  - L500 assign tdir = _template_dir(template_id, kind=kind)
  - L502 assign template_html_path = tdir / 'report_final.html'
  - L503 if not template_html_path.exists():
    - L504 assign template_html_path = tdir / 'template_p1.html'
  - L505 if not template_html_path.exists():
    - L506 raise _http_error(status_code=404, code='template_html_missing', message='No template HTML found (report_final.html or template_p1.html).')
  - L512 assign contract_path = tdir / 'contract.json'
  - L513 if not contract_path.exists():
    - L514 raise _http_error(status_code=400, code='contract_missing', message='Missing contract.json. Finish template approval/mapping to create a contract for generation.')
  - L519 return (template_html_path, contract_path)
- L522 def _artifact_map_from_pathsout_html: Path, out_pdf: Path, out_docx: Path | None, out_xlsx: Path | None:
  - L528 assign artifacts = {out_html.name: out_html, out_pdf.name: out_pdf}
  - L529 if out_docx:
    - L530 assign artifacts[out_docx.name] = out_docx
  - L531 if out_xlsx:
    - L532 assign artifacts[out_xlsx.name] = out_xlsx
  - L533 return artifacts
- L536 def _run_report_internalp: RunPayload, *, kind: str='pdf', correlation_id: str | None=None, job_tracker: JobRunTracker | None=None:
  - L543 def _ensure_not_cancelled:
    - L544 expr _raise_if_cancelled(job_tracker)
  - L546 assign run_started = time.time()
  - L547 expr logger.info('reports_run_start', extra={'event': 'reports_run_start', 'template_id': p.template_id, 'connection_id': p.connection_id, 'template_kind': kind, 'correlation_id': correlation_id})
  - L557 expr _ensure_not_cancelled()
  - L558 if job_tracker:
    - L559 expr job_tracker.step_running('dataLoad', label='Load database connection')
  - L560 assign db_path = db_path_from_payload_or_default(p.connection_id)
  - L561 if not db_path.exists():
    - L562 if job_tracker:
      - L563 expr job_tracker.step_failed('dataLoad', f'DB not found: {db_path}')
    - L564 raise _http_error(400, 'db_not_found', f'DB not found: {db_path}')
  - L565 if job_tracker:
    - L566 expr job_tracker.step_succeeded('dataLoad')
  - L568 expr _ensure_not_cancelled()
  - L570 if job_tracker:
    - L571 expr job_tracker.step_running('contractCheck', label='Prepare contract')
  - L572 try:
    - L573 assign (template_html_path, contract_path) = _ensure_contract_files(p.template_id, kind=kind)
    - L574 except HTTPException as exc:
      - L575 if job_tracker:
        - L576 expr job_tracker.step_failed('contractCheck', _job_error_message(exc.detail))
      - L577 raise
  - L578 assign tdir = template_html_path.parent
  - L580 try:
    - L581 assign contract_data = json.loads(contract_path.read_text(encoding='utf-8'))
    - L582 except Exception as exc:
      - L583 if job_tracker:
        - L584 expr job_tracker.step_failed('contractCheck', f'Invalid contract.json: {exc}')
      - L585 raise _http_error(500, 'invalid_contract', f'Invalid contract.json: {exc}')
    - L587 else:
      - L587 try:
        - L588 assign api_mod = importlib.import_module('backend.api')
        - L589 assign validate_fn = getattr(api_mod, 'validate_contract_schema', validate_contract_schema)
        - L590 except Exception:
          - L591 assign validate_fn = validate_contract_schema
      - L592 try:
        - L593 expr validate_fn(contract_data)
        - L594 except Exception as exc:
          - L595 if job_tracker:
            - L596 expr job_tracker.step_failed('contractCheck', str(exc))
          - L597 raise _http_error(500, 'invalid_contract', str(exc))
  - L598 if job_tracker:
    - L599 expr job_tracker.step_succeeded('contractCheck')
  - L601 assign key_values_payload = clean_key_values(p.key_values)
  - L603 assign docx_requested = bool(p.docx)
  - L604 assign xlsx_requested = bool(p.xlsx)
  - L605 assign docx_landscape = kind == 'excel'
  - L606 assign docx_enabled = docx_requested or docx_landscape or kind == 'pdf'
  - L607 assign xlsx_enabled = xlsx_requested or kind == 'excel'
  - L608 assign render_strategy = RENDER_STRATEGIES.resolve('excel' if docx_landscape or xlsx_enabled else 'pdf')
  - L609 expr _ensure_not_cancelled()
  - L611 assign ts = str(int(time.time()))
  - L612 assign out_html = tdir / f'filled_{ts}.html'
  - L613 assign out_pdf = tdir / f'filled_{ts}.pdf'
  - L614 assign out_docx = tdir / f'filled_{ts}.docx' if docx_enabled else None
  - L615 assign out_xlsx = tdir / f'filled_{ts}.xlsx' if xlsx_enabled else None
  - L616 assign tmp_html = out_html.with_name(out_html.name + '.tmp')
  - L617 assign tmp_pdf = out_pdf.with_name(out_pdf.name + '.tmp')
  - L618 assign tmp_docx = out_docx.with_name(out_docx.name + '.tmp') if out_docx else None
  - L619 assign tmp_xlsx = out_xlsx.with_name(out_xlsx.name + '.tmp') if out_xlsx else None
  - L620 annotated assign docx_path: Path | None = None
  - L621 annotated assign docx_font_scale: float | None = None
  - L622 annotated assign xlsx_path: Path | None = None
  - L624 try:
    - L625 assign lock_ctx = acquire_template_lock(tdir, 'reports_run', correlation_id)
    - L626 except TemplateLockError:
      - L627 raise _http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L629 with lock_ctx:
    - L630 try:
      - L631 expr _ensure_not_cancelled()
      - L632 if job_tracker:
        - L633 expr job_tracker.step_running('renderPdf', label='Render PDF artifacts')
      - L634 if kind == 'excel':
        - L635 from backend.app.services.reports import ReportGenerateExcel as report_generate_module
        - L637 else:
          - L637 from backend.app.services.reports import ReportGenerate as report_generate_module
      - L639 assign fill_and_print = report_generate_module.fill_and_print
      - L641 expr fill_and_print(OBJ=contract_data, TEMPLATE_PATH=template_html_path, DB_PATH=db_path, OUT_HTML=tmp_html, OUT_PDF=tmp_pdf, START_DATE=p.start_date, END_DATE=p.end_date, batch_ids=p.batch_ids, KEY_VALUES=key_values_payload)
      - L652 expr _ensure_not_cancelled()
      - L653 if tmp_html.exists():
        - L654 expr tmp_html.replace(out_html)
      - L655 if tmp_pdf.exists():
        - L656 expr tmp_pdf.replace(out_pdf)
      - L657 assign docx_step_tracked = bool(job_tracker and job_tracker.has_step('renderDocx'))
      - L658 if docx_enabled and out_docx and tmp_docx:
        - L659 expr _ensure_not_cancelled()
        - L660 if docx_step_tracked:
          - L661 expr job_tracker.step_running('renderDocx', label='Render DOCX')
        - L662 annotated assign docx_tmp_result: Path | None = None
        - L663 annotated assign docx_error: str | None = None
        - L664 try:
          - L665 if docx_landscape:
            - L666 assign docx_font_scale = _extract_excel_print_scale_from_html(out_html) or docx_font_scale
          - L667 assign docx_tmp_result = render_strategy.render_docx(out_html, out_pdf if out_pdf and Path(out_pdf).exists() else None, tmp_docx, landscape=docx_landscape, font_scale=docx_font_scale or (0.82 if docx_landscape else None))
          - L674 except Exception as exc:
            - L675 with contextlib.suppress(FileNotFoundError):
              - L676 expr tmp_docx.unlink(missing_ok=True)
            - L677 assign docx_error = f'DOCX export failed: {exc}'
            - L678 expr logger.exception('docx_export_failed', extra={'event': 'docx_export_failed', 'template_id': p.template_id, 'template_kind': kind, 'correlation_id': correlation_id})
          - L688 else:
            - L688 if docx_tmp_result:
              - L689 assign docx_tmp_path = Path(docx_tmp_result)
              - L690 if docx_tmp_path != out_docx:
                - L691 expr docx_tmp_path.replace(out_docx)
              - L692 assign docx_path = out_docx
              - L694 else:
                - L694 with contextlib.suppress(FileNotFoundError):
                  - L695 expr tmp_docx.unlink(missing_ok=True)
        - L696 if docx_step_tracked:
          - L697 if docx_error:
            - L698 expr job_tracker.step_failed('renderDocx', docx_error)
            - L700 else:
              - L700 expr job_tracker.step_succeeded('renderDocx')
        - L701 if docx_path and (not docx_error):
          - L702 expr _publish_event_safe(Event(name='render.completed', payload={'template_id': p.template_id, 'kind': 'docx'}, correlation_id=correlation_id))
      - L709 assign xlsx_step_tracked = bool(job_tracker and job_tracker.has_step('renderXlsx'))
      - L710 if xlsx_enabled and out_xlsx and tmp_xlsx:
        - L711 expr _ensure_not_cancelled()
        - L712 if xlsx_step_tracked:
          - L713 expr job_tracker.step_running('renderXlsx', label='Render XLSX')
        - L714 annotated assign xlsx_error: str | None = None
        - L715 try:
          - L716 assign xlsx_tmp_result = render_strategy.render_xlsx(out_html, tmp_xlsx)
          - L717 except Exception as exc:
            - L718 with contextlib.suppress(FileNotFoundError):
              - L719 expr tmp_xlsx.unlink(missing_ok=True)
            - L720 expr logger.exception('xlsx_export_failed', extra={'event': 'xlsx_export_failed', 'template_id': p.template_id, 'template_kind': kind, 'correlation_id': correlation_id})
            - L729 assign xlsx_error = f'XLSX export failed: {exc}'
          - L731 else:
            - L731 if xlsx_tmp_result:
              - L732 assign xlsx_tmp_path = Path(xlsx_tmp_result)
              - L733 if xlsx_tmp_path != out_xlsx:
                - L734 expr xlsx_tmp_path.replace(out_xlsx)
              - L735 assign xlsx_path = out_xlsx
              - L737 else:
                - L737 with contextlib.suppress(FileNotFoundError):
                  - L738 expr tmp_xlsx.unlink(missing_ok=True)
        - L739 if xlsx_step_tracked:
          - L740 if xlsx_error:
            - L741 expr job_tracker.step_failed('renderXlsx', xlsx_error)
            - L743 else:
              - L743 expr job_tracker.step_succeeded('renderXlsx')
        - L744 if xlsx_path and (not xlsx_error):
          - L745 expr _publish_event_safe(Event(name='render.completed', payload={'template_id': p.template_id, 'kind': 'xlsx'}, correlation_id=correlation_id))
      - L752 except ImportError:
        - L753 raise _http_error(501, 'report_module_missing', 'Report generation module not found. Add .app.services.reports.ReportGenerate.fill_and_print(OBJ, TEMPLATE_PATH, DB_PATH, OUT_HTML, OUT_PDF, START_DATE, END_DATE, batch_ids=None).')
      - L762 except Exception as exc:
        - L763 with contextlib.suppress(FileNotFoundError):
          - L764 expr tmp_html.unlink(missing_ok=True)
        - L765 with contextlib.suppress(FileNotFoundError):
          - L766 expr tmp_pdf.unlink(missing_ok=True)
        - L767 if tmp_docx is not None:
          - L768 with contextlib.suppress(FileNotFoundError):
            - L769 expr tmp_docx.unlink(missing_ok=True)
        - L770 if tmp_xlsx is not None:
          - L771 with contextlib.suppress(FileNotFoundError):
            - L772 expr tmp_xlsx.unlink(missing_ok=True)
        - L773 if job_tracker:
          - L774 expr job_tracker.step_failed('renderPdf', f'Report generation failed: {exc}')
        - L775 raise _http_error(500, 'report_generation_failed', f'Report generation failed: {exc}')
  - L776 if job_tracker:
    - L777 expr job_tracker.step_succeeded('renderPdf')
  - L779 expr _ensure_not_cancelled()
  - L781 assign artifact_files = _artifact_map_from_paths(out_html, out_pdf, out_docx, out_xlsx)
  - L783 if job_tracker and job_tracker.has_step('finalize'):
    - L784 expr job_tracker.step_running('finalize', label='Finalize artifacts')
  - L785 expr write_artifact_manifest(tdir, step='reports_run', files=artifact_files, inputs=[str(contract_path), str(db_path)], correlation_id=correlation_id)
  - L792 if job_tracker and job_tracker.has_step('finalize'):
    - L793 expr job_tracker.step_succeeded('finalize')
  - L795 assign manifest_data = load_manifest(tdir) or {}
  - L796 assign manifest_url = _manifest_endpoint(p.template_id, kind=kind)
  - L797 expr _state_store().record_template_run(p.template_id, p.connection_id)
  - L798 expr _state_store().set_last_used(p.connection_id, p.template_id)
  - L800 expr logger.info('reports_run_complete', extra={'event': 'reports_run_complete', 'template_id': p.template_id, 'html': str(out_html.name), 'pdf': str(out_pdf.name), 'docx': str(out_docx.name) if docx_path and out_docx else None, 'xlsx': str(out_xlsx.name) if xlsx_path and out_xlsx else None, 'correlation_id': correlation_id, 'elapsed_ms': int((time.time() - run_started) * 1000)})
  - L814 assign run_id = str(uuid.uuid4())
  - L815 assign result = {'ok': True, 'run_id': run_id, 'template_id': p.template_id, 'start_date': p.start_date, 'end_date': p.end_date, 'html_url': _artifact_url(out_html), 'pdf_url': _artifact_url(out_pdf), 'docx_url': _artifact_url(out_docx) if docx_path and out_docx else None, 'xlsx_url': _artifact_url(out_xlsx) if xlsx_path and out_xlsx else None, 'manifest_url': manifest_url, 'manifest_produced_at': manifest_data.get('produced_at'), 'correlation_id': correlation_id}
  - L829 try:
    - L830 assign template_record = _state_store().get_template_record(p.template_id) or {}
    - L831 assign connection_record = _state_store().get_connection_record(p.connection_id) if p.connection_id else {}
    - L832 expr _state_store().record_report_run(run_id, template_id=p.template_id, template_name=template_record.get('name') or p.template_id, template_kind=kind, connection_id=p.connection_id, connection_name=(connection_record or {}).get('name'), start_date=p.start_date, end_date=p.end_date, batch_ids=p.batch_ids, key_values=key_values_payload, status='succeeded', artifacts={'html_url': result.get('html_url'), 'pdf_url': result.get('pdf_url'), 'docx_url': result.get('docx_url'), 'xlsx_url': result.get('xlsx_url'), 'manifest_url': result.get('manifest_url')}, schedule_id=p.schedule_id, schedule_name=p.schedule_name)
    - L854 except Exception:
      - L855 expr logger.exception('report_run_history_record_failed', extra={'event': 'report_run_history_record_failed', 'template_id': p.template_id, 'correlation_id': correlation_id})
  - L863 assign artifact_paths = {'html': out_html if out_html.exists() else None, 'pdf': out_pdf if out_pdf.exists() else None, 'docx': docx_path if docx_path and docx_path.exists() else None, 'xlsx': xlsx_path if xlsx_path and xlsx_path.exists() else None}
  - L869 return (result, artifact_paths)
- L872 def _maybe_send_emailp: RunPayload, artifact_paths: Mapping[str, Optional[Path]], run_result: Mapping[str, Any], *, kind: str, correlation_id: str | None, job_tracker: JobRunTracker | None=None:
  - L881 assign notification_strategy = NOTIFICATION_STRATEGIES.resolve('email')
  - L882 expr _raise_if_cancelled(job_tracker)
  - L883 assign recipients = normalize_email_targets(p.email_recipients)
  - L884 assign email_step_tracked = bool(job_tracker and job_tracker.has_step('email'))
  - L885 if not recipients:
    - L886 if email_step_tracked:
      - L887 expr job_tracker.step_succeeded('email')
    - L888 return None
  - L889 if email_step_tracked:
    - L890 expr job_tracker.step_running('email', label='Send notification email')
  - L891 annotated assign attachments: list[Path] = []
  - L892 for key in ('pdf', 'docx', 'xlsx'):
    - L893 assign path = artifact_paths.get(key)
    - L894 if isinstance(path, Path) and path.exists():
      - L895 expr attachments.append(path)
  - L896 if not attachments:
    - L897 assign fallback = artifact_paths.get('html')
    - L898 if isinstance(fallback, Path) and fallback.exists():
      - L899 expr attachments.append(fallback)
  - L900 if not attachments:
    - L901 return None
  - L902 assign template_record = _state_store().get_template_record(p.template_id) or {}
  - L903 assign template_name = template_record.get('name') or p.template_id
  - L904 assign default_subject = f'Report run for {template_name}'
  - L905 assign subject = (p.email_subject or default_subject).strip()
  - L906 if not subject:
    - L907 assign subject = default_subject
  - L908 if p.email_message:
    - L909 assign body = p.email_message.strip()
    - L911 else:
      - L911 assign artifact_lines = []
      - L912 for key in ('pdf_url', 'docx_url', 'xlsx_url', 'html_url'):
        - L913 assign url = run_result.get(key)
        - L914 if url:
          - L915 assign label = key.replace('_url', '').upper()
          - L916 expr artifact_lines.append(f'{label}: {url}')
      - L917 assign lines = [f'Template: {template_name} ({p.template_id})', f'Run kind: {kind}', f'Range: {p.start_date} -> {p.end_date}']
      - L922 if artifact_lines:
        - L923 expr lines.append('')
        - L924 expr lines.append('Artifacts:')
        - L925 expr lines.extend(artifact_lines)
      - L926 expr lines.append('')
      - L927 expr lines.append('This notification was generated automatically by NeuraReport.')
      - L928 assign body = '\n'.join(lines)
  - L930 assign success = notification_strategy.send(recipients=recipients, subject=subject, body=body, attachments=attachments)
  - L936 if email_step_tracked:
    - L937 if success:
      - L938 expr job_tracker.step_succeeded('email')
      - L940 else:
        - L940 expr job_tracker.step_failed('email', 'Email delivery failed')
  - L941 expr _publish_event_safe(Event(name='notification.sent' if success else 'notification.failed', payload={'template_id': p.template_id, 'kind': kind, 'recipients': len(recipients)}, correlation_id=correlation_id))
  - L952 expr logger.info('report_email_attempt', extra={'event': 'report_email_attempt', 'template_id': p.template_id, 'recipients': len(recipients), 'correlation_id': correlation_id, 'status': 'sent' if success else 'skipped'})
- L964 def _run_report_with_emailp: RunPayload, *, kind: str, correlation_id: str | None=None, job_tracker: JobRunTracker | None=None:
  - L971 assign (result, artifact_paths) = _run_report_internal(p, kind=kind, correlation_id=correlation_id, job_tracker=job_tracker)
  - L972 expr _maybe_send_email(p, artifact_paths, result, kind=kind, correlation_id=correlation_id, job_tracker=job_tracker)
  - L980 return result
- L983 def _run_report_job_syncjob_id: str, payload_data: Mapping[str, Any], kind: str, correlation_id: str, step_progress: Mapping[str, float]:
  - L991 if _is_job_cancelled(job_id):
    - L992 expr logger.info('report_job_skipped_cancelled', extra={'event': 'report_job_skipped_cancelled', 'job_id': job_id})
    - L993 return None
  - L994 expr _register_job_thread(job_id)
  - L995 assign tracker = JobRunTracker(job_id, correlation_id=correlation_id, step_progress=step_progress)
  - L996 expr tracker.start()
  - L997 expr _publish_event_safe(Event(name='job.started', payload={'job_id': job_id, 'kind': kind}, correlation_id=correlation_id))
  - L999 def _patch_subprocess_tracking:
    - L1000 if not job_id:
      - L1001 expr (yield)
      - L1002 return None
    - L1003 assign original = subprocess.Popen
    - L1004 def _job_popen*args, **kwargs:
      - L1005 assign proc = _SUBPROCESS_POPEN(*args, **kwargs)
      - L1006 if proc and getattr(proc, 'pid', None):
        - L1007 expr _register_job_process(job_id, proc.pid)
      - L1008 return proc
    - L1009 assign subprocess.Popen = _job_popen
    - L1010 try:
      - L1011 expr (yield)
      - L1013 finally:
        - L1013 assign subprocess.Popen = original
  - L1015 try:
    - L1016 assign api_mod = importlib.import_module('backend.api')
    - L1017 assign run_fn = getattr(api_mod, '_run_report_with_email', _run_report_with_email)
    - L1018 except Exception:
      - L1019 assign run_fn = _run_report_with_email
  - L1020 try:
    - L1021 assign run_payload = RunPayload(**payload_data)
    - L1022 except Exception as exc:
      - L1023 expr tracker.fail(f'Invalid payload: {exc}')
      - L1024 expr logger.exception('report_job_payload_invalid', extra={'event': 'report_job_payload_invalid', 'job_id': job_id, 'error': str(exc)})
      - L1028 return None
  - L1029 try:
    - L1030 with _patch_subprocess_tracking():
      - L1031 assign result = run_fn(run_payload, kind=kind, correlation_id=correlation_id, job_tracker=tracker)
    - L1032 except HTTPException as exc:
      - L1033 assign error_message = _job_error_message(exc.detail)
      - L1034 assign error_code = str(exc.detail.get('code') or '').lower() if isinstance(exc.detail, Mapping) else ''
      - L1035 assign is_cancelled = error_code == 'job_cancelled'
      - L1036 expr tracker.fail(error_message, status='cancelled' if is_cancelled else 'failed')
      - L1037 assign log_extra = {'event': 'report_job_cancelled' if is_cancelled else 'report_job_http_error', 'job_id': job_id, 'template_id': run_payload.template_id, 'correlation_id': correlation_id}
      - L1043 if is_cancelled:
        - L1044 expr logger.info('report_job_cancelled', extra=log_extra)
        - L1045 expr _publish_event_safe(Event(name='job.cancelled', payload={'job_id': job_id, 'kind': kind, 'status': 'cancelled'}, correlation_id=correlation_id))
        - L1053 else:
          - L1053 expr logger.exception('report_job_http_error', extra=log_extra)
          - L1054 expr _publish_event_safe(Event(name='job.failed', payload={'job_id': job_id, 'kind': kind, 'error': error_message}, correlation_id=correlation_id))
    - L1061 except (asyncio.CancelledError, KeyboardInterrupt, SystemExit) as exc:
      - L1062 expr tracker.fail('Job cancelled', status='cancelled')
      - L1063 expr logger.info('report_job_force_cancelled', extra={'event': 'report_job_force_cancelled', 'job_id': job_id, 'template_id': run_payload.template_id, 'correlation_id': correlation_id, 'exc': str(exc)})
      - L1073 expr _publish_event_safe(Event(name='job.cancelled', payload={'job_id': job_id, 'kind': kind, 'status': 'cancelled'}, correlation_id=correlation_id))
    - L1080 except Exception as exc:
      - L1081 expr tracker.fail(str(exc))
      - L1082 expr logger.exception('report_job_failed', extra={'event': 'report_job_failed', 'job_id': job_id, 'template_id': run_payload.template_id, 'correlation_id': correlation_id})
      - L1091 expr _publish_event_safe(Event(name='job.failed', payload={'job_id': job_id, 'kind': kind, 'error': str(exc)}, correlation_id=correlation_id))
    - L1099 else:
      - L1099 expr tracker.succeed(result)
      - L1100 expr _publish_event_safe(Event(name='job.completed', payload={'job_id': job_id, 'kind': kind, 'status': 'succeeded'}, correlation_id=correlation_id))
    - L1108 finally:
      - L1108 expr _clear_job_thread(job_id)
      - L1109 expr _clear_job_processes(job_id)
- L1112 def _schedule_report_jobjob_id: str, payload_data: Mapping[str, Any], kind: str, correlation_id: str, step_progress: Mapping[str, float]:
  - L1119 expr _publish_event_safe(Event(name='job.enqueued', payload={'job_id': job_id, 'kind': kind}, correlation_id=correlation_id))
  - L1127 async def runner:
    - L1128 try:
      - L1129 assign future = asyncio.get_running_loop().run_in_executor(REPORT_JOB_EXECUTOR, _run_report_job_sync, job_id, payload_data, kind, correlation_id, step_progress)
      - L1138 expr _track_job_future(job_id, future)
      - L1139 expr await future
      - L1140 except Exception:
        - L1141 expr logger.exception('report_job_task_failed', extra={'event': 'report_job_task_failed', 'job_id': job_id, 'correlation_id': correlation_id})
  - L1146 assign task = asyncio.create_task(runner())
  - L1147 expr _track_background_task(task)
- L1150 def _normalize_run_payloadsraw: RunPayload | Sequence[Any]:
  - L1151 docstring: "\n    Accept a single run payload or a sequence of payloads and normalize to Run..."
  - L1154 if isinstance(raw, RunPayload):
    - L1155 return [raw]
  - L1156 if isinstance(raw, Mapping) and 'runs' in raw:
    - L1157 return _normalize_run_payloads(raw.get('runs') or [])
  - L1158 if isinstance(raw, Iterable) and (not isinstance(raw, (str, bytes))):
    - L1159 annotated assign normalized: list[RunPayload] = []
    - L1160 for (idx, item) in enumerate(raw):
      - L1161 if isinstance(item, RunPayload):
        - L1162 expr normalized.append(item)
        - L1163 continue
      - L1164 if isinstance(item, Mapping):
        - L1165 try:
          - L1166 expr normalized.append(RunPayload(**item))
          - L1167 continue
          - L1168 except Exception as exc:
            - L1169 raise _http_error(400, 'invalid_payload', f'Invalid run payload at index {idx}: {exc}')
      - L1170 raise _http_error(400, 'invalid_payload', 'Payload entries must be run payload objects or mappings')
    - L1171 if not normalized:
      - L1172 raise _http_error(400, 'invalid_payload', 'At least one run payload is required')
    - L1173 return normalized
  - L1174 raise _http_error(400, 'invalid_payload', 'Payload must be a run payload or a list of run payloads')
- L1177 async def queue_report_jobp: RunPayload | Sequence[Any], request: Request, *, kind: str:
  - L1178 assign correlation_base = getattr(request.state, 'correlation_id', None) or f'job-{uuid.uuid4().hex[:10]}'
  - L1179 assign payloads = _normalize_run_payloads(p)
  - L1180 try:
    - L1181 assign api_mod = importlib.import_module('backend.api')
    - L1182 assign schedule_fn = getattr(api_mod, '_schedule_report_job', _schedule_report_job)
    - L1183 except Exception:
      - L1184 assign schedule_fn = _schedule_report_job
  - L1186 annotated assign scheduled_jobs: list[dict[str, Any]] = []
  - L1187 for (idx, payload) in enumerate(payloads):
    - L1188 assign correlation_id = correlation_base if len(payloads) == 1 else f'{correlation_base}-{idx + 1}'
    - L1189 assign steps = _build_job_steps(payload, kind=kind)
    - L1190 assign template_rec = _state_store().get_template_record(payload.template_id) or {}
    - L1191 assign job_record = _state_store().create_job(job_type='run_report', template_id=payload.template_id, connection_id=payload.connection_id, template_name=template_rec.get('name') or f'Template {payload.template_id[:8]}', template_kind=template_rec.get('kind') or kind, schedule_id=payload.schedule_id, correlation_id=correlation_id, steps=steps, meta={'start_date': payload.start_date, 'end_date': payload.end_date, 'docx': bool(payload.docx), 'xlsx': bool(payload.xlsx)})
    - L1207 assign payload_data = payload.dict()
    - L1208 assign step_progress = _step_progress_from_steps(steps)
    - L1209 expr schedule_fn(job_record['id'], payload_data, kind, correlation_id, step_progress)
    - L1210 expr logger.info('job_enqueued', extra={'event': 'job_enqueued', 'job_id': job_record['id'], 'template_id': payload.template_id, 'template_kind': kind, 'correlation_id': correlation_id})
    - L1220 expr scheduled_jobs.append({'job_id': job_record['id'], 'template_id': payload.template_id, 'correlation_id': correlation_id, 'kind': kind})
  - L1229 assign job_ids = [job['job_id'] for job in scheduled_jobs]
  - L1230 annotated assign response: dict[str, Any] = {'job_id': job_ids[0], 'job_ids': job_ids, 'jobs': scheduled_jobs, 'count': len(job_ids)}
  - L1236 if len(job_ids) == 1:
    - L1237 return {'job_id': job_ids[0]}
  - L1238 return response
- L1241 def list_report_runs*, template_id: str | None=None, connection_id: str | None=None, schedule_id: str | None=None, limit: int=50:
  - L1248 return _state_store().list_report_runs(template_id=template_id, connection_id=connection_id, schedule_id=schedule_id, limit=limit)
- L1256 def get_report_runrun_id: str:
  - L1257 return _state_store().get_report_run(run_id)
- L1260 def scheduler_runnerpayload: dict, kind: str, *, job_tracker: JobRunTracker | None=None:
  - L1261 assign run_payload = RunPayload(**payload)
  - L1262 assign correlation_id = payload.get('correlation_id') or f"sched-{payload.get('schedule_id') or uuid.uuid4()}"
  - L1263 return _run_report_with_email(run_payload, kind=kind, correlation_id=correlation_id, job_tracker=job_tracker)
- L1266 def run_reportp: RunPayload, request: Request, *, kind: str='pdf':
  - L1267 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L1268 return _run_report_with_email(p, kind=kind, correlation_id=correlation_id)

## src\services\scheduler_service.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 from typing import Any, Optional
- L6 from fastapi import HTTPException
- L8 from backend.app.services import state as state_store_module
- L9 from src.services import report_service as report_service_module
- L10 from src.schemas.report_schema import ScheduleCreatePayload, ScheduleUpdatePayload
- L11 from src.utils.email_utils import normalize_email_targets
- L12 from src.utils.schedule_utils import clean_key_values, resolve_schedule_interval, utcnow_iso
- L15 def _http_errorstatus_code: int, code: str, message: str:
  - L16 return HTTPException(status_code=status_code, detail={'status': 'error', 'code': code, 'message': message})
- L19 def _state_store:
  - L20 try:
    - L21 assign api_mod = importlib.import_module('backend.api')
    - L22 return getattr(api_mod, 'state_store', state_store_module.state_store)
    - L23 except Exception:
      - L24 return state_store_module.state_store
- L27 def _report_service:
  - L28 try:
    - L29 assign api_mod = importlib.import_module('backend.api')
    - L30 return getattr(api_mod, 'report_service', report_service_module)
    - L31 except Exception:
      - L32 return report_service_module
- L35 def list_jobsstatus: Optional[list[str]], job_type: Optional[list[str]], limit: int, active_only: bool:
  - L36 return _state_store().list_jobs(statuses=status, types=job_type, limit=limit, active_only=active_only)
- L39 def list_active_jobslimit: int:
  - L40 return _state_store().list_jobs(limit=limit, active_only=True)
- L43 def get_jobjob_id: str:
  - L44 return _state_store().get_job(job_id)
- L47 def list_schedules:
  - L48 return _state_store().list_schedules()
- L51 def get_scheduleschedule_id: str:
  - L52 docstring: "Get a specific schedule by ID."
  - L53 return _state_store().get_schedule(schedule_id)
- L56 def cancel_jobjob_id: str, *, force: bool=False:
  - L57 assign job = _state_store().cancel_job(job_id)
  - L58 if not job:
    - L59 raise _http_error(404, 'job_not_found', 'Job not found.')
  - L60 try:
    - L61 expr _report_service().force_cancel_job(job_id, force=force)
    - L62 except Exception:
      - L64 pass
  - L65 return job
- L68 def create_schedulepayload: ScheduleCreatePayload:
  - L69 assign store = _state_store()
  - L70 assign template = store.get_template_record(payload.template_id) or {}
  - L71 if not template:
    - L72 raise _http_error(404, 'template_not_found', 'Template not found.')
  - L73 if str(template.get('status')).lower() != 'approved':
    - L74 raise _http_error(400, 'template_not_ready', 'Template must be approved before scheduling runs.')
  - L75 assign connection = store.get_connection_record(payload.connection_id)
  - L76 if not connection:
    - L77 raise _http_error(404, 'connection_not_found', 'Connection not found.')
  - L78 if not payload.start_date or not payload.end_date:
    - L79 raise _http_error(400, 'invalid_schedule_range', 'Provide both start_date and end_date.')
  - L80 assign interval_minutes = resolve_schedule_interval(payload.frequency, payload.interval_minutes)
  - L81 assign now_iso = utcnow_iso()
  - L82 assign schedule = store.create_schedule(name=(payload.name or template.get('name') or f'Schedule {payload.template_id}')[:120], template_id=payload.template_id, template_name=template.get('name') or payload.template_id, template_kind=template.get('kind') or 'pdf', connection_id=payload.connection_id, connection_name=connection.get('name') or connection.get('connection_name') or payload.connection_id, start_date=payload.start_date, end_date=payload.end_date, key_values=clean_key_values(payload.key_values), batch_ids=payload.batch_ids, docx=payload.docx, xlsx=payload.xlsx, email_recipients=normalize_email_targets(payload.email_recipients or []), email_subject=payload.email_subject, email_message=payload.email_message, frequency=payload.frequency, interval_minutes=interval_minutes, next_run_at=now_iso, first_run_at=now_iso, active=payload.active)
  - L104 return schedule
- L107 def delete_scheduleschedule_id: str:
  - L108 return _state_store().delete_schedule(schedule_id)
- L111 def update_scheduleschedule_id: str, payload: ScheduleUpdatePayload:
  - L112 docstring: "Update an existing schedule with partial data."
  - L113 assign store = _state_store()
  - L114 assign existing = store.get_schedule(schedule_id)
  - L115 if not existing:
    - L116 raise _http_error(404, 'schedule_not_found', 'Schedule not found.')
  - L119 annotated assign changes: dict[str, Any] = {}
  - L120 if payload.name is not None:
    - L121 assign changes['name'] = payload.name[:120]
  - L122 if payload.start_date is not None:
    - L123 assign changes['start_date'] = payload.start_date
  - L124 if payload.end_date is not None:
    - L125 assign changes['end_date'] = payload.end_date
  - L126 if payload.key_values is not None:
    - L127 assign changes['key_values'] = clean_key_values(payload.key_values)
  - L128 if payload.batch_ids is not None:
    - L129 assign changes['batch_ids'] = payload.batch_ids
  - L130 if payload.docx is not None:
    - L131 assign changes['docx'] = payload.docx
  - L132 if payload.xlsx is not None:
    - L133 assign changes['xlsx'] = payload.xlsx
  - L134 if payload.email_recipients is not None:
    - L135 assign changes['email_recipients'] = normalize_email_targets(payload.email_recipients)
  - L136 if payload.email_subject is not None:
    - L137 assign changes['email_subject'] = payload.email_subject
  - L138 if payload.email_message is not None:
    - L139 assign changes['email_message'] = payload.email_message
  - L140 if payload.frequency is not None:
    - L141 assign interval_minutes = resolve_schedule_interval(payload.frequency, payload.interval_minutes)
    - L142 assign changes['frequency'] = payload.frequency
    - L143 assign changes['interval_minutes'] = interval_minutes
    - L144 else:
      - L144 if payload.interval_minutes is not None:
        - L146 assign changes['interval_minutes'] = payload.interval_minutes
  - L147 if payload.active is not None:
    - L148 assign changes['active'] = payload.active
  - L150 if not changes:
    - L152 return existing
  - L154 assign updated = store.update_schedule(schedule_id, **changes)
  - L155 if not updated:
    - L156 raise _http_error(404, 'schedule_not_found', 'Schedule not found after update.')
  - L157 return updated

## src\services\template_service.py
- L1 from __future__ import annotations
- L3 import contextlib
- L4 import os
- L5 import re
- L6 import shutil
- L7 import tempfile
- L8 from pathlib import Path
- L9 from typing import Any, Optional
- L11 from fastapi import HTTPException, Request, UploadFile
- L12 from fastapi.responses import FileResponse
- L13 from starlette.background import BackgroundTask
- L15 from backend.app.core.config import get_settings as get_api_settings
- L16 from backend.app.domain.templates.errors import TemplateImportError
- L17 from backend.app.domain.templates.service import TemplateService
- L18 from backend.app.services.utils import TemplateLockError, acquire_template_lock
- L19 from backend.app.services.templates.catalog import build_unified_template_catalog
- L20 from backend.app.services.utils import get_correlation_id
- L21 from backend.app.services.state import store as state_store_module
- L22 from backend.app.services.prompts.llm_prompts_templates import recommend_templates_from_catalog
- L23 from backend.app.services.utils.zip_tools import create_zip_from_dir
- L24 from backend.app.core.validation import is_safe_name
- L25 from src.services.file_service import apply_chat_template_edit as apply_chat_template_edit_service, chat_template_edit as chat_template_edit_service, edit_template_ai as edit_template_ai_service, edit_template_manual as edit_template_manual_service, generator_assets as generator_assets_service, get_template_html as get_template_html_service, undo_last_template_edit as undo_last_template_edit_service, verify_excel as verify_excel_service, verify_template as verify_template_service
- L36 from src.services.file_service.helpers import load_template_generator_summary as _load_template_generator_summary, resolve_template_kind as _resolve_template_kind, update_template_generator_summary_for_edit as _update_template_generator_summary_for_edit
- L41 from src.schemas.template_schema import GeneratorAssetsPayload, TemplateAiEditPayload, TemplateChatPayload, TemplateManualEditPayload, TemplateRecommendPayload, TemplateRecommendResponse, TemplateRecommendation, TemplateUpdatePayload
- L51 from src.utils.schedule_utils import utcnow_iso
- L52 from src.utils.template_utils import template_dir
- L53 from src.utils.mapping_utils import load_mapping_keys
- L55 annotated assign _TEMPLATE_SERVICE: TemplateService | None = None
- L58 def _get_template_service:
  - L59 Global
  - L60 if _TEMPLATE_SERVICE is None:
    - L61 assign settings = get_api_settings()
    - L62 assign _TEMPLATE_SERVICE = TemplateService(uploads_root=settings.uploads_dir, excel_uploads_root=settings.excel_uploads_dir, max_bytes=settings.max_upload_bytes, max_zip_entries=settings.max_zip_entries, max_zip_uncompressed_bytes=settings.max_zip_uncompressed_bytes, max_concurrency=settings.template_import_max_concurrency)
  - L70 return _TEMPLATE_SERVICE
- L73 def _http_errorstatus_code: int, code: str, message: str, details: str | None=None:
  - L74 assign payload = {'status': 'error', 'code': code, 'message': message}
  - L75 if details:
    - L76 assign payload['details'] = details
  - L77 return HTTPException(status_code=status_code, detail=payload)
- L80 def _state_store:
  - L81 return state_store_module.state_store
- L84 def _normalize_tagsvalues: Optional[list[str]]:
  - L85 if not values:
    - L86 return []
  - L87 annotated assign seen: set[str] = set()
  - L88 annotated assign normalized: list[str] = []
  - L89 for raw in values:
    - L90 assign text = re.sub('[^A-Za-z0-9 _-]', '', str(raw or '')).strip()
    - L91 if not text:
      - L92 continue
    - L93 assign key = text.lower()
    - L94 if key in seen:
      - L95 continue
    - L96 expr seen.add(key)
    - L97 expr normalized.append(text[:32])
    - L98 if len(normalized) >= 12:
      - L99 break
  - L100 return normalized
- L103 def export_template_ziptemplate_id: str, request: Request:
  - L104 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L105 assign kind = _resolve_template_kind(template_id)
  - L106 assign tdir = template_dir(template_id, must_exist=True, create=False, kind=kind)
  - L107 try:
    - L108 assign lock_ctx = acquire_template_lock(tdir, 'template_export', correlation_id)
    - L109 except TemplateLockError:
      - L110 raise _http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L116 assign (fd, tmp_name) = tempfile.mkstemp(prefix=f'{template_id}-', suffix='.zip')
  - L117 expr os.close(fd)
  - L118 assign zip_path = Path(tmp_name)
  - L120 def _cleanuppath: Path=zip_path:
    - L121 with contextlib.suppress(FileNotFoundError):
      - L122 expr path.unlink(missing_ok=True)
  - L124 with lock_ctx:
    - L125 expr create_zip_from_dir(tdir, zip_path, include_root=True)
  - L127 assign headers = {'X-Correlation-ID': correlation_id} if correlation_id else None
  - L128 return FileResponse(zip_path, media_type='application/zip', filename=f'{template_id}.zip', background=BackgroundTask(_cleanup), headers=headers)
- L137 async def import_template_zipfile: UploadFile, request: Request, name: str | None=None:
  - L138 if not file:
    - L139 raise _http_error(400, 'file_missing', 'No file provided for import.')
  - L141 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L142 assign service = _get_template_service()
  - L143 try:
    - L144 assign result = await service.import_zip(file, name, correlation_id)
    - L145 except TemplateImportError as exc:
      - L146 assign detail = {'status': 'error', 'code': exc.code, 'message': exc.message}
      - L147 if exc.detail:
        - L148 assign detail['detail'] = exc.detail
      - L149 if correlation_id:
        - L150 assign detail['correlation_id'] = correlation_id
      - L151 raise HTTPException(status_code=exc.status_code, detail=detail)
  - L153 assign normalized = dict(result or {})
  - L154 expr normalized.setdefault('status', 'ok')
  - L155 expr normalized.setdefault('correlation_id', correlation_id)
  - L156 return normalized
- L159 def verify_templatefile: UploadFile, connection_id: str | None, request: Request, refine_iters: int=0:
  - L160 return verify_template_service(file=file, connection_id=connection_id, request=request)
- L163 def verify_excelfile: UploadFile, request: Request, connection_id: str | None=None:
  - L164 return verify_excel_service(file=file, request=request, connection_id=connection_id)
- L167 def get_template_htmltemplate_id: str, request: Request:
  - L168 return get_template_html_service(template_id, request)
- L171 def edit_template_manualtemplate_id: str, payload: TemplateManualEditPayload, request: Request:
  - L172 return edit_template_manual_service(template_id, payload, request)
- L175 def edit_template_aitemplate_id: str, payload: TemplateAiEditPayload, request: Request:
  - L176 return edit_template_ai_service(template_id, payload, request)
- L179 def undo_last_template_edittemplate_id: str, request: Request:
  - L180 return undo_last_template_edit_service(template_id, request)
- L183 def chat_template_edittemplate_id: str, payload: TemplateChatPayload, request: Request:
  - L184 return chat_template_edit_service(template_id, payload, request)
- L187 def apply_chat_template_edittemplate_id: str, html: str, request: Request:
  - L188 return apply_chat_template_edit_service(template_id, html, request)
- L191 def generator_assetstemplate_id: str, payload: GeneratorAssetsPayload, request: Request, *, kind: str='pdf':
  - L192 return generator_assets_service(template_id, payload, request, kind=kind)
- L195 def bootstrap_staterequest: Request:
  - L196 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L197 assign templates = _state_store().list_templates()
  - L198 assign hydrated_templates = _ensure_template_mapping_keys(templates)
  - L199 return {'status': 'ok', 'connections': _state_store().list_connections(), 'templates': hydrated_templates, 'last_used': _state_store().get_last_used(), 'correlation_id': correlation_id}
- L208 def templates_catalogrequest: Request:
  - L209 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L210 assign catalog = build_unified_template_catalog()
  - L211 return {'status': 'ok', 'templates': catalog, 'correlation_id': correlation_id}
- L214 def list_templatesstatus: Optional[str], request: Request:
  - L215 assign templates = _state_store().list_templates()
  - L216 if status:
    - L217 assign status_lower = status.lower()
    - L218 assign templates = [t for t in templates if (t.get('status') or '').lower() == status_lower]
  - L219 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L220 assign hydrated = _ensure_template_mapping_keys(templates)
  - L221 return {'status': 'ok', 'templates': hydrated, 'correlation_id': correlation_id}
- L224 def recommend_templatespayload: TemplateRecommendPayload, request: Request:
  - L225 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L226 assign catalog = build_unified_template_catalog()
  - L228 def _dedupe_str_listvalues: list[str] | None:
    - L229 if not values:
      - L230 return []
    - L231 annotated assign seen: set[str] = set()
    - L232 annotated assign cleaned: list[str] = []
    - L233 for raw in values:
      - L234 assign text = str(raw or '').strip()
      - L235 if not text or text in seen:
        - L236 continue
      - L237 expr seen.add(text)
      - L238 expr cleaned.append(text)
    - L239 return cleaned
  - L241 annotated assign hints: dict[str, Any] = {}
  - L243 annotated assign kind_values: list[str] = []
  - L244 if payload.kind:
    - L245 expr kind_values.append(payload.kind)
  - L246 if getattr(payload, 'kinds', None):
    - L247 expr kind_values.extend(payload.kinds or [])
  - L248 assign kinds = _dedupe_str_list(kind_values)
  - L249 if payload.kind:
    - L250 assign kind = str(payload.kind or '').strip()
    - L251 if kind:
      - L252 assign hints['kind'] = kind
  - L253 if kinds:
    - L254 assign hints['kinds'] = kinds
  - L256 annotated assign domain_values: list[str] = []
  - L257 if payload.domain:
    - L258 expr domain_values.append(payload.domain)
  - L259 if getattr(payload, 'domains', None):
    - L260 expr domain_values.extend(payload.domains or [])
  - L261 assign domains = _dedupe_str_list(domain_values)
  - L262 if payload.domain:
    - L263 assign domain = str(payload.domain or '').strip()
    - L264 if domain:
      - L265 assign hints['domain'] = domain
  - L266 if domains:
    - L267 assign hints['domains'] = domains
  - L269 if payload.schema_snapshot is not None:
    - L270 assign hints['schema_snapshot'] = payload.schema_snapshot
  - L271 assign tables = _dedupe_str_list(payload.tables)
  - L272 if tables:
    - L273 assign hints['tables'] = tables
  - L275 assign raw_recs = recommend_templates_from_catalog(catalog, requirement=payload.requirement, hints=hints, max_results=6)
  - L282 annotated assign catalog_by_id: dict[str, dict[str, Any]] = {}
  - L283 for item in catalog:
    - L284 if isinstance(item, dict):
      - L285 assign tid = str(item.get('id') or '').strip()
      - L286 if tid and tid not in catalog_by_id:
        - L287 assign catalog_by_id[tid] = item
  - L289 annotated assign recommendations: list[TemplateRecommendation] = []
  - L290 for rec in raw_recs:
    - L291 assign tid = str(rec.get('id') or '').strip()
    - L292 if not tid:
      - L293 continue
    - L294 assign template = catalog_by_id.get(tid)
    - L295 if not template:
      - L296 continue
    - L297 assign explanation = str(rec.get('explanation') or '').strip()
    - L298 try:
      - L299 assign score = float(rec.get('score') or 0.0)
      - L300 except Exception:
        - L301 assign score = 0.0
    - L302 expr recommendations.append(TemplateRecommendation(template=template, explanation=explanation, score=score))
  - L310 return TemplateRecommendResponse(recommendations=recommendations)
- L313 def delete_templatetemplate_id: str, request: Request:
  - L314 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L315 assign existing_record = _state_store().get_template_record(template_id)
  - L316 assign template_kind = _resolve_template_kind(template_id)
  - L317 assign tdir = template_dir(template_id, must_exist=False, create=False, kind=template_kind)
  - L319 annotated assign lock_ctx: contextlib.AbstractContextManager[Any] = contextlib.nullcontext()
  - L320 if tdir.exists():
    - L321 try:
      - L322 assign lock_ctx = acquire_template_lock(tdir, 'template_delete', correlation_id)
      - L323 except TemplateLockError:
        - L324 raise _http_error(409, 'template_locked', 'Template is currently processing another request.')
  - L330 assign removed_dir = False
  - L331 with lock_ctx:
    - L332 if tdir.exists():
      - L333 try:
        - L334 expr shutil.rmtree(tdir)
        - L335 assign removed_dir = True
        - L336 except FileNotFoundError:
          - L337 assign removed_dir = False
        - L338 except Exception as exc:
          - L339 raise _http_error(500, 'template_delete_failed', f'Failed to remove template files: {exc}')
    - L345 assign removed_state = _state_store().delete_template(template_id)
  - L347 if not removed_state and (not removed_dir) and (existing_record is None):
    - L348 raise _http_error(404, 'template_not_found', 'template_id not found')
  - L350 return {'status': 'ok', 'template_id': template_id, 'correlation_id': correlation_id}
- L357 def update_template_metadatatemplate_id: str, payload: TemplateUpdatePayload, request: Request:
  - L358 assign correlation_id = getattr(request.state, 'correlation_id', None) or get_correlation_id()
  - L359 assign record = _state_store().get_template_record(template_id)
  - L360 if not record:
    - L361 raise _http_error(404, 'template_not_found', 'template_id not found')
  - L363 assign name = payload.name if payload.name is not None else record.get('name') or template_id
  - L364 if payload.name is not None and (not is_safe_name(name)):
    - L365 raise _http_error(400, 'invalid_name', 'Template name contains invalid characters')
  - L367 assign description = payload.description if payload.description is not None else record.get('description')
  - L368 if description is not None:
    - L369 assign description = str(description).strip()
    - L370 if len(description) > 280:
      - L371 raise _http_error(400, 'description_too_long', 'Description is limited to 280 characters')
  - L373 assign tags = _normalize_tags(payload.tags) if payload.tags is not None else list(record.get('tags') or [])
  - L374 assign status = payload.status if payload.status is not None else record.get('status') or 'draft'
  - L375 assign status_norm = str(status or '').strip().lower()
  - L376 if status_norm not in {'draft', 'pending', 'approved', 'archived'}:
    - L377 raise _http_error(400, 'invalid_status', 'Status must be draft, pending, approved, or archived')
  - L379 assign updated = _state_store().upsert_template(template_id, name=name, status=status_norm, description=description, artifacts=record.get('artifacts') or {}, tags=tags, connection_id=record.get('last_connection_id'), mapping_keys=record.get('mapping_keys'), template_type=record.get('kind') or 'pdf')
  - L391 return {'status': 'ok', 'template': updated, 'correlation_id': correlation_id}
- L398 def _ensure_template_mapping_keysrecords: list[dict]:
  - L399 annotated assign hydrated: list[dict] = []
  - L400 for record in records:
    - L401 assign mapping_keys = record.get('mappingKeys') or []
    - L402 if mapping_keys:
      - L403 expr hydrated.append(record)
      - L404 continue
    - L405 assign template_id = record.get('id')
    - L406 if not template_id:
      - L407 expr hydrated.append(record)
      - L408 continue
    - L410 assign kind = record.get('kind') or 'pdf'
    - L411 try:
      - L412 assign tdir = template_dir(template_id, must_exist=False, create=False, kind=kind)
      - L413 except HTTPException:
        - L414 expr hydrated.append(record)
        - L415 continue
    - L417 assign keys = load_mapping_keys(tdir)
    - L418 if not keys:
      - L419 expr hydrated.append(record)
      - L420 continue
    - L422 assign new_record = dict(record)
    - L423 assign new_record['mappingKeys'] = keys
    - L424 expr hydrated.append(new_record)
    - L426 try:
      - L427 expr _state_store().upsert_template(template_id, name=record.get('name') or f'Template {template_id[:8]}', status=record.get('status') or 'unknown', artifacts=record.get('artifacts') or {}, tags=record.get('tags') or [], connection_id=record.get('lastConnectionId'), mapping_keys=keys, template_type=kind)
      - L437 except Exception:
        - L438 pass
  - L439 return hydrated

## src\utils\__init__.py
- L1 docstring: "Shared utility functions."

## src\utils\connection_utils.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 import os
- L5 from pathlib import Path
- L6 from typing import Optional
- L8 from fastapi import HTTPException
- L10 from backend.app.services.connections.db_connection import resolve_db_path
- L11 from backend.app.services.state import store as state_store_module
- L14 def _http_errorstatus_code: int, code: str, message: str:
  - L15 return HTTPException(status_code=status_code, detail={'status': 'error', 'code': code, 'message': message})
- L18 def _state_store:
  - L19 return state_store_module.state_store
- L22 def display_name_for_pathdb_path: Path, db_type: str='sqlite':
  - L23 assign base = db_path.name
  - L24 if db_type.lower() == 'sqlite':
    - L25 return base
  - L26 return f'{db_type}:{base}'
- L29 def db_path_from_payload_or_defaultconn_id: Optional[str]:
  - L30 docstring: "\n    Resolve a database path using the same precedence as the legacy api.py hel..."
  - L33 assign resolve_db_path_fn = resolve_db_path
  - L34 try:
    - L35 assign api_mod = importlib.import_module('backend.api')
    - L36 assign override = getattr(api_mod, '_db_path_from_payload_or_default', None)
    - L37 if override and override is not db_path_from_payload_or_default:
      - L38 return override(conn_id)
    - L39 assign resolve_db_path_fn = getattr(api_mod, 'resolve_db_path', resolve_db_path)
    - L40 except Exception:
      - L41 pass
  - L43 if conn_id:
    - L44 assign secrets = _state_store().get_connection_secrets(conn_id)
    - L45 if secrets and secrets.get('database_path'):
      - L46 return Path(secrets['database_path'])
    - L47 assign record = _state_store().get_connection_record(conn_id)
    - L48 if record and record.get('database_path'):
      - L49 return Path(record['database_path'])
    - L50 try:
      - L51 return resolve_db_path_fn(connection_id=conn_id, db_url=None, db_path=None)
      - L52 except Exception:
        - L53 pass
  - L55 assign last_used = _state_store().get_last_used()
  - L56 if last_used.get('connection_id'):
    - L57 assign secrets = _state_store().get_connection_secrets(last_used['connection_id'])
    - L58 if secrets and secrets.get('database_path'):
      - L59 return Path(secrets['database_path'])
    - L60 assign record = _state_store().get_connection_record(last_used['connection_id'])
    - L61 if record and record.get('database_path'):
      - L62 return Path(record['database_path'])
  - L64 assign env_db = os.getenv('NR_DEFAULT_DB') or os.getenv('DB_PATH')
  - L65 if env_db:
    - L66 return Path(env_db)
  - L68 assign latest = _state_store().get_latest_connection()
  - L69 if latest and latest.get('database_path'):
    - L70 return Path(latest['database_path'])
  - L72 raise _http_error(400, 'db_missing', 'No database configured. Connect once or set NR_DEFAULT_DB/DB_PATH env.')

## src\utils\email_utils.py
- L1 from __future__ import annotations
- L3 import re
- L4 from typing import Iterable, Optional
- L7 def normalize_email_targetsraw: Optional[Iterable[str] | str]:
  - L8 docstring: "Normalize comma/semicolon-delimited inputs, deduplicated and trimmed."
  - L9 if raw is None:
    - L10 return []
  - L11 assign candidates = []
  - L12 if isinstance(raw, str):
    - L13 assign candidates = [piece for piece in re.split('[;,]', raw) if piece is not None]
    - L15 else:
      - L15 assign candidates = list(raw)
  - L17 annotated assign normalized: list[str] = []
  - L18 annotated assign seen: set[str] = set()
  - L19 for value in candidates:
    - L20 assign text = str(value or '').strip()
    - L21 if not text:
      - L22 continue
    - L23 assign lower = text.lower()
    - L24 if lower in seen:
      - L25 continue
    - L26 expr seen.add(lower)
    - L27 expr normalized.append(text)
  - L28 return normalized

## src\utils\health_utils.py
- L1 from __future__ import annotations
- L3 import os
- L4 import time
- L5 from pathlib import Path
- L6 from typing import Dict, Tuple
- L8 from fastapi import Request
- L9 from fastapi.responses import JSONResponse
- L11 from src.core.config import APP_COMMIT, APP_VERSION
- L14 def check_fs_writableroot: Path:
  - L15 try:
    - L16 assign marker = root / '.healthcheck'
    - L17 expr marker.parent.mkdir(parents=True, exist_ok=True)
    - L18 expr marker.write_text(str(time.time()))
    - L19 expr marker.unlink(missing_ok=True)
    - L20 return (True, 'ok')
    - L21 except Exception as exc:
      - L22 return (False, str(exc))
- L25 def check_clock:
  - L26 try:
    - L27 assign now = time.time()
    - L28 if now <= 0:
      - L29 return (False, 'invalid_time')
    - L30 return (True, 'ok')
    - L31 except Exception as exc:
      - L32 return (False, str(exc))
- L35 def check_external_headurl: str, api_key: str | None:
  - L36 import urllib.error
  - L37 import urllib.request
  - L39 assign req = urllib.request.Request(url, method='HEAD')
  - L40 if api_key:
    - L41 expr req.add_header('Authorization', f'Bearer {api_key}')
  - L42 try:
    - L43 with urllib.request.urlopen(req, timeout=10) as resp:
      - L44 assign status = getattr(resp, 'status', resp.getcode())
      - L45 assign ok = 200 <= status < 400
      - L46 return (ok, f'status={status}')
    - L47 except urllib.error.HTTPError as exc:
      - L48 if exc.code in {401, 403, 405}:
        - L49 return (True, f'status={exc.code}')
      - L50 return (False, str(exc))
    - L51 except Exception as exc:
      - L52 return (False, str(exc))
- L55 def health_responserequest: Request, checks: Dict[str, Tuple[bool, str]]:
  - L56 assign status_ok = all((ok for ok, _ in checks.values()))
  - L57 assign correlation_id = getattr(request.state, 'correlation_id', None)
  - L58 assign payload = {'status': 'ok' if status_ok else 'error', 'checks': {name: {'ok': ok, 'detail': detail} for name, (ok, detail) in checks.items()}, 'version': APP_VERSION, 'commit': APP_COMMIT, 'correlation_id': correlation_id}
  - L65 return JSONResponse(status_code=200 if status_ok else 503, content=payload)

## src\utils\mapping_utils.py
- L1 from __future__ import annotations
- L3 import json
- L4 import time
- L5 from pathlib import Path
- L6 from typing import Iterable
- L8 from backend.app.services.utils import write_json_atomic
- L10 assign _MAPPING_KEYS_FILENAME = 'mapping_keys.json'
- L13 def mapping_keys_pathtemplate_dir: Path:
  - L14 return template_dir / _MAPPING_KEYS_FILENAME
- L17 def normalize_key_tokensraw: Iterable[str] | None:
  - L18 if raw is None:
    - L19 return []
  - L20 annotated assign seen: set[str] = set()
  - L21 annotated assign normalized: list[str] = []
  - L22 for item in raw:
    - L23 assign text = str(item or '').strip()
    - L24 if not text or text in seen:
      - L25 continue
    - L26 expr seen.add(text)
    - L27 expr normalized.append(text)
  - L28 return normalized
- L31 def load_mapping_keystemplate_dir: Path:
  - L32 assign path = mapping_keys_path(template_dir)
  - L33 if not path.exists():
    - L34 return []
  - L35 try:
    - L36 assign data = json.loads(path.read_text(encoding='utf-8'))
    - L37 except Exception:
      - L38 return []
  - L40 if isinstance(data, dict):
    - L41 assign raw_keys = data.get('keys')
    - L42 else:
      - L42 if isinstance(data, list):
        - L43 assign raw_keys = data
        - L45 else:
          - L45 assign raw_keys = None
  - L46 return normalize_key_tokens(raw_keys if isinstance(raw_keys, Iterable) else None)
- L49 def write_mapping_keystemplate_dir: Path, keys: Iterable[str]:
  - L50 assign normalized = normalize_key_tokens(keys)
  - L51 assign path = mapping_keys_path(template_dir)
  - L52 assign payload = {'keys': normalized, 'updated_at': int(time.time())}
  - L56 expr write_json_atomic(path, payload, ensure_ascii=False, indent=2, step='mapping_keys')
  - L57 return normalized

## src\utils\schedule_utils.py
- L1 from __future__ import annotations
- L3 from datetime import datetime, timezone
- L4 from typing import Any, Mapping, Optional
- L6 assign _SCHEDULE_INTERVALS = {'hourly': 60, 'six_hours': 360, 'daily': 1440, 'weekly': 10080}
- L14 def utcnow_iso:
  - L15 return datetime.now(timezone.utc).replace(microsecond=0).isoformat()
- L18 def resolve_schedule_intervalfrequency: str, override: Optional[int]:
  - L19 if override and override > 0:
    - L20 return max(int(override), 5)
  - L21 if not frequency:
    - L22 return 60
  - L23 assign key = frequency.strip().lower()
  - L24 return _SCHEDULE_INTERVALS.get(key, 60)
- L27 def clean_key_valuesraw: Optional[Mapping[str, Any]]:
  - L28 annotated assign cleaned: dict[str, Any] = {}
  - L29 if isinstance(raw, Mapping):
    - L30 for (token, value) in raw.items():
      - L31 assign name = str(token or '').strip()
      - L32 if not name or value is None:
        - L33 continue
      - L34 assign cleaned[name] = value
  - L35 return cleaned

## src\utils\static_files.py
- L1 from __future__ import annotations
- L3 from email.utils import formatdate
- L4 from pathlib import Path
- L5 from urllib.parse import parse_qs, quote
- L7 from fastapi.staticfiles import StaticFiles
- L10 class UploadsStaticFiles(StaticFiles):
  - L11 docstring: "Static file handler that adds ETag, cache, and download headers."
  - L13 async def get_responseself, path: str, scope:
    - L14 assign response = await super().get_response(path, scope)
    - L15 if response.status_code == 404:
      - L16 return response
    - L18 assign query_params = {}
    - L19 if scope:
      - L20 assign raw_qs = scope.get('query_string') or b''
      - L21 if raw_qs:
        - L22 try:
          - L23 assign query_params = parse_qs(raw_qs.decode('utf-8', errors='ignore'))
          - L24 except Exception:
            - L25 assign query_params = {}
    - L27 try:
      - L28 assign (full_path, stat_result) = await self.lookup_path(path)
      - L29 except Exception:
        - L30 assign full_path = None
        - L31 assign stat_result = None
    - L33 if full_path and stat_result:
      - L34 assign etag = f'"{stat_result.st_mtime_ns:x}-{stat_result.st_size:x}"'
      - L35 assign response.headers['Cache-Control'] = 'no-store, max-age=0'
      - L36 assign response.headers['ETag'] = etag
      - L37 assign response.headers['Last-Modified'] = formatdate(stat_result.st_mtime, usegmt=True)
      - L38 if query_params.get('download'):
        - L39 assign filename = Path(full_path).name
        - L40 assign quoted = quote(filename)
        - L41 assign response.headers['Content-Disposition'] = f"""attachment; filename="{filename}"; filename*=UTF-8''{quoted}"""
    - L45 return response

## src\utils\template_utils.py
- L1 from __future__ import annotations
- L3 import importlib
- L4 import re
- L5 import uuid
- L6 from pathlib import Path
- L7 from typing import Optional
- L9 from fastapi import HTTPException
- L11 from src.core.config import EXCEL_UPLOAD_ROOT, UPLOAD_ROOT
- L13 annotated assign UPLOAD_KIND_PREFIXES: dict[str, str] = {'pdf': '/uploads', 'excel': '/excel-uploads'}
- L19 def _http_errorstatus_code: int, code: str, message: str:
  - L20 return HTTPException(status_code=status_code, detail={'status': 'error', 'code': code, 'message': message})
- L23 def _build_upload_kind_bases:
  - L24 return {'pdf': (UPLOAD_ROOT.resolve(), UPLOAD_KIND_PREFIXES['pdf']), 'excel': (EXCEL_UPLOAD_ROOT.resolve(), UPLOAD_KIND_PREFIXES['excel'])}
- L30 def _get_upload_kind_bases:
  - L31 docstring: "\n    Resolve upload roots dynamically so tests can monkeypatch backend.api.UPLO..."
  - L34 assign bases = _build_upload_kind_bases()
  - L35 try:
    - L36 assign api_mod = importlib.import_module('backend.api')
    - L37 except Exception:
      - L38 return bases
  - L39 assign pdf_root = getattr(api_mod, 'UPLOAD_ROOT', bases['pdf'][0])
  - L40 assign excel_root = getattr(api_mod, 'EXCEL_UPLOAD_ROOT', bases['excel'][0])
  - L41 try:
    - L42 assign bases['pdf'] = (Path(pdf_root).resolve(), UPLOAD_KIND_PREFIXES['pdf'])
    - L43 except Exception:
      - L44 pass
  - L45 try:
    - L46 assign bases['excel'] = (Path(excel_root).resolve(), UPLOAD_KIND_PREFIXES['excel'])
    - L47 except Exception:
      - L48 pass
  - L49 return bases
- L52 annotated assign _UPLOAD_KIND_BASES: dict[str, tuple[Path, str]] = _build_upload_kind_bases()
- L53 assign _TEMPLATE_ID_SAFE_RE = re.compile('^[a-z0-9][a-z0-9_-]{2,180}$')
- L56 def normalize_template_idtemplate_id: str:
  - L57 assign raw = str(template_id or '').strip()
  - L58 assign candidate = raw.replace('\\', '/').split('/')[-1].strip()
  - L59 if not candidate or candidate in {'.', '..'}:
    - L60 raise _http_error(400, 'invalid_template_id', 'Invalid template_id format')
  - L61 assign normalized = candidate.lower()
  - L62 if _TEMPLATE_ID_SAFE_RE.fullmatch(normalized):
    - L63 return normalized
  - L64 try:
    - L65 return str(uuid.UUID(candidate))
    - L66 except (ValueError, TypeError):
      - L67 raise _http_error(400, 'invalid_template_id', 'Invalid template_id format')
- L70 def template_dirtemplate_id: str, *, must_exist: bool=True, create: bool=False, kind: str='pdf':
  - L71 assign normalized_kind = (kind or 'pdf').lower()
  - L72 assign bases = _get_upload_kind_bases()
  - L73 if normalized_kind not in bases:
    - L74 raise _http_error(400, 'invalid_template_kind', f'Unsupported template kind: {kind}')
  - L76 assign base_dir = bases[normalized_kind][0]
  - L77 assign tid = normalize_template_id(template_id)
  - L79 assign tdir = (base_dir / tid).resolve()
  - L80 if base_dir not in tdir.parents:
    - L81 raise _http_error(400, 'invalid_template_path', 'Invalid template_id path')
  - L83 if must_exist and (not tdir.exists()):
    - L84 raise _http_error(404, 'template_not_found', 'template_id not found')
  - L86 if create:
    - L87 expr tdir.mkdir(parents=True, exist_ok=True)
  - L89 return tdir
- L92 def artifact_urlpath: Path | None:
  - L93 if path is None:
    - L94 return None
  - L95 try:
    - L96 assign resolved = path.resolve()
    - L97 except Exception:
      - L98 return None
  - L99 if not resolved.exists():
    - L100 return None
  - L101 for (base_dir, prefix) in _get_upload_kind_bases().values():
    - L102 try:
      - L103 assign rel = resolved.relative_to(base_dir)
      - L104 except ValueError:
        - L105 continue
    - L106 return f'{prefix}/{rel.as_posix()}'
  - L107 return None
- L110 def manifest_endpointtemplate_id: str, kind: str='pdf':
  - L111 return f'/excel/{template_id}/artifacts/manifest' if (kind or 'pdf').lower() == 'excel' else f'/templates/{template_id}/artifacts/manifest'
- L118 def find_reference_pdftemplate_dir_path: Path:
  - L119 for name in ('source.pdf', 'upload.pdf', 'template.pdf', 'report.pdf'):
    - L120 assign candidate = template_dir_path / name
    - L121 if candidate.exists():
      - L122 return candidate
  - L123 return None
- L126 def find_reference_pngtemplate_dir_path: Path:
  - L127 for name in ('report_final.png', 'reference_p1.png', 'render_p1.png'):
    - L128 assign candidate = template_dir_path / name
    - L129 if candidate.exists():
      - L130 return candidate
  - L131 return None
